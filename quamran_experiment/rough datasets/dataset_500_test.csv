repo,path,func_name,original_string,language,code,code_tokens/0,code_tokens/1,code_tokens/2,code_tokens/3,code_tokens/4,code_tokens/5,code_tokens/6,code_tokens/7,code_tokens/8,code_tokens/9,code_tokens/10,code_tokens/11,code_tokens/12,code_tokens/13,code_tokens/14,code_tokens/15,code_tokens/16,code_tokens/17,code_tokens/18,code_tokens/19,code_tokens/20,code_tokens/21,code_tokens/22,code_tokens/23,code_tokens/24,code_tokens/25,code_tokens/26,code_tokens/27,code_tokens/28,code_tokens/29,code_tokens/30,code_tokens/31,code_tokens/32,code_tokens/33,code_tokens/34,code_tokens/35,code_tokens/36,code_tokens/37,code_tokens/38,code_tokens/39,code_tokens/40,code_tokens/41,code_tokens/42,code_tokens/43,code_tokens/44,code_tokens/45,code_tokens/46,code_tokens/47,code_tokens/48,code_tokens/49,code_tokens/50,code_tokens/51,docstring,docstring_tokens/0,docstring_tokens/1,docstring_tokens/2,docstring_tokens/3,docstring_tokens/4,docstring_tokens/5,docstring_tokens/6,docstring_tokens/7,docstring_tokens/8,docstring_tokens/9,docstring_tokens/10,docstring_tokens/11,docstring_tokens/12,sha,url,partition,code_tokens/52,code_tokens/53,code_tokens/54,code_tokens/55,code_tokens/56,code_tokens/57,code_tokens/58,code_tokens/59,code_tokens/60,code_tokens/61,code_tokens/62,code_tokens/63,code_tokens/64,code_tokens/65,code_tokens/66,code_tokens/67,code_tokens/68,code_tokens/69,code_tokens/70,code_tokens/71,code_tokens/72,code_tokens/73,code_tokens/74,code_tokens/75,code_tokens/76,code_tokens/77,code_tokens/78,code_tokens/79,code_tokens/80,code_tokens/81,code_tokens/82,code_tokens/83,code_tokens/84,code_tokens/85,code_tokens/86,code_tokens/87,code_tokens/88,code_tokens/89,code_tokens/90,code_tokens/91,code_tokens/92,code_tokens/93,code_tokens/94,code_tokens/95,code_tokens/96,code_tokens/97,code_tokens/98,code_tokens/99,code_tokens/100,code_tokens/101,code_tokens/102,code_tokens/103,code_tokens/104,code_tokens/105,code_tokens/106,code_tokens/107,code_tokens/108,code_tokens/109,code_tokens/110,code_tokens/111,code_tokens/112,code_tokens/113,code_tokens/114,code_tokens/115,code_tokens/116,code_tokens/117,code_tokens/118,code_tokens/119,code_tokens/120,code_tokens/121,code_tokens/122,code_tokens/123,code_tokens/124,code_tokens/125,code_tokens/126,code_tokens/127,code_tokens/128,code_tokens/129,code_tokens/130,code_tokens/131,code_tokens/132,code_tokens/133,code_tokens/134,code_tokens/135,code_tokens/136,code_tokens/137,code_tokens/138,code_tokens/139,code_tokens/140,code_tokens/141,code_tokens/142,code_tokens/143,code_tokens/144,code_tokens/145,code_tokens/146,code_tokens/147,code_tokens/148,code_tokens/149,code_tokens/150,code_tokens/151,code_tokens/152,code_tokens/153,code_tokens/154,code_tokens/155,code_tokens/156,code_tokens/157,code_tokens/158,code_tokens/159,code_tokens/160,code_tokens/161,code_tokens/162,code_tokens/163,code_tokens/164,code_tokens/165,code_tokens/166,code_tokens/167,code_tokens/168,code_tokens/169,code_tokens/170,code_tokens/171,code_tokens/172,code_tokens/173,code_tokens/174,code_tokens/175,code_tokens/176,code_tokens/177,code_tokens/178,code_tokens/179,code_tokens/180,code_tokens/181,code_tokens/182,code_tokens/183,code_tokens/184,code_tokens/185,code_tokens/186,code_tokens/187,code_tokens/188,code_tokens/189,code_tokens/190,code_tokens/191,code_tokens/192,code_tokens/193,code_tokens/194,code_tokens/195,code_tokens/196,code_tokens/197,code_tokens/198,code_tokens/199,code_tokens/200,code_tokens/201,code_tokens/202,code_tokens/203,code_tokens/204,code_tokens/205,code_tokens/206,code_tokens/207,code_tokens/208,code_tokens/209,code_tokens/210,code_tokens/211,code_tokens/212,code_tokens/213,code_tokens/214,code_tokens/215,code_tokens/216,code_tokens/217,code_tokens/218,code_tokens/219,code_tokens/220,code_tokens/221,code_tokens/222,code_tokens/223,code_tokens/224,code_tokens/225,code_tokens/226,code_tokens/227,code_tokens/228,code_tokens/229,code_tokens/230,code_tokens/231,code_tokens/232,code_tokens/233,code_tokens/234,code_tokens/235,code_tokens/236,code_tokens/237,code_tokens/238,code_tokens/239,code_tokens/240,code_tokens/241,code_tokens/242,code_tokens/243,code_tokens/244,code_tokens/245,code_tokens/246,code_tokens/247,code_tokens/248,code_tokens/249,code_tokens/250,code_tokens/251,code_tokens/252,code_tokens/253,code_tokens/254,code_tokens/255,code_tokens/256,code_tokens/257,code_tokens/258,code_tokens/259,code_tokens/260,code_tokens/261,code_tokens/262,code_tokens/263,code_tokens/264,code_tokens/265,code_tokens/266,code_tokens/267,code_tokens/268,code_tokens/269,code_tokens/270,code_tokens/271,code_tokens/272,code_tokens/273,code_tokens/274,code_tokens/275,code_tokens/276,code_tokens/277,code_tokens/278,code_tokens/279,code_tokens/280,code_tokens/281,code_tokens/282,code_tokens/283,code_tokens/284,code_tokens/285,code_tokens/286,code_tokens/287,code_tokens/288,code_tokens/289,code_tokens/290,code_tokens/291,code_tokens/292,code_tokens/293,code_tokens/294,code_tokens/295,code_tokens/296,code_tokens/297,code_tokens/298,code_tokens/299,code_tokens/300,code_tokens/301,code_tokens/302,code_tokens/303,code_tokens/304,code_tokens/305,code_tokens/306,code_tokens/307,code_tokens/308,code_tokens/309,code_tokens/310,code_tokens/311,code_tokens/312,code_tokens/313,code_tokens/314,code_tokens/315,code_tokens/316,code_tokens/317,code_tokens/318,code_tokens/319,code_tokens/320,code_tokens/321,code_tokens/322,code_tokens/323,code_tokens/324,code_tokens/325,code_tokens/326,code_tokens/327,code_tokens/328,code_tokens/329,code_tokens/330,code_tokens/331,code_tokens/332,code_tokens/333,code_tokens/334,code_tokens/335,code_tokens/336,code_tokens/337,code_tokens/338,code_tokens/339,code_tokens/340,code_tokens/341,code_tokens/342,code_tokens/343,code_tokens/344,code_tokens/345,code_tokens/346,code_tokens/347,code_tokens/348,code_tokens/349,code_tokens/350,code_tokens/351,code_tokens/352,code_tokens/353,code_tokens/354,code_tokens/355,code_tokens/356,code_tokens/357,code_tokens/358,code_tokens/359,code_tokens/360,code_tokens/361,code_tokens/362,code_tokens/363,code_tokens/364,code_tokens/365,code_tokens/366,code_tokens/367,code_tokens/368,code_tokens/369,code_tokens/370,code_tokens/371,code_tokens/372,code_tokens/373,code_tokens/374,code_tokens/375,code_tokens/376,code_tokens/377,code_tokens/378,code_tokens/379,code_tokens/380,code_tokens/381,code_tokens/382,code_tokens/383,code_tokens/384,code_tokens/385,code_tokens/386,code_tokens/387,code_tokens/388,code_tokens/389,code_tokens/390,code_tokens/391,code_tokens/392,code_tokens/393,code_tokens/394,code_tokens/395,code_tokens/396,code_tokens/397,code_tokens/398,code_tokens/399,code_tokens/400,code_tokens/401,code_tokens/402,code_tokens/403,code_tokens/404,code_tokens/405,code_tokens/406,code_tokens/407,code_tokens/408,code_tokens/409,code_tokens/410,code_tokens/411,code_tokens/412,code_tokens/413,code_tokens/414,code_tokens/415,code_tokens/416,code_tokens/417,code_tokens/418,code_tokens/419,code_tokens/420,code_tokens/421,code_tokens/422,code_tokens/423,code_tokens/424,code_tokens/425,code_tokens/426,code_tokens/427,code_tokens/428,code_tokens/429,code_tokens/430,code_tokens/431,code_tokens/432,code_tokens/433,code_tokens/434,code_tokens/435,code_tokens/436,code_tokens/437,code_tokens/438,code_tokens/439,code_tokens/440,code_tokens/441,code_tokens/442,code_tokens/443,code_tokens/444,code_tokens/445,code_tokens/446,code_tokens/447,code_tokens/448,code_tokens/449,code_tokens/450,code_tokens/451,code_tokens/452,code_tokens/453,code_tokens/454,code_tokens/455,code_tokens/456,code_tokens/457,code_tokens/458,code_tokens/459,code_tokens/460,code_tokens/461,code_tokens/462,code_tokens/463,code_tokens/464,code_tokens/465,code_tokens/466,code_tokens/467,code_tokens/468,code_tokens/469,code_tokens/470,code_tokens/471,code_tokens/472,code_tokens/473,code_tokens/474,code_tokens/475,code_tokens/476,code_tokens/477,code_tokens/478,code_tokens/479,code_tokens/480,code_tokens/481,code_tokens/482,code_tokens/483,code_tokens/484,code_tokens/485,code_tokens/486,code_tokens/487,code_tokens/488,docstring_tokens/13,docstring_tokens/14,docstring_tokens/15,docstring_tokens/16,docstring_tokens/17,docstring_tokens/18,docstring_tokens/19,docstring_tokens/20,docstring_tokens/21,docstring_tokens/22,docstring_tokens/23,docstring_tokens/24,docstring_tokens/25,docstring_tokens/26,docstring_tokens/27,docstring_tokens/28,docstring_tokens/29,docstring_tokens/30,docstring_tokens/31,docstring_tokens/32,docstring_tokens/33,docstring_tokens/34,docstring_tokens/35,docstring_tokens/36,docstring_tokens/37,docstring_tokens/38,docstring_tokens/39,docstring_tokens/40,docstring_tokens/41,docstring_tokens/42,docstring_tokens/43,docstring_tokens/44,docstring_tokens/45,docstring_tokens/46,docstring_tokens/47,docstring_tokens/48,docstring_tokens/49,docstring_tokens/50,docstring_tokens/51,docstring_tokens/52
soimort/you-get,src/you_get/extractors/miomio.py,sina_xml_to_url_list,"def sina_xml_to_url_list(xml_data):
    """"""str->list
    Convert XML to URL List.
    From Biligrab.
    """"""
    rawurl = []
    dom = parseString(xml_data)
    for node in dom.getElementsByTagName('durl'):
        url = node.getElementsByTagName('url')[0]
        rawurl.append(url.childNodes[0].data)
    return rawurl",python,"def sina_xml_to_url_list(xml_data):
    """"""str->list
    Convert XML to URL List.
    From Biligrab.
    """"""
    rawurl = []
    dom = parseString(xml_data)
    for node in dom.getElementsByTagName('durl'):
        url = node.getElementsByTagName('url')[0]
        rawurl.append(url.childNodes[0].data)
    return rawurl",def,sina_xml_to_url_list,(,xml_data,),:,rawurl,=,[,],dom,=,parseString,(,xml_data,),for,node,in,dom,.,getElementsByTagName,(,'durl',),:,url,=,node,.,getElementsByTagName,(,'url',),[,0,],rawurl,.,append,(,url,.,childNodes,[,0,],.,data,),return,rawurl,"str->list
    Convert XML to URL List.
    From Biligrab.",str,-,>,list,Convert,XML,to,URL,List,.,From,Biligrab,.,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/dailymotion.py,dailymotion_download,"def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Dailymotion videos by URL.
    """"""

    html = get_content(rebuilt_url(url))
    info = json.loads(match1(html, r'qualities"":({.+?}),""'))
    title = match1(html, r'""video_title""\s*:\s*""([^""]+)""') or \
            match1(html, r'""title""\s*:\s*""([^""]+)""')
    title = unicodize(title)

    for quality in ['1080','720','480','380','240','144','auto']:
        try:
            real_url = info[quality][1][""url""]
            if real_url:
                break
        except KeyError:
            pass

    mime, ext, size = url_info(real_url)

    print_info(site_info, title, mime, size)
    if not info_only:
        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",python,"def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Dailymotion videos by URL.
    """"""

    html = get_content(rebuilt_url(url))
    info = json.loads(match1(html, r'qualities"":({.+?}),""'))
    title = match1(html, r'""video_title""\s*:\s*""([^""]+)""') or \
            match1(html, r'""title""\s*:\s*""([^""]+)""')
    title = unicodize(title)

    for quality in ['1080','720','480','380','240','144','auto']:
        try:
            real_url = info[quality][1][""url""]
            if real_url:
                break
        except KeyError:
            pass

    mime, ext, size = url_info(real_url)

    print_info(site_info, title, mime, size)
    if not info_only:
        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",def,dailymotion_download,(,url,",",output_dir,=,'.',",",merge,=,True,",",info_only,=,False,",",*,*,kwargs,),:,html,=,get_content,(,rebuilt_url,(,url,),),info,=,json,.,loads,(,match1,(,html,",","r'qualities"":({.+?}),""'",),),title,=,match1,(,html,",","r'""video_title""\s*:\s*""([^""]+)""'",),Downloads Dailymotion videos by URL.,Downloads,Dailymotion,videos,by,URL,.,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35,test,or,match1,(,html,",","r'""title""\s*:\s*""([^""]+)""'",),title,=,unicodize,(,title,),for,quality,in,[,'1080',",",'720',",",'480',",",'380',",",'240',",",'144',",",'auto',],:,try,:,real_url,=,info,[,quality,],[,1,],[,"""url""",],if,real_url,:,break,except,KeyError,:,pass,mime,",",ext,",",size,=,url_info,(,real_url,),print_info,(,site_info,",",title,",",mime,",",size,),if,not,info_only,:,download_urls,(,[,real_url,],",",title,",",ext,",",size,",",output_dir,=,output_dir,",",merge,=,merge,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/sina.py,sina_download,"def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Sina videos by URL.
    """"""
    if 'news.sina.com.cn/zxt' in url:
        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)
        return

    vid = match1(url, r'vid=(\d+)')
    if vid is None:
        video_page = get_content(url)
        vid = hd_vid = match1(video_page, r'hd_vid\s*:\s*\'([^\']+)\'')
        if hd_vid == '0':
            vids = match1(video_page, r'[^\w]vid\s*:\s*\'([^\']+)\'').split('|')
            vid = vids[-1]

    if vid is None:
        vid = match1(video_page, r'vid:""?(\d+)""?')
    if vid:
        #title = match1(video_page, r'title\s*:\s*\'([^\']+)\'')
        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)
    else:
        vkey = match1(video_page, r'vkey\s*:\s*""([^""]+)""')
        if vkey is None:
            vid = match1(url, r'#(\d+)')
            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)
            return
        title = match1(video_page, r'title\s*:\s*""([^""]+)""')
        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",python,"def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Sina videos by URL.
    """"""
    if 'news.sina.com.cn/zxt' in url:
        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)
        return

    vid = match1(url, r'vid=(\d+)')
    if vid is None:
        video_page = get_content(url)
        vid = hd_vid = match1(video_page, r'hd_vid\s*:\s*\'([^\']+)\'')
        if hd_vid == '0':
            vids = match1(video_page, r'[^\w]vid\s*:\s*\'([^\']+)\'').split('|')
            vid = vids[-1]

    if vid is None:
        vid = match1(video_page, r'vid:""?(\d+)""?')
    if vid:
        #title = match1(video_page, r'title\s*:\s*\'([^\']+)\'')
        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)
    else:
        vkey = match1(video_page, r'vkey\s*:\s*""([^""]+)""')
        if vkey is None:
            vid = match1(url, r'#(\d+)')
            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)
            return
        title = match1(video_page, r'title\s*:\s*""([^""]+)""')
        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",def,sina_download,(,url,",",output_dir,=,'.',",",merge,=,True,",",info_only,=,False,",",*,*,kwargs,),:,if,'news.sina.com.cn/zxt',in,url,:,sina_zxt,(,url,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,",",*,*,kwargs,),return,vid,=,match1,(,Downloads Sina videos by URL.,Downloads,Sina,videos,by,URL,.,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121,test,url,",",r'vid=(\d+)',),if,vid,is,None,:,video_page,=,get_content,(,url,),vid,=,hd_vid,=,match1,(,video_page,",",r'hd_vid\s*:\s*\'([^\']+)\'',),if,hd_vid,==,'0',:,vids,=,match1,(,video_page,",",r'[^\w]vid\s*:\s*\'([^\']+)\'',),.,split,(,'|',),vid,=,vids,[,-,1,],if,vid,is,None,:,vid,=,match1,(,video_page,",","r'vid:""?(\d+)""?'",),if,vid,:,"#title = match1(video_page, r'title\s*:\s*\'([^\']+)\'')",sina_download_by_vid,(,vid,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),else,:,vkey,=,match1,(,video_page,",","r'vkey\s*:\s*""([^""]+)""'",),if,vkey,is,None,:,vid,=,match1,(,url,",",r'#(\d+)',),sina_download_by_vid,(,vid,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),return,title,=,match1,(,video_page,",","r'title\s*:\s*""([^""]+)""'",),sina_download_by_vkey,(,vkey,",",title,=,title,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/util/log.py,sprint,"def sprint(text, *colors):
    """"""Format text with color or other effects into ANSI escaped string.""""""
    return ""\33[{}m{content}\33[{}m"".format("";"".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",python,"def sprint(text, *colors):
    """"""Format text with color or other effects into ANSI escaped string.""""""
    return ""\33[{}m{content}\33[{}m"".format("";"".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",def,sprint,(,text,",",*,colors,),:,return,"""\33[{}m{content}\33[{}m""",.,format,(,""";""",.,join,(,[,str,(,color,),for,color,in,colors,],),",",RESET,",",content,=,text,),if,IS_ANSI_TERMINAL,and,colors,else,text,,,,,,,,,,,Format text with color or other effects into ANSI escaped string.,Format,text,with,color,or,other,effects,into,ANSI,escaped,string,.,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/util/log.py,print_log,"def print_log(text, *colors):
    """"""Print a log message to standard error.""""""
    sys.stderr.write(sprint(""{}: {}"".format(script_name, text), *colors) + ""\n"")",python,"def print_log(text, *colors):
    """"""Print a log message to standard error.""""""
    sys.stderr.write(sprint(""{}: {}"".format(script_name, text), *colors) + ""\n"")",def,print_log,(,text,",",*,colors,),:,sys,.,stderr,.,write,(,sprint,(,"""{}: {}""",.,format,(,script_name,",",text,),",",*,colors,),+,"""\n""",),,,,,,,,,,,,,,,,,,,,,Print a log message to standard error.,Print,a,log,message,to,standard,error,.,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/util/log.py,e,"def e(message, exit_code=None):
    """"""Print an error log message.""""""
    print_log(message, YELLOW, BOLD)
    if exit_code is not None:
        sys.exit(exit_code)",python,"def e(message, exit_code=None):
    """"""Print an error log message.""""""
    print_log(message, YELLOW, BOLD)
    if exit_code is not None:
        sys.exit(exit_code)",def,e,(,message,",",exit_code,=,None,),:,print_log,(,message,",",YELLOW,",",BOLD,),if,exit_code,is,not,None,:,sys,.,exit,(,exit_code,),,,,,,,,,,,,,,,,,,,,,,,Print an error log message.,Print,an,error,log,message,.,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/util/log.py,wtf,"def wtf(message, exit_code=1):
    """"""What a Terrible Failure!""""""
    print_log(message, RED, BOLD)
    if exit_code is not None:
        sys.exit(exit_code)",python,"def wtf(message, exit_code=1):
    """"""What a Terrible Failure!""""""
    print_log(message, RED, BOLD)
    if exit_code is not None:
        sys.exit(exit_code)",def,wtf,(,message,",",exit_code,=,1,),:,print_log,(,message,",",RED,",",BOLD,),if,exit_code,is,not,None,:,sys,.,exit,(,exit_code,),,,,,,,,,,,,,,,,,,,,,,,What a Terrible Failure!,What,a,Terrible,Failure!,,,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/util/os.py,detect_os,"def detect_os():
    """"""Detect operating system.
    """"""

    # Inspired by:
    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py

    syst = system().lower()
    os = 'unknown'

    if 'cygwin' in syst:
        os = 'cygwin'
    elif 'darwin' in syst:
        os = 'mac'
    elif 'linux' in syst:
        os = 'linux'
        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423
        try:
            with open('/proc/version', 'r') as f:
                if 'microsoft' in f.read().lower():
                    os = 'wsl'
        except: pass
    elif 'windows' in syst:
        os = 'windows'
    elif 'bsd' in syst:
        os = 'bsd'

    return os",python,"def detect_os():
    """"""Detect operating system.
    """"""

    # Inspired by:
    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py

    syst = system().lower()
    os = 'unknown'

    if 'cygwin' in syst:
        os = 'cygwin'
    elif 'darwin' in syst:
        os = 'mac'
    elif 'linux' in syst:
        os = 'linux'
        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423
        try:
            with open('/proc/version', 'r') as f:
                if 'microsoft' in f.read().lower():
                    os = 'wsl'
        except: pass
    elif 'windows' in syst:
        os = 'windows'
    elif 'bsd' in syst:
        os = 'bsd'

    return os",def,detect_os,(,),:,# Inspired by:,# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py,syst,=,system,(,),.,lower,(,),os,=,'unknown',if,'cygwin',in,syst,:,os,=,'cygwin',elif,'darwin',in,syst,:,os,=,'mac',elif,'linux',in,syst,:,os,=,'linux',# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423,try,:,with,open,(,'/proc/version',",",'r',Detect operating system.,Detect,operating,system,.,,,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32,test,),as,f,:,if,'microsoft',in,f,.,read,(,),.,lower,(,),:,os,=,'wsl',except,:,pass,elif,'windows',in,syst,:,os,=,'windows',elif,'bsd',in,syst,:,os,=,'bsd',return,os,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/vimeo.py,vimeo_download_by_channel,"def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):
    """"""str->None""""""
    # https://vimeo.com/channels/464686
    channel_id = match1(url, r'http://vimeo.com/channels/(\w+)')
    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",python,"def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):
    """"""str->None""""""
    # https://vimeo.com/channels/464686
    channel_id = match1(url, r'http://vimeo.com/channels/(\w+)')
    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",def,vimeo_download_by_channel,(,url,",",output_dir,=,'.',",",merge,=,False,",",info_only,=,False,",",*,*,kwargs,),:,# https://vimeo.com/channels/464686,channel_id,=,match1,(,url,",",r'http://vimeo.com/channels/(\w+)',),vimeo_download_by_channel_id,(,channel_id,",",output_dir,",",merge,",",info_only,",",*,*,kwargs,),,,,,,,,str->None,str,-,>,None,,,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/ckplayer.py,ckplayer_get_info_by_xml,"def ckplayer_get_info_by_xml(ckinfo):
    """"""str->dict
    Information for CKPlayer API content.""""""
    e = ET.XML(ckinfo)
    video_dict = {'title': '',
                  #'duration': 0,
                  'links': [],
                  'size': 0,
                  'flashvars': '',}
    dictified = dictify(e)['ckplayer']
    if 'info' in dictified:
        if '_text' in dictified['info'][0]['title'][0]:  #title
            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()

    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration
        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()

    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece
        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])

    if '_text' in dictified['video'][0]['file'][0]:  #link exist
        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]

    if '_text' in dictified['flashvars'][0]:
        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()

    return video_dict",python,"def ckplayer_get_info_by_xml(ckinfo):
    """"""str->dict
    Information for CKPlayer API content.""""""
    e = ET.XML(ckinfo)
    video_dict = {'title': '',
                  #'duration': 0,
                  'links': [],
                  'size': 0,
                  'flashvars': '',}
    dictified = dictify(e)['ckplayer']
    if 'info' in dictified:
        if '_text' in dictified['info'][0]['title'][0]:  #title
            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()

    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration
        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()

    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece
        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])

    if '_text' in dictified['video'][0]['file'][0]:  #link exist
        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]

    if '_text' in dictified['flashvars'][0]:
        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()

    return video_dict",def,ckplayer_get_info_by_xml,(,ckinfo,),:,e,=,ET,.,XML,(,ckinfo,),video_dict,=,{,'title',:,'',",","#'duration': 0,",'links',:,[,],",",'size',:,0,",",'flashvars',:,'',",",},dictified,=,dictify,(,e,),[,'ckplayer',],if,'info',in,dictified,:,if,'_text',"str->dict
    Information for CKPlayer API content.",str,-,>,dict,Information,for,CKPlayer,API,content,.,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39,test,in,dictified,[,'info',],[,0,],[,'title',],[,0,],:,#title,video_dict,[,'title',],=,dictified,[,'info',],[,0,],[,'title',],[,0,],[,'_text',],.,strip,(,),#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration,#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip(),if,'_text',in,dictified,[,'video',],[,0,],[,'size',],[,0,],:,#size exists for 1 piece,video_dict,[,'size',],=,sum,(,[,int,(,i,[,'size',],[,0,],[,'_text',],),for,i,in,dictified,[,'video',],],),if,'_text',in,dictified,[,'video',],[,0,],[,'file',],[,0,],:,#link exist,video_dict,[,'links',],=,[,i,[,'file',],[,0,],[,'_text',],.,strip,(,),for,i,in,dictified,[,'video',],],if,'_text',in,dictified,[,'flashvars',],[,0,],:,video_dict,[,'flashvars',],=,dictified,[,'flashvars',],[,0,],[,'_text',],.,strip,(,),return,video_dict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/ixigua.py,get_video_url_from_video_id,"def get_video_url_from_video_id(video_id):
    """"""Splicing URLs according to video ID to get video details""""""
    # from js
    data = [""""] * 256
    for index, _ in enumerate(data):
        t = index
        for i in range(8):
            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)
        data[index] = t

    def tmp():
        rand_num = random.random()
        path = ""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}"".format(video_id=video_id,
                                                                              random_num=str(rand_num)[2:])
        e = o = r = -1
        i, a = 0, len(path)
        while i < a:
            e = ord(path[i])
            i += 1
            if e < 128:
                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]
            else:
                if e < 2048:
                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]
                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]
                else:
                    if 55296 <= e < 57344:
                        e = (1023 & e) + 64
                        i += 1
                        o = 1023 & t.url(i)
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]
                    else:
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]

        return ""https://ib.365yg.com{path}&s={param}"".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))

    while 1:
        url = tmp()
        if url.split(""="")[-1][0] != ""-"":  # 参数s不能为负数
            return url",python,"def get_video_url_from_video_id(video_id):
    """"""Splicing URLs according to video ID to get video details""""""
    # from js
    data = [""""] * 256
    for index, _ in enumerate(data):
        t = index
        for i in range(8):
            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)
        data[index] = t

    def tmp():
        rand_num = random.random()
        path = ""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}"".format(video_id=video_id,
                                                                              random_num=str(rand_num)[2:])
        e = o = r = -1
        i, a = 0, len(path)
        while i < a:
            e = ord(path[i])
            i += 1
            if e < 128:
                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]
            else:
                if e < 2048:
                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]
                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]
                else:
                    if 55296 <= e < 57344:
                        e = (1023 & e) + 64
                        i += 1
                        o = 1023 & t.url(i)
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]
                    else:
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]

        return ""https://ib.365yg.com{path}&s={param}"".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))

    while 1:
        url = tmp()
        if url.split(""="")[-1][0] != ""-"":  # 参数s不能为负数
            return url",def,get_video_url_from_video_id,(,video_id,),:,# from js,data,=,[,"""""",],*,256,for,index,",",_,in,enumerate,(,data,),:,t,=,index,for,i,in,range,(,8,),:,t,=,-,306674912,^,unsigned_right_shitf,(,t,",",1,),if,1,&,t,else,unsigned_right_shitf,Splicing URLs according to video ID to get video details,Splicing,URLs,according,to,video,ID,to,get,video,details,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78,test,(,t,",",1,),data,[,index,],=,t,def,tmp,(,),:,rand_num,=,random,.,random,(,),path,=,"""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}""",.,format,(,video_id,=,video_id,",",random_num,=,str,(,rand_num,),[,2,:,],),e,=,o,=,r,=,-,1,i,",",a,=,0,",",len,(,path,),while,i,<,a,:,e,=,ord,(,path,[,i,],),i,+=,1,if,e,<,128,:,r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,e,),],else,:,if,e,<,2048,:,r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,192,|,e,>>,6,&,31,),),],r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,128,|,63,&,e,),),],else,:,if,55296,<=,e,<,57344,:,e,=,(,1023,&,e,),+,64,i,+=,1,o,=,1023,&,t,.,url,(,i,),r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,240,|,e,>>,8,&,7,),),],r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,128,|,e,>>,2,&,63,),),],r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,128,|,o,>>,6,&,15,|,(,3,&,e,),<<,4,),),],r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,128,|,63,&,o,),),],else,:,r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,224,|,e,>>,12,&,15,),),],r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,128,|,e,>>,6,&,63,),),],r,=,unsigned_right_shitf,(,r,",",8,),^,data,[,255,&,(,r,^,(,128,|,63,&,e,),),],return,"""https://ib.365yg.com{path}&s={param}""",.,format,(,path,=,path,",",param,=,unsigned_right_shitf,(,r,^,-,1,",",0,),),while,1,:,url,=,tmp,(,),if,url,.,split,(,"""=""",),[,-,1,],[,0,],!=,"""-""",:,# 参数s不能为负数,return,url,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/mgtv.py,MGTV.get_mgtv_real_url,"def get_mgtv_real_url(url):
        """"""str->list of str
        Give you the real URLs.""""""
        content = loads(get_content(url))
        m3u_url = content['info']
        split = urlsplit(m3u_url)
        
        base_url = ""{scheme}://{netloc}{path}/"".format(scheme = split[0],
                                                      netloc = split[1],
                                                      path = dirname(split[2]))

        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?
        segment_list = []
        segments_size = 0
        for i in content.split():
            if not i.startswith('#'):  #not the best way, better we use the m3u8 package
                segment_list.append(base_url + i)
            # use ext-info for fast size calculate
            elif i.startswith('#EXT-MGTV-File-SIZE:'):
                segments_size += int(i[i.rfind(':')+1:])

        return m3u_url, segments_size, segment_list",python,"def get_mgtv_real_url(url):
        """"""str->list of str
        Give you the real URLs.""""""
        content = loads(get_content(url))
        m3u_url = content['info']
        split = urlsplit(m3u_url)
        
        base_url = ""{scheme}://{netloc}{path}/"".format(scheme = split[0],
                                                      netloc = split[1],
                                                      path = dirname(split[2]))

        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?
        segment_list = []
        segments_size = 0
        for i in content.split():
            if not i.startswith('#'):  #not the best way, better we use the m3u8 package
                segment_list.append(base_url + i)
            # use ext-info for fast size calculate
            elif i.startswith('#EXT-MGTV-File-SIZE:'):
                segments_size += int(i[i.rfind(':')+1:])

        return m3u_url, segments_size, segment_list",def,get_mgtv_real_url,(,url,),:,content,=,loads,(,get_content,(,url,),),m3u_url,=,content,[,'info',],split,=,urlsplit,(,m3u_url,),base_url,=,"""{scheme}://{netloc}{path}/""",.,format,(,scheme,=,split,[,0,],",",netloc,=,split,[,1,],",",path,=,dirname,(,split,"str->list of str
        Give you the real URLs.",str,-,>,list,of,str,Give,you,the,real,URLs,.,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58,test,[,2,],),),content,=,get_content,(,content,[,'info',],),"#get the REAL M3U url, maybe to be changed later?",segment_list,=,[,],segments_size,=,0,for,i,in,content,.,split,(,),:,if,not,i,.,startswith,(,'#',),:,"#not the best way, better we use the m3u8 package",segment_list,.,append,(,base_url,+,i,),# use ext-info for fast size calculate,elif,i,.,startswith,(,'#EXT-MGTV-File-SIZE:',),:,segments_size,+=,int,(,i,[,i,.,rfind,(,':',),+,1,:,],),return,m3u_url,",",segments_size,",",segment_list,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/util/fs.py,legitimize,"def legitimize(text, os=detect_os()):
    """"""Converts a string to a valid filename.
    """"""

    # POSIX systems
    text = text.translate({
        0: None,
        ord('/'): '-',
        ord('|'): '-',
    })

    # FIXME: do some filesystem detection
    if os == 'windows' or os == 'cygwin' or os == 'wsl':
        # Windows (non-POSIX namespace)
        text = text.translate({
            # Reserved in Windows VFAT and NTFS
            ord(':'): '-',
            ord('*'): '-',
            ord('?'): '-',
            ord('\\'): '-',
            ord('\""'): '\'',
            # Reserved in Windows VFAT
            ord('+'): '-',
            ord('<'): '-',
            ord('>'): '-',
            ord('['): '(',
            ord(']'): ')',
            ord('\t'): ' ',
        })
    else:
        # *nix
        if os == 'mac':
            # Mac OS HFS+
            text = text.translate({
                ord(':'): '-',
            })

        # Remove leading .
        if text.startswith("".""):
            text = text[1:]

    text = text[:80] # Trim to 82 Unicode characters long
    return text",python,"def legitimize(text, os=detect_os()):
    """"""Converts a string to a valid filename.
    """"""

    # POSIX systems
    text = text.translate({
        0: None,
        ord('/'): '-',
        ord('|'): '-',
    })

    # FIXME: do some filesystem detection
    if os == 'windows' or os == 'cygwin' or os == 'wsl':
        # Windows (non-POSIX namespace)
        text = text.translate({
            # Reserved in Windows VFAT and NTFS
            ord(':'): '-',
            ord('*'): '-',
            ord('?'): '-',
            ord('\\'): '-',
            ord('\""'): '\'',
            # Reserved in Windows VFAT
            ord('+'): '-',
            ord('<'): '-',
            ord('>'): '-',
            ord('['): '(',
            ord(']'): ')',
            ord('\t'): ' ',
        })
    else:
        # *nix
        if os == 'mac':
            # Mac OS HFS+
            text = text.translate({
                ord(':'): '-',
            })

        # Remove leading .
        if text.startswith("".""):
            text = text[1:]

    text = text[:80] # Trim to 82 Unicode characters long
    return text",def,legitimize,(,text,",",os,=,detect_os,(,),),:,# POSIX systems,text,=,text,.,translate,(,{,0,:,None,",",ord,(,'/',),:,'-',",",ord,(,'|',),:,'-',",",},),# FIXME: do some filesystem detection,if,os,==,'windows',or,os,==,'cygwin',or,os,==,Converts a string to a valid filename.,Converts,a,string,to,a,valid,filename,.,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47,test,'wsl',:,# Windows (non-POSIX namespace),text,=,text,.,translate,(,{,# Reserved in Windows VFAT and NTFS,ord,(,':',),:,'-',",",ord,(,'*',),:,'-',",",ord,(,'?',),:,'-',",",ord,(,'\\',),:,'-',",",ord,(,"'\""'",),:,'\'',",",# Reserved in Windows VFAT,ord,(,'+',),:,'-',",",ord,(,'<',),:,'-',",",ord,(,'>',),:,'-',",",ord,(,'[',),:,'(',",",ord,(,']',),:,')',",",ord,(,'\t',),:,' ',",",},),else,:,# *nix,if,os,==,'mac',:,# Mac OS HFS+,text,=,text,.,translate,(,{,ord,(,':',),:,'-',",",},),# Remove leading .,if,text,.,startswith,(,""".""",),:,text,=,text,[,1,:,],text,=,text,[,:,80,],# Trim to 82 Unicode characters long,return,text,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/cbs.py,cbs_download,"def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads CBS videos by URL.
    """"""

    html = get_content(url)
    pid = match1(html, r'video\.settings\.pid\s*=\s*\'([^\']+)\'')
    title = match1(html, r'video\.settings\.title\s*=\s*\""([^\""]+)\""')

    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",python,"def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads CBS videos by URL.
    """"""

    html = get_content(url)
    pid = match1(html, r'video\.settings\.pid\s*=\s*\'([^\']+)\'')
    title = match1(html, r'video\.settings\.title\s*=\s*\""([^\""]+)\""')

    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",def,cbs_download,(,url,",",output_dir,=,'.',",",merge,=,True,",",info_only,=,False,",",*,*,kwargs,),:,html,=,get_content,(,url,),pid,=,match1,(,html,",",r'video\.settings\.pid\s*=\s*\'([^\']+)\'',),title,=,match1,(,html,",","r'video\.settings\.title\s*=\s*\""([^\""]+)\""'",),theplatform_download_by_pid,(,pid,",",title,",",output_dir,=,Downloads CBS videos by URL.,Downloads,CBS,videos,by,URL,.,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17,test,output_dir,",",merge,=,merge,",",info_only,=,info_only,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/iqiyi.py,Iqiyi.download,"def download(self, **kwargs):
        """"""Override the original one
        Ugly ugly dirty hack""""""
        if 'json_output' in kwargs and kwargs['json_output']:
            json_output.output(self)
        elif 'info_only' in kwargs and kwargs['info_only']:
            if 'stream_id' in kwargs and kwargs['stream_id']:
                # Display the stream
                stream_id = kwargs['stream_id']
                if 'index' not in kwargs:
                    self.p(stream_id)
                else:
                    self.p_i(stream_id)
            else:
                # Display all available streams
                if 'index' not in kwargs:
                    self.p([])
                else:
                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']
                    self.p_i(stream_id)

        else:
            if 'stream_id' in kwargs and kwargs['stream_id']:
                # Download the stream
                stream_id = kwargs['stream_id']
            else:
                # Download stream with the best quality
                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']

            if 'index' not in kwargs:
                self.p(stream_id)
            else:
                self.p_i(stream_id)

            if stream_id in self.streams:
                urls = self.streams[stream_id]['src']
                ext = self.streams[stream_id]['container']
                total_size = self.streams[stream_id]['size']
            else:
                urls = self.dash_streams[stream_id]['src']
                ext = self.dash_streams[stream_id]['container']
                total_size = self.dash_streams[stream_id]['size']

            if not urls:
                log.wtf('[Failed] Cannot extract video source.')
            # For legacy main()
            
            #Here's the change!!
            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)

            if not kwargs['caption']:
                print('Skipping captions.')
                return
            for lang in self.caption_tracks:
                filename = '%s.%s.srt' % (get_filename(self.title), lang)
                print('Saving %s ... ' % filename, end="""", flush=True)
                srt = self.caption_tracks[lang]
                with open(os.path.join(kwargs['output_dir'], filename),
                          'w', encoding='utf-8') as x:
                    x.write(srt)
                print('Done.')",python,"def download(self, **kwargs):
        """"""Override the original one
        Ugly ugly dirty hack""""""
        if 'json_output' in kwargs and kwargs['json_output']:
            json_output.output(self)
        elif 'info_only' in kwargs and kwargs['info_only']:
            if 'stream_id' in kwargs and kwargs['stream_id']:
                # Display the stream
                stream_id = kwargs['stream_id']
                if 'index' not in kwargs:
                    self.p(stream_id)
                else:
                    self.p_i(stream_id)
            else:
                # Display all available streams
                if 'index' not in kwargs:
                    self.p([])
                else:
                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']
                    self.p_i(stream_id)

        else:
            if 'stream_id' in kwargs and kwargs['stream_id']:
                # Download the stream
                stream_id = kwargs['stream_id']
            else:
                # Download stream with the best quality
                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']

            if 'index' not in kwargs:
                self.p(stream_id)
            else:
                self.p_i(stream_id)

            if stream_id in self.streams:
                urls = self.streams[stream_id]['src']
                ext = self.streams[stream_id]['container']
                total_size = self.streams[stream_id]['size']
            else:
                urls = self.dash_streams[stream_id]['src']
                ext = self.dash_streams[stream_id]['container']
                total_size = self.dash_streams[stream_id]['size']

            if not urls:
                log.wtf('[Failed] Cannot extract video source.')
            # For legacy main()
            
            #Here's the change!!
            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)

            if not kwargs['caption']:
                print('Skipping captions.')
                return
            for lang in self.caption_tracks:
                filename = '%s.%s.srt' % (get_filename(self.title), lang)
                print('Saving %s ... ' % filename, end="""", flush=True)
                srt = self.caption_tracks[lang]
                with open(os.path.join(kwargs['output_dir'], filename),
                          'w', encoding='utf-8') as x:
                    x.write(srt)
                print('Done.')",def,download,(,self,",",*,*,kwargs,),:,if,'json_output',in,kwargs,and,kwargs,[,'json_output',],:,json_output,.,output,(,self,),elif,'info_only',in,kwargs,and,kwargs,[,'info_only',],:,if,'stream_id',in,kwargs,and,kwargs,[,'stream_id',],:,# Display the stream,stream_id,=,kwargs,[,'stream_id',"Override the original one
        Ugly ugly dirty hack",Override,the,original,one,Ugly,ugly,dirty,hack,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218,test,],if,'index',not,in,kwargs,:,self,.,p,(,stream_id,),else,:,self,.,p_i,(,stream_id,),else,:,# Display all available streams,if,'index',not,in,kwargs,:,self,.,p,(,[,],),else,:,stream_id,=,self,.,streams_sorted,[,0,],[,'id',],if,'id',in,self,.,streams_sorted,[,0,],else,self,.,streams_sorted,[,0,],[,'itag',],self,.,p_i,(,stream_id,),else,:,if,'stream_id',in,kwargs,and,kwargs,[,'stream_id',],:,# Download the stream,stream_id,=,kwargs,[,'stream_id',],else,:,# Download stream with the best quality,stream_id,=,self,.,streams_sorted,[,0,],[,'id',],if,'id',in,self,.,streams_sorted,[,0,],else,self,.,streams_sorted,[,0,],[,'itag',],if,'index',not,in,kwargs,:,self,.,p,(,stream_id,),else,:,self,.,p_i,(,stream_id,),if,stream_id,in,self,.,streams,:,urls,=,self,.,streams,[,stream_id,],[,'src',],ext,=,self,.,streams,[,stream_id,],[,'container',],total_size,=,self,.,streams,[,stream_id,],[,'size',],else,:,urls,=,self,.,dash_streams,[,stream_id,],[,'src',],ext,=,self,.,dash_streams,[,stream_id,],[,'container',],total_size,=,self,.,dash_streams,[,stream_id,],[,'size',],if,not,urls,:,log,.,wtf,(,'[Failed] Cannot extract video source.',),# For legacy main(),#Here's the change!!,download_url_ffmpeg,(,urls,[,0,],",",self,.,title,",",'mp4',",",output_dir,=,kwargs,[,'output_dir',],",",merge,=,kwargs,[,'merge',],",",stream,=,False,),if,not,kwargs,[,'caption',],:,print,(,'Skipping captions.',),return,for,lang,in,self,.,caption_tracks,:,filename,=,'%s.%s.srt',%,(,get_filename,(,self,.,title,),",",lang,),print,(,'Saving %s ... ',%,filename,",",end,=,"""""",",",flush,=,True,),srt,=,self,.,caption_tracks,[,lang,],with,open,(,os,.,path,.,join,(,kwargs,[,'output_dir',],",",filename,),",",'w',",",encoding,=,'utf-8',),as,x,:,x,.,write,(,srt,),print,(,'Done.',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/acfun.py,acfun_download_by_vid,"def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""str, str, str, bool, bool ->None

    Download Acfun video by vid.

    Call Acfun API, decide which site to use, and pass the job to its
    extractor.
    """"""

    #first call the main parasing API
    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))

    sourceType = info['sourceType']

    #decide sourceId to know which extractor to use
    if 'sourceId' in info: sourceId = info['sourceId']
    # danmakuId = info['danmakuId']

    #call extractor decided by sourceId
    if sourceType == 'sina':
        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'youku':
        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)
    elif sourceType == 'tudou':
        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'qq':
        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'letv':
        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'zhuzhan':
        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this
#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player
#old code removed
        url = 'http://www.acfun.cn/v/ac' + vid
        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)
        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']
        for t in seq:
            if yk_streams.get(t):
                preferred = yk_streams[t]
                break
#total_size in the json could be incorrect(F.I. 0)
        size = 0
        for url in preferred[0]:
            _, _, seg_size = url_info(url)
            size += seg_size
#fallback to flvhd is not quite possible
        if re.search(r'fid=[0-9A-Z\-]*.flv', preferred[0][0]):
            ext = 'flv'
        else:
            ext = 'mp4'
        print_info(site_info, title, ext, size)
        if not info_only:
            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)
    else:
        raise NotImplementedError(sourceType)

    if not info_only and not dry_run:
        if not kwargs['caption']:
            print('Skipping danmaku.')
            return
        try:
            title = get_filename(title)
            print('Downloading %s ...\n' % (title + '.cmt.json'))
            cmt = get_srt_json(vid)
            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:
                x.write(cmt)
        except:
            pass",python,"def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""str, str, str, bool, bool ->None

    Download Acfun video by vid.

    Call Acfun API, decide which site to use, and pass the job to its
    extractor.
    """"""

    #first call the main parasing API
    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))

    sourceType = info['sourceType']

    #decide sourceId to know which extractor to use
    if 'sourceId' in info: sourceId = info['sourceId']
    # danmakuId = info['danmakuId']

    #call extractor decided by sourceId
    if sourceType == 'sina':
        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'youku':
        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)
    elif sourceType == 'tudou':
        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'qq':
        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'letv':
        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'zhuzhan':
        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this
#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player
#old code removed
        url = 'http://www.acfun.cn/v/ac' + vid
        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)
        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']
        for t in seq:
            if yk_streams.get(t):
                preferred = yk_streams[t]
                break
#total_size in the json could be incorrect(F.I. 0)
        size = 0
        for url in preferred[0]:
            _, _, seg_size = url_info(url)
            size += seg_size
#fallback to flvhd is not quite possible
        if re.search(r'fid=[0-9A-Z\-]*.flv', preferred[0][0]):
            ext = 'flv'
        else:
            ext = 'mp4'
        print_info(site_info, title, ext, size)
        if not info_only:
            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)
    else:
        raise NotImplementedError(sourceType)

    if not info_only and not dry_run:
        if not kwargs['caption']:
            print('Skipping danmaku.')
            return
        try:
            title = get_filename(title)
            print('Downloading %s ...\n' % (title + '.cmt.json'))
            cmt = get_srt_json(vid)
            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:
                x.write(cmt)
        except:
            pass",def,acfun_download_by_vid,(,vid,",",title,",",output_dir,=,'.',",",merge,=,True,",",info_only,=,False,",",*,*,kwargs,),:,#first call the main parasing API,info,=,json,.,loads,(,get_content,(,'http://www.acfun.cn/video/getVideo.aspx?id=',+,vid,),),sourceType,=,info,[,'sourceType',],#decide sourceId to know which extractor to use,if,'sourceId',in,info,:,sourceId,=,"str, str, str, bool, bool ->None

    Download Acfun video by vid.

    Call Acfun API, decide which site to use, and pass the job to its
    extractor.",str,str,str,bool,bool,-,>,None,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109,test,info,[,'sourceId',],# danmakuId = info['danmakuId'],#call extractor decided by sourceId,if,sourceType,==,'sina',:,sina_download_by_vid,(,sourceId,",",title,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),elif,sourceType,==,'youku',:,youku_download_by_vid,(,sourceId,",",title,=,title,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,",",*,*,kwargs,),elif,sourceType,==,'tudou',:,tudou_download_by_iid,(,sourceId,",",title,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),elif,sourceType,==,'qq',:,qq_download_by_vid,(,sourceId,",",title,",",True,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),elif,sourceType,==,'letv',:,letvcloud_download_by_vu,(,sourceId,",",'2d8c027396',",",title,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,),elif,sourceType,==,'zhuzhan',:,"#As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this",#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player,#old code removed,url,=,'http://www.acfun.cn/v/ac',+,vid,yk_streams,=,youku_acfun_proxy,(,info,[,'sourceId',],",",info,[,'encode',],",",url,),seq,=,[,'mp4hd3',",",'mp4hd2',",",'mp4hd',",",'flvhd',],for,t,in,seq,:,if,yk_streams,.,get,(,t,),:,preferred,=,yk_streams,[,t,],break,#total_size in the json could be incorrect(F.I. 0),size,=,0,for,url,in,preferred,[,0,],:,_,",",_,",",seg_size,=,url_info,(,url,),size,+=,seg_size,#fallback to flvhd is not quite possible,if,re,.,search,(,r'fid=[0-9A-Z\-]*.flv',",",preferred,[,0,],[,0,],),:,ext,=,'flv',else,:,ext,=,'mp4',print_info,(,site_info,",",title,",",ext,",",size,),if,not,info_only,:,download_urls,(,preferred,[,0,],",",title,",",ext,",",size,",",output_dir,=,output_dir,",",merge,=,merge,),else,:,raise,NotImplementedError,(,sourceType,),if,not,info_only,and,not,dry_run,:,if,not,kwargs,[,'caption',],:,print,(,'Skipping danmaku.',),return,try,:,title,=,get_filename,(,title,),print,(,'Downloading %s ...\n',%,(,title,+,'.cmt.json',),),cmt,=,get_srt_json,(,vid,),with,open,(,os,.,path,.,join,(,output_dir,",",title,+,'.cmt.json',),",",'w',",",encoding,=,'utf-8',),as,x,:,x,.,write,(,cmt,),except,:,pass,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/common.py,matchall,"def matchall(text, patterns):
    """"""Scans through a string for substrings matched some patterns.

    Args:
        text: A string to be scanned.
        patterns: a list of regex pattern.

    Returns:
        a list if matched. empty if not.
    """"""

    ret = []
    for pattern in patterns:
        match = re.findall(pattern, text)
        ret += match

    return ret",python,"def matchall(text, patterns):
    """"""Scans through a string for substrings matched some patterns.

    Args:
        text: A string to be scanned.
        patterns: a list of regex pattern.

    Returns:
        a list if matched. empty if not.
    """"""

    ret = []
    for pattern in patterns:
        match = re.findall(pattern, text)
        ret += match

    return ret",def,matchall,(,text,",",patterns,),:,ret,=,[,],for,pattern,in,patterns,:,match,=,re,.,findall,(,pattern,",",text,),ret,+=,match,return,ret,,,,,,,,,,,,,,,,,,,,,"Scans through a string for substrings matched some patterns.

    Args:
        text: A string to be scanned.
        patterns: a list of regex pattern.

    Returns:
        a list if matched. empty if not.",Scans,through,a,string,for,substrings,matched,some,patterns,.,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/common.py,parse_query_param,"def parse_query_param(url, param):
    """"""Parses the query string of a URL and returns the value of a parameter.

    Args:
        url: A URL.
        param: A string representing the name of the parameter.

    Returns:
        The value of the parameter.
    """"""

    try:
        return parse.parse_qs(parse.urlparse(url).query)[param][0]
    except:
        return None",python,"def parse_query_param(url, param):
    """"""Parses the query string of a URL and returns the value of a parameter.

    Args:
        url: A URL.
        param: A string representing the name of the parameter.

    Returns:
        The value of the parameter.
    """"""

    try:
        return parse.parse_qs(parse.urlparse(url).query)[param][0]
    except:
        return None",def,parse_query_param,(,url,",",param,),:,try,:,return,parse,.,parse_qs,(,parse,.,urlparse,(,url,),.,query,),[,param,],[,0,],except,:,return,None,,,,,,,,,,,,,,,,,,,"Parses the query string of a URL and returns the value of a parameter.

    Args:
        url: A URL.
        param: A string representing the name of the parameter.

    Returns:
        The value of the parameter.",Parses,the,query,string,of,a,URL,and,returns,the,value,of,a,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,parameter,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/common.py,get_content,"def get_content(url, headers={}, decoded=True):
    """"""Gets the content of a URL via sending a HTTP GET request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.
    """"""

    logging.debug('get_content: %s' % url)

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)

    response = urlopen_with_retry(req)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type', ''), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset, 'ignore')
        else:
            data = data.decode('utf-8', 'ignore')

    return data",python,"def get_content(url, headers={}, decoded=True):
    """"""Gets the content of a URL via sending a HTTP GET request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.
    """"""

    logging.debug('get_content: %s' % url)

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)

    response = urlopen_with_retry(req)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type', ''), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset, 'ignore')
        else:
            data = data.decode('utf-8', 'ignore')

    return data",def,get_content,(,url,",",headers,=,{,},",",decoded,=,True,),:,logging,.,debug,(,'get_content: %s',%,url,),req,=,request,.,Request,(,url,",",headers,=,headers,),if,cookies,:,cookies,.,add_cookie_header,(,req,),req,.,headers,.,update,(,req,.,"Gets the content of a URL via sending a HTTP GET request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.",Gets,the,content,of,a,URL,via,sending,a,HTTP,GET,request,.,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454,test,unredirected_hdrs,),response,=,urlopen_with_retry,(,req,),data,=,response,.,read,(,),# Handle HTTP compression for gzip and deflate (zlib),content_encoding,=,response,.,getheader,(,'Content-Encoding',),if,content_encoding,==,'gzip',:,data,=,ungzip,(,data,),elif,content_encoding,==,'deflate',:,data,=,undeflate,(,data,),# Decode the response body,if,decoded,:,charset,=,match1,(,response,.,getheader,(,'Content-Type',",",'',),",",r'charset=([\w-]+)',),if,charset,is,not,None,:,data,=,data,.,decode,(,charset,",",'ignore',),else,:,data,=,data,.,decode,(,'utf-8',",",'ignore',),return,data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/common.py,post_content,"def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):
    """"""Post the content of a URL via sending a HTTP POST request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.
    """"""
    if kwargs.get('post_data_raw'):
        logging.debug('post_content: %s\npost_data_raw: %s' % (url, kwargs['post_data_raw']))
    else:
        logging.debug('post_content: %s\npost_data: %s' % (url, post_data))

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)
    if kwargs.get('post_data_raw'):
        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')
    else:
        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')
    response = urlopen_with_retry(req, data=post_data_enc)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type'), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset)
        else:
            data = data.decode('utf-8')

    return data",python,"def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):
    """"""Post the content of a URL via sending a HTTP POST request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.
    """"""
    if kwargs.get('post_data_raw'):
        logging.debug('post_content: %s\npost_data_raw: %s' % (url, kwargs['post_data_raw']))
    else:
        logging.debug('post_content: %s\npost_data: %s' % (url, post_data))

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)
    if kwargs.get('post_data_raw'):
        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')
    else:
        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')
    response = urlopen_with_retry(req, data=post_data_enc)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type'), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset)
        else:
            data = data.decode('utf-8')

    return data",def,post_content,(,url,",",headers,=,{,},",",post_data,=,{,},",",decoded,=,True,",",*,*,kwargs,),:,if,kwargs,.,get,(,'post_data_raw',),:,logging,.,debug,(,'post_content: %s\npost_data_raw: %s',%,(,url,",",kwargs,[,'post_data_raw',],),),else,:,logging,.,debug,"Post the content of a URL via sending a HTTP POST request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.",Post,the,content,of,a,URL,via,sending,a,HTTP,POST,request,.,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501,test,(,'post_content: %s\npost_data: %s',%,(,url,",",post_data,),),req,=,request,.,Request,(,url,",",headers,=,headers,),if,cookies,:,cookies,.,add_cookie_header,(,req,),req,.,headers,.,update,(,req,.,unredirected_hdrs,),if,kwargs,.,get,(,'post_data_raw',),:,post_data_enc,=,bytes,(,kwargs,[,'post_data_raw',],",",'utf-8',),else,:,post_data_enc,=,bytes,(,parse,.,urlencode,(,post_data,),",",'utf-8',),response,=,urlopen_with_retry,(,req,",",data,=,post_data_enc,),data,=,response,.,read,(,),# Handle HTTP compression for gzip and deflate (zlib),content_encoding,=,response,.,getheader,(,'Content-Encoding',),if,content_encoding,==,'gzip',:,data,=,ungzip,(,data,),elif,content_encoding,==,'deflate',:,data,=,undeflate,(,data,),# Decode the response body,if,decoded,:,charset,=,match1,(,response,.,getheader,(,'Content-Type',),",",r'charset=([\w-]+)',),if,charset,is,not,None,:,data,=,data,.,decode,(,charset,),else,:,data,=,data,.,decode,(,'utf-8',),return,data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/common.py,parse_host,"def parse_host(host):
    """"""Parses host name and port number from a string.
    """"""
    if re.match(r'^(\d+)$', host) is not None:
        return (""0.0.0.0"", int(host))
    if re.match(r'^(\w+)://', host) is None:
        host = ""//"" + host
    o = parse.urlparse(host)
    hostname = o.hostname or ""0.0.0.0""
    port = o.port or 0
    return (hostname, port)",python,"def parse_host(host):
    """"""Parses host name and port number from a string.
    """"""
    if re.match(r'^(\d+)$', host) is not None:
        return (""0.0.0.0"", int(host))
    if re.match(r'^(\w+)://', host) is None:
        host = ""//"" + host
    o = parse.urlparse(host)
    hostname = o.hostname or ""0.0.0.0""
    port = o.port or 0
    return (hostname, port)",def,parse_host,(,host,),:,if,re,.,match,(,r'^(\d+)$',",",host,),is,not,None,:,return,(,"""0.0.0.0""",",",int,(,host,),),if,re,.,match,(,r'^(\w+)://',",",host,),is,None,:,host,=,"""//""",+,host,o,=,parse,.,urlparse,(,host,Parses host name and port number from a string.,Parses,host,name,and,port,number,from,a,string,.,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1216-L1226,test,),hostname,=,o,.,hostname,or,"""0.0.0.0""",port,=,o,.,port,or,0,return,(,hostname,",",port,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/showroom.py,showroom_get_roomid_by_room_url_key,"def showroom_get_roomid_by_room_url_key(room_url_key):
    """"""str->str""""""
    fake_headers_mobile = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Charset': 'UTF-8,*;q=0.5',
        'Accept-Encoding': 'gzip,deflate,sdch',
        'Accept-Language': 'en-US,en;q=0.8',
        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'
    }
    webpage_url = 'https://www.showroom-live.com/' + room_url_key
    html = get_content(webpage_url, headers = fake_headers_mobile)
    roomid = match1(html, r'room\?room_id\=(\d+)')
    assert roomid
    return roomid",python,"def showroom_get_roomid_by_room_url_key(room_url_key):
    """"""str->str""""""
    fake_headers_mobile = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Charset': 'UTF-8,*;q=0.5',
        'Accept-Encoding': 'gzip,deflate,sdch',
        'Accept-Language': 'en-US,en;q=0.8',
        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'
    }
    webpage_url = 'https://www.showroom-live.com/' + room_url_key
    html = get_content(webpage_url, headers = fake_headers_mobile)
    roomid = match1(html, r'room\?room_id\=(\d+)')
    assert roomid
    return roomid",def,showroom_get_roomid_by_room_url_key,(,room_url_key,),:,fake_headers_mobile,=,{,'Accept',:,"'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'",",",'Accept-Charset',:,"'UTF-8,*;q=0.5'",",",'Accept-Encoding',:,"'gzip,deflate,sdch'",",",'Accept-Language',:,"'en-US,en;q=0.8'",",",'User-Agent',:,"'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'",},webpage_url,=,'https://www.showroom-live.com/',+,room_url_key,html,=,get_content,(,webpage_url,",",headers,=,fake_headers_mobile,),roomid,=,match1,(,html,",",r'room\?room_id\=(\d+)',),str->str,str,-,>,str,,,,,,,,,,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L11-L24,test,assert,roomid,return,roomid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/wanmen.py,_wanmen_get_title_by_json_topic_part,"def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):
    """"""JSON, int, int, int->str
    
    Get a proper title with courseid+topicID+partID.""""""

    return '_'.join([json_content[0]['name'],
                    json_content[0]['Topics'][tIndex]['name'],
                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",python,"def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):
    """"""JSON, int, int, int->str
    
    Get a proper title with courseid+topicID+partID.""""""

    return '_'.join([json_content[0]['name'],
                    json_content[0]['Topics'][tIndex]['name'],
                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",def,_wanmen_get_title_by_json_topic_part,(,json_content,",",tIndex,",",pIndex,),:,return,'_',.,join,(,[,json_content,[,0,],[,'name',],",",json_content,[,0,],[,'Topics',],[,tIndex,],[,'name',],",",json_content,[,0,],[,'Topics',],[,tIndex,],[,'Parts',],[,"JSON, int, int, int->str
    
    Get a proper title with courseid+topicID+partID.",JSON,int,int,int,-,>,str,Get,a,proper,title,with,courseid,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L18-L25,test,pIndex,],[,'name',],],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,+,topicID,+,partID,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/wanmen.py,wanmen_download_by_course,"def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""int->None
    
    Download a WHOLE course.
    Reuse the API call to save time.""""""

    for tIndex in range(len(json_api_content[0]['Topics'])):
        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):
            wanmen_download_by_course_topic_part(json_api_content,
                                                 tIndex,
                                                 pIndex,
                                                 output_dir=output_dir,
                                                 merge=merge,
                                                 info_only=info_only,
                                                 **kwargs)",python,"def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""int->None
    
    Download a WHOLE course.
    Reuse the API call to save time.""""""

    for tIndex in range(len(json_api_content[0]['Topics'])):
        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):
            wanmen_download_by_course_topic_part(json_api_content,
                                                 tIndex,
                                                 pIndex,
                                                 output_dir=output_dir,
                                                 merge=merge,
                                                 info_only=info_only,
                                                 **kwargs)",def,wanmen_download_by_course,(,json_api_content,",",output_dir,=,'.',",",merge,=,True,",",info_only,=,False,",",*,*,kwargs,),:,for,tIndex,in,range,(,len,(,json_api_content,[,0,],[,'Topics',],),),:,for,pIndex,in,range,(,len,(,json_api_content,[,0,],[,'Topics',"int->None
    
    Download a WHOLE course.
    Reuse the API call to save time.",int,-,>,None,Download,a,WHOLE,course,.,Reuse,the,API,call,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L37-L51,test,],[,tIndex,],[,'Parts',],),),:,wanmen_download_by_course_topic_part,(,json_api_content,",",tIndex,",",pIndex,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,save,time,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
soimort/you-get,src/you_get/extractors/wanmen.py,wanmen_download_by_course_topic_part,"def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""int, int, int->None
    
    Download ONE PART of the course.""""""

    html = json_api_content

    title = _wanmen_get_title_by_json_topic_part(html, 
                                                  tIndex, 
                                                  pIndex)

    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,
                                                      tIndex, 
                                                     pIndex)

    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",python,"def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""int, int, int->None
    
    Download ONE PART of the course.""""""

    html = json_api_content

    title = _wanmen_get_title_by_json_topic_part(html, 
                                                  tIndex, 
                                                  pIndex)

    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,
                                                      tIndex, 
                                                     pIndex)

    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",def,wanmen_download_by_course_topic_part,(,json_api_content,",",tIndex,",",pIndex,",",output_dir,=,'.',",",merge,=,True,",",info_only,=,False,",",*,*,kwargs,),:,html,=,json_api_content,title,=,_wanmen_get_title_by_json_topic_part,(,html,",",tIndex,",",pIndex,),bokeccID,=,_wanmen_get_boke_id_by_json_topic_part,(,html,",",tIndex,",",pIndex,),bokecc_download_by_id,(,vid,"int, int, int->None
    
    Download ONE PART of the course.",int,int,int,-,>,None,Download,ONE,PART,of,the,course,.,b746ac01c9f39de94cac2d56f665285b0523b974,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L69-L84,test,=,bokeccID,",",title,=,title,",",output_dir,=,output_dir,",",merge,=,merge,",",info_only,=,info_only,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/base_executor.py,BaseExecutor.has_task,"def has_task(self, task_instance):
        """"""
        Checks if a task is either queued or running in this executor

        :param task_instance: TaskInstance
        :return: True if the task is known to this executor
        """"""
        if task_instance.key in self.queued_tasks or task_instance.key in self.running:
            return True",python,"def has_task(self, task_instance):
        """"""
        Checks if a task is either queued or running in this executor

        :param task_instance: TaskInstance
        :return: True if the task is known to this executor
        """"""
        if task_instance.key in self.queued_tasks or task_instance.key in self.running:
            return True",def,has_task,(,self,",",task_instance,),:,if,task_instance,.,key,in,self,.,queued_tasks,or,task_instance,.,key,in,self,.,running,:,return,True,,,,,,,,,,,,,,,,,,,,,,,,,,"Checks if a task is either queued or running in this executor

        :param task_instance: TaskInstance
        :return: True if the task is known to this executor",Checks,if,a,task,is,either,queued,or,running,in,this,executor,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L97-L105,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/base_executor.py,BaseExecutor.get_event_buffer,"def get_event_buffer(self, dag_ids=None):
        """"""
        Returns and flush the event buffer. In case dag_ids is specified
        it will only return and flush events for the given dag_ids. Otherwise
        it returns and flushes all

        :param dag_ids: to dag_ids to return events for, if None returns all
        :return: a dict of events
        """"""
        cleared_events = dict()
        if dag_ids is None:
            cleared_events = self.event_buffer
            self.event_buffer = dict()
        else:
            for key in list(self.event_buffer.keys()):
                dag_id, _, _, _ = key
                if dag_id in dag_ids:
                    cleared_events[key] = self.event_buffer.pop(key)

        return cleared_events",python,"def get_event_buffer(self, dag_ids=None):
        """"""
        Returns and flush the event buffer. In case dag_ids is specified
        it will only return and flush events for the given dag_ids. Otherwise
        it returns and flushes all

        :param dag_ids: to dag_ids to return events for, if None returns all
        :return: a dict of events
        """"""
        cleared_events = dict()
        if dag_ids is None:
            cleared_events = self.event_buffer
            self.event_buffer = dict()
        else:
            for key in list(self.event_buffer.keys()):
                dag_id, _, _, _ = key
                if dag_id in dag_ids:
                    cleared_events[key] = self.event_buffer.pop(key)

        return cleared_events",def,get_event_buffer,(,self,",",dag_ids,=,None,),:,cleared_events,=,dict,(,),if,dag_ids,is,None,:,cleared_events,=,self,.,event_buffer,self,.,event_buffer,=,dict,(,),else,:,for,key,in,list,(,self,.,event_buffer,.,keys,(,),),:,dag_id,",",_,",","Returns and flush the event buffer. In case dag_ids is specified
        it will only return and flush events for the given dag_ids. Otherwise
        it returns and flushes all

        :param dag_ids: to dag_ids to return events for, if None returns all
        :return: a dict of events",Returns,and,flush,the,event,buffer,.,In,case,dag_ids,is,specified,it,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L160-L179,test,_,",",_,=,key,if,dag_id,in,dag_ids,:,cleared_events,[,key,],=,self,.,event_buffer,.,pop,(,key,),return,cleared_events,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,will,only,return,and,flush,events,for,the,given,dag_ids,.,Otherwise,it,returns,and,flushes,all,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/snowflake_hook.py,SnowflakeHook.get_conn,"def get_conn(self):
        """"""
        Returns a snowflake.connection object
        """"""
        conn_config = self._get_conn_params()
        conn = snowflake.connector.connect(**conn_config)
        return conn",python,"def get_conn(self):
        """"""
        Returns a snowflake.connection object
        """"""
        conn_config = self._get_conn_params()
        conn = snowflake.connector.connect(**conn_config)
        return conn",def,get_conn,(,self,),:,conn_config,=,self,.,_get_conn_params,(,),conn,=,snowflake,.,connector,.,connect,(,*,*,conn_config,),return,conn,,,,,,,,,,,,,,,,,,,,,,,,,,Returns a snowflake.connection object,Returns,a,snowflake,.,connection,object,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L107-L113,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/snowflake_hook.py,SnowflakeHook._get_aws_credentials,"def _get_aws_credentials(self):
        """"""
        returns aws_access_key_id, aws_secret_access_key
        from extra

        intended to be used by external import and export statements
        """"""
        if self.snowflake_conn_id:
            connection_object = self.get_connection(self.snowflake_conn_id)
            if 'aws_secret_access_key' in connection_object.extra_dejson:
                aws_access_key_id = connection_object.extra_dejson.get(
                    'aws_access_key_id')
                aws_secret_access_key = connection_object.extra_dejson.get(
                    'aws_secret_access_key')
        return aws_access_key_id, aws_secret_access_key",python,"def _get_aws_credentials(self):
        """"""
        returns aws_access_key_id, aws_secret_access_key
        from extra

        intended to be used by external import and export statements
        """"""
        if self.snowflake_conn_id:
            connection_object = self.get_connection(self.snowflake_conn_id)
            if 'aws_secret_access_key' in connection_object.extra_dejson:
                aws_access_key_id = connection_object.extra_dejson.get(
                    'aws_access_key_id')
                aws_secret_access_key = connection_object.extra_dejson.get(
                    'aws_secret_access_key')
        return aws_access_key_id, aws_secret_access_key",def,_get_aws_credentials,(,self,),:,if,self,.,snowflake_conn_id,:,connection_object,=,self,.,get_connection,(,self,.,snowflake_conn_id,),if,'aws_secret_access_key',in,connection_object,.,extra_dejson,:,aws_access_key_id,=,connection_object,.,extra_dejson,.,get,(,'aws_access_key_id',),aws_secret_access_key,=,connection_object,.,extra_dejson,.,get,(,'aws_secret_access_key',),return,aws_access_key_id,",",aws_secret_access_key,"returns aws_access_key_id, aws_secret_access_key
        from extra

        intended to be used by external import and export statements",returns,aws_access_key_id,aws_secret_access_key,from,extra,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L115-L129,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/grpc_hook.py,GrpcHook._get_field,"def _get_field(self, field_name, default=None):
        """"""
        Fetches a field from extras, and returns it. This is some Airflow
        magic. The grpc hook type adds custom UI elements
        to the hook page, which allow admins to specify scopes, credential pem files, etc.
        They get formatted as shown below.
        """"""
        full_field_name = 'extra__grpc__{}'.format(field_name)
        if full_field_name in self.extras:
            return self.extras[full_field_name]
        else:
            return default",python,"def _get_field(self, field_name, default=None):
        """"""
        Fetches a field from extras, and returns it. This is some Airflow
        magic. The grpc hook type adds custom UI elements
        to the hook page, which allow admins to specify scopes, credential pem files, etc.
        They get formatted as shown below.
        """"""
        full_field_name = 'extra__grpc__{}'.format(field_name)
        if full_field_name in self.extras:
            return self.extras[full_field_name]
        else:
            return default",def,_get_field,(,self,",",field_name,",",default,=,None,),:,full_field_name,=,'extra__grpc__{}',.,format,(,field_name,),if,full_field_name,in,self,.,extras,:,return,self,.,extras,[,full_field_name,],else,:,return,default,,,,,,,,,,,,,,,"Fetches a field from extras, and returns it. This is some Airflow
        magic. The grpc hook type adds custom UI elements
        to the hook page, which allow admins to specify scopes, credential pem files, etc.
        They get formatted as shown below.",Fetches,a,field,from,extras,and,returns,it,.,This,is,some,Airflow,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/grpc_hook.py#L112-L123,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,magic,.,The,grpc,hook,type,adds,custom,UI,elements,to,the,hook,page,which,allow,admins,to,specify,scopes,credential,pem,files,etc,.,They,get,formatted,as,shown,below,.,,,,,,,,
apache/airflow,airflow/hooks/postgres_hook.py,PostgresHook.copy_expert,"def copy_expert(self, sql, filename, open=open):
        """"""
        Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a ""COPY FROM"" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.
        """"""
        if not os.path.isfile(filename):
            with open(filename, 'w'):
                pass

        with open(filename, 'r+') as f:
            with closing(self.get_conn()) as conn:
                with closing(conn.cursor()) as cur:
                    cur.copy_expert(sql, f)
                    f.truncate(f.tell())
                    conn.commit()",python,"def copy_expert(self, sql, filename, open=open):
        """"""
        Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a ""COPY FROM"" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.
        """"""
        if not os.path.isfile(filename):
            with open(filename, 'w'):
                pass

        with open(filename, 'r+') as f:
            with closing(self.get_conn()) as conn:
                with closing(conn.cursor()) as cur:
                    cur.copy_expert(sql, f)
                    f.truncate(f.tell())
                    conn.commit()",def,copy_expert,(,self,",",sql,",",filename,",",open,=,open,),:,if,not,os,.,path,.,isfile,(,filename,),:,with,open,(,filename,",",'w',),:,pass,with,open,(,filename,",",'r+',),as,f,:,with,closing,(,self,.,get_conn,(,),"Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a ""COPY FROM"" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.",Executes,SQL,using,psycopg2,copy_expert,method,.,Necessary,to,execute,COPY,command,without,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L63-L83,test,),as,conn,:,with,closing,(,conn,.,cursor,(,),),as,cur,:,cur,.,copy_expert,(,sql,",",f,),f,.,truncate,(,f,.,tell,(,),),conn,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,access,to,a,superuser,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/postgres_hook.py,PostgresHook.bulk_dump,"def bulk_dump(self, table, tmp_file):
        """"""
        Dumps a database table into a tab-delimited file
        """"""
        self.copy_expert(""COPY {table} TO STDOUT"".format(table=table), tmp_file)",python,"def bulk_dump(self, table, tmp_file):
        """"""
        Dumps a database table into a tab-delimited file
        """"""
        self.copy_expert(""COPY {table} TO STDOUT"".format(table=table), tmp_file)",def,bulk_dump,(,self,",",table,",",tmp_file,),:,self,.,copy_expert,(,"""COPY {table} TO STDOUT""",.,format,(,table,=,table,),",",tmp_file,),,,,,,,,,,,,,,,,,,,,,,,,,,,,Dumps a database table into a tab-delimited file,Dumps,a,database,table,into,a,tab,-,delimited,file,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L91-L95,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/file_to_gcs.py,FileToGoogleCloudStorageOperator.execute,"def execute(self, context):
        """"""
        Uploads the file to Google cloud storage
        """"""
        hook = GoogleCloudStorageHook(
            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,
            delegate_to=self.delegate_to)

        hook.upload(
            bucket_name=self.bucket,
            object_name=self.dst,
            mime_type=self.mime_type,
            filename=self.src,
            gzip=self.gzip,
        )",python,"def execute(self, context):
        """"""
        Uploads the file to Google cloud storage
        """"""
        hook = GoogleCloudStorageHook(
            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,
            delegate_to=self.delegate_to)

        hook.upload(
            bucket_name=self.bucket,
            object_name=self.dst,
            mime_type=self.mime_type,
            filename=self.src,
            gzip=self.gzip,
        )",def,execute,(,self,",",context,),:,hook,=,GoogleCloudStorageHook,(,google_cloud_storage_conn_id,=,self,.,google_cloud_storage_conn_id,",",delegate_to,=,self,.,delegate_to,),hook,.,upload,(,bucket_name,=,self,.,bucket,",",object_name,=,self,.,dst,",",mime_type,=,self,.,mime_type,",",filename,=,self,.,src,",",Uploads the file to Google cloud storage,Uploads,the,file,to,Google,cloud,storage,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_gcs.py#L68-L82,test,gzip,=,self,.,gzip,",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/macros/hive.py,max_partition,"def max_partition(
        table, schema=""default"", field=None, filter_map=None,
        metastore_conn_id='metastore_default'):
    """"""
    Gets the max partition for a table.

    :param schema: The hive schema the table lives in
    :type schema: str
    :param table: The hive table you are interested in, supports the dot
        notation as in ""my_database.my_table"", if a dot is found,
        the schema param is disregarded
    :type table: str
    :param metastore_conn_id: The hive connection you are interested in.
        If your default is set you don't need to use this parameter.
    :type metastore_conn_id: str
    :param filter_map: partition_key:partition_value map used for partition filtering,
                       e.g. {'key1': 'value1', 'key2': 'value2'}.
                       Only partitions matching all partition_key:partition_value
                       pairs will be considered as candidates of max partition.
    :type filter_map: map
    :param field: the field to get the max value from. If there's only
        one partition field, this will be inferred
    :type field: str

    >>> max_partition('airflow.static_babynames_partitioned')
    '2015-01-01'
    """"""
    from airflow.hooks.hive_hooks import HiveMetastoreHook
    if '.' in table:
        schema, table = table.split('.')
    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)
    return hh.max_partition(
        schema=schema, table_name=table, field=field, filter_map=filter_map)",python,"def max_partition(
        table, schema=""default"", field=None, filter_map=None,
        metastore_conn_id='metastore_default'):
    """"""
    Gets the max partition for a table.

    :param schema: The hive schema the table lives in
    :type schema: str
    :param table: The hive table you are interested in, supports the dot
        notation as in ""my_database.my_table"", if a dot is found,
        the schema param is disregarded
    :type table: str
    :param metastore_conn_id: The hive connection you are interested in.
        If your default is set you don't need to use this parameter.
    :type metastore_conn_id: str
    :param filter_map: partition_key:partition_value map used for partition filtering,
                       e.g. {'key1': 'value1', 'key2': 'value2'}.
                       Only partitions matching all partition_key:partition_value
                       pairs will be considered as candidates of max partition.
    :type filter_map: map
    :param field: the field to get the max value from. If there's only
        one partition field, this will be inferred
    :type field: str

    >>> max_partition('airflow.static_babynames_partitioned')
    '2015-01-01'
    """"""
    from airflow.hooks.hive_hooks import HiveMetastoreHook
    if '.' in table:
        schema, table = table.split('.')
    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)
    return hh.max_partition(
        schema=schema, table_name=table, field=field, filter_map=filter_map)",def,max_partition,(,table,",",schema,=,"""default""",",",field,=,None,",",filter_map,=,None,",",metastore_conn_id,=,'metastore_default',),:,from,airflow,.,hooks,.,hive_hooks,import,HiveMetastoreHook,if,'.',in,table,:,schema,",",table,=,table,.,split,(,'.',),hh,=,HiveMetastoreHook,(,metastore_conn_id,=,metastore_conn_id,"Gets the max partition for a table.

    :param schema: The hive schema the table lives in
    :type schema: str
    :param table: The hive table you are interested in, supports the dot
        notation as in ""my_database.my_table"", if a dot is found,
        the schema param is disregarded
    :type table: str
    :param metastore_conn_id: The hive connection you are interested in.
        If your default is set you don't need to use this parameter.
    :type metastore_conn_id: str
    :param filter_map: partition_key:partition_value map used for partition filtering,
                       e.g. {'key1': 'value1', 'key2': 'value2'}.
                       Only partitions matching all partition_key:partition_value
                       pairs will be considered as candidates of max partition.
    :type filter_map: map
    :param field: the field to get the max value from. If there's only
        one partition field, this will be inferred
    :type field: str

    >>> max_partition('airflow.static_babynames_partitioned')
    '2015-01-01'",Gets,the,max,partition,for,a,table,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L23-L55,test,),return,hh,.,max_partition,(,schema,=,schema,",",table_name,=,table,",",field,=,field,",",filter_map,=,filter_map,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/mysql_hook.py,MySqlHook.get_conn,"def get_conn(self):
        """"""
        Returns a mysql connection object
        """"""
        conn = self.get_connection(self.mysql_conn_id)
        conn_config = {
            ""user"": conn.login,
            ""passwd"": conn.password or '',
            ""host"": conn.host or 'localhost',
            ""db"": self.schema or conn.schema or ''
        }

        if not conn.port:
            conn_config[""port""] = 3306
        else:
            conn_config[""port""] = int(conn.port)

        if conn.extra_dejson.get('charset', False):
            conn_config[""charset""] = conn.extra_dejson[""charset""]
            if (conn_config[""charset""]).lower() == 'utf8' or\
                    (conn_config[""charset""]).lower() == 'utf-8':
                conn_config[""use_unicode""] = True
        if conn.extra_dejson.get('cursor', False):
            if (conn.extra_dejson[""cursor""]).lower() == 'sscursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.SSCursor
            elif (conn.extra_dejson[""cursor""]).lower() == 'dictcursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.DictCursor
            elif (conn.extra_dejson[""cursor""]).lower() == 'ssdictcursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.SSDictCursor
        local_infile = conn.extra_dejson.get('local_infile', False)
        if conn.extra_dejson.get('ssl', False):
            # SSL parameter for MySQL has to be a dictionary and in case
            # of extra/dejson we can get string if extra is passed via
            # URL parameters
            dejson_ssl = conn.extra_dejson['ssl']
            if isinstance(dejson_ssl, six.string_types):
                dejson_ssl = json.loads(dejson_ssl)
            conn_config['ssl'] = dejson_ssl
        if conn.extra_dejson.get('unix_socket'):
            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']
        if local_infile:
            conn_config[""local_infile""] = 1
        conn = MySQLdb.connect(**conn_config)
        return conn",python,"def get_conn(self):
        """"""
        Returns a mysql connection object
        """"""
        conn = self.get_connection(self.mysql_conn_id)
        conn_config = {
            ""user"": conn.login,
            ""passwd"": conn.password or '',
            ""host"": conn.host or 'localhost',
            ""db"": self.schema or conn.schema or ''
        }

        if not conn.port:
            conn_config[""port""] = 3306
        else:
            conn_config[""port""] = int(conn.port)

        if conn.extra_dejson.get('charset', False):
            conn_config[""charset""] = conn.extra_dejson[""charset""]
            if (conn_config[""charset""]).lower() == 'utf8' or\
                    (conn_config[""charset""]).lower() == 'utf-8':
                conn_config[""use_unicode""] = True
        if conn.extra_dejson.get('cursor', False):
            if (conn.extra_dejson[""cursor""]).lower() == 'sscursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.SSCursor
            elif (conn.extra_dejson[""cursor""]).lower() == 'dictcursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.DictCursor
            elif (conn.extra_dejson[""cursor""]).lower() == 'ssdictcursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.SSDictCursor
        local_infile = conn.extra_dejson.get('local_infile', False)
        if conn.extra_dejson.get('ssl', False):
            # SSL parameter for MySQL has to be a dictionary and in case
            # of extra/dejson we can get string if extra is passed via
            # URL parameters
            dejson_ssl = conn.extra_dejson['ssl']
            if isinstance(dejson_ssl, six.string_types):
                dejson_ssl = json.loads(dejson_ssl)
            conn_config['ssl'] = dejson_ssl
        if conn.extra_dejson.get('unix_socket'):
            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']
        if local_infile:
            conn_config[""local_infile""] = 1
        conn = MySQLdb.connect(**conn_config)
        return conn",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,mysql_conn_id,),conn_config,=,{,"""user""",:,conn,.,login,",","""passwd""",:,conn,.,password,or,'',",","""host""",:,conn,.,host,or,'localhost',",","""db""",:,self,.,schema,or,conn,.,schema,or,'',Returns a mysql connection object,Returns,a,mysql,connection,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L62-L105,test,},if,not,conn,.,port,:,conn_config,[,"""port""",],=,3306,else,:,conn_config,[,"""port""",],=,int,(,conn,.,port,),if,conn,.,extra_dejson,.,get,(,'charset',",",False,),:,conn_config,[,"""charset""",],=,conn,.,extra_dejson,[,"""charset""",],if,(,conn_config,[,"""charset""",],),.,lower,(,),==,'utf8',or,(,conn_config,[,"""charset""",],),.,lower,(,),==,'utf-8',:,conn_config,[,"""use_unicode""",],=,True,if,conn,.,extra_dejson,.,get,(,'cursor',",",False,),:,if,(,conn,.,extra_dejson,[,"""cursor""",],),.,lower,(,),==,'sscursor',:,conn_config,[,"""cursorclass""",],=,MySQLdb,.,cursors,.,SSCursor,elif,(,conn,.,extra_dejson,[,"""cursor""",],),.,lower,(,),==,'dictcursor',:,conn_config,[,"""cursorclass""",],=,MySQLdb,.,cursors,.,DictCursor,elif,(,conn,.,extra_dejson,[,"""cursor""",],),.,lower,(,),==,'ssdictcursor',:,conn_config,[,"""cursorclass""",],=,MySQLdb,.,cursors,.,SSDictCursor,local_infile,=,conn,.,extra_dejson,.,get,(,'local_infile',",",False,),if,conn,.,extra_dejson,.,get,(,'ssl',",",False,),:,# SSL parameter for MySQL has to be a dictionary and in case,# of extra/dejson we can get string if extra is passed via,# URL parameters,dejson_ssl,=,conn,.,extra_dejson,[,'ssl',],if,isinstance,(,dejson_ssl,",",six,.,string_types,),:,dejson_ssl,=,json,.,loads,(,dejson_ssl,),conn_config,[,'ssl',],=,dejson_ssl,if,conn,.,extra_dejson,.,get,(,'unix_socket',),:,conn_config,[,'unix_socket',],=,conn,.,extra_dejson,[,'unix_socket',],if,local_infile,:,conn_config,[,"""local_infile""",],=,1,conn,=,MySQLdb,.,connect,(,*,*,conn_config,),return,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/bin/cli.py,task_state,"def task_state(args):
    """"""
    Returns the state of a TaskInstance at the command line.
    >>> airflow task_state tutorial sleep 2015-01-01
    success
    """"""
    dag = get_dag(args)
    task = dag.get_task(task_id=args.task_id)
    ti = TaskInstance(task, args.execution_date)
    print(ti.current_state())",python,"def task_state(args):
    """"""
    Returns the state of a TaskInstance at the command line.
    >>> airflow task_state tutorial sleep 2015-01-01
    success
    """"""
    dag = get_dag(args)
    task = dag.get_task(task_id=args.task_id)
    ti = TaskInstance(task, args.execution_date)
    print(ti.current_state())",def,task_state,(,args,),:,dag,=,get_dag,(,args,),task,=,dag,.,get_task,(,task_id,=,args,.,task_id,),ti,=,TaskInstance,(,task,",",args,.,execution_date,),print,(,ti,.,current_state,(,),),,,,,,,,,,,"Returns the state of a TaskInstance at the command line.
    >>> airflow task_state tutorial sleep 2015-01-01
    success",Returns,the,state,of,a,TaskInstance,at,the,command,line,.,>>>,airflow,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L554-L563,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,task_state,tutorial,sleep,2015,-,01,-,01,success,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/bin/cli.py,restart_workers,"def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):
    """"""
    Runs forever, monitoring the child processes of @gunicorn_master_proc and
    restarting workers occasionally.
    Each iteration of the loop traverses one edge of this state transition
    diagram, where each state (node) represents
    [ num_ready_workers_running / num_workers_running ]. We expect most time to
    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.
    The horizontal transition at ? happens after the new worker parses all the
    dags (so it could take a while!)
       V ────────────────────────────────────────────────────────────────────────┐
    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘
       ^                          ^───────────────┘
       │
       │      ┌────────────────v
       └──────┴────── [ [0, n) / n ] <─── start
    We change the number of workers by sending TTIN and TTOU to the gunicorn
    master process, which increases and decreases the number of child workers
    respectively. Gunicorn guarantees that on TTOU workers are terminated
    gracefully and that the oldest worker is terminated.
    """"""

    def wait_until_true(fn, timeout=0):
        """"""
        Sleeps until fn is true
        """"""
        t = time.time()
        while not fn():
            if 0 < timeout <= time.time() - t:
                raise AirflowWebServerTimeout(
                    ""No response from gunicorn master within {0} seconds""
                    .format(timeout))
            time.sleep(0.1)

    def start_refresh(gunicorn_master_proc):
        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')
        log.debug('%s doing a refresh of %s workers', state, batch_size)
        sys.stdout.flush()
        sys.stderr.flush()

        excess = 0
        for _ in range(batch_size):
            gunicorn_master_proc.send_signal(signal.SIGTTIN)
            excess += 1
            wait_until_true(lambda: num_workers_expected + excess ==
                            get_num_workers_running(gunicorn_master_proc),
                            master_timeout)

    try:
        wait_until_true(lambda: num_workers_expected ==
                        get_num_workers_running(gunicorn_master_proc),
                        master_timeout)
        while True:
            num_workers_running = get_num_workers_running(gunicorn_master_proc)
            num_ready_workers_running = \
                get_num_ready_workers_running(gunicorn_master_proc)

            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)

            # Whenever some workers are not ready, wait until all workers are ready
            if num_ready_workers_running < num_workers_running:
                log.debug('%s some workers are starting up, waiting...', state)
                sys.stdout.flush()
                time.sleep(1)

            # Kill a worker gracefully by asking gunicorn to reduce number of workers
            elif num_workers_running > num_workers_expected:
                excess = num_workers_running - num_workers_expected
                log.debug('%s killing %s workers', state, excess)

                for _ in range(excess):
                    gunicorn_master_proc.send_signal(signal.SIGTTOU)
                    excess -= 1
                    wait_until_true(lambda: num_workers_expected + excess ==
                                    get_num_workers_running(gunicorn_master_proc),
                                    master_timeout)

            # Start a new worker by asking gunicorn to increase number of workers
            elif num_workers_running == num_workers_expected:
                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')
                log.debug(
                    '%s sleeping for %ss starting doing a refresh...',
                    state, refresh_interval
                )
                time.sleep(refresh_interval)
                start_refresh(gunicorn_master_proc)

            else:
                # num_ready_workers_running == num_workers_running < num_workers_expected
                log.error((
                    ""%s some workers seem to have died and gunicorn""
                    ""did not restart them as expected""
                ), state)
                time.sleep(10)
                if len(
                    psutil.Process(gunicorn_master_proc.pid).children()
                ) < num_workers_expected:
                    start_refresh(gunicorn_master_proc)
    except (AirflowWebServerTimeout, OSError) as err:
        log.error(err)
        log.error(""Shutting down webserver"")
        try:
            gunicorn_master_proc.terminate()
            gunicorn_master_proc.wait()
        finally:
            sys.exit(1)",python,"def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):
    """"""
    Runs forever, monitoring the child processes of @gunicorn_master_proc and
    restarting workers occasionally.
    Each iteration of the loop traverses one edge of this state transition
    diagram, where each state (node) represents
    [ num_ready_workers_running / num_workers_running ]. We expect most time to
    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.
    The horizontal transition at ? happens after the new worker parses all the
    dags (so it could take a while!)
       V ────────────────────────────────────────────────────────────────────────┐
    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘
       ^                          ^───────────────┘
       │
       │      ┌────────────────v
       └──────┴────── [ [0, n) / n ] <─── start
    We change the number of workers by sending TTIN and TTOU to the gunicorn
    master process, which increases and decreases the number of child workers
    respectively. Gunicorn guarantees that on TTOU workers are terminated
    gracefully and that the oldest worker is terminated.
    """"""

    def wait_until_true(fn, timeout=0):
        """"""
        Sleeps until fn is true
        """"""
        t = time.time()
        while not fn():
            if 0 < timeout <= time.time() - t:
                raise AirflowWebServerTimeout(
                    ""No response from gunicorn master within {0} seconds""
                    .format(timeout))
            time.sleep(0.1)

    def start_refresh(gunicorn_master_proc):
        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')
        log.debug('%s doing a refresh of %s workers', state, batch_size)
        sys.stdout.flush()
        sys.stderr.flush()

        excess = 0
        for _ in range(batch_size):
            gunicorn_master_proc.send_signal(signal.SIGTTIN)
            excess += 1
            wait_until_true(lambda: num_workers_expected + excess ==
                            get_num_workers_running(gunicorn_master_proc),
                            master_timeout)

    try:
        wait_until_true(lambda: num_workers_expected ==
                        get_num_workers_running(gunicorn_master_proc),
                        master_timeout)
        while True:
            num_workers_running = get_num_workers_running(gunicorn_master_proc)
            num_ready_workers_running = \
                get_num_ready_workers_running(gunicorn_master_proc)

            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)

            # Whenever some workers are not ready, wait until all workers are ready
            if num_ready_workers_running < num_workers_running:
                log.debug('%s some workers are starting up, waiting...', state)
                sys.stdout.flush()
                time.sleep(1)

            # Kill a worker gracefully by asking gunicorn to reduce number of workers
            elif num_workers_running > num_workers_expected:
                excess = num_workers_running - num_workers_expected
                log.debug('%s killing %s workers', state, excess)

                for _ in range(excess):
                    gunicorn_master_proc.send_signal(signal.SIGTTOU)
                    excess -= 1
                    wait_until_true(lambda: num_workers_expected + excess ==
                                    get_num_workers_running(gunicorn_master_proc),
                                    master_timeout)

            # Start a new worker by asking gunicorn to increase number of workers
            elif num_workers_running == num_workers_expected:
                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')
                log.debug(
                    '%s sleeping for %ss starting doing a refresh...',
                    state, refresh_interval
                )
                time.sleep(refresh_interval)
                start_refresh(gunicorn_master_proc)

            else:
                # num_ready_workers_running == num_workers_running < num_workers_expected
                log.error((
                    ""%s some workers seem to have died and gunicorn""
                    ""did not restart them as expected""
                ), state)
                time.sleep(10)
                if len(
                    psutil.Process(gunicorn_master_proc.pid).children()
                ) < num_workers_expected:
                    start_refresh(gunicorn_master_proc)
    except (AirflowWebServerTimeout, OSError) as err:
        log.error(err)
        log.error(""Shutting down webserver"")
        try:
            gunicorn_master_proc.terminate()
            gunicorn_master_proc.wait()
        finally:
            sys.exit(1)",def,restart_workers,(,gunicorn_master_proc,",",num_workers_expected,",",master_timeout,),:,def,wait_until_true,(,fn,",",timeout,=,0,),:,"""""""
        Sleeps until fn is true
        """"""",t,=,time,.,time,(,),while,not,fn,(,),:,if,0,<,timeout,<=,time,.,time,(,),-,t,:,raise,AirflowWebServerTimeout,(,"""No response from gunicorn master within {0} seconds""",.,"Runs forever, monitoring the child processes of @gunicorn_master_proc and
    restarting workers occasionally.
    Each iteration of the loop traverses one edge of this state transition
    diagram, where each state (node) represents
    [ num_ready_workers_running / num_workers_running ]. We expect most time to
    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.
    The horizontal transition at ? happens after the new worker parses all the
    dags (so it could take a while!)
       V ────────────────────────────────────────────────────────────────────────┐
    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘
       ^                          ^───────────────┘
       │
       │      ┌────────────────v
       └──────┴────── [ [0, n) / n ] <─── start
    We change the number of workers by sending TTIN and TTOU to the gunicorn
    master process, which increases and decreases the number of child workers
    respectively. Gunicorn guarantees that on TTOU workers are terminated
    gracefully and that the oldest worker is terminated.",Runs,forever,monitoring,the,child,processes,of,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L763-L868,test,format,(,timeout,),),time,.,sleep,(,0.1,),def,start_refresh,(,gunicorn_master_proc,),:,batch_size,=,conf,.,getint,(,'webserver',",",'worker_refresh_batch_size',),log,.,debug,(,'%s doing a refresh of %s workers',",",state,",",batch_size,),sys,.,stdout,.,flush,(,),sys,.,stderr,.,flush,(,),excess,=,0,for,_,in,range,(,batch_size,),:,gunicorn_master_proc,.,send_signal,(,signal,.,SIGTTIN,),excess,+=,1,wait_until_true,(,lambda,:,num_workers_expected,+,excess,==,get_num_workers_running,(,gunicorn_master_proc,),",",master_timeout,),try,:,wait_until_true,(,lambda,:,num_workers_expected,==,get_num_workers_running,(,gunicorn_master_proc,),",",master_timeout,),while,True,:,num_workers_running,=,get_num_workers_running,(,gunicorn_master_proc,),num_ready_workers_running,=,get_num_ready_workers_running,(,gunicorn_master_proc,),state,=,'[{0} / {1}]',.,format,(,num_ready_workers_running,",",num_workers_running,),"# Whenever some workers are not ready, wait until all workers are ready",if,num_ready_workers_running,<,num_workers_running,:,log,.,debug,(,"'%s some workers are starting up, waiting...'",",",state,),sys,.,stdout,.,flush,(,),time,.,sleep,(,1,),# Kill a worker gracefully by asking gunicorn to reduce number of workers,elif,num_workers_running,>,num_workers_expected,:,excess,=,num_workers_running,-,num_workers_expected,log,.,debug,(,'%s killing %s workers',",",state,",",excess,),for,_,in,range,(,excess,),:,gunicorn_master_proc,.,send_signal,(,signal,.,SIGTTOU,),excess,-=,1,wait_until_true,(,lambda,:,num_workers_expected,+,excess,==,get_num_workers_running,(,gunicorn_master_proc,),",",master_timeout,),# Start a new worker by asking gunicorn to increase number of workers,elif,num_workers_running,==,num_workers_expected,:,refresh_interval,=,conf,.,getint,(,'webserver',",",'worker_refresh_interval',),log,.,debug,(,'%s sleeping for %ss starting doing a refresh...',",",state,",",refresh_interval,),time,.,sleep,(,refresh_interval,),start_refresh,(,gunicorn_master_proc,),else,:,# num_ready_workers_running == num_workers_running < num_workers_expected,log,.,error,(,(,"""%s some workers seem to have died and gunicorn""","""did not restart them as expected""",),",",state,),time,.,sleep,(,10,),if,len,(,psutil,.,Process,(,gunicorn_master_proc,.,pid,),.,children,(,),),<,num_workers_expected,:,start_refresh,(,gunicorn_master_proc,),except,(,AirflowWebServerTimeout,",",OSError,),as,err,:,log,.,error,(,err,),log,.,error,(,"""Shutting down webserver""",),try,:,gunicorn_master_proc,.,terminate,(,),gunicorn_master_proc,.,wait,(,),finally,:,sys,.,exit,(,1,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_translate_hook.py,CloudTranslateHook.get_conn,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Translate

        :return: Google Cloud Translate client object.
        :rtype: Client
        """"""
        if not self._client:
            self._client = Client(credentials=self._get_credentials())
        return self._client",python,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Translate

        :return: Google Cloud Translate client object.
        :rtype: Client
        """"""
        if not self._client:
            self._client = Client(credentials=self._get_credentials())
        return self._client",def,get_conn,(,self,),:,if,not,self,.,_client,:,self,.,_client,=,Client,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_client,,,,,,,,,,,,,,,,,,,,,,,"Retrieves connection to Cloud Translate

        :return: Google Cloud Translate client object.
        :rtype: Client",Retrieves,connection,to,Cloud,Translate,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L34-L43,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_translate_hook.py,CloudTranslateHook.translate,"def translate(
        self, values, target_language, format_=None, source_language=None, model=None
    ):
        """"""Translate a string or list of strings.

        See https://cloud.google.com/translate/docs/translating-text

        :type values: str or list
        :param values: String or list of strings to translate.

        :type target_language: str
        :param target_language: The language to translate results into. This
                                is required by the API and defaults to
                                the target language of the current instance.

        :type format_: str
        :param format_: (Optional) One of ``text`` or ``html``, to specify
                        if the input text is plain text or HTML.

        :type source_language: str or None
        :param source_language: (Optional) The language of the text to
                                be translated.

        :type model: str or None
        :param model: (Optional) The model used to translate the text, such
                      as ``'base'`` or ``'nmt'``.

        :rtype: str or list
        :returns: A list of dictionaries for each queried value. Each
                  dictionary typically contains three keys (though not
                  all will be present in all cases)

                  * ``detectedSourceLanguage``: The detected language (as an
                    ISO 639-1 language code) of the text.
                  * ``translatedText``: The translation of the text into the
                    target language.
                  * ``input``: The corresponding input value.
                  * ``model``: The model used to translate the text.

                  If only a single value is passed, then only a single
                  dictionary will be returned.
        :raises: :class:`~exceptions.ValueError` if the number of
                 values and translations differ.
        """"""
        client = self.get_conn()

        return client.translate(
            values=values,
            target_language=target_language,
            format_=format_,
            source_language=source_language,
            model=model,
        )",python,"def translate(
        self, values, target_language, format_=None, source_language=None, model=None
    ):
        """"""Translate a string or list of strings.

        See https://cloud.google.com/translate/docs/translating-text

        :type values: str or list
        :param values: String or list of strings to translate.

        :type target_language: str
        :param target_language: The language to translate results into. This
                                is required by the API and defaults to
                                the target language of the current instance.

        :type format_: str
        :param format_: (Optional) One of ``text`` or ``html``, to specify
                        if the input text is plain text or HTML.

        :type source_language: str or None
        :param source_language: (Optional) The language of the text to
                                be translated.

        :type model: str or None
        :param model: (Optional) The model used to translate the text, such
                      as ``'base'`` or ``'nmt'``.

        :rtype: str or list
        :returns: A list of dictionaries for each queried value. Each
                  dictionary typically contains three keys (though not
                  all will be present in all cases)

                  * ``detectedSourceLanguage``: The detected language (as an
                    ISO 639-1 language code) of the text.
                  * ``translatedText``: The translation of the text into the
                    target language.
                  * ``input``: The corresponding input value.
                  * ``model``: The model used to translate the text.

                  If only a single value is passed, then only a single
                  dictionary will be returned.
        :raises: :class:`~exceptions.ValueError` if the number of
                 values and translations differ.
        """"""
        client = self.get_conn()

        return client.translate(
            values=values,
            target_language=target_language,
            format_=format_,
            source_language=source_language,
            model=model,
        )",def,translate,(,self,",",values,",",target_language,",",format_,=,None,",",source_language,=,None,",",model,=,None,),:,client,=,self,.,get_conn,(,),return,client,.,translate,(,values,=,values,",",target_language,=,target_language,",",format_,=,format_,",",source_language,=,source_language,",",model,=,"Translate a string or list of strings.

        See https://cloud.google.com/translate/docs/translating-text

        :type values: str or list
        :param values: String or list of strings to translate.

        :type target_language: str
        :param target_language: The language to translate results into. This
                                is required by the API and defaults to
                                the target language of the current instance.

        :type format_: str
        :param format_: (Optional) One of ``text`` or ``html``, to specify
                        if the input text is plain text or HTML.

        :type source_language: str or None
        :param source_language: (Optional) The language of the text to
                                be translated.

        :type model: str or None
        :param model: (Optional) The model used to translate the text, such
                      as ``'base'`` or ``'nmt'``.

        :rtype: str or list
        :returns: A list of dictionaries for each queried value. Each
                  dictionary typically contains three keys (though not
                  all will be present in all cases)

                  * ``detectedSourceLanguage``: The detected language (as an
                    ISO 639-1 language code) of the text.
                  * ``translatedText``: The translation of the text into the
                    target language.
                  * ``input``: The corresponding input value.
                  * ``model``: The model used to translate the text.

                  If only a single value is passed, then only a single
                  dictionary will be returned.
        :raises: :class:`~exceptions.ValueError` if the number of
                 values and translations differ.",Translate,a,string,or,list,of,strings,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L45-L97,test,model,",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.get_instance,"def get_instance(self, instance, project_id=None):
        """"""
        Retrieves a resource containing information about a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL instance resource.
        :rtype: dict
        """"""
        return self.get_conn().instances().get(
            project=project_id,
            instance=instance
        ).execute(num_retries=self.num_retries)",python,"def get_instance(self, instance, project_id=None):
        """"""
        Retrieves a resource containing information about a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL instance resource.
        :rtype: dict
        """"""
        return self.get_conn().instances().get(
            project=project_id,
            instance=instance
        ).execute(num_retries=self.num_retries)",def,get_instance,(,self,",",instance,",",project_id,=,None,),:,return,self,.,get_conn,(,),.,instances,(,),.,get,(,project,=,project_id,",",instance,=,instance,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,,,,,"Retrieves a resource containing information about a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL instance resource.
        :rtype: dict",Retrieves,a,resource,containing,information,about,a,Cloud,SQL,instance,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L97-L112,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.create_instance,"def create_instance(self, body, project_id=None):
        """"""
        Creates a new Cloud SQL instance.

        :param body: Body required by the Cloud SQL insert API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().insert(
            project=project_id,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def create_instance(self, body, project_id=None):
        """"""
        Creates a new Cloud SQL instance.

        :param body: Body required by the Cloud SQL insert API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().insert(
            project=project_id,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,create_instance,(,self,",",body,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instances,(,),.,insert,(,project,=,project_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,[,"""name""",],self,.,_wait_for_operation_to_complete,"Creates a new Cloud SQL instance.

        :param body: Body required by the Cloud SQL insert API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Creates,a,new,Cloud,SQL,instance,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L115-L133,test,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.patch_instance,"def patch_instance(self, body, instance, project_id=None):
        """"""
        Updates settings of a Cloud SQL instance.

        Caution: This is not a partial update, so you must include values for
        all the settings that you want to retain.

        :param body: Body required by the Cloud SQL patch API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.
        :type body: dict
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().patch(
            project=project_id,
            instance=instance,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def patch_instance(self, body, instance, project_id=None):
        """"""
        Updates settings of a Cloud SQL instance.

        Caution: This is not a partial update, so you must include values for
        all the settings that you want to retain.

        :param body: Body required by the Cloud SQL patch API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.
        :type body: dict
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().patch(
            project=project_id,
            instance=instance,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,patch_instance,(,self,",",body,",",instance,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instances,(,),.,patch,(,project,=,project_id,",",instance,=,instance,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,"Updates settings of a Cloud SQL instance.

        Caution: This is not a partial update, so you must include values for
        all the settings that you want to retain.

        :param body: Body required by the Cloud SQL patch API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.
        :type body: dict
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Updates,settings,of,a,Cloud,SQL,instance,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L136-L160,test,[,"""name""",],self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.delete_instance,"def delete_instance(self, instance, project_id=None):
        """"""
        Deletes a Cloud SQL instance.

        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :return: None
        """"""
        response = self.get_conn().instances().delete(
            project=project_id,
            instance=instance,
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def delete_instance(self, instance, project_id=None):
        """"""
        Deletes a Cloud SQL instance.

        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :return: None
        """"""
        response = self.get_conn().instances().delete(
            project=project_id,
            instance=instance,
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,delete_instance,(,self,",",instance,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instances,(,),.,delete,(,project,=,project_id,",",instance,=,instance,",",),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,[,"""name""",],self,.,"Deletes a Cloud SQL instance.

        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :return: None",Deletes,a,Cloud,SQL,instance,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L163-L180,test,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.get_database,"def get_database(self, instance, database, project_id=None):
        """"""
        Retrieves a database resource from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL database resource, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.
        :rtype: dict
        """"""
        return self.get_conn().databases().get(
            project=project_id,
            instance=instance,
            database=database
        ).execute(num_retries=self.num_retries)",python,"def get_database(self, instance, database, project_id=None):
        """"""
        Retrieves a database resource from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL database resource, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.
        :rtype: dict
        """"""
        return self.get_conn().databases().get(
            project=project_id,
            instance=instance,
            database=database
        ).execute(num_retries=self.num_retries)",def,get_database,(,self,",",instance,",",database,",",project_id,=,None,),:,return,self,.,get_conn,(,),.,databases,(,),.,get,(,project,=,project_id,",",instance,=,instance,",",database,=,database,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,"Retrieves a database resource from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL database resource, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.
        :rtype: dict",Retrieves,a,database,resource,from,a,Cloud,SQL,instance,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L183-L202,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.create_database,"def create_database(self, instance, body, project_id=None):
        """"""
        Creates a new database inside a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().insert(
            project=project_id,
            instance=instance,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def create_database(self, instance, body, project_id=None):
        """"""
        Creates a new database inside a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().insert(
            project=project_id,
            instance=instance,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,create_database,(,self,",",instance,",",body,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,databases,(,),.,insert,(,project,=,project_id,",",instance,=,instance,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,"Creates a new database inside a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Creates,a,new,database,inside,a,Cloud,SQL,instance,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L205-L226,test,[,"""name""",],self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.patch_database,"def patch_database(self, instance, database, body, project_id=None):
        """"""
        Updates a database resource inside a Cloud SQL instance.

        This method supports patch semantics.
        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be updated in the instance.
        :type database: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().patch(
            project=project_id,
            instance=instance,
            database=database,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def patch_database(self, instance, database, body, project_id=None):
        """"""
        Updates a database resource inside a Cloud SQL instance.

        This method supports patch semantics.
        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be updated in the instance.
        :type database: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().patch(
            project=project_id,
            instance=instance,
            database=database,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,patch_database,(,self,",",instance,",",database,",",body,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,databases,(,),.,patch,(,project,=,project_id,",",instance,=,instance,",",database,=,database,",",body,=,body,),.,execute,(,num_retries,=,self,"Updates a database resource inside a Cloud SQL instance.

        This method supports patch semantics.
        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be updated in the instance.
        :type database: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Updates,a,database,resource,inside,a,Cloud,SQL,instance,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L229-L256,test,.,num_retries,),operation_name,=,response,[,"""name""",],self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.delete_database,"def delete_database(self, instance, database, project_id=None):
        """"""
        Deletes a database from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be deleted in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().delete(
            project=project_id,
            instance=instance,
            database=database
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def delete_database(self, instance, database, project_id=None):
        """"""
        Deletes a database from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be deleted in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().delete(
            project=project_id,
            instance=instance,
            database=database
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,delete_database,(,self,",",instance,",",database,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,databases,(,),.,delete,(,project,=,project_id,",",instance,=,instance,",",database,=,database,),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,"Deletes a database from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be deleted in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Deletes,a,database,from,a,Cloud,SQL,instance,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L259-L279,test,[,"""name""",],self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlHook.export_instance,"def export_instance(self, instance, body, project_id=None):
        """"""
        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump
        or CSV file.

        :param instance: Database instance ID of the Cloud SQL instance. This does not include the
            project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        try:
            response = self.get_conn().instances().export(
                project=project_id,
                instance=instance,
                body=body
            ).execute(num_retries=self.num_retries)
            operation_name = response[""name""]
            self._wait_for_operation_to_complete(project_id=project_id,
                                                 operation_name=operation_name)
        except HttpError as ex:
            raise AirflowException(
                'Exporting instance {} failed: {}'.format(instance, ex.content)
            )",python,"def export_instance(self, instance, body, project_id=None):
        """"""
        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump
        or CSV file.

        :param instance: Database instance ID of the Cloud SQL instance. This does not include the
            project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        try:
            response = self.get_conn().instances().export(
                project=project_id,
                instance=instance,
                body=body
            ).execute(num_retries=self.num_retries)
            operation_name = response[""name""]
            self._wait_for_operation_to_complete(project_id=project_id,
                                                 operation_name=operation_name)
        except HttpError as ex:
            raise AirflowException(
                'Exporting instance {} failed: {}'.format(instance, ex.content)
            )",def,export_instance,(,self,",",instance,",",body,",",project_id,=,None,),:,try,:,response,=,self,.,get_conn,(,),.,instances,(,),.,export,(,project,=,project_id,",",instance,=,instance,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,"Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump
        or CSV file.

        :param instance: Database instance ID of the Cloud SQL instance. This does not include the
            project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Exports,data,from,a,Cloud,SQL,instance,to,a,Cloud,Storage,bucket,as,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L282-L310,test,=,response,[,"""name""",],self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),except,HttpError,as,ex,:,raise,AirflowException,(,'Exporting instance {} failed: {}',.,format,(,instance,",",ex,.,content,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,a,SQL,dump,or,CSV,file,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlProxyRunner.start_proxy,"def start_proxy(self):
        """"""
        Starts Cloud SQL Proxy.

        You have to remember to stop the proxy if you started it!
        """"""
        self._download_sql_proxy_if_needed()
        if self.sql_proxy_process:
            raise AirflowException(""The sql proxy is already running: {}"".format(
                self.sql_proxy_process))
        else:
            command_to_run = [self.sql_proxy_path]
            command_to_run.extend(self.command_line_parameters)
            try:
                self.log.info(""Creating directory %s"",
                              self.cloud_sql_proxy_socket_directory)
                os.makedirs(self.cloud_sql_proxy_socket_directory)
            except OSError:
                # Needed for python 2 compatibility (exists_ok missing)
                pass
            command_to_run.extend(self._get_credential_parameters())
            self.log.info(""Running the command: `%s`"", "" "".join(command_to_run))
            self.sql_proxy_process = Popen(command_to_run,
                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)
            self.log.info(""The pid of cloud_sql_proxy: %s"", self.sql_proxy_process.pid)
            while True:
                line = self.sql_proxy_process.stderr.readline().decode('utf-8')
                return_code = self.sql_proxy_process.poll()
                if line == '' and return_code is not None:
                    self.sql_proxy_process = None
                    raise AirflowException(
                        ""The cloud_sql_proxy finished early with return code {}!"".format(
                            return_code))
                if line != '':
                    self.log.info(line)
                if ""googleapi: Error"" in line or ""invalid instance name:"" in line:
                    self.stop_proxy()
                    raise AirflowException(
                        ""Error when starting the cloud_sql_proxy {}!"".format(
                            line))
                if ""Ready for new connections"" in line:
                    return",python,"def start_proxy(self):
        """"""
        Starts Cloud SQL Proxy.

        You have to remember to stop the proxy if you started it!
        """"""
        self._download_sql_proxy_if_needed()
        if self.sql_proxy_process:
            raise AirflowException(""The sql proxy is already running: {}"".format(
                self.sql_proxy_process))
        else:
            command_to_run = [self.sql_proxy_path]
            command_to_run.extend(self.command_line_parameters)
            try:
                self.log.info(""Creating directory %s"",
                              self.cloud_sql_proxy_socket_directory)
                os.makedirs(self.cloud_sql_proxy_socket_directory)
            except OSError:
                # Needed for python 2 compatibility (exists_ok missing)
                pass
            command_to_run.extend(self._get_credential_parameters())
            self.log.info(""Running the command: `%s`"", "" "".join(command_to_run))
            self.sql_proxy_process = Popen(command_to_run,
                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)
            self.log.info(""The pid of cloud_sql_proxy: %s"", self.sql_proxy_process.pid)
            while True:
                line = self.sql_proxy_process.stderr.readline().decode('utf-8')
                return_code = self.sql_proxy_process.poll()
                if line == '' and return_code is not None:
                    self.sql_proxy_process = None
                    raise AirflowException(
                        ""The cloud_sql_proxy finished early with return code {}!"".format(
                            return_code))
                if line != '':
                    self.log.info(line)
                if ""googleapi: Error"" in line or ""invalid instance name:"" in line:
                    self.stop_proxy()
                    raise AirflowException(
                        ""Error when starting the cloud_sql_proxy {}!"".format(
                            line))
                if ""Ready for new connections"" in line:
                    return",def,start_proxy,(,self,),:,self,.,_download_sql_proxy_if_needed,(,),if,self,.,sql_proxy_process,:,raise,AirflowException,(,"""The sql proxy is already running: {}""",.,format,(,self,.,sql_proxy_process,),),else,:,command_to_run,=,[,self,.,sql_proxy_path,],command_to_run,.,extend,(,self,.,command_line_parameters,),try,:,self,.,log,.,info,"Starts Cloud SQL Proxy.

        You have to remember to stop the proxy if you started it!",Starts,Cloud,SQL,Proxy,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L524-L565,test,(,"""Creating directory %s""",",",self,.,cloud_sql_proxy_socket_directory,),os,.,makedirs,(,self,.,cloud_sql_proxy_socket_directory,),except,OSError,:,# Needed for python 2 compatibility (exists_ok missing),pass,command_to_run,.,extend,(,self,.,_get_credential_parameters,(,),),self,.,log,.,info,(,"""Running the command: `%s`""",",",""" """,.,join,(,command_to_run,),),self,.,sql_proxy_process,=,Popen,(,command_to_run,",",stdin,=,PIPE,",",stdout,=,PIPE,",",stderr,=,PIPE,),self,.,log,.,info,(,"""The pid of cloud_sql_proxy: %s""",",",self,.,sql_proxy_process,.,pid,),while,True,:,line,=,self,.,sql_proxy_process,.,stderr,.,readline,(,),.,decode,(,'utf-8',),return_code,=,self,.,sql_proxy_process,.,poll,(,),if,line,==,'',and,return_code,is,not,None,:,self,.,sql_proxy_process,=,None,raise,AirflowException,(,"""The cloud_sql_proxy finished early with return code {}!""",.,format,(,return_code,),),if,line,!=,'',:,self,.,log,.,info,(,line,),if,"""googleapi: Error""",in,line,or,"""invalid instance name:""",in,line,:,self,.,stop_proxy,(,),raise,AirflowException,(,"""Error when starting the cloud_sql_proxy {}!""",.,format,(,line,),),if,"""Ready for new connections""",in,line,:,return,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlProxyRunner.stop_proxy,"def stop_proxy(self):
        """"""
        Stops running proxy.

        You should stop the proxy after you stop using it.
        """"""
        if not self.sql_proxy_process:
            raise AirflowException(""The sql proxy is not started yet"")
        else:
            self.log.info(""Stopping the cloud_sql_proxy pid: %s"",
                          self.sql_proxy_process.pid)
            self.sql_proxy_process.kill()
            self.sql_proxy_process = None
        # Cleanup!
        self.log.info(""Removing the socket directory: %s"",
                      self.cloud_sql_proxy_socket_directory)
        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)
        if self.sql_proxy_was_downloaded:
            self.log.info(""Removing downloaded proxy: %s"", self.sql_proxy_path)
            # Silently ignore if the file has already been removed (concurrency)
            try:
                os.remove(self.sql_proxy_path)
            except OSError as e:
                if not e.errno == errno.ENOENT:
                    raise
        else:
            self.log.info(""Skipped removing proxy - it was not downloaded: %s"",
                          self.sql_proxy_path)
        if os.path.isfile(self.credentials_path):
            self.log.info(""Removing generated credentials file %s"",
                          self.credentials_path)
            # Here file cannot be delete by concurrent task (each task has its own copy)
            os.remove(self.credentials_path)",python,"def stop_proxy(self):
        """"""
        Stops running proxy.

        You should stop the proxy after you stop using it.
        """"""
        if not self.sql_proxy_process:
            raise AirflowException(""The sql proxy is not started yet"")
        else:
            self.log.info(""Stopping the cloud_sql_proxy pid: %s"",
                          self.sql_proxy_process.pid)
            self.sql_proxy_process.kill()
            self.sql_proxy_process = None
        # Cleanup!
        self.log.info(""Removing the socket directory: %s"",
                      self.cloud_sql_proxy_socket_directory)
        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)
        if self.sql_proxy_was_downloaded:
            self.log.info(""Removing downloaded proxy: %s"", self.sql_proxy_path)
            # Silently ignore if the file has already been removed (concurrency)
            try:
                os.remove(self.sql_proxy_path)
            except OSError as e:
                if not e.errno == errno.ENOENT:
                    raise
        else:
            self.log.info(""Skipped removing proxy - it was not downloaded: %s"",
                          self.sql_proxy_path)
        if os.path.isfile(self.credentials_path):
            self.log.info(""Removing generated credentials file %s"",
                          self.credentials_path)
            # Here file cannot be delete by concurrent task (each task has its own copy)
            os.remove(self.credentials_path)",def,stop_proxy,(,self,),:,if,not,self,.,sql_proxy_process,:,raise,AirflowException,(,"""The sql proxy is not started yet""",),else,:,self,.,log,.,info,(,"""Stopping the cloud_sql_proxy pid: %s""",",",self,.,sql_proxy_process,.,pid,),self,.,sql_proxy_process,.,kill,(,),self,.,sql_proxy_process,=,None,# Cleanup!,self,.,log,.,info,(,"Stops running proxy.

        You should stop the proxy after you stop using it.",Stops,running,proxy,.,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L567-L599,test,"""Removing the socket directory: %s""",",",self,.,cloud_sql_proxy_socket_directory,),shutil,.,rmtree,(,self,.,cloud_sql_proxy_socket_directory,",",ignore_errors,=,True,),if,self,.,sql_proxy_was_downloaded,:,self,.,log,.,info,(,"""Removing downloaded proxy: %s""",",",self,.,sql_proxy_path,),# Silently ignore if the file has already been removed (concurrency),try,:,os,.,remove,(,self,.,sql_proxy_path,),except,OSError,as,e,:,if,not,e,.,errno,==,errno,.,ENOENT,:,raise,else,:,self,.,log,.,info,(,"""Skipped removing proxy - it was not downloaded: %s""",",",self,.,sql_proxy_path,),if,os,.,path,.,isfile,(,self,.,credentials_path,),:,self,.,log,.,info,(,"""Removing generated credentials file %s""",",",self,.,credentials_path,),# Here file cannot be delete by concurrent task (each task has its own copy),os,.,remove,(,self,.,credentials_path,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlProxyRunner.get_proxy_version,"def get_proxy_version(self):
        """"""
        Returns version of the Cloud SQL Proxy.
        """"""
        self._download_sql_proxy_if_needed()
        command_to_run = [self.sql_proxy_path]
        command_to_run.extend(['--version'])
        command_to_run.extend(self._get_credential_parameters())
        result = subprocess.check_output(command_to_run).decode('utf-8')
        pattern = re.compile(""^.*[V|v]ersion ([^;]*);.*$"")
        m = pattern.match(result)
        if m:
            return m.group(1)
        else:
            return None",python,"def get_proxy_version(self):
        """"""
        Returns version of the Cloud SQL Proxy.
        """"""
        self._download_sql_proxy_if_needed()
        command_to_run = [self.sql_proxy_path]
        command_to_run.extend(['--version'])
        command_to_run.extend(self._get_credential_parameters())
        result = subprocess.check_output(command_to_run).decode('utf-8')
        pattern = re.compile(""^.*[V|v]ersion ([^;]*);.*$"")
        m = pattern.match(result)
        if m:
            return m.group(1)
        else:
            return None",def,get_proxy_version,(,self,),:,self,.,_download_sql_proxy_if_needed,(,),command_to_run,=,[,self,.,sql_proxy_path,],command_to_run,.,extend,(,[,'--version',],),command_to_run,.,extend,(,self,.,_get_credential_parameters,(,),),result,=,subprocess,.,check_output,(,command_to_run,),.,decode,(,'utf-8',),pattern,=,re,Returns version of the Cloud SQL Proxy.,Returns,version,of,the,Cloud,SQL,Proxy,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L601-L615,test,.,compile,(,"""^.*[V|v]ersion ([^;]*);.*$""",),m,=,pattern,.,match,(,result,),if,m,:,return,m,.,group,(,1,),else,:,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.create_connection,"def create_connection(self, session=None):
        """"""
        Create connection in the Connection table, according to whether it uses
        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        connection = Connection(conn_id=self.db_conn_id)
        uri = self._generate_connection_uri()
        self.log.info(""Creating connection %s"", self.db_conn_id)
        connection.parse_from_uri(uri)
        session.add(connection)
        session.commit()",python,"def create_connection(self, session=None):
        """"""
        Create connection in the Connection table, according to whether it uses
        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        connection = Connection(conn_id=self.db_conn_id)
        uri = self._generate_connection_uri()
        self.log.info(""Creating connection %s"", self.db_conn_id)
        connection.parse_from_uri(uri)
        session.add(connection)
        session.commit()",def,create_connection,(,self,",",session,=,None,),:,connection,=,Connection,(,conn_id,=,self,.,db_conn_id,),uri,=,self,.,_generate_connection_uri,(,),self,.,log,.,info,(,"""Creating connection %s""",",",self,.,db_conn_id,),connection,.,parse_from_uri,(,uri,),session,.,add,(,connection,),session,"Create connection in the Connection table, according to whether it uses
        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).",Create,connection,in,the,Connection,table,according,to,whether,it,uses,proxy,TCP,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L895-L908,test,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,UNIX,sockets,SSL,.,Connection,ID,will,be,randomly,generated,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.retrieve_connection,"def retrieve_connection(self, session=None):
        """"""
        Retrieves the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        self.log.info(""Retrieving connection %s"", self.db_conn_id)
        connections = session.query(Connection).filter(
            Connection.conn_id == self.db_conn_id)
        if connections.count():
            return connections[0]
        return None",python,"def retrieve_connection(self, session=None):
        """"""
        Retrieves the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        self.log.info(""Retrieving connection %s"", self.db_conn_id)
        connections = session.query(Connection).filter(
            Connection.conn_id == self.db_conn_id)
        if connections.count():
            return connections[0]
        return None",def,retrieve_connection,(,self,",",session,=,None,),:,self,.,log,.,info,(,"""Retrieving connection %s""",",",self,.,db_conn_id,),connections,=,session,.,query,(,Connection,),.,filter,(,Connection,.,conn_id,==,self,.,db_conn_id,),if,connections,.,count,(,),:,return,connections,[,0,"Retrieves the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).",Retrieves,the,dynamically,created,connection,from,the,Connection,table,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923,test,],return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.delete_connection,"def delete_connection(self, session=None):
        """"""
        Delete the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        self.log.info(""Deleting connection %s"", self.db_conn_id)
        connections = session.query(Connection).filter(
            Connection.conn_id == self.db_conn_id)
        if connections.count():
            connection = connections[0]
            session.delete(connection)
            session.commit()
        else:
            self.log.info(""Connection was already deleted!"")",python,"def delete_connection(self, session=None):
        """"""
        Delete the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        self.log.info(""Deleting connection %s"", self.db_conn_id)
        connections = session.query(Connection).filter(
            Connection.conn_id == self.db_conn_id)
        if connections.count():
            connection = connections[0]
            session.delete(connection)
            session.commit()
        else:
            self.log.info(""Connection was already deleted!"")",def,delete_connection,(,self,",",session,=,None,),:,self,.,log,.,info,(,"""Deleting connection %s""",",",self,.,db_conn_id,),connections,=,session,.,query,(,Connection,),.,filter,(,Connection,.,conn_id,==,self,.,db_conn_id,),if,connections,.,count,(,),:,connection,=,connections,[,"Delete the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).",Delete,the,dynamically,created,connection,from,the,Connection,table,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L926-L941,test,0,],session,.,delete,(,connection,),session,.,commit,(,),else,:,self,.,log,.,info,(,"""Connection was already deleted!""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.get_sqlproxy_runner,"def get_sqlproxy_runner(self):
        """"""
        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy
        lifecycle per task.

        :return: The Cloud SQL Proxy runner.
        :rtype: CloudSqlProxyRunner
        """"""
        if not self.use_proxy:
            raise AirflowException(""Proxy runner can only be retrieved in case of use_proxy = True"")
        return CloudSqlProxyRunner(
            path_prefix=self.sql_proxy_unique_path,
            instance_specification=self._get_sqlproxy_instance_specification(),
            project_id=self.project_id,
            sql_proxy_version=self.sql_proxy_version,
            sql_proxy_binary_path=self.sql_proxy_binary_path
        )",python,"def get_sqlproxy_runner(self):
        """"""
        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy
        lifecycle per task.

        :return: The Cloud SQL Proxy runner.
        :rtype: CloudSqlProxyRunner
        """"""
        if not self.use_proxy:
            raise AirflowException(""Proxy runner can only be retrieved in case of use_proxy = True"")
        return CloudSqlProxyRunner(
            path_prefix=self.sql_proxy_unique_path,
            instance_specification=self._get_sqlproxy_instance_specification(),
            project_id=self.project_id,
            sql_proxy_version=self.sql_proxy_version,
            sql_proxy_binary_path=self.sql_proxy_binary_path
        )",def,get_sqlproxy_runner,(,self,),:,if,not,self,.,use_proxy,:,raise,AirflowException,(,"""Proxy runner can only be retrieved in case of use_proxy = True""",),return,CloudSqlProxyRunner,(,path_prefix,=,self,.,sql_proxy_unique_path,",",instance_specification,=,self,.,_get_sqlproxy_instance_specification,(,),",",project_id,=,self,.,project_id,",",sql_proxy_version,=,self,.,sql_proxy_version,",",sql_proxy_binary_path,=,self,.,sql_proxy_binary_path,),"Retrieve Cloud SQL Proxy runner. It is used to manage the proxy
        lifecycle per task.

        :return: The Cloud SQL Proxy runner.
        :rtype: CloudSqlProxyRunner",Retrieve,Cloud,SQL,Proxy,runner,.,It,is,used,to,manage,the,proxy,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L943-L959,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,lifecycle,per,task,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.get_database_hook,"def get_database_hook(self):
        """"""
        Retrieve database hook. This is the actual Postgres or MySQL database hook
        that uses proxy or connects directly to the Google Cloud SQL database.
        """"""
        if self.database_type == 'postgres':
            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,
                                        schema=self.database)
        else:
            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,
                                     schema=self.database)
        return self.db_hook",python,"def get_database_hook(self):
        """"""
        Retrieve database hook. This is the actual Postgres or MySQL database hook
        that uses proxy or connects directly to the Google Cloud SQL database.
        """"""
        if self.database_type == 'postgres':
            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,
                                        schema=self.database)
        else:
            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,
                                     schema=self.database)
        return self.db_hook",def,get_database_hook,(,self,),:,if,self,.,database_type,==,'postgres',:,self,.,db_hook,=,PostgresHook,(,postgres_conn_id,=,self,.,db_conn_id,",",schema,=,self,.,database,),else,:,self,.,db_hook,=,MySqlHook,(,mysql_conn_id,=,self,.,db_conn_id,",",schema,=,self,.,database,),return,"Retrieve database hook. This is the actual Postgres or MySQL database hook
        that uses proxy or connects directly to the Google Cloud SQL database.",Retrieve,database,hook,.,This,is,the,actual,Postgres,or,MySQL,database,hook,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L961-L972,test,self,.,db_hook,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,that,uses,proxy,or,connects,directly,to,the,Google,Cloud,SQL,database,.,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.cleanup_database_hook,"def cleanup_database_hook(self):
        """"""
        Clean up database hook after it was used.
        """"""
        if self.database_type == 'postgres':
            if hasattr(self.db_hook,
                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:
                for output in self.db_hook.conn.notices:
                    self.log.info(output)",python,"def cleanup_database_hook(self):
        """"""
        Clean up database hook after it was used.
        """"""
        if self.database_type == 'postgres':
            if hasattr(self.db_hook,
                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:
                for output in self.db_hook.conn.notices:
                    self.log.info(output)",def,cleanup_database_hook,(,self,),:,if,self,.,database_type,==,'postgres',:,if,hasattr,(,self,.,db_hook,",",'conn',),and,self,.,db_hook,.,conn,and,self,.,db_hook,.,conn,.,notices,:,for,output,in,self,.,db_hook,.,conn,.,notices,:,self,.,log,.,Clean up database hook after it was used.,Clean,up,database,hook,after,it,was,used,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L974-L982,test,info,(,output,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,CloudSqlDatabaseHook.reserve_free_tcp_port,"def reserve_free_tcp_port(self):
        """"""
        Reserve free TCP port to be used by Cloud SQL Proxy
        """"""
        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.reserved_tcp_socket.bind(('127.0.0.1', 0))
        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",python,"def reserve_free_tcp_port(self):
        """"""
        Reserve free TCP port to be used by Cloud SQL Proxy
        """"""
        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.reserved_tcp_socket.bind(('127.0.0.1', 0))
        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",def,reserve_free_tcp_port,(,self,),:,self,.,reserved_tcp_socket,=,socket,.,socket,(,socket,.,AF_INET,",",socket,.,SOCK_STREAM,),self,.,reserved_tcp_socket,.,bind,(,(,'127.0.0.1',",",0,),),self,.,sql_proxy_tcp_port,=,self,.,reserved_tcp_socket,.,getsockname,(,),[,1,],,,,,Reserve free TCP port to be used by Cloud SQL Proxy,Reserve,free,TCP,port,to,be,used,by,Cloud,SQL,Proxy,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mlengine_operator.py,_normalize_mlengine_job_id,"def _normalize_mlengine_job_id(job_id):
    """"""
    Replaces invalid MLEngine job_id characters with '_'.

    This also adds a leading 'z' in case job_id starts with an invalid
    character.

    Args:
        job_id: A job_id str that may have invalid characters.

    Returns:
        A valid job_id representation.
    """"""

    # Add a prefix when a job_id starts with a digit or a template
    match = re.search(r'\d|\{{2}', job_id)
    if match and match.start() == 0:
        job = 'z_{}'.format(job_id)
    else:
        job = job_id

    # Clean up 'bad' characters except templates
    tracker = 0
    cleansed_job_id = ''
    for m in re.finditer(r'\{{2}.+?\}{2}', job):
        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',
                                  job[tracker:m.start()])
        cleansed_job_id += job[m.start():m.end()]
        tracker = m.end()

    # Clean up last substring or the full string if no templates
    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])

    return cleansed_job_id",python,"def _normalize_mlengine_job_id(job_id):
    """"""
    Replaces invalid MLEngine job_id characters with '_'.

    This also adds a leading 'z' in case job_id starts with an invalid
    character.

    Args:
        job_id: A job_id str that may have invalid characters.

    Returns:
        A valid job_id representation.
    """"""

    # Add a prefix when a job_id starts with a digit or a template
    match = re.search(r'\d|\{{2}', job_id)
    if match and match.start() == 0:
        job = 'z_{}'.format(job_id)
    else:
        job = job_id

    # Clean up 'bad' characters except templates
    tracker = 0
    cleansed_job_id = ''
    for m in re.finditer(r'\{{2}.+?\}{2}', job):
        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',
                                  job[tracker:m.start()])
        cleansed_job_id += job[m.start():m.end()]
        tracker = m.end()

    # Clean up last substring or the full string if no templates
    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])

    return cleansed_job_id",def,_normalize_mlengine_job_id,(,job_id,),:,# Add a prefix when a job_id starts with a digit or a template,match,=,re,.,search,(,r'\d|\{{2}',",",job_id,),if,match,and,match,.,start,(,),==,0,:,job,=,'z_{}',.,format,(,job_id,),else,:,job,=,job_id,# Clean up 'bad' characters except templates,tracker,=,0,cleansed_job_id,=,'',for,m,in,re,"Replaces invalid MLEngine job_id characters with '_'.

    This also adds a leading 'z' in case job_id starts with an invalid
    character.

    Args:
        job_id: A job_id str that may have invalid characters.

    Returns:
        A valid job_id representation.",Replaces,invalid,MLEngine,job_id,characters,with,_,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mlengine_operator.py#L29-L62,test,.,finditer,(,r'\{{2}.+?\}{2}',",",job,),:,cleansed_job_id,+=,re,.,sub,(,r'[^0-9a-zA-Z]+',",",'_',",",job,[,tracker,:,m,.,start,(,),],),cleansed_job_id,+=,job,[,m,.,start,(,),:,m,.,end,(,),],tracker,=,m,.,end,(,),# Clean up last substring or the full string if no templates,cleansed_job_id,+=,re,.,sub,(,r'[^0-9a-zA-Z]+',",",'_',",",job,[,tracker,:,],),return,cleansed_job_id,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/ftp_sensor.py,FTPSensor._get_error_code,"def _get_error_code(self, e):
        """"""Extract error code from ftp exception""""""
        try:
            matches = self.error_code_pattern.match(str(e))
            code = int(matches.group(0))
            return code
        except ValueError:
            return e",python,"def _get_error_code(self, e):
        """"""Extract error code from ftp exception""""""
        try:
            matches = self.error_code_pattern.match(str(e))
            code = int(matches.group(0))
            return code
        except ValueError:
            return e",def,_get_error_code,(,self,",",e,),:,try,:,matches,=,self,.,error_code_pattern,.,match,(,str,(,e,),),code,=,int,(,matches,.,group,(,0,),),return,code,except,ValueError,:,return,e,,,,,,,,,,,,Extract error code from ftp exception,Extract,error,code,from,ftp,exception,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/ftp_sensor.py#L69-L76,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,scripts/perf/scheduler_ops_metrics.py,clear_dag_runs,"def clear_dag_runs():
    """"""
    Remove any existing DAG runs for the perf test DAGs.
    """"""
    session = settings.Session()
    drs = session.query(DagRun).filter(
        DagRun.dag_id.in_(DAG_IDS),
    ).all()
    for dr in drs:
        logging.info('Deleting DagRun :: {}'.format(dr))
        session.delete(dr)",python,"def clear_dag_runs():
    """"""
    Remove any existing DAG runs for the perf test DAGs.
    """"""
    session = settings.Session()
    drs = session.query(DagRun).filter(
        DagRun.dag_id.in_(DAG_IDS),
    ).all()
    for dr in drs:
        logging.info('Deleting DagRun :: {}'.format(dr))
        session.delete(dr)",def,clear_dag_runs,(,),:,session,=,settings,.,Session,(,),drs,=,session,.,query,(,DagRun,),.,filter,(,DagRun,.,dag_id,.,in_,(,DAG_IDS,),",",),.,all,(,),for,dr,in,drs,:,logging,.,info,(,'Deleting DagRun :: {}',.,format,(,dr,),Remove any existing DAG runs for the perf test DAGs.,Remove,any,existing,DAG,runs,for,the,perf,test,DAGs,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L138-L148,test,),session,.,delete,(,dr,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,scripts/perf/scheduler_ops_metrics.py,clear_dag_task_instances,"def clear_dag_task_instances():
    """"""
    Remove any existing task instances for the perf test DAGs.
    """"""
    session = settings.Session()
    TI = TaskInstance
    tis = (
        session
        .query(TI)
        .filter(TI.dag_id.in_(DAG_IDS))
        .all()
    )
    for ti in tis:
        logging.info('Deleting TaskInstance :: {}'.format(ti))
        session.delete(ti)
    session.commit()",python,"def clear_dag_task_instances():
    """"""
    Remove any existing task instances for the perf test DAGs.
    """"""
    session = settings.Session()
    TI = TaskInstance
    tis = (
        session
        .query(TI)
        .filter(TI.dag_id.in_(DAG_IDS))
        .all()
    )
    for ti in tis:
        logging.info('Deleting TaskInstance :: {}'.format(ti))
        session.delete(ti)
    session.commit()",def,clear_dag_task_instances,(,),:,session,=,settings,.,Session,(,),TI,=,TaskInstance,tis,=,(,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,.,in_,(,DAG_IDS,),),.,all,(,),),for,ti,in,tis,:,logging,.,info,(,'Deleting TaskInstance :: {}',.,Remove any existing task instances for the perf test DAGs.,Remove,any,existing,task,instances,for,the,perf,test,DAGs,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L151-L166,test,format,(,ti,),),session,.,delete,(,ti,),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,scripts/perf/scheduler_ops_metrics.py,set_dags_paused_state,"def set_dags_paused_state(is_paused):
    """"""
    Toggle the pause state of the DAGs in the test.
    """"""
    session = settings.Session()
    dms = session.query(DagModel).filter(
        DagModel.dag_id.in_(DAG_IDS))
    for dm in dms:
        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))
        dm.is_paused = is_paused
    session.commit()",python,"def set_dags_paused_state(is_paused):
    """"""
    Toggle the pause state of the DAGs in the test.
    """"""
    session = settings.Session()
    dms = session.query(DagModel).filter(
        DagModel.dag_id.in_(DAG_IDS))
    for dm in dms:
        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))
        dm.is_paused = is_paused
    session.commit()",def,set_dags_paused_state,(,is_paused,),:,session,=,settings,.,Session,(,),dms,=,session,.,query,(,DagModel,),.,filter,(,DagModel,.,dag_id,.,in_,(,DAG_IDS,),),for,dm,in,dms,:,logging,.,info,(,'Setting DAG :: {} is_paused={}',.,format,(,dm,",",is_paused,),),dm,Toggle the pause state of the DAGs in the test.,Toggle,the,pause,state,of,the,DAGs,in,the,test,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L169-L179,test,.,is_paused,=,is_paused,session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,scripts/perf/scheduler_ops_metrics.py,SchedulerMetricsJob.print_stats,"def print_stats(self):
        """"""
        Print operational metrics for the scheduler test.
        """"""
        session = settings.Session()
        TI = TaskInstance
        tis = (
            session
            .query(TI)
            .filter(TI.dag_id.in_(DAG_IDS))
            .all()
        )
        successful_tis = [x for x in tis if x.state == State.SUCCESS]
        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,
                    (ti.queued_dttm - self.start_date).total_seconds(),
                    (ti.start_date - self.start_date).total_seconds(),
                    (ti.end_date - self.start_date).total_seconds(),
                    ti.duration) for ti in successful_tis]
        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',
                                                    'execution_date',
                                                    'queue_delay',
                                                    'start_delay', 'land_time',
                                                    'duration'])

        print('Performance Results')
        print('###################')
        for dag_id in DAG_IDS:
            print('DAG {}'.format(dag_id))
            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])
        print('###################')
        if len(tis) > len(successful_tis):
            print(""WARNING!! The following task instances haven't completed"")
            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)
                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],
                  columns=['dag_id', 'task_id', 'execution_date', 'state']))

        session.commit()",python,"def print_stats(self):
        """"""
        Print operational metrics for the scheduler test.
        """"""
        session = settings.Session()
        TI = TaskInstance
        tis = (
            session
            .query(TI)
            .filter(TI.dag_id.in_(DAG_IDS))
            .all()
        )
        successful_tis = [x for x in tis if x.state == State.SUCCESS]
        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,
                    (ti.queued_dttm - self.start_date).total_seconds(),
                    (ti.start_date - self.start_date).total_seconds(),
                    (ti.end_date - self.start_date).total_seconds(),
                    ti.duration) for ti in successful_tis]
        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',
                                                    'execution_date',
                                                    'queue_delay',
                                                    'start_delay', 'land_time',
                                                    'duration'])

        print('Performance Results')
        print('###################')
        for dag_id in DAG_IDS:
            print('DAG {}'.format(dag_id))
            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])
        print('###################')
        if len(tis) > len(successful_tis):
            print(""WARNING!! The following task instances haven't completed"")
            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)
                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],
                  columns=['dag_id', 'task_id', 'execution_date', 'state']))

        session.commit()",def,print_stats,(,self,),:,session,=,settings,.,Session,(,),TI,=,TaskInstance,tis,=,(,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,.,in_,(,DAG_IDS,),),.,all,(,),),successful_tis,=,[,x,for,x,in,tis,if,x,Print operational metrics for the scheduler test.,Print,operational,metrics,for,the,scheduler,test,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L65-L101,test,.,state,==,State,.,SUCCESS,],ti_perf,=,[,(,ti,.,dag_id,",",ti,.,task_id,",",ti,.,execution_date,",",(,ti,.,queued_dttm,-,self,.,start_date,),.,total_seconds,(,),",",(,ti,.,start_date,-,self,.,start_date,),.,total_seconds,(,),",",(,ti,.,end_date,-,self,.,start_date,),.,total_seconds,(,),",",ti,.,duration,),for,ti,in,successful_tis,],ti_perf_df,=,pd,.,DataFrame,(,ti_perf,",",columns,=,[,'dag_id',",",'task_id',",",'execution_date',",",'queue_delay',",",'start_delay',",",'land_time',",",'duration',],),print,(,'Performance Results',),print,(,'###################',),for,dag_id,in,DAG_IDS,:,print,(,'DAG {}',.,format,(,dag_id,),),print,(,ti_perf_df,[,ti_perf_df,[,'dag_id',],==,dag_id,],),print,(,'###################',),if,len,(,tis,),>,len,(,successful_tis,),:,print,(,"""WARNING!! The following task instances haven't completed""",),print,(,pd,.,DataFrame,(,[,(,ti,.,dag_id,",",ti,.,task_id,",",ti,.,execution_date,",",ti,.,state,),for,ti,in,filter,(,lambda,x,:,x,.,state,!=,State,.,SUCCESS,",",tis,),],",",columns,=,[,'dag_id',",",'task_id',",",'execution_date',",",'state',],),),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,scripts/perf/scheduler_ops_metrics.py,SchedulerMetricsJob.heartbeat,"def heartbeat(self):
        """"""
        Override the scheduler heartbeat to determine when the test is complete
        """"""
        super(SchedulerMetricsJob, self).heartbeat()
        session = settings.Session()
        # Get all the relevant task instances
        TI = TaskInstance
        successful_tis = (
            session
            .query(TI)
            .filter(TI.dag_id.in_(DAG_IDS))
            .filter(TI.state.in_([State.SUCCESS]))
            .all()
        )
        session.commit()

        dagbag = DagBag(SUBDIR)
        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]
        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.
        num_task_instances = sum([(timezone.utcnow() - task.start_date).days
                                 for dag in dags for task in dag.tasks])

        if (len(successful_tis) == num_task_instances or
                (timezone.utcnow() - self.start_date).total_seconds() >
                MAX_RUNTIME_SECS):
            if len(successful_tis) == num_task_instances:
                self.log.info(""All tasks processed! Printing stats."")
            else:
                self.log.info(""Test timeout reached. Printing available stats."")
            self.print_stats()
            set_dags_paused_state(True)
            sys.exit()",python,"def heartbeat(self):
        """"""
        Override the scheduler heartbeat to determine when the test is complete
        """"""
        super(SchedulerMetricsJob, self).heartbeat()
        session = settings.Session()
        # Get all the relevant task instances
        TI = TaskInstance
        successful_tis = (
            session
            .query(TI)
            .filter(TI.dag_id.in_(DAG_IDS))
            .filter(TI.state.in_([State.SUCCESS]))
            .all()
        )
        session.commit()

        dagbag = DagBag(SUBDIR)
        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]
        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.
        num_task_instances = sum([(timezone.utcnow() - task.start_date).days
                                 for dag in dags for task in dag.tasks])

        if (len(successful_tis) == num_task_instances or
                (timezone.utcnow() - self.start_date).total_seconds() >
                MAX_RUNTIME_SECS):
            if len(successful_tis) == num_task_instances:
                self.log.info(""All tasks processed! Printing stats."")
            else:
                self.log.info(""Test timeout reached. Printing available stats."")
            self.print_stats()
            set_dags_paused_state(True)
            sys.exit()",def,heartbeat,(,self,),:,super,(,SchedulerMetricsJob,",",self,),.,heartbeat,(,),session,=,settings,.,Session,(,),# Get all the relevant task instances,TI,=,TaskInstance,successful_tis,=,(,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,.,in_,(,DAG_IDS,),),.,filter,(,TI,Override the scheduler heartbeat to determine when the test is complete,Override,the,scheduler,heartbeat,to,determine,when,the,test,is,complete,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L103-L135,test,.,state,.,in_,(,[,State,.,SUCCESS,],),),.,all,(,),),session,.,commit,(,),dagbag,=,DagBag,(,SUBDIR,),dags,=,[,dagbag,.,dags,[,dag_id,],for,dag_id,in,DAG_IDS,],# the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.,num_task_instances,=,sum,(,[,(,timezone,.,utcnow,(,),-,task,.,start_date,),.,days,for,dag,in,dags,for,task,in,dag,.,tasks,],),if,(,len,(,successful_tis,),==,num_task_instances,or,(,timezone,.,utcnow,(,),-,self,.,start_date,),.,total_seconds,(,),>,MAX_RUNTIME_SECS,),:,if,len,(,successful_tis,),==,num_task_instances,:,self,.,log,.,info,(,"""All tasks processed! Printing stats.""",),else,:,self,.,log,.,info,(,"""Test timeout reached. Printing available stats.""",),self,.,print_stats,(,),set_dags_paused_state,(,True,),sys,.,exit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_lambda_hook.py,AwsLambdaHook.invoke_lambda,"def invoke_lambda(self, payload):
        """"""
        Invoke Lambda Function
        """"""

        awslambda_conn = self.get_conn()

        response = awslambda_conn.invoke(
            FunctionName=self.function_name,
            InvocationType=self.invocation_type,
            LogType=self.log_type,
            Payload=payload,
            Qualifier=self.qualifier
        )

        return response",python,"def invoke_lambda(self, payload):
        """"""
        Invoke Lambda Function
        """"""

        awslambda_conn = self.get_conn()

        response = awslambda_conn.invoke(
            FunctionName=self.function_name,
            InvocationType=self.invocation_type,
            LogType=self.log_type,
            Payload=payload,
            Qualifier=self.qualifier
        )

        return response",def,invoke_lambda,(,self,",",payload,),:,awslambda_conn,=,self,.,get_conn,(,),response,=,awslambda_conn,.,invoke,(,FunctionName,=,self,.,function_name,",",InvocationType,=,self,.,invocation_type,",",LogType,=,self,.,log_type,",",Payload,=,payload,",",Qualifier,=,self,.,qualifier,),return,response,,Invoke Lambda Function,Invoke,Lambda,Function,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_lambda_hook.py#L53-L68,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/utils/mlengine_operator_utils.py,create_evaluate_ops,"def create_evaluate_ops(task_prefix,
                        data_format,
                        input_paths,
                        prediction_path,
                        metric_fn_and_keys,
                        validate_fn,
                        batch_prediction_job_id=None,
                        project_id=None,
                        region=None,
                        dataflow_options=None,
                        model_uri=None,
                        model_name=None,
                        version_name=None,
                        dag=None):
    """"""
    Creates Operators needed for model evaluation and returns.

    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by
    calling MLEngineBatchPredictionOperator, then summarize and validate
    the result via Cloud Dataflow using DataFlowPythonOperator.

    For details and pricing about Batch prediction, please refer to the website
    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict
    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/

    It returns three chained operators for prediction, summary, and validation,
    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,
    respectively.
    (<prefix> should contain only alphanumeric characters or hyphen.)

    The upstream and downstream can be set accordingly like:
      pred, _, val = create_evaluate_ops(...)
      pred.set_upstream(upstream_op)
      ...
      downstream_op.set_upstream(val)

    Callers will provide two python callables, metric_fn and validate_fn, in
    order to customize the evaluation behavior as they wish.
    - metric_fn receives a dictionary per instance derived from json in the
      batch prediction result. The keys might vary depending on the model.
      It should return a tuple of metrics.
    - validation_fn receives a dictionary of the averaged metrics that metric_fn
      generated over all instances.
      The key/value of the dictionary matches to what's given by
      metric_fn_and_keys arg.
      The dictionary contains an additional metric, 'count' to represent the
      total number of instances received for evaluation.
      The function would raise an exception to mark the task as failed, in a
      case the validation result is not okay to proceed (i.e. to set the trained
      version as default).

    Typical examples are like this:

    def get_metric_fn_and_keys():
        import math  # imports should be outside of the metric_fn below.
        def error_and_squared_error(inst):
            label = float(inst['input_label'])
            classes = float(inst['classes'])  # 0 or 1
            err = abs(classes-label)
            squared_err = math.pow(classes-label, 2)
            return (err, squared_err)  # returns a tuple.
        return error_and_squared_error, ['err', 'mse']  # key order must match.

    def validate_err_and_count(summary):
        if summary['err'] > 0.2:
            raise ValueError('Too high err>0.2; summary=%s' % summary)
        if summary['mse'] > 0.05:
            raise ValueError('Too high mse>0.05; summary=%s' % summary)
        if summary['count'] < 1000:
            raise ValueError('Too few instances<1000; summary=%s' % summary)
        return summary

    For the details on the other BatchPrediction-related arguments (project_id,
    job_id, region, data_format, input_paths, prediction_path, model_uri),
    please refer to MLEngineBatchPredictionOperator too.

    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and
        hyphen are allowed (no underscores), since this will be used as dataflow
        job name, which doesn't allow other characters.
    :type task_prefix: str

    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'
    :type data_format: str

    :param input_paths: a list of input paths to be sent to BatchPrediction.
    :type input_paths: list[str]

    :param prediction_path: GCS path to put the prediction results in.
    :type prediction_path: str

    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:
        - metric_fn is a function that accepts a dictionary (for an instance),
          and returns a tuple of metric(s) that it calculates.
        - metric_keys is a list of strings to denote the key of each metric.
    :type metric_fn_and_keys: tuple of a function and a list[str]

    :param validate_fn: a function to validate whether the averaged metric(s) is
        good enough to push the model.
    :type validate_fn: function

    :param batch_prediction_job_id: the id to use for the Cloud ML Batch
        prediction job. Passed directly to the MLEngineBatchPredictionOperator as
        the job_id argument.
    :type batch_prediction_job_id: str

    :param project_id: the Google Cloud Platform project id in which to execute
        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['project_id']` will be used.
    :type project_id: str

    :param region: the Google Cloud Platform region in which to execute Cloud ML
        Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['region']` will be used.
    :type region: str

    :param dataflow_options: options to run Dataflow jobs. If None, then the
        `dag`'s `default_args['dataflow_default_options']` will be used.
    :type dataflow_options: dictionary

    :param model_uri: GCS path of the model exported by Tensorflow using
        tensorflow.estimator.export_savedmodel(). It cannot be used with
        model_name or version_name below. See MLEngineBatchPredictionOperator for
        more detail.
    :type model_uri: str

    :param model_name: Used to indicate a model to use for prediction. Can be
        used in combination with version_name, but cannot be used together with
        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,
        then the `dag`'s `default_args['model_name']` will be used.
    :type model_name: str

    :param version_name: Used to indicate a model version to use for prediction,
        in combination with model_name. Cannot be used together with model_uri.
        See MLEngineBatchPredictionOperator for more detail. If None, then the
        `dag`'s `default_args['version_name']` will be used.
    :type version_name: str

    :param dag: The `DAG` to use for all Operators.
    :type dag: airflow.models.DAG

    :returns: a tuple of three operators, (prediction, summary, validation)
    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,
                  PythonOperator)
    """"""

    # Verify that task_prefix doesn't have any special characters except hyphen
    # '-', which is the only allowed non-alphanumeric character by Dataflow.
    if not re.match(r""^[a-zA-Z][-A-Za-z0-9]*$"", task_prefix):
        raise AirflowException(
            ""Malformed task_id for DataFlowPythonOperator (only alphanumeric ""
            ""and hyphens are allowed but got: "" + task_prefix)

    metric_fn, metric_keys = metric_fn_and_keys
    if not callable(metric_fn):
        raise AirflowException(""`metric_fn` param must be callable."")
    if not callable(validate_fn):
        raise AirflowException(""`validate_fn` param must be callable."")

    if dag is not None and dag.default_args is not None:
        default_args = dag.default_args
        project_id = project_id or default_args.get('project_id')
        region = region or default_args.get('region')
        model_name = model_name or default_args.get('model_name')
        version_name = version_name or default_args.get('version_name')
        dataflow_options = dataflow_options or \
            default_args.get('dataflow_default_options')

    evaluate_prediction = MLEngineBatchPredictionOperator(
        task_id=(task_prefix + ""-prediction""),
        project_id=project_id,
        job_id=batch_prediction_job_id,
        region=region,
        data_format=data_format,
        input_paths=input_paths,
        output_path=prediction_path,
        uri=model_uri,
        model_name=model_name,
        version_name=version_name,
        dag=dag)

    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))
    evaluate_summary = DataFlowPythonOperator(
        task_id=(task_prefix + ""-summary""),
        py_options=[""-m""],
        py_file=""airflow.contrib.utils.mlengine_prediction_summary"",
        dataflow_default_options=dataflow_options,
        options={
            ""prediction_path"": prediction_path,
            ""metric_fn_encoded"": metric_fn_encoded,
            ""metric_keys"": ','.join(metric_keys)
        },
        dag=dag)
    evaluate_summary.set_upstream(evaluate_prediction)

    def apply_validate_fn(*args, **kwargs):
        prediction_path = kwargs[""templates_dict""][""prediction_path""]
        scheme, bucket, obj, _, _ = urlsplit(prediction_path)
        if scheme != ""gs"" or not bucket or not obj:
            raise ValueError(""Wrong format prediction_path: %s"",
                             prediction_path)
        summary = os.path.join(obj.strip(""/""),
                               ""prediction.summary.json"")
        gcs_hook = GoogleCloudStorageHook()
        summary = json.loads(gcs_hook.download(bucket, summary))
        return validate_fn(summary)

    evaluate_validation = PythonOperator(
        task_id=(task_prefix + ""-validation""),
        python_callable=apply_validate_fn,
        provide_context=True,
        templates_dict={""prediction_path"": prediction_path},
        dag=dag)
    evaluate_validation.set_upstream(evaluate_summary)

    return evaluate_prediction, evaluate_summary, evaluate_validation",python,"def create_evaluate_ops(task_prefix,
                        data_format,
                        input_paths,
                        prediction_path,
                        metric_fn_and_keys,
                        validate_fn,
                        batch_prediction_job_id=None,
                        project_id=None,
                        region=None,
                        dataflow_options=None,
                        model_uri=None,
                        model_name=None,
                        version_name=None,
                        dag=None):
    """"""
    Creates Operators needed for model evaluation and returns.

    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by
    calling MLEngineBatchPredictionOperator, then summarize and validate
    the result via Cloud Dataflow using DataFlowPythonOperator.

    For details and pricing about Batch prediction, please refer to the website
    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict
    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/

    It returns three chained operators for prediction, summary, and validation,
    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,
    respectively.
    (<prefix> should contain only alphanumeric characters or hyphen.)

    The upstream and downstream can be set accordingly like:
      pred, _, val = create_evaluate_ops(...)
      pred.set_upstream(upstream_op)
      ...
      downstream_op.set_upstream(val)

    Callers will provide two python callables, metric_fn and validate_fn, in
    order to customize the evaluation behavior as they wish.
    - metric_fn receives a dictionary per instance derived from json in the
      batch prediction result. The keys might vary depending on the model.
      It should return a tuple of metrics.
    - validation_fn receives a dictionary of the averaged metrics that metric_fn
      generated over all instances.
      The key/value of the dictionary matches to what's given by
      metric_fn_and_keys arg.
      The dictionary contains an additional metric, 'count' to represent the
      total number of instances received for evaluation.
      The function would raise an exception to mark the task as failed, in a
      case the validation result is not okay to proceed (i.e. to set the trained
      version as default).

    Typical examples are like this:

    def get_metric_fn_and_keys():
        import math  # imports should be outside of the metric_fn below.
        def error_and_squared_error(inst):
            label = float(inst['input_label'])
            classes = float(inst['classes'])  # 0 or 1
            err = abs(classes-label)
            squared_err = math.pow(classes-label, 2)
            return (err, squared_err)  # returns a tuple.
        return error_and_squared_error, ['err', 'mse']  # key order must match.

    def validate_err_and_count(summary):
        if summary['err'] > 0.2:
            raise ValueError('Too high err>0.2; summary=%s' % summary)
        if summary['mse'] > 0.05:
            raise ValueError('Too high mse>0.05; summary=%s' % summary)
        if summary['count'] < 1000:
            raise ValueError('Too few instances<1000; summary=%s' % summary)
        return summary

    For the details on the other BatchPrediction-related arguments (project_id,
    job_id, region, data_format, input_paths, prediction_path, model_uri),
    please refer to MLEngineBatchPredictionOperator too.

    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and
        hyphen are allowed (no underscores), since this will be used as dataflow
        job name, which doesn't allow other characters.
    :type task_prefix: str

    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'
    :type data_format: str

    :param input_paths: a list of input paths to be sent to BatchPrediction.
    :type input_paths: list[str]

    :param prediction_path: GCS path to put the prediction results in.
    :type prediction_path: str

    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:
        - metric_fn is a function that accepts a dictionary (for an instance),
          and returns a tuple of metric(s) that it calculates.
        - metric_keys is a list of strings to denote the key of each metric.
    :type metric_fn_and_keys: tuple of a function and a list[str]

    :param validate_fn: a function to validate whether the averaged metric(s) is
        good enough to push the model.
    :type validate_fn: function

    :param batch_prediction_job_id: the id to use for the Cloud ML Batch
        prediction job. Passed directly to the MLEngineBatchPredictionOperator as
        the job_id argument.
    :type batch_prediction_job_id: str

    :param project_id: the Google Cloud Platform project id in which to execute
        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['project_id']` will be used.
    :type project_id: str

    :param region: the Google Cloud Platform region in which to execute Cloud ML
        Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['region']` will be used.
    :type region: str

    :param dataflow_options: options to run Dataflow jobs. If None, then the
        `dag`'s `default_args['dataflow_default_options']` will be used.
    :type dataflow_options: dictionary

    :param model_uri: GCS path of the model exported by Tensorflow using
        tensorflow.estimator.export_savedmodel(). It cannot be used with
        model_name or version_name below. See MLEngineBatchPredictionOperator for
        more detail.
    :type model_uri: str

    :param model_name: Used to indicate a model to use for prediction. Can be
        used in combination with version_name, but cannot be used together with
        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,
        then the `dag`'s `default_args['model_name']` will be used.
    :type model_name: str

    :param version_name: Used to indicate a model version to use for prediction,
        in combination with model_name. Cannot be used together with model_uri.
        See MLEngineBatchPredictionOperator for more detail. If None, then the
        `dag`'s `default_args['version_name']` will be used.
    :type version_name: str

    :param dag: The `DAG` to use for all Operators.
    :type dag: airflow.models.DAG

    :returns: a tuple of three operators, (prediction, summary, validation)
    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,
                  PythonOperator)
    """"""

    # Verify that task_prefix doesn't have any special characters except hyphen
    # '-', which is the only allowed non-alphanumeric character by Dataflow.
    if not re.match(r""^[a-zA-Z][-A-Za-z0-9]*$"", task_prefix):
        raise AirflowException(
            ""Malformed task_id for DataFlowPythonOperator (only alphanumeric ""
            ""and hyphens are allowed but got: "" + task_prefix)

    metric_fn, metric_keys = metric_fn_and_keys
    if not callable(metric_fn):
        raise AirflowException(""`metric_fn` param must be callable."")
    if not callable(validate_fn):
        raise AirflowException(""`validate_fn` param must be callable."")

    if dag is not None and dag.default_args is not None:
        default_args = dag.default_args
        project_id = project_id or default_args.get('project_id')
        region = region or default_args.get('region')
        model_name = model_name or default_args.get('model_name')
        version_name = version_name or default_args.get('version_name')
        dataflow_options = dataflow_options or \
            default_args.get('dataflow_default_options')

    evaluate_prediction = MLEngineBatchPredictionOperator(
        task_id=(task_prefix + ""-prediction""),
        project_id=project_id,
        job_id=batch_prediction_job_id,
        region=region,
        data_format=data_format,
        input_paths=input_paths,
        output_path=prediction_path,
        uri=model_uri,
        model_name=model_name,
        version_name=version_name,
        dag=dag)

    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))
    evaluate_summary = DataFlowPythonOperator(
        task_id=(task_prefix + ""-summary""),
        py_options=[""-m""],
        py_file=""airflow.contrib.utils.mlengine_prediction_summary"",
        dataflow_default_options=dataflow_options,
        options={
            ""prediction_path"": prediction_path,
            ""metric_fn_encoded"": metric_fn_encoded,
            ""metric_keys"": ','.join(metric_keys)
        },
        dag=dag)
    evaluate_summary.set_upstream(evaluate_prediction)

    def apply_validate_fn(*args, **kwargs):
        prediction_path = kwargs[""templates_dict""][""prediction_path""]
        scheme, bucket, obj, _, _ = urlsplit(prediction_path)
        if scheme != ""gs"" or not bucket or not obj:
            raise ValueError(""Wrong format prediction_path: %s"",
                             prediction_path)
        summary = os.path.join(obj.strip(""/""),
                               ""prediction.summary.json"")
        gcs_hook = GoogleCloudStorageHook()
        summary = json.loads(gcs_hook.download(bucket, summary))
        return validate_fn(summary)

    evaluate_validation = PythonOperator(
        task_id=(task_prefix + ""-validation""),
        python_callable=apply_validate_fn,
        provide_context=True,
        templates_dict={""prediction_path"": prediction_path},
        dag=dag)
    evaluate_validation.set_upstream(evaluate_summary)

    return evaluate_prediction, evaluate_summary, evaluate_validation",def,create_evaluate_ops,(,task_prefix,",",data_format,",",input_paths,",",prediction_path,",",metric_fn_and_keys,",",validate_fn,",",batch_prediction_job_id,=,None,",",project_id,=,None,",",region,=,None,",",dataflow_options,=,None,",",model_uri,=,None,",",model_name,=,None,",",version_name,=,None,",",dag,=,None,),:,# Verify that task_prefix doesn't have any special characters except hyphen,"# '-', which is the only allowed non-alphanumeric character by Dataflow.",if,not,"Creates Operators needed for model evaluation and returns.

    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by
    calling MLEngineBatchPredictionOperator, then summarize and validate
    the result via Cloud Dataflow using DataFlowPythonOperator.

    For details and pricing about Batch prediction, please refer to the website
    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict
    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/

    It returns three chained operators for prediction, summary, and validation,
    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,
    respectively.
    (<prefix> should contain only alphanumeric characters or hyphen.)

    The upstream and downstream can be set accordingly like:
      pred, _, val = create_evaluate_ops(...)
      pred.set_upstream(upstream_op)
      ...
      downstream_op.set_upstream(val)

    Callers will provide two python callables, metric_fn and validate_fn, in
    order to customize the evaluation behavior as they wish.
    - metric_fn receives a dictionary per instance derived from json in the
      batch prediction result. The keys might vary depending on the model.
      It should return a tuple of metrics.
    - validation_fn receives a dictionary of the averaged metrics that metric_fn
      generated over all instances.
      The key/value of the dictionary matches to what's given by
      metric_fn_and_keys arg.
      The dictionary contains an additional metric, 'count' to represent the
      total number of instances received for evaluation.
      The function would raise an exception to mark the task as failed, in a
      case the validation result is not okay to proceed (i.e. to set the trained
      version as default).

    Typical examples are like this:

    def get_metric_fn_and_keys():
        import math  # imports should be outside of the metric_fn below.
        def error_and_squared_error(inst):
            label = float(inst['input_label'])
            classes = float(inst['classes'])  # 0 or 1
            err = abs(classes-label)
            squared_err = math.pow(classes-label, 2)
            return (err, squared_err)  # returns a tuple.
        return error_and_squared_error, ['err', 'mse']  # key order must match.

    def validate_err_and_count(summary):
        if summary['err'] > 0.2:
            raise ValueError('Too high err>0.2; summary=%s' % summary)
        if summary['mse'] > 0.05:
            raise ValueError('Too high mse>0.05; summary=%s' % summary)
        if summary['count'] < 1000:
            raise ValueError('Too few instances<1000; summary=%s' % summary)
        return summary

    For the details on the other BatchPrediction-related arguments (project_id,
    job_id, region, data_format, input_paths, prediction_path, model_uri),
    please refer to MLEngineBatchPredictionOperator too.

    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and
        hyphen are allowed (no underscores), since this will be used as dataflow
        job name, which doesn't allow other characters.
    :type task_prefix: str

    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'
    :type data_format: str

    :param input_paths: a list of input paths to be sent to BatchPrediction.
    :type input_paths: list[str]

    :param prediction_path: GCS path to put the prediction results in.
    :type prediction_path: str

    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:
        - metric_fn is a function that accepts a dictionary (for an instance),
          and returns a tuple of metric(s) that it calculates.
        - metric_keys is a list of strings to denote the key of each metric.
    :type metric_fn_and_keys: tuple of a function and a list[str]

    :param validate_fn: a function to validate whether the averaged metric(s) is
        good enough to push the model.
    :type validate_fn: function

    :param batch_prediction_job_id: the id to use for the Cloud ML Batch
        prediction job. Passed directly to the MLEngineBatchPredictionOperator as
        the job_id argument.
    :type batch_prediction_job_id: str

    :param project_id: the Google Cloud Platform project id in which to execute
        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['project_id']` will be used.
    :type project_id: str

    :param region: the Google Cloud Platform region in which to execute Cloud ML
        Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['region']` will be used.
    :type region: str

    :param dataflow_options: options to run Dataflow jobs. If None, then the
        `dag`'s `default_args['dataflow_default_options']` will be used.
    :type dataflow_options: dictionary

    :param model_uri: GCS path of the model exported by Tensorflow using
        tensorflow.estimator.export_savedmodel(). It cannot be used with
        model_name or version_name below. See MLEngineBatchPredictionOperator for
        more detail.
    :type model_uri: str

    :param model_name: Used to indicate a model to use for prediction. Can be
        used in combination with version_name, but cannot be used together with
        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,
        then the `dag`'s `default_args['model_name']` will be used.
    :type model_name: str

    :param version_name: Used to indicate a model version to use for prediction,
        in combination with model_name. Cannot be used together with model_uri.
        See MLEngineBatchPredictionOperator for more detail. If None, then the
        `dag`'s `default_args['version_name']` will be used.
    :type version_name: str

    :param dag: The `DAG` to use for all Operators.
    :type dag: airflow.models.DAG

    :returns: a tuple of three operators, (prediction, summary, validation)
    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,
                  PythonOperator)",Creates,Operators,needed,for,model,evaluation,and,returns,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/mlengine_operator_utils.py#L32-L246,test,re,.,match,(,"r""^[a-zA-Z][-A-Za-z0-9]*$""",",",task_prefix,),:,raise,AirflowException,(,"""Malformed task_id for DataFlowPythonOperator (only alphanumeric ""","""and hyphens are allowed but got: """,+,task_prefix,),metric_fn,",",metric_keys,=,metric_fn_and_keys,if,not,callable,(,metric_fn,),:,raise,AirflowException,(,"""`metric_fn` param must be callable.""",),if,not,callable,(,validate_fn,),:,raise,AirflowException,(,"""`validate_fn` param must be callable.""",),if,dag,is,not,None,and,dag,.,default_args,is,not,None,:,default_args,=,dag,.,default_args,project_id,=,project_id,or,default_args,.,get,(,'project_id',),region,=,region,or,default_args,.,get,(,'region',),model_name,=,model_name,or,default_args,.,get,(,'model_name',),version_name,=,version_name,or,default_args,.,get,(,'version_name',),dataflow_options,=,dataflow_options,or,default_args,.,get,(,'dataflow_default_options',),evaluate_prediction,=,MLEngineBatchPredictionOperator,(,task_id,=,(,task_prefix,+,"""-prediction""",),",",project_id,=,project_id,",",job_id,=,batch_prediction_job_id,",",region,=,region,",",data_format,=,data_format,",",input_paths,=,input_paths,",",output_path,=,prediction_path,",",uri,=,model_uri,",",model_name,=,model_name,",",version_name,=,version_name,",",dag,=,dag,),metric_fn_encoded,=,base64,.,b64encode,(,dill,.,dumps,(,metric_fn,",",recurse,=,True,),),evaluate_summary,=,DataFlowPythonOperator,(,task_id,=,(,task_prefix,+,"""-summary""",),",",py_options,=,[,"""-m""",],",",py_file,=,"""airflow.contrib.utils.mlengine_prediction_summary""",",",dataflow_default_options,=,dataflow_options,",",options,=,{,"""prediction_path""",:,prediction_path,",","""metric_fn_encoded""",:,metric_fn_encoded,",","""metric_keys""",:,"','",.,join,(,metric_keys,),},",",dag,=,dag,),evaluate_summary,.,set_upstream,(,evaluate_prediction,),def,apply_validate_fn,(,*,args,",",*,*,kwargs,),:,prediction_path,=,kwargs,[,"""templates_dict""",],[,"""prediction_path""",],scheme,",",bucket,",",obj,",",_,",",_,=,urlsplit,(,prediction_path,),if,scheme,!=,"""gs""",or,not,bucket,or,not,obj,:,raise,ValueError,(,"""Wrong format prediction_path: %s""",",",prediction_path,),summary,=,os,.,path,.,join,(,obj,.,strip,(,"""/""",),",","""prediction.summary.json""",),gcs_hook,=,GoogleCloudStorageHook,(,),summary,=,json,.,loads,(,gcs_hook,.,download,(,bucket,",",summary,),),return,validate_fn,(,summary,),evaluate_validation,=,PythonOperator,(,task_id,=,(,task_prefix,+,"""-validation""",),",",python_callable,=,apply_validate_fn,",",provide_context,=,True,",",templates_dict,=,{,"""prediction_path""",:,prediction_path,},",",dag,=,dag,),evaluate_validation,.,set_upstream,(,evaluate_summary,),return,evaluate_prediction,",",evaluate_summary,",",evaluate_validation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/file.py,mkdirs,"def mkdirs(path, mode):
    """"""
    Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int
    """"""
    try:
        o_umask = os.umask(0)
        os.makedirs(path, mode)
    except OSError:
        if not os.path.isdir(path):
            raise
    finally:
        os.umask(o_umask)",python,"def mkdirs(path, mode):
    """"""
    Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int
    """"""
    try:
        o_umask = os.umask(0)
        os.makedirs(path, mode)
    except OSError:
        if not os.path.isdir(path):
            raise
    finally:
        os.umask(o_umask)",def,mkdirs,(,path,",",mode,),:,try,:,o_umask,=,os,.,umask,(,0,),os,.,makedirs,(,path,",",mode,),except,OSError,:,if,not,os,.,path,.,isdir,(,path,),:,raise,finally,:,os,.,umask,(,o_umask,),,,,"Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int",Creates,the,directory,specified,by,path,creating,intermediate,directories,as,necessary,.,If,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/file.py#L42-L59,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,directory,already,exists,this,is,a,no,-,op,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/operators/check_operator.py,_convert_to_float_if_possible,"def _convert_to_float_if_possible(s):
    """"""
    A small helper function to convert a string to a numeric value
    if appropriate

    :param s: the string to be converted
    :type s: str
    """"""
    try:
        ret = float(s)
    except (ValueError, TypeError):
        ret = s
    return ret",python,"def _convert_to_float_if_possible(s):
    """"""
    A small helper function to convert a string to a numeric value
    if appropriate

    :param s: the string to be converted
    :type s: str
    """"""
    try:
        ret = float(s)
    except (ValueError, TypeError):
        ret = s
    return ret",def,_convert_to_float_if_possible,(,s,),:,try,:,ret,=,float,(,s,),except,(,ValueError,",",TypeError,),:,ret,=,s,return,ret,,,,,,,,,,,,,,,,,,,,,,,,,,,"A small helper function to convert a string to a numeric value
    if appropriate

    :param s: the string to be converted
    :type s: str",A,small,helper,function,to,convert,a,string,to,a,numeric,value,if,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/check_operator.py#L98-L110,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,appropriate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/timezone.py,make_aware,"def make_aware(value, timezone=None):
    """"""
    Make a naive datetime.datetime in a given time zone aware.

    :param value: datetime
    :param timezone: timezone
    :return: localized datetime in settings.TIMEZONE or timezone

    """"""
    if timezone is None:
        timezone = TIMEZONE

    # Check that we won't overwrite the timezone of an aware datetime.
    if is_localized(value):
        raise ValueError(
            ""make_aware expects a naive datetime, got %s"" % value)
    if hasattr(value, 'fold'):
        # In case of python 3.6 we want to do the same that pendulum does for python3.5
        # i.e in case we move clock back we want to schedule the run at the time of the second
        # instance of the same clock time rather than the first one.
        # Fold parameter has no impact in other cases so we can safely set it to 1 here
        value = value.replace(fold=1)
    if hasattr(timezone, 'localize'):
        # This method is available for pytz time zones.
        return timezone.localize(value)
    elif hasattr(timezone, 'convert'):
        # For pendulum
        return timezone.convert(value)
    else:
        # This may be wrong around DST changes!
        return value.replace(tzinfo=timezone)",python,"def make_aware(value, timezone=None):
    """"""
    Make a naive datetime.datetime in a given time zone aware.

    :param value: datetime
    :param timezone: timezone
    :return: localized datetime in settings.TIMEZONE or timezone

    """"""
    if timezone is None:
        timezone = TIMEZONE

    # Check that we won't overwrite the timezone of an aware datetime.
    if is_localized(value):
        raise ValueError(
            ""make_aware expects a naive datetime, got %s"" % value)
    if hasattr(value, 'fold'):
        # In case of python 3.6 we want to do the same that pendulum does for python3.5
        # i.e in case we move clock back we want to schedule the run at the time of the second
        # instance of the same clock time rather than the first one.
        # Fold parameter has no impact in other cases so we can safely set it to 1 here
        value = value.replace(fold=1)
    if hasattr(timezone, 'localize'):
        # This method is available for pytz time zones.
        return timezone.localize(value)
    elif hasattr(timezone, 'convert'):
        # For pendulum
        return timezone.convert(value)
    else:
        # This may be wrong around DST changes!
        return value.replace(tzinfo=timezone)",def,make_aware,(,value,",",timezone,=,None,),:,if,timezone,is,None,:,timezone,=,TIMEZONE,# Check that we won't overwrite the timezone of an aware datetime.,if,is_localized,(,value,),:,raise,ValueError,(,"""make_aware expects a naive datetime, got %s""",%,value,),if,hasattr,(,value,",",'fold',),:,# In case of python 3.6 we want to do the same that pendulum does for python3.5,# i.e in case we move clock back we want to schedule the run at the time of the second,# instance of the same clock time rather than the first one.,# Fold parameter has no impact in other cases so we can safely set it to 1 here,value,=,value,.,replace,(,fold,=,"Make a naive datetime.datetime in a given time zone aware.

    :param value: datetime
    :param timezone: timezone
    :return: localized datetime in settings.TIMEZONE or timezone",Make,a,naive,datetime,.,datetime,in,a,given,time,zone,aware,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L98-L128,test,1,),if,hasattr,(,timezone,",",'localize',),:,# This method is available for pytz time zones.,return,timezone,.,localize,(,value,),elif,hasattr,(,timezone,",",'convert',),:,# For pendulum,return,timezone,.,convert,(,value,),else,:,# This may be wrong around DST changes!,return,value,.,replace,(,tzinfo,=,timezone,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/timezone.py,make_naive,"def make_naive(value, timezone=None):
    """"""
    Make an aware datetime.datetime naive in a given time zone.

    :param value: datetime
    :param timezone: timezone
    :return: naive datetime
    """"""
    if timezone is None:
        timezone = TIMEZONE

    # Emulate the behavior of astimezone() on Python < 3.6.
    if is_naive(value):
        raise ValueError(""make_naive() cannot be applied to a naive datetime"")

    o = value.astimezone(timezone)

    # cross library compatibility
    naive = dt.datetime(o.year,
                        o.month,
                        o.day,
                        o.hour,
                        o.minute,
                        o.second,
                        o.microsecond)

    return naive",python,"def make_naive(value, timezone=None):
    """"""
    Make an aware datetime.datetime naive in a given time zone.

    :param value: datetime
    :param timezone: timezone
    :return: naive datetime
    """"""
    if timezone is None:
        timezone = TIMEZONE

    # Emulate the behavior of astimezone() on Python < 3.6.
    if is_naive(value):
        raise ValueError(""make_naive() cannot be applied to a naive datetime"")

    o = value.astimezone(timezone)

    # cross library compatibility
    naive = dt.datetime(o.year,
                        o.month,
                        o.day,
                        o.hour,
                        o.minute,
                        o.second,
                        o.microsecond)

    return naive",def,make_naive,(,value,",",timezone,=,None,),:,if,timezone,is,None,:,timezone,=,TIMEZONE,# Emulate the behavior of astimezone() on Python < 3.6.,if,is_naive,(,value,),:,raise,ValueError,(,"""make_naive() cannot be applied to a naive datetime""",),o,=,value,.,astimezone,(,timezone,),# cross library compatibility,naive,=,dt,.,datetime,(,o,.,year,",",o,.,month,"Make an aware datetime.datetime naive in a given time zone.

    :param value: datetime
    :param timezone: timezone
    :return: naive datetime",Make,an,aware,datetime,.,datetime,naive,in,a,given,time,zone,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L131-L157,test,",",o,.,day,",",o,.,hour,",",o,.,minute,",",o,.,second,",",o,.,microsecond,),return,naive,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/timezone.py,datetime,"def datetime(*args, **kwargs):
    """"""
    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified

    :return: datetime.datetime
    """"""
    if 'tzinfo' not in kwargs:
        kwargs['tzinfo'] = TIMEZONE

    return dt.datetime(*args, **kwargs)",python,"def datetime(*args, **kwargs):
    """"""
    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified

    :return: datetime.datetime
    """"""
    if 'tzinfo' not in kwargs:
        kwargs['tzinfo'] = TIMEZONE

    return dt.datetime(*args, **kwargs)",def,datetime,(,*,args,",",*,*,kwargs,),:,if,'tzinfo',not,in,kwargs,:,kwargs,[,'tzinfo',],=,TIMEZONE,return,dt,.,datetime,(,*,args,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,"Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified

    :return: datetime.datetime",Wrapper,around,datetime,.,datetime,that,adds,settings,.,TIMEZONE,if,tzinfo,not,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L160-L169,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,specified,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/druid_hook.py,DruidDbApiHook.get_conn,"def get_conn(self):
        """"""
        Establish a connection to druid broker.
        """"""
        conn = self.get_connection(self.druid_broker_conn_id)
        druid_broker_conn = connect(
            host=conn.host,
            port=conn.port,
            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),
            scheme=conn.extra_dejson.get('schema', 'http')
        )
        self.log.info('Get the connection to druid broker on %s', conn.host)
        return druid_broker_conn",python,"def get_conn(self):
        """"""
        Establish a connection to druid broker.
        """"""
        conn = self.get_connection(self.druid_broker_conn_id)
        druid_broker_conn = connect(
            host=conn.host,
            port=conn.port,
            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),
            scheme=conn.extra_dejson.get('schema', 'http')
        )
        self.log.info('Get the connection to druid broker on %s', conn.host)
        return druid_broker_conn",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,druid_broker_conn_id,),druid_broker_conn,=,connect,(,host,=,conn,.,host,",",port,=,conn,.,port,",",path,=,conn,.,extra_dejson,.,get,(,'endpoint',",",'/druid/v2/sql',),",",scheme,=,conn,.,extra_dejson,.,get,Establish a connection to druid broker.,Establish,a,connection,to,druid,broker,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/druid_hook.py#L127-L139,test,(,'schema',",",'http',),),self,.,log,.,info,(,'Get the connection to druid broker on %s',",",conn,.,host,),return,druid_broker_conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/http_hook.py,HttpHook.get_conn,"def get_conn(self, headers=None):
        """"""
        Returns http session for use with requests

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        """"""
        session = requests.Session()
        if self.http_conn_id:
            conn = self.get_connection(self.http_conn_id)

            if ""://"" in conn.host:
                self.base_url = conn.host
            else:
                # schema defaults to HTTP
                schema = conn.schema if conn.schema else ""http""
                self.base_url = schema + ""://"" + conn.host

            if conn.port:
                self.base_url = self.base_url + "":"" + str(conn.port)
            if conn.login:
                session.auth = (conn.login, conn.password)
            if conn.extra:
                try:
                    session.headers.update(conn.extra_dejson)
                except TypeError:
                    self.log.warn('Connection to %s has invalid extra field.', conn.host)
        if headers:
            session.headers.update(headers)

        return session",python,"def get_conn(self, headers=None):
        """"""
        Returns http session for use with requests

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        """"""
        session = requests.Session()
        if self.http_conn_id:
            conn = self.get_connection(self.http_conn_id)

            if ""://"" in conn.host:
                self.base_url = conn.host
            else:
                # schema defaults to HTTP
                schema = conn.schema if conn.schema else ""http""
                self.base_url = schema + ""://"" + conn.host

            if conn.port:
                self.base_url = self.base_url + "":"" + str(conn.port)
            if conn.login:
                session.auth = (conn.login, conn.password)
            if conn.extra:
                try:
                    session.headers.update(conn.extra_dejson)
                except TypeError:
                    self.log.warn('Connection to %s has invalid extra field.', conn.host)
        if headers:
            session.headers.update(headers)

        return session",def,get_conn,(,self,",",headers,=,None,),:,session,=,requests,.,Session,(,),if,self,.,http_conn_id,:,conn,=,self,.,get_connection,(,self,.,http_conn_id,),if,"""://""",in,conn,.,host,:,self,.,base_url,=,conn,.,host,else,:,# schema defaults to HTTP,schema,=,conn,"Returns http session for use with requests

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict",Returns,http,session,for,use,with,requests,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L53-L83,test,.,schema,if,conn,.,schema,else,"""http""",self,.,base_url,=,schema,+,"""://""",+,conn,.,host,if,conn,.,port,:,self,.,base_url,=,self,.,base_url,+,""":""",+,str,(,conn,.,port,),if,conn,.,login,:,session,.,auth,=,(,conn,.,login,",",conn,.,password,),if,conn,.,extra,:,try,:,session,.,headers,.,update,(,conn,.,extra_dejson,),except,TypeError,:,self,.,log,.,warn,(,'Connection to %s has invalid extra field.',",",conn,.,host,),if,headers,:,session,.,headers,.,update,(,headers,),return,session,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/http_hook.py,HttpHook.run,"def run(self, endpoint, data=None, headers=None, extra_options=None):
        """"""
        Performs the request

        :param endpoint: the endpoint to be called i.e. resource/v1/query?
        :type endpoint: str
        :param data: payload to be uploaded or request parameters
        :type data: dict
        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non
            2XX or 3XX status codes
        :type extra_options: dict
        """"""
        extra_options = extra_options or {}

        session = self.get_conn(headers)

        if self.base_url and not self.base_url.endswith('/') and \
           endpoint and not endpoint.startswith('/'):
            url = self.base_url + '/' + endpoint
        else:
            url = (self.base_url or '') + (endpoint or '')

        req = None
        if self.method == 'GET':
            # GET uses params
            req = requests.Request(self.method,
                                   url,
                                   params=data,
                                   headers=headers)
        elif self.method == 'HEAD':
            # HEAD doesn't use params
            req = requests.Request(self.method,
                                   url,
                                   headers=headers)
        else:
            # Others use data
            req = requests.Request(self.method,
                                   url,
                                   data=data,
                                   headers=headers)

        prepped_request = session.prepare_request(req)
        self.log.info(""Sending '%s' to url: %s"", self.method, url)
        return self.run_and_check(session, prepped_request, extra_options)",python,"def run(self, endpoint, data=None, headers=None, extra_options=None):
        """"""
        Performs the request

        :param endpoint: the endpoint to be called i.e. resource/v1/query?
        :type endpoint: str
        :param data: payload to be uploaded or request parameters
        :type data: dict
        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non
            2XX or 3XX status codes
        :type extra_options: dict
        """"""
        extra_options = extra_options or {}

        session = self.get_conn(headers)

        if self.base_url and not self.base_url.endswith('/') and \
           endpoint and not endpoint.startswith('/'):
            url = self.base_url + '/' + endpoint
        else:
            url = (self.base_url or '') + (endpoint or '')

        req = None
        if self.method == 'GET':
            # GET uses params
            req = requests.Request(self.method,
                                   url,
                                   params=data,
                                   headers=headers)
        elif self.method == 'HEAD':
            # HEAD doesn't use params
            req = requests.Request(self.method,
                                   url,
                                   headers=headers)
        else:
            # Others use data
            req = requests.Request(self.method,
                                   url,
                                   data=data,
                                   headers=headers)

        prepped_request = session.prepare_request(req)
        self.log.info(""Sending '%s' to url: %s"", self.method, url)
        return self.run_and_check(session, prepped_request, extra_options)",def,run,(,self,",",endpoint,",",data,=,None,",",headers,=,None,",",extra_options,=,None,),:,extra_options,=,extra_options,or,{,},session,=,self,.,get_conn,(,headers,),if,self,.,base_url,and,not,self,.,base_url,.,endswith,(,'/',),and,endpoint,and,not,"Performs the request

        :param endpoint: the endpoint to be called i.e. resource/v1/query?
        :type endpoint: str
        :param data: payload to be uploaded or request parameters
        :type data: dict
        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non
            2XX or 3XX status codes
        :type extra_options: dict",Performs,the,request,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L85-L131,test,endpoint,.,startswith,(,'/',),:,url,=,self,.,base_url,+,'/',+,endpoint,else,:,url,=,(,self,.,base_url,or,'',),+,(,endpoint,or,'',),req,=,None,if,self,.,method,==,'GET',:,# GET uses params,req,=,requests,.,Request,(,self,.,method,",",url,",",params,=,data,",",headers,=,headers,),elif,self,.,method,==,'HEAD',:,# HEAD doesn't use params,req,=,requests,.,Request,(,self,.,method,",",url,",",headers,=,headers,),else,:,# Others use data,req,=,requests,.,Request,(,self,.,method,",",url,",",data,=,data,",",headers,=,headers,),prepped_request,=,session,.,prepare_request,(,req,),self,.,log,.,info,(,"""Sending '%s' to url: %s""",",",self,.,method,",",url,),return,self,.,run_and_check,(,session,",",prepped_request,",",extra_options,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/http_hook.py,HttpHook.check_response,"def check_response(self, response):
        """"""
        Checks the status code and raise an AirflowException exception on non 2XX or 3XX
        status codes

        :param response: A requests response object
        :type response: requests.response
        """"""
        try:
            response.raise_for_status()
        except requests.exceptions.HTTPError:
            self.log.error(""HTTP error: %s"", response.reason)
            if self.method not in ['GET', 'HEAD']:
                self.log.error(response.text)
            raise AirflowException(str(response.status_code) + "":"" + response.reason)",python,"def check_response(self, response):
        """"""
        Checks the status code and raise an AirflowException exception on non 2XX or 3XX
        status codes

        :param response: A requests response object
        :type response: requests.response
        """"""
        try:
            response.raise_for_status()
        except requests.exceptions.HTTPError:
            self.log.error(""HTTP error: %s"", response.reason)
            if self.method not in ['GET', 'HEAD']:
                self.log.error(response.text)
            raise AirflowException(str(response.status_code) + "":"" + response.reason)",def,check_response,(,self,",",response,),:,try,:,response,.,raise_for_status,(,),except,requests,.,exceptions,.,HTTPError,:,self,.,log,.,error,(,"""HTTP error: %s""",",",response,.,reason,),if,self,.,method,not,in,[,'GET',",",'HEAD',],:,self,.,log,.,error,(,"Checks the status code and raise an AirflowException exception on non 2XX or 3XX
        status codes

        :param response: A requests response object
        :type response: requests.response",Checks,the,status,code,and,raise,an,AirflowException,exception,on,non,2XX,or,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L133-L147,test,response,.,text,),raise,AirflowException,(,str,(,response,.,status_code,),+,""":""",+,response,.,reason,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3XX,status,codes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/http_hook.py,HttpHook.run_and_check,"def run_and_check(self, session, prepped_request, extra_options):
        """"""
        Grabs extra options like timeout and actually runs the request,
        checking for the result

        :param session: the session to be used to execute the request
        :type session: requests.Session
        :param prepped_request: the prepared request generated in run()
        :type prepped_request: session.prepare_request
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX
            or 3XX status codes
        :type extra_options: dict
        """"""
        extra_options = extra_options or {}

        try:
            response = session.send(
                prepped_request,
                stream=extra_options.get(""stream"", False),
                verify=extra_options.get(""verify"", True),
                proxies=extra_options.get(""proxies"", {}),
                cert=extra_options.get(""cert""),
                timeout=extra_options.get(""timeout""),
                allow_redirects=extra_options.get(""allow_redirects"", True))

            if extra_options.get('check_response', True):
                self.check_response(response)
            return response

        except requests.exceptions.ConnectionError as ex:
            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')
            raise ex",python,"def run_and_check(self, session, prepped_request, extra_options):
        """"""
        Grabs extra options like timeout and actually runs the request,
        checking for the result

        :param session: the session to be used to execute the request
        :type session: requests.Session
        :param prepped_request: the prepared request generated in run()
        :type prepped_request: session.prepare_request
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX
            or 3XX status codes
        :type extra_options: dict
        """"""
        extra_options = extra_options or {}

        try:
            response = session.send(
                prepped_request,
                stream=extra_options.get(""stream"", False),
                verify=extra_options.get(""verify"", True),
                proxies=extra_options.get(""proxies"", {}),
                cert=extra_options.get(""cert""),
                timeout=extra_options.get(""timeout""),
                allow_redirects=extra_options.get(""allow_redirects"", True))

            if extra_options.get('check_response', True):
                self.check_response(response)
            return response

        except requests.exceptions.ConnectionError as ex:
            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')
            raise ex",def,run_and_check,(,self,",",session,",",prepped_request,",",extra_options,),:,extra_options,=,extra_options,or,{,},try,:,response,=,session,.,send,(,prepped_request,",",stream,=,extra_options,.,get,(,"""stream""",",",False,),",",verify,=,extra_options,.,get,(,"""verify""",",",True,),",",proxies,=,"Grabs extra options like timeout and actually runs the request,
        checking for the result

        :param session: the session to be used to execute the request
        :type session: requests.Session
        :param prepped_request: the prepared request generated in run()
        :type prepped_request: session.prepare_request
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX
            or 3XX status codes
        :type extra_options: dict",Grabs,extra,options,like,timeout,and,actually,runs,the,request,checking,for,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181,test,extra_options,.,get,(,"""proxies""",",",{,},),",",cert,=,extra_options,.,get,(,"""cert""",),",",timeout,=,extra_options,.,get,(,"""timeout""",),",",allow_redirects,=,extra_options,.,get,(,"""allow_redirects""",",",True,),),if,extra_options,.,get,(,'check_response',",",True,),:,self,.,check_response,(,response,),return,response,except,requests,.,exceptions,.,ConnectionError,as,ex,:,self,.,log,.,warn,(,str,(,ex,),+,' Tenacity will retry to execute the operation',),raise,ex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/db.py,create_session,"def create_session():
    """"""
    Contextmanager that will create and teardown a session.
    """"""
    session = settings.Session()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()",python,"def create_session():
    """"""
    Contextmanager that will create and teardown a session.
    """"""
    session = settings.Session()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()",def,create_session,(,),:,session,=,settings,.,Session,(,),try,:,yield,session,session,.,commit,(,),except,Exception,:,session,.,rollback,(,),raise,finally,:,session,.,close,(,),,,,,,,,,,,,,,,,Contextmanager that will create and teardown a session.,Contextmanager,that,will,create,and,teardown,a,session,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L32-L44,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/db.py,provide_session,"def provide_session(func):
    """"""
    Function decorator that provides a session if it isn't provided.
    If you want to reuse a session or run the function as part of a
    database transaction, you pass it to the function, if not this wrapper
    will create one and close it for you.
    """"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        arg_session = 'session'

        func_params = func.__code__.co_varnames
        session_in_args = arg_session in func_params and \
            func_params.index(arg_session) < len(args)
        session_in_kwargs = arg_session in kwargs

        if session_in_kwargs or session_in_args:
            return func(*args, **kwargs)
        else:
            with create_session() as session:
                kwargs[arg_session] = session
                return func(*args, **kwargs)

    return wrapper",python,"def provide_session(func):
    """"""
    Function decorator that provides a session if it isn't provided.
    If you want to reuse a session or run the function as part of a
    database transaction, you pass it to the function, if not this wrapper
    will create one and close it for you.
    """"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        arg_session = 'session'

        func_params = func.__code__.co_varnames
        session_in_args = arg_session in func_params and \
            func_params.index(arg_session) < len(args)
        session_in_kwargs = arg_session in kwargs

        if session_in_kwargs or session_in_args:
            return func(*args, **kwargs)
        else:
            with create_session() as session:
                kwargs[arg_session] = session
                return func(*args, **kwargs)

    return wrapper",def,provide_session,(,func,),:,@,wraps,(,func,),def,wrapper,(,*,args,",",*,*,kwargs,),:,arg_session,=,'session',func_params,=,func,.,__code__,.,co_varnames,session_in_args,=,arg_session,in,func_params,and,func_params,.,index,(,arg_session,),<,len,(,args,),session_in_kwargs,=,arg_session,"Function decorator that provides a session if it isn't provided.
    If you want to reuse a session or run the function as part of a
    database transaction, you pass it to the function, if not this wrapper
    will create one and close it for you.",Function,decorator,that,provides,a,session,if,it,isn,t,provided,.,If,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L47-L70,test,in,kwargs,if,session_in_kwargs,or,session_in_args,:,return,func,(,*,args,",",*,*,kwargs,),else,:,with,create_session,(,),as,session,:,kwargs,[,arg_session,],=,session,return,func,(,*,args,",",*,*,kwargs,),return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,you,want,to,reuse,a,session,or,run,the,function,as,part,of,a,database,transaction,you,pass,it,to,the,function,if,not,this,wrapper,will,create,one,and,close,it,for,you,.,,,,,
apache/airflow,airflow/utils/db.py,resetdb,"def resetdb():
    """"""
    Clear out the database
    """"""
    from airflow import models

    # alembic adds significant import time, so we import it lazily
    from alembic.migration import MigrationContext

    log.info(""Dropping tables that exist"")

    models.base.Base.metadata.drop_all(settings.engine)
    mc = MigrationContext.configure(settings.engine)
    if mc._version.exists(settings.engine):
        mc._version.drop(settings.engine)

    from flask_appbuilder.models.sqla import Base
    Base.metadata.drop_all(settings.engine)

    initdb()",python,"def resetdb():
    """"""
    Clear out the database
    """"""
    from airflow import models

    # alembic adds significant import time, so we import it lazily
    from alembic.migration import MigrationContext

    log.info(""Dropping tables that exist"")

    models.base.Base.metadata.drop_all(settings.engine)
    mc = MigrationContext.configure(settings.engine)
    if mc._version.exists(settings.engine):
        mc._version.drop(settings.engine)

    from flask_appbuilder.models.sqla import Base
    Base.metadata.drop_all(settings.engine)

    initdb()",def,resetdb,(,),:,from,airflow,import,models,"# alembic adds significant import time, so we import it lazily",from,alembic,.,migration,import,MigrationContext,log,.,info,(,"""Dropping tables that exist""",),models,.,base,.,Base,.,metadata,.,drop_all,(,settings,.,engine,),mc,=,MigrationContext,.,configure,(,settings,.,engine,),if,mc,.,_version,.,exists,Clear out the database,Clear,out,the,database,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L312-L331,test,(,settings,.,engine,),:,mc,.,_version,.,drop,(,settings,.,engine,),from,flask_appbuilder,.,models,.,sqla,import,Base,Base,.,metadata,.,drop_all,(,settings,.,engine,),initdb,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/presto_hook.py,PrestoHook._get_pretty_exception_message,"def _get_pretty_exception_message(e):
        """"""
        Parses some DatabaseError to provide a better error message
        """"""
        if (hasattr(e, 'message') and
            'errorName' in e.message and
                'message' in e.message):
            return ('{name}: {message}'.format(
                    name=e.message['errorName'],
                    message=e.message['message']))
        else:
            return str(e)",python,"def _get_pretty_exception_message(e):
        """"""
        Parses some DatabaseError to provide a better error message
        """"""
        if (hasattr(e, 'message') and
            'errorName' in e.message and
                'message' in e.message):
            return ('{name}: {message}'.format(
                    name=e.message['errorName'],
                    message=e.message['message']))
        else:
            return str(e)",def,_get_pretty_exception_message,(,e,),:,if,(,hasattr,(,e,",",'message',),and,'errorName',in,e,.,message,and,'message',in,e,.,message,),:,return,(,'{name}: {message}',.,format,(,name,=,e,.,message,[,'errorName',],",",message,=,e,.,message,[,'message',],),Parses some DatabaseError to provide a better error message,Parses,some,DatabaseError,to,provide,a,better,error,message,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L67-L78,test,),else,:,return,str,(,e,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/presto_hook.py,PrestoHook.get_records,"def get_records(self, hql, parameters=None):
        """"""
        Get a set of records from Presto
        """"""
        try:
            return super().get_records(
                self._strip_sql(hql), parameters)
        except DatabaseError as e:
            raise PrestoException(self._get_pretty_exception_message(e))",python,"def get_records(self, hql, parameters=None):
        """"""
        Get a set of records from Presto
        """"""
        try:
            return super().get_records(
                self._strip_sql(hql), parameters)
        except DatabaseError as e:
            raise PrestoException(self._get_pretty_exception_message(e))",def,get_records,(,self,",",hql,",",parameters,=,None,),:,try,:,return,super,(,),.,get_records,(,self,.,_strip_sql,(,hql,),",",parameters,),except,DatabaseError,as,e,:,raise,PrestoException,(,self,.,_get_pretty_exception_message,(,e,),),,,,,,,,Get a set of records from Presto,Get,a,set,of,records,from,Presto,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L80-L88,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/presto_hook.py,PrestoHook.get_pandas_df,"def get_pandas_df(self, hql, parameters=None):
        """"""
        Get a pandas dataframe from a sql query.
        """"""
        import pandas
        cursor = self.get_cursor()
        try:
            cursor.execute(self._strip_sql(hql), parameters)
            data = cursor.fetchall()
        except DatabaseError as e:
            raise PrestoException(self._get_pretty_exception_message(e))
        column_descriptions = cursor.description
        if data:
            df = pandas.DataFrame(data)
            df.columns = [c[0] for c in column_descriptions]
        else:
            df = pandas.DataFrame()
        return df",python,"def get_pandas_df(self, hql, parameters=None):
        """"""
        Get a pandas dataframe from a sql query.
        """"""
        import pandas
        cursor = self.get_cursor()
        try:
            cursor.execute(self._strip_sql(hql), parameters)
            data = cursor.fetchall()
        except DatabaseError as e:
            raise PrestoException(self._get_pretty_exception_message(e))
        column_descriptions = cursor.description
        if data:
            df = pandas.DataFrame(data)
            df.columns = [c[0] for c in column_descriptions]
        else:
            df = pandas.DataFrame()
        return df",def,get_pandas_df,(,self,",",hql,",",parameters,=,None,),:,import,pandas,cursor,=,self,.,get_cursor,(,),try,:,cursor,.,execute,(,self,.,_strip_sql,(,hql,),",",parameters,),data,=,cursor,.,fetchall,(,),except,DatabaseError,as,e,:,raise,PrestoException,(,self,Get a pandas dataframe from a sql query.,Get,a,pandas,dataframe,from,a,sql,query,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L101-L118,test,.,_get_pretty_exception_message,(,e,),),column_descriptions,=,cursor,.,description,if,data,:,df,=,pandas,.,DataFrame,(,data,),df,.,columns,=,[,c,[,0,],for,c,in,column_descriptions,],else,:,df,=,pandas,.,DataFrame,(,),return,df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/presto_hook.py,PrestoHook.run,"def run(self, hql, parameters=None):
        """"""
        Execute the statement against Presto. Can be used to create views.
        """"""
        return super().run(self._strip_sql(hql), parameters)",python,"def run(self, hql, parameters=None):
        """"""
        Execute the statement against Presto. Can be used to create views.
        """"""
        return super().run(self._strip_sql(hql), parameters)",def,run,(,self,",",hql,",",parameters,=,None,),:,return,super,(,),.,run,(,self,.,_strip_sql,(,hql,),",",parameters,),,,,,,,,,,,,,,,,,,,,,,,,,Execute the statement against Presto. Can be used to create views.,Execute,the,statement,against,Presto,.,Can,be,used,to,create,views,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L120-L124,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/presto_hook.py,PrestoHook.insert_rows,"def insert_rows(self, table, rows, target_fields=None):
        """"""
        A generic way to insert a set of tuples into a table.

        :param table: Name of the target table
        :type table: str
        :param rows: The rows to insert into the table
        :type rows: iterable of tuples
        :param target_fields: The names of the columns to fill in the table
        :type target_fields: iterable of strings
        """"""
        super().insert_rows(table, rows, target_fields, 0)",python,"def insert_rows(self, table, rows, target_fields=None):
        """"""
        A generic way to insert a set of tuples into a table.

        :param table: Name of the target table
        :type table: str
        :param rows: The rows to insert into the table
        :type rows: iterable of tuples
        :param target_fields: The names of the columns to fill in the table
        :type target_fields: iterable of strings
        """"""
        super().insert_rows(table, rows, target_fields, 0)",def,insert_rows,(,self,",",table,",",rows,",",target_fields,=,None,),:,super,(,),.,insert_rows,(,table,",",rows,",",target_fields,",",0,),,,,,,,,,,,,,,,,,,,,,,,,,"A generic way to insert a set of tuples into a table.

        :param table: Name of the target table
        :type table: str
        :param rows: The rows to insert into the table
        :type rows: iterable of tuples
        :param target_fields: The names of the columns to fill in the table
        :type target_fields: iterable of strings",A,generic,way,to,insert,a,set,of,tuples,into,a,table,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L129-L140,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.get_conn,"def get_conn(self):
        """"""
        Return a cosmos db client.
        """"""
        if self.cosmos_client is not None:
            return self.cosmos_client

        # Initialize the Python Azure Cosmos DB client
        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})

        return self.cosmos_client",python,"def get_conn(self):
        """"""
        Return a cosmos db client.
        """"""
        if self.cosmos_client is not None:
            return self.cosmos_client

        # Initialize the Python Azure Cosmos DB client
        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})

        return self.cosmos_client",def,get_conn,(,self,),:,if,self,.,cosmos_client,is,not,None,:,return,self,.,cosmos_client,# Initialize the Python Azure Cosmos DB client,self,.,cosmos_client,=,cosmos_client,.,CosmosClient,(,self,.,endpoint_uri,",",{,'masterKey',:,self,.,master_key,},),return,self,.,cosmos_client,,,,,,,,,,Return a cosmos db client.,Return,a,cosmos,db,client,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L50-L60,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.does_collection_exist,"def does_collection_exist(self, collection_name, database_name=None):
        """"""
        Checks if a collection exists in CosmosDB.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        existing_container = list(self.get_conn().QueryContainers(
            get_database_link(self.__get_database_name(database_name)), {
                ""query"": ""SELECT * FROM r WHERE r.id=@id"",
                ""parameters"": [
                    {""name"": ""@id"", ""value"": collection_name}
                ]
            }))
        if len(existing_container) == 0:
            return False

        return True",python,"def does_collection_exist(self, collection_name, database_name=None):
        """"""
        Checks if a collection exists in CosmosDB.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        existing_container = list(self.get_conn().QueryContainers(
            get_database_link(self.__get_database_name(database_name)), {
                ""query"": ""SELECT * FROM r WHERE r.id=@id"",
                ""parameters"": [
                    {""name"": ""@id"", ""value"": collection_name}
                ]
            }))
        if len(existing_container) == 0:
            return False

        return True",def,does_collection_exist,(,self,",",collection_name,",",database_name,=,None,),:,if,collection_name,is,None,:,raise,AirflowBadRequest,(,"""Collection name cannot be None.""",),existing_container,=,list,(,self,.,get_conn,(,),.,QueryContainers,(,get_database_link,(,self,.,__get_database_name,(,database_name,),),",",{,"""query""",:,"""SELECT * FROM r WHERE r.id=@id""",",","""parameters""",:,[,Checks if a collection exists in CosmosDB.,Checks,if,a,collection,exists,in,CosmosDB,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L82-L99,test,{,"""name""",:,"""@id""",",","""value""",:,collection_name,},],},),),if,len,(,existing_container,),==,0,:,return,False,return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.create_collection,"def create_collection(self, collection_name, database_name=None):
        """"""
        Creates a new collection in the CosmosDB database.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        # We need to check to see if this container already exists so we don't try
        # to create it twice
        existing_container = list(self.get_conn().QueryContainers(
            get_database_link(self.__get_database_name(database_name)), {
                ""query"": ""SELECT * FROM r WHERE r.id=@id"",
                ""parameters"": [
                    {""name"": ""@id"", ""value"": collection_name}
                ]
            }))

        # Only create if we did not find it already existing
        if len(existing_container) == 0:
            self.get_conn().CreateContainer(
                get_database_link(self.__get_database_name(database_name)),
                {""id"": collection_name})",python,"def create_collection(self, collection_name, database_name=None):
        """"""
        Creates a new collection in the CosmosDB database.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        # We need to check to see if this container already exists so we don't try
        # to create it twice
        existing_container = list(self.get_conn().QueryContainers(
            get_database_link(self.__get_database_name(database_name)), {
                ""query"": ""SELECT * FROM r WHERE r.id=@id"",
                ""parameters"": [
                    {""name"": ""@id"", ""value"": collection_name}
                ]
            }))

        # Only create if we did not find it already existing
        if len(existing_container) == 0:
            self.get_conn().CreateContainer(
                get_database_link(self.__get_database_name(database_name)),
                {""id"": collection_name})",def,create_collection,(,self,",",collection_name,",",database_name,=,None,),:,if,collection_name,is,None,:,raise,AirflowBadRequest,(,"""Collection name cannot be None.""",),# We need to check to see if this container already exists so we don't try,# to create it twice,existing_container,=,list,(,self,.,get_conn,(,),.,QueryContainers,(,get_database_link,(,self,.,__get_database_name,(,database_name,),),",",{,"""query""",:,"""SELECT * FROM r WHERE r.id=@id""",",","""parameters""",Creates a new collection in the CosmosDB database.,Creates,a,new,collection,in,the,CosmosDB,database,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L101-L122,test,:,[,{,"""name""",:,"""@id""",",","""value""",:,collection_name,},],},),),# Only create if we did not find it already existing,if,len,(,existing_container,),==,0,:,self,.,get_conn,(,),.,CreateContainer,(,get_database_link,(,self,.,__get_database_name,(,database_name,),),",",{,"""id""",:,collection_name,},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.does_database_exist,"def does_database_exist(self, database_name):
        """"""
        Checks if a database exists in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        existing_database = list(self.get_conn().QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": database_name}
            ]
        }))
        if len(existing_database) == 0:
            return False

        return True",python,"def does_database_exist(self, database_name):
        """"""
        Checks if a database exists in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        existing_database = list(self.get_conn().QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": database_name}
            ]
        }))
        if len(existing_database) == 0:
            return False

        return True",def,does_database_exist,(,self,",",database_name,),:,if,database_name,is,None,:,raise,AirflowBadRequest,(,"""Database name cannot be None.""",),existing_database,=,list,(,self,.,get_conn,(,),.,QueryDatabases,(,{,"""query""",:,"""SELECT * FROM r WHERE r.id=@id""",",","""parameters""",:,[,{,"""name""",:,"""@id""",",","""value""",:,database_name,},],},),),if,Checks if a database exists in CosmosDB.,Checks,if,a,database,exists,in,CosmosDB,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L124-L140,test,len,(,existing_database,),==,0,:,return,False,return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.create_database,"def create_database(self, database_name):
        """"""
        Creates a new database in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        # We need to check to see if this database already exists so we don't try
        # to create it twice
        existing_database = list(self.get_conn().QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": database_name}
            ]
        }))

        # Only create if we did not find it already existing
        if len(existing_database) == 0:
            self.get_conn().CreateDatabase({""id"": database_name})",python,"def create_database(self, database_name):
        """"""
        Creates a new database in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        # We need to check to see if this database already exists so we don't try
        # to create it twice
        existing_database = list(self.get_conn().QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": database_name}
            ]
        }))

        # Only create if we did not find it already existing
        if len(existing_database) == 0:
            self.get_conn().CreateDatabase({""id"": database_name})",def,create_database,(,self,",",database_name,),:,if,database_name,is,None,:,raise,AirflowBadRequest,(,"""Database name cannot be None.""",),# We need to check to see if this database already exists so we don't try,# to create it twice,existing_database,=,list,(,self,.,get_conn,(,),.,QueryDatabases,(,{,"""query""",:,"""SELECT * FROM r WHERE r.id=@id""",",","""parameters""",:,[,{,"""name""",:,"""@id""",",","""value""",:,database_name,},],},),Creates a new database in CosmosDB.,Creates,a,new,database,in,CosmosDB,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L142-L160,test,),# Only create if we did not find it already existing,if,len,(,existing_database,),==,0,:,self,.,get_conn,(,),.,CreateDatabase,(,{,"""id""",:,database_name,},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.delete_database,"def delete_database(self, database_name):
        """"""
        Deletes an existing database in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        self.get_conn().DeleteDatabase(get_database_link(database_name))",python,"def delete_database(self, database_name):
        """"""
        Deletes an existing database in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        self.get_conn().DeleteDatabase(get_database_link(database_name))",def,delete_database,(,self,",",database_name,),:,if,database_name,is,None,:,raise,AirflowBadRequest,(,"""Database name cannot be None.""",),self,.,get_conn,(,),.,DeleteDatabase,(,get_database_link,(,database_name,),),,,,,,,,,,,,,,,,,,,,,,Deletes an existing database in CosmosDB.,Deletes,an,existing,database,in,CosmosDB,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L162-L169,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.delete_collection,"def delete_collection(self, collection_name, database_name=None):
        """"""
        Deletes an existing collection in the CosmosDB database.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        self.get_conn().DeleteContainer(
            get_collection_link(self.__get_database_name(database_name), collection_name))",python,"def delete_collection(self, collection_name, database_name=None):
        """"""
        Deletes an existing collection in the CosmosDB database.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        self.get_conn().DeleteContainer(
            get_collection_link(self.__get_database_name(database_name), collection_name))",def,delete_collection,(,self,",",collection_name,",",database_name,=,None,),:,if,collection_name,is,None,:,raise,AirflowBadRequest,(,"""Collection name cannot be None.""",),self,.,get_conn,(,),.,DeleteContainer,(,get_collection_link,(,self,.,__get_database_name,(,database_name,),",",collection_name,),),,,,,,,,,,,Deletes an existing collection in the CosmosDB database.,Deletes,an,existing,collection,in,the,CosmosDB,database,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L171-L179,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.insert_documents,"def insert_documents(self, documents, database_name=None, collection_name=None):
        """"""
        Insert a list of new documents into an existing collection in the CosmosDB database.
        """"""
        if documents is None:
            raise AirflowBadRequest(""You cannot insert empty documents"")

        created_documents = []
        for single_document in documents:
            created_documents.append(
                self.get_conn().CreateItem(
                    get_collection_link(
                        self.__get_database_name(database_name),
                        self.__get_collection_name(collection_name)),
                    single_document))

        return created_documents",python,"def insert_documents(self, documents, database_name=None, collection_name=None):
        """"""
        Insert a list of new documents into an existing collection in the CosmosDB database.
        """"""
        if documents is None:
            raise AirflowBadRequest(""You cannot insert empty documents"")

        created_documents = []
        for single_document in documents:
            created_documents.append(
                self.get_conn().CreateItem(
                    get_collection_link(
                        self.__get_database_name(database_name),
                        self.__get_collection_name(collection_name)),
                    single_document))

        return created_documents",def,insert_documents,(,self,",",documents,",",database_name,=,None,",",collection_name,=,None,),:,if,documents,is,None,:,raise,AirflowBadRequest,(,"""You cannot insert empty documents""",),created_documents,=,[,],for,single_document,in,documents,:,created_documents,.,append,(,self,.,get_conn,(,),.,CreateItem,(,get_collection_link,(,self,.,__get_database_name,Insert a list of new documents into an existing collection in the CosmosDB database.,Insert,a,list,of,new,documents,into,an,existing,collection,in,the,CosmosDB,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L208-L224,test,(,database_name,),",",self,.,__get_collection_name,(,collection_name,),),",",single_document,),),return,created_documents,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,database,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.delete_document,"def delete_document(self, document_id, database_name=None, collection_name=None):
        """"""
        Delete an existing document out of a collection in the CosmosDB database.
        """"""
        if document_id is None:
            raise AirflowBadRequest(""Cannot delete a document without an id"")

        self.get_conn().DeleteItem(
            get_document_link(
                self.__get_database_name(database_name),
                self.__get_collection_name(collection_name),
                document_id))",python,"def delete_document(self, document_id, database_name=None, collection_name=None):
        """"""
        Delete an existing document out of a collection in the CosmosDB database.
        """"""
        if document_id is None:
            raise AirflowBadRequest(""Cannot delete a document without an id"")

        self.get_conn().DeleteItem(
            get_document_link(
                self.__get_database_name(database_name),
                self.__get_collection_name(collection_name),
                document_id))",def,delete_document,(,self,",",document_id,",",database_name,=,None,",",collection_name,=,None,),:,if,document_id,is,None,:,raise,AirflowBadRequest,(,"""Cannot delete a document without an id""",),self,.,get_conn,(,),.,DeleteItem,(,get_document_link,(,self,.,__get_database_name,(,database_name,),",",self,.,__get_collection_name,(,collection_name,),",",document_id,),Delete an existing document out of a collection in the CosmosDB database.,Delete,an,existing,document,out,of,a,collection,in,the,CosmosDB,database,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L226-L237,test,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.get_document,"def get_document(self, document_id, database_name=None, collection_name=None):
        """"""
        Get a document from an existing collection in the CosmosDB database.
        """"""
        if document_id is None:
            raise AirflowBadRequest(""Cannot get a document without an id"")

        try:
            return self.get_conn().ReadItem(
                get_document_link(
                    self.__get_database_name(database_name),
                    self.__get_collection_name(collection_name),
                    document_id))
        except HTTPFailure:
            return None",python,"def get_document(self, document_id, database_name=None, collection_name=None):
        """"""
        Get a document from an existing collection in the CosmosDB database.
        """"""
        if document_id is None:
            raise AirflowBadRequest(""Cannot get a document without an id"")

        try:
            return self.get_conn().ReadItem(
                get_document_link(
                    self.__get_database_name(database_name),
                    self.__get_collection_name(collection_name),
                    document_id))
        except HTTPFailure:
            return None",def,get_document,(,self,",",document_id,",",database_name,=,None,",",collection_name,=,None,),:,if,document_id,is,None,:,raise,AirflowBadRequest,(,"""Cannot get a document without an id""",),try,:,return,self,.,get_conn,(,),.,ReadItem,(,get_document_link,(,self,.,__get_database_name,(,database_name,),",",self,.,__get_collection_name,(,collection_name,),Get a document from an existing collection in the CosmosDB database.,Get,a,document,from,an,existing,collection,in,the,CosmosDB,database,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L239-L253,test,",",document_id,),),except,HTTPFailure,:,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,AzureCosmosDBHook.get_documents,"def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):
        """"""
        Get a list of documents from an existing collection in the CosmosDB database via SQL query.
        """"""
        if sql_string is None:
            raise AirflowBadRequest(""SQL query string cannot be None"")

        # Query them in SQL
        query = {'query': sql_string}

        try:
            result_iterable = self.get_conn().QueryItems(
                get_collection_link(
                    self.__get_database_name(database_name),
                    self.__get_collection_name(collection_name)),
                query,
                partition_key)

            return list(result_iterable)
        except HTTPFailure:
            return None",python,"def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):
        """"""
        Get a list of documents from an existing collection in the CosmosDB database via SQL query.
        """"""
        if sql_string is None:
            raise AirflowBadRequest(""SQL query string cannot be None"")

        # Query them in SQL
        query = {'query': sql_string}

        try:
            result_iterable = self.get_conn().QueryItems(
                get_collection_link(
                    self.__get_database_name(database_name),
                    self.__get_collection_name(collection_name)),
                query,
                partition_key)

            return list(result_iterable)
        except HTTPFailure:
            return None",def,get_documents,(,self,",",sql_string,",",database_name,=,None,",",collection_name,=,None,",",partition_key,=,None,),:,if,sql_string,is,None,:,raise,AirflowBadRequest,(,"""SQL query string cannot be None""",),# Query them in SQL,query,=,{,'query',:,sql_string,},try,:,result_iterable,=,self,.,get_conn,(,),.,QueryItems,(,get_collection_link,(,Get a list of documents from an existing collection in the CosmosDB database via SQL query.,Get,a,list,of,documents,from,an,existing,collection,in,the,CosmosDB,database,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275,test,self,.,__get_database_name,(,database_name,),",",self,.,__get_collection_name,(,collection_name,),),",",query,",",partition_key,),return,list,(,result_iterable,),except,HTTPFailure,:,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,via,SQL,query,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,GcfHook.get_function,"def get_function(self, name):
        """"""
        Returns the Cloud Function with the given name.

        :param name: Name of the function.
        :type name: str
        :return: A Cloud Functions object representing the function.
        :rtype: dict
        """"""
        return self.get_conn().projects().locations().functions().get(
            name=name).execute(num_retries=self.num_retries)",python,"def get_function(self, name):
        """"""
        Returns the Cloud Function with the given name.

        :param name: Name of the function.
        :type name: str
        :return: A Cloud Functions object representing the function.
        :rtype: dict
        """"""
        return self.get_conn().projects().locations().functions().get(
            name=name).execute(num_retries=self.num_retries)",def,get_function,(,self,",",name,),:,return,self,.,get_conn,(,),.,projects,(,),.,locations,(,),.,functions,(,),.,get,(,name,=,name,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,,,,,"Returns the Cloud Function with the given name.

        :param name: Name of the function.
        :type name: str
        :return: A Cloud Functions object representing the function.
        :rtype: dict",Returns,the,Cloud,Function,with,the,given,name,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L76-L86,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,GcfHook.create_new_function,"def create_new_function(self, location, body, project_id=None):
        """"""
        Creates a new function in Cloud Function in the location specified in the body.

        :param location: The location of the function.
        :type location: str
        :param body: The body required by the Cloud Functions insert API.
        :type body: dict
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().create(
            location=self._full_location(project_id, location),
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)",python,"def create_new_function(self, location, body, project_id=None):
        """"""
        Creates a new function in Cloud Function in the location specified in the body.

        :param location: The location of the function.
        :type location: str
        :param body: The body required by the Cloud Functions insert API.
        :type body: dict
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().create(
            location=self._full_location(project_id, location),
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)",def,create_new_function,(,self,",",location,",",body,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,projects,(,),.,locations,(,),.,functions,(,),.,create,(,location,=,self,.,_full_location,(,project_id,",",location,),",",body,=,body,),.,"Creates a new function in Cloud Function in the location specified in the body.

        :param location: The location of the function.
        :type location: str
        :param body: The body required by the Cloud Functions insert API.
        :type body: dict
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Creates,a,new,function,in,Cloud,Function,in,the,location,specified,in,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L89-L107,test,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,[,"""name""",],self,.,_wait_for_operation_to_complete,(,operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,body,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,GcfHook.update_function,"def update_function(self, name, body, update_mask):
        """"""
        Updates Cloud Functions according to the specified update mask.

        :param name: The name of the function.
        :type name: str
        :param body: The body required by the cloud function patch API.
        :type body: dict
        :param update_mask: The update mask - array of fields that should be patched.
        :type update_mask: [str]
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().patch(
            updateMask="","".join(update_mask),
            name=name,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)",python,"def update_function(self, name, body, update_mask):
        """"""
        Updates Cloud Functions according to the specified update mask.

        :param name: The name of the function.
        :type name: str
        :param body: The body required by the cloud function patch API.
        :type body: dict
        :param update_mask: The update mask - array of fields that should be patched.
        :type update_mask: [str]
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().patch(
            updateMask="","".join(update_mask),
            name=name,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)",def,update_function,(,self,",",name,",",body,",",update_mask,),:,response,=,self,.,get_conn,(,),.,projects,(,),.,locations,(,),.,functions,(,),.,patch,(,updateMask,=,""",""",.,join,(,update_mask,),",",name,=,name,",",body,=,body,),.,"Updates Cloud Functions according to the specified update mask.

        :param name: The name of the function.
        :type name: str
        :param body: The body required by the cloud function patch API.
        :type body: dict
        :param update_mask: The update mask - array of fields that should be patched.
        :type update_mask: [str]
        :return: None",Updates,Cloud,Functions,according,to,the,specified,update,mask,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L109-L127,test,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,[,"""name""",],self,.,_wait_for_operation_to_complete,(,operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,GcfHook.upload_function_zip,"def upload_function_zip(self, location, zip_path, project_id=None):
        """"""
        Uploads zip file with sources.

        :param location: The location where the function is created.
        :type location: str
        :param zip_path: The path of the valid .zip file to upload.
        :type zip_path: str
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: The upload URL that was returned by generateUploadUrl method.
        """"""
        response = self.get_conn().projects().locations().functions().generateUploadUrl(
            parent=self._full_location(project_id, location)
        ).execute(num_retries=self.num_retries)
        upload_url = response.get('uploadUrl')
        with open(zip_path, 'rb') as fp:
            requests.put(
                url=upload_url,
                data=fp,
                # Those two headers needs to be specified according to:
                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl
                # nopep8
                headers={
                    'Content-type': 'application/zip',
                    'x-goog-content-length-range': '0,104857600',
                }
            )
        return upload_url",python,"def upload_function_zip(self, location, zip_path, project_id=None):
        """"""
        Uploads zip file with sources.

        :param location: The location where the function is created.
        :type location: str
        :param zip_path: The path of the valid .zip file to upload.
        :type zip_path: str
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: The upload URL that was returned by generateUploadUrl method.
        """"""
        response = self.get_conn().projects().locations().functions().generateUploadUrl(
            parent=self._full_location(project_id, location)
        ).execute(num_retries=self.num_retries)
        upload_url = response.get('uploadUrl')
        with open(zip_path, 'rb') as fp:
            requests.put(
                url=upload_url,
                data=fp,
                # Those two headers needs to be specified according to:
                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl
                # nopep8
                headers={
                    'Content-type': 'application/zip',
                    'x-goog-content-length-range': '0,104857600',
                }
            )
        return upload_url",def,upload_function_zip,(,self,",",location,",",zip_path,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,projects,(,),.,locations,(,),.,functions,(,),.,generateUploadUrl,(,parent,=,self,.,_full_location,(,project_id,",",location,),),.,execute,(,num_retries,=,"Uploads zip file with sources.

        :param location: The location where the function is created.
        :type location: str
        :param zip_path: The path of the valid .zip file to upload.
        :type zip_path: str
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: The upload URL that was returned by generateUploadUrl method.",Uploads,zip,file,with,sources,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L130-L159,test,self,.,num_retries,),upload_url,=,response,.,get,(,'uploadUrl',),with,open,(,zip_path,",",'rb',),as,fp,:,requests,.,put,(,url,=,upload_url,",",data,=,fp,",",# Those two headers needs to be specified according to:,# https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl,# nopep8,headers,=,{,'Content-type',:,'application/zip',",",'x-goog-content-length-range',:,"'0,104857600'",",",},),return,upload_url,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,GcfHook.delete_function,"def delete_function(self, name):
        """"""
        Deletes the specified Cloud Function.

        :param name: The name of the function.
        :type name: str
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().delete(
            name=name).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)",python,"def delete_function(self, name):
        """"""
        Deletes the specified Cloud Function.

        :param name: The name of the function.
        :type name: str
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().delete(
            name=name).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)",def,delete_function,(,self,",",name,),:,response,=,self,.,get_conn,(,),.,projects,(,),.,locations,(,),.,functions,(,),.,delete,(,name,=,name,),.,execute,(,num_retries,=,self,.,num_retries,),operation_name,=,response,[,"""name""",],self,.,_wait_for_operation_to_complete,"Deletes the specified Cloud Function.

        :param name: The name of the function.
        :type name: str
        :return: None",Deletes,the,specified,Cloud,Function,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L161-L172,test,(,operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/ti_deps/deps/base_ti_dep.py,BaseTIDep.get_dep_statuses,"def get_dep_statuses(self, ti, session, dep_context=None):
        """"""
        Wrapper around the private _get_dep_statuses method that contains some global
        checks for all dependencies.

        :param ti: the task instance to get the dependency status for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: the context for which this dependency should be evaluated for
        :type dep_context: DepContext
        """"""
        # this avoids a circular dependency
        from airflow.ti_deps.dep_context import DepContext

        if dep_context is None:
            dep_context = DepContext()

        if self.IGNOREABLE and dep_context.ignore_all_deps:
            yield self._passing_status(
                reason=""Context specified all dependencies should be ignored."")
            return

        if self.IS_TASK_DEP and dep_context.ignore_task_deps:
            yield self._passing_status(
                reason=""Context specified all task dependencies should be ignored."")
            return

        for dep_status in self._get_dep_statuses(ti, session, dep_context):
            yield dep_status",python,"def get_dep_statuses(self, ti, session, dep_context=None):
        """"""
        Wrapper around the private _get_dep_statuses method that contains some global
        checks for all dependencies.

        :param ti: the task instance to get the dependency status for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: the context for which this dependency should be evaluated for
        :type dep_context: DepContext
        """"""
        # this avoids a circular dependency
        from airflow.ti_deps.dep_context import DepContext

        if dep_context is None:
            dep_context = DepContext()

        if self.IGNOREABLE and dep_context.ignore_all_deps:
            yield self._passing_status(
                reason=""Context specified all dependencies should be ignored."")
            return

        if self.IS_TASK_DEP and dep_context.ignore_task_deps:
            yield self._passing_status(
                reason=""Context specified all task dependencies should be ignored."")
            return

        for dep_status in self._get_dep_statuses(ti, session, dep_context):
            yield dep_status",def,get_dep_statuses,(,self,",",ti,",",session,",",dep_context,=,None,),:,# this avoids a circular dependency,from,airflow,.,ti_deps,.,dep_context,import,DepContext,if,dep_context,is,None,:,dep_context,=,DepContext,(,),if,self,.,IGNOREABLE,and,dep_context,.,ignore_all_deps,:,yield,self,.,_passing_status,(,reason,=,"""Context specified all dependencies should be ignored.""",),return,"Wrapper around the private _get_dep_statuses method that contains some global
        checks for all dependencies.

        :param ti: the task instance to get the dependency status for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: the context for which this dependency should be evaluated for
        :type dep_context: DepContext",Wrapper,around,the,private,_get_dep_statuses,method,that,contains,some,global,checks,for,all,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/base_ti_dep.py#L78-L107,test,if,self,.,IS_TASK_DEP,and,dep_context,.,ignore_task_deps,:,yield,self,.,_passing_status,(,reason,=,"""Context specified all task dependencies should be ignored.""",),return,for,dep_status,in,self,.,_get_dep_statuses,(,ti,",",session,",",dep_context,),:,yield,dep_status,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dependencies,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/ti_deps/deps/base_ti_dep.py,BaseTIDep.is_met,"def is_met(self, ti, session, dep_context=None):
        """"""
        Returns whether or not this dependency is met for a given task instance. A
        dependency is considered met if all of the dependency statuses it reports are
        passing.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext
        """"""
        return all(status.passed for status in
                   self.get_dep_statuses(ti, session, dep_context))",python,"def is_met(self, ti, session, dep_context=None):
        """"""
        Returns whether or not this dependency is met for a given task instance. A
        dependency is considered met if all of the dependency statuses it reports are
        passing.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext
        """"""
        return all(status.passed for status in
                   self.get_dep_statuses(ti, session, dep_context))",def,is_met,(,self,",",ti,",",session,",",dep_context,=,None,),:,return,all,(,status,.,passed,for,status,in,self,.,get_dep_statuses,(,ti,",",session,",",dep_context,),),,,,,,,,,,,,,,,,,,,"Returns whether or not this dependency is met for a given task instance. A
        dependency is considered met if all of the dependency statuses it reports are
        passing.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext",Returns,whether,or,not,this,dependency,is,met,for,a,given,task,instance,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/base_ti_dep.py#L110-L125,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,A,dependency,is,considered,met,if,all,of,the,dependency,statuses,it,reports,are,passing,.,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/ti_deps/deps/base_ti_dep.py,BaseTIDep.get_failure_reasons,"def get_failure_reasons(self, ti, session, dep_context=None):
        """"""
        Returns an iterable of strings that explain why this dependency wasn't met.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext
        """"""
        for dep_status in self.get_dep_statuses(ti, session, dep_context):
            if not dep_status.passed:
                yield dep_status.reason",python,"def get_failure_reasons(self, ti, session, dep_context=None):
        """"""
        Returns an iterable of strings that explain why this dependency wasn't met.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext
        """"""
        for dep_status in self.get_dep_statuses(ti, session, dep_context):
            if not dep_status.passed:
                yield dep_status.reason",def,get_failure_reasons,(,self,",",ti,",",session,",",dep_context,=,None,),:,for,dep_status,in,self,.,get_dep_statuses,(,ti,",",session,",",dep_context,),:,if,not,dep_status,.,passed,:,yield,dep_status,.,reason,,,,,,,,,,,,,,,"Returns an iterable of strings that explain why this dependency wasn't met.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext",Returns,an,iterable,of,strings,that,explain,why,this,dependency,wasn,t,met,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/base_ti_dep.py#L128-L142,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_hook.py,_parse_s3_config,"def _parse_s3_config(config_file_name, config_format='boto', profile=None):
    """"""
    Parses a config file for s3 credentials. Can currently
    parse boto, s3cmd.conf and AWS SDK config formats

    :param config_file_name: path to the config file
    :type config_file_name: str
    :param config_format: config type. One of ""boto"", ""s3cmd"" or ""aws"".
        Defaults to ""boto""
    :type config_format: str
    :param profile: profile name in AWS type config file
    :type profile: str
    """"""
    config = configparser.ConfigParser()
    if config.read(config_file_name):  # pragma: no cover
        sections = config.sections()
    else:
        raise AirflowException(""Couldn't read {0}"".format(config_file_name))
    # Setting option names depending on file format
    if config_format is None:
        config_format = 'boto'
    conf_format = config_format.lower()
    if conf_format == 'boto':  # pragma: no cover
        if profile is not None and 'profile ' + profile in sections:
            cred_section = 'profile ' + profile
        else:
            cred_section = 'Credentials'
    elif conf_format == 'aws' and profile is not None:
        cred_section = profile
    else:
        cred_section = 'default'
    # Option names
    if conf_format in ('boto', 'aws'):  # pragma: no cover
        key_id_option = 'aws_access_key_id'
        secret_key_option = 'aws_secret_access_key'
        # security_token_option = 'aws_security_token'
    else:
        key_id_option = 'access_key'
        secret_key_option = 'secret_key'
    # Actual Parsing
    if cred_section not in sections:
        raise AirflowException(""This config file format is not recognized"")
    else:
        try:
            access_key = config.get(cred_section, key_id_option)
            secret_key = config.get(cred_section, secret_key_option)
        except Exception:
            logging.warning(""Option Error in parsing s3 config file"")
            raise
        return access_key, secret_key",python,"def _parse_s3_config(config_file_name, config_format='boto', profile=None):
    """"""
    Parses a config file for s3 credentials. Can currently
    parse boto, s3cmd.conf and AWS SDK config formats

    :param config_file_name: path to the config file
    :type config_file_name: str
    :param config_format: config type. One of ""boto"", ""s3cmd"" or ""aws"".
        Defaults to ""boto""
    :type config_format: str
    :param profile: profile name in AWS type config file
    :type profile: str
    """"""
    config = configparser.ConfigParser()
    if config.read(config_file_name):  # pragma: no cover
        sections = config.sections()
    else:
        raise AirflowException(""Couldn't read {0}"".format(config_file_name))
    # Setting option names depending on file format
    if config_format is None:
        config_format = 'boto'
    conf_format = config_format.lower()
    if conf_format == 'boto':  # pragma: no cover
        if profile is not None and 'profile ' + profile in sections:
            cred_section = 'profile ' + profile
        else:
            cred_section = 'Credentials'
    elif conf_format == 'aws' and profile is not None:
        cred_section = profile
    else:
        cred_section = 'default'
    # Option names
    if conf_format in ('boto', 'aws'):  # pragma: no cover
        key_id_option = 'aws_access_key_id'
        secret_key_option = 'aws_secret_access_key'
        # security_token_option = 'aws_security_token'
    else:
        key_id_option = 'access_key'
        secret_key_option = 'secret_key'
    # Actual Parsing
    if cred_section not in sections:
        raise AirflowException(""This config file format is not recognized"")
    else:
        try:
            access_key = config.get(cred_section, key_id_option)
            secret_key = config.get(cred_section, secret_key_option)
        except Exception:
            logging.warning(""Option Error in parsing s3 config file"")
            raise
        return access_key, secret_key",def,_parse_s3_config,(,config_file_name,",",config_format,=,'boto',",",profile,=,None,),:,config,=,configparser,.,ConfigParser,(,),if,config,.,read,(,config_file_name,),:,# pragma: no cover,sections,=,config,.,sections,(,),else,:,raise,AirflowException,(,"""Couldn't read {0}""",.,format,(,config_file_name,),),# Setting option names depending on file format,if,config_format,"Parses a config file for s3 credentials. Can currently
    parse boto, s3cmd.conf and AWS SDK config formats

    :param config_file_name: path to the config file
    :type config_file_name: str
    :param config_format: config type. One of ""boto"", ""s3cmd"" or ""aws"".
        Defaults to ""boto""
    :type config_format: str
    :param profile: profile name in AWS type config file
    :type profile: str",Parses,a,config,file,for,s3,credentials,.,Can,currently,parse,boto,s3cmd,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_hook.py#L28-L77,test,is,None,:,config_format,=,'boto',conf_format,=,config_format,.,lower,(,),if,conf_format,==,'boto',:,# pragma: no cover,if,profile,is,not,None,and,'profile ',+,profile,in,sections,:,cred_section,=,'profile ',+,profile,else,:,cred_section,=,'Credentials',elif,conf_format,==,'aws',and,profile,is,not,None,:,cred_section,=,profile,else,:,cred_section,=,'default',# Option names,if,conf_format,in,(,'boto',",",'aws',),:,# pragma: no cover,key_id_option,=,'aws_access_key_id',secret_key_option,=,'aws_secret_access_key',# security_token_option = 'aws_security_token',else,:,key_id_option,=,'access_key',secret_key_option,=,'secret_key',# Actual Parsing,if,cred_section,not,in,sections,:,raise,AirflowException,(,"""This config file format is not recognized""",),else,:,try,:,access_key,=,config,.,get,(,cred_section,",",key_id_option,),secret_key,=,config,.,get,(,cred_section,",",secret_key_option,),except,Exception,:,logging,.,warning,(,"""Option Error in parsing s3 config file""",),raise,return,access_key,",",secret_key,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,conf,and,AWS,SDK,config,formats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_hook.py,AwsHook.get_credentials,"def get_credentials(self, region_name=None):
        """"""Get the underlying `botocore.Credentials` object.

        This contains the following authentication attributes: access_key, secret_key and token.
        """"""
        session, _ = self._get_credentials(region_name)
        # Credentials are refreshable, so accessing your access key and
        # secret key separately can lead to a race condition.
        # See https://stackoverflow.com/a/36291428/8283373
        return session.get_credentials().get_frozen_credentials()",python,"def get_credentials(self, region_name=None):
        """"""Get the underlying `botocore.Credentials` object.

        This contains the following authentication attributes: access_key, secret_key and token.
        """"""
        session, _ = self._get_credentials(region_name)
        # Credentials are refreshable, so accessing your access key and
        # secret key separately can lead to a race condition.
        # See https://stackoverflow.com/a/36291428/8283373
        return session.get_credentials().get_frozen_credentials()",def,get_credentials,(,self,",",region_name,=,None,),:,session,",",_,=,self,.,_get_credentials,(,region_name,),"# Credentials are refreshable, so accessing your access key and",# secret key separately can lead to a race condition.,# See https://stackoverflow.com/a/36291428/8283373,return,session,.,get_credentials,(,),.,get_frozen_credentials,(,),,,,,,,,,,,,,,,,,,,,"Get the underlying `botocore.Credentials` object.

        This contains the following authentication attributes: access_key, secret_key and token.",Get,the,underlying,botocore,.,Credentials,object,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_hook.py#L183-L192,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/vertica_hook.py,VerticaHook.get_conn,"def get_conn(self):
        """"""
        Returns verticaql connection object
        """"""
        conn = self.get_connection(self.vertica_conn_id)
        conn_config = {
            ""user"": conn.login,
            ""password"": conn.password or '',
            ""database"": conn.schema,
            ""host"": conn.host or 'localhost'
        }

        if not conn.port:
            conn_config[""port""] = 5433
        else:
            conn_config[""port""] = int(conn.port)

        conn = connect(**conn_config)
        return conn",python,"def get_conn(self):
        """"""
        Returns verticaql connection object
        """"""
        conn = self.get_connection(self.vertica_conn_id)
        conn_config = {
            ""user"": conn.login,
            ""password"": conn.password or '',
            ""database"": conn.schema,
            ""host"": conn.host or 'localhost'
        }

        if not conn.port:
            conn_config[""port""] = 5433
        else:
            conn_config[""port""] = int(conn.port)

        conn = connect(**conn_config)
        return conn",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,vertica_conn_id,),conn_config,=,{,"""user""",:,conn,.,login,",","""password""",:,conn,.,password,or,'',",","""database""",:,conn,.,schema,",","""host""",:,conn,.,host,or,'localhost',},if,not,conn,.,port,Returns verticaql connection object,Returns,verticaql,connection,object,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/vertica_hook.py#L35-L53,test,:,conn_config,[,"""port""",],=,5433,else,:,conn_config,[,"""port""",],=,int,(,conn,.,port,),conn,=,connect,(,*,*,conn_config,),return,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/log/logging_mixin.py,StreamLogWriter.flush,"def flush(self):
        """"""
        Ensure all logging output has been flushed
        """"""
        if len(self._buffer) > 0:
            self.logger.log(self.level, self._buffer)
            self._buffer = str()",python,"def flush(self):
        """"""
        Ensure all logging output has been flushed
        """"""
        if len(self._buffer) > 0:
            self.logger.log(self.level, self._buffer)
            self._buffer = str()",def,flush,(,self,),:,if,len,(,self,.,_buffer,),>,0,:,self,.,logger,.,log,(,self,.,level,",",self,.,_buffer,),self,.,_buffer,=,str,(,),,,,,,,,,,,,,,,,Ensure all logging output has been flushed,Ensure,all,logging,output,has,been,flushed,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/logging_mixin.py#L104-L110,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,correct_maybe_zipped,"def correct_maybe_zipped(fileloc):
    """"""
    If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive and path to zip is returned.
    """"""

    _, archive, filename = re.search(
        r'((.*\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()
    if archive and zipfile.is_zipfile(archive):
        return archive
    else:
        return fileloc",python,"def correct_maybe_zipped(fileloc):
    """"""
    If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive and path to zip is returned.
    """"""

    _, archive, filename = re.search(
        r'((.*\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()
    if archive and zipfile.is_zipfile(archive):
        return archive
    else:
        return fileloc",def,correct_maybe_zipped,(,fileloc,),:,_,",",archive,",",filename,=,re,.,search,(,r'((.*\.zip){})?(.*)',.,format,(,re,.,escape,(,os,.,sep,),),",",fileloc,),.,groups,(,),if,archive,and,zipfile,.,is_zipfile,(,archive,),:,return,archive,else,:,return,fileloc,"If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive and path to zip is returned.",If,the,path,contains,a,folder,with,a,.,zip,suffix,then,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L275-L286,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,folder,is,treated,as,a,zip,archive,and,path,to,zip,is,returned,.,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,list_py_file_paths,"def list_py_file_paths(directory, safe_mode=True,
                       include_examples=None):
    """"""
    Traverse a directory and look for Python files.

    :param directory: the directory to traverse
    :type directory: unicode
    :param safe_mode: whether to use a heuristic to determine whether a file
        contains Airflow DAG definitions
    :return: a list of paths to Python files in the specified directory
    :rtype: list[unicode]
    """"""
    if include_examples is None:
        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')
    file_paths = []
    if directory is None:
        return []
    elif os.path.isfile(directory):
        return [directory]
    elif os.path.isdir(directory):
        patterns_by_dir = {}
        for root, dirs, files in os.walk(directory, followlinks=True):
            patterns = patterns_by_dir.get(root, [])
            ignore_file = os.path.join(root, '.airflowignore')
            if os.path.isfile(ignore_file):
                with open(ignore_file, 'r') as f:
                    # If we have new patterns create a copy so we don't change
                    # the previous list (which would affect other subdirs)
                    patterns += [re.compile(p) for p in f.read().split('\n') if p]

            # If we can ignore any subdirs entirely we should - fewer paths
            # to walk is better. We have to modify the ``dirs`` array in
            # place for this to affect os.walk
            dirs[:] = [
                d
                for d in dirs
                if not any(p.search(os.path.join(root, d)) for p in patterns)
            ]

            # We want patterns defined in a parent folder's .airflowignore to
            # apply to subdirs too
            for d in dirs:
                patterns_by_dir[os.path.join(root, d)] = patterns

            for f in files:
                try:
                    file_path = os.path.join(root, f)
                    if not os.path.isfile(file_path):
                        continue
                    mod_name, file_ext = os.path.splitext(
                        os.path.split(file_path)[-1])
                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):
                        continue
                    if any([re.findall(p, file_path) for p in patterns]):
                        continue

                    # Heuristic that guesses whether a Python file contains an
                    # Airflow DAG definition.
                    might_contain_dag = True
                    if safe_mode and not zipfile.is_zipfile(file_path):
                        with open(file_path, 'rb') as fp:
                            content = fp.read()
                            might_contain_dag = all(
                                [s in content for s in (b'DAG', b'airflow')])

                    if not might_contain_dag:
                        continue

                    file_paths.append(file_path)
                except Exception:
                    log = LoggingMixin().log
                    log.exception(""Error while examining %s"", f)
    if include_examples:
        import airflow.example_dags
        example_dag_folder = airflow.example_dags.__path__[0]
        file_paths.extend(list_py_file_paths(example_dag_folder, safe_mode, False))
    return file_paths",python,"def list_py_file_paths(directory, safe_mode=True,
                       include_examples=None):
    """"""
    Traverse a directory and look for Python files.

    :param directory: the directory to traverse
    :type directory: unicode
    :param safe_mode: whether to use a heuristic to determine whether a file
        contains Airflow DAG definitions
    :return: a list of paths to Python files in the specified directory
    :rtype: list[unicode]
    """"""
    if include_examples is None:
        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')
    file_paths = []
    if directory is None:
        return []
    elif os.path.isfile(directory):
        return [directory]
    elif os.path.isdir(directory):
        patterns_by_dir = {}
        for root, dirs, files in os.walk(directory, followlinks=True):
            patterns = patterns_by_dir.get(root, [])
            ignore_file = os.path.join(root, '.airflowignore')
            if os.path.isfile(ignore_file):
                with open(ignore_file, 'r') as f:
                    # If we have new patterns create a copy so we don't change
                    # the previous list (which would affect other subdirs)
                    patterns += [re.compile(p) for p in f.read().split('\n') if p]

            # If we can ignore any subdirs entirely we should - fewer paths
            # to walk is better. We have to modify the ``dirs`` array in
            # place for this to affect os.walk
            dirs[:] = [
                d
                for d in dirs
                if not any(p.search(os.path.join(root, d)) for p in patterns)
            ]

            # We want patterns defined in a parent folder's .airflowignore to
            # apply to subdirs too
            for d in dirs:
                patterns_by_dir[os.path.join(root, d)] = patterns

            for f in files:
                try:
                    file_path = os.path.join(root, f)
                    if not os.path.isfile(file_path):
                        continue
                    mod_name, file_ext = os.path.splitext(
                        os.path.split(file_path)[-1])
                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):
                        continue
                    if any([re.findall(p, file_path) for p in patterns]):
                        continue

                    # Heuristic that guesses whether a Python file contains an
                    # Airflow DAG definition.
                    might_contain_dag = True
                    if safe_mode and not zipfile.is_zipfile(file_path):
                        with open(file_path, 'rb') as fp:
                            content = fp.read()
                            might_contain_dag = all(
                                [s in content for s in (b'DAG', b'airflow')])

                    if not might_contain_dag:
                        continue

                    file_paths.append(file_path)
                except Exception:
                    log = LoggingMixin().log
                    log.exception(""Error while examining %s"", f)
    if include_examples:
        import airflow.example_dags
        example_dag_folder = airflow.example_dags.__path__[0]
        file_paths.extend(list_py_file_paths(example_dag_folder, safe_mode, False))
    return file_paths",def,list_py_file_paths,(,directory,",",safe_mode,=,True,",",include_examples,=,None,),:,if,include_examples,is,None,:,include_examples,=,conf,.,getboolean,(,'core',",",'LOAD_EXAMPLES',),file_paths,=,[,],if,directory,is,None,:,return,[,],elif,os,.,path,.,isfile,(,directory,),:,return,"Traverse a directory and look for Python files.

    :param directory: the directory to traverse
    :type directory: unicode
    :param safe_mode: whether to use a heuristic to determine whether a file
        contains Airflow DAG definitions
    :return: a list of paths to Python files in the specified directory
    :rtype: list[unicode]",Traverse,a,directory,and,look,for,Python,files,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L289-L365,test,[,directory,],elif,os,.,path,.,isdir,(,directory,),:,patterns_by_dir,=,{,},for,root,",",dirs,",",files,in,os,.,walk,(,directory,",",followlinks,=,True,),:,patterns,=,patterns_by_dir,.,get,(,root,",",[,],),ignore_file,=,os,.,path,.,join,(,root,",",'.airflowignore',),if,os,.,path,.,isfile,(,ignore_file,),:,with,open,(,ignore_file,",",'r',),as,f,:,# If we have new patterns create a copy so we don't change,# the previous list (which would affect other subdirs),patterns,+=,[,re,.,compile,(,p,),for,p,in,f,.,read,(,),.,split,(,'\n',),if,p,],# If we can ignore any subdirs entirely we should - fewer paths,# to walk is better. We have to modify the ``dirs`` array in,# place for this to affect os.walk,dirs,[,:,],=,[,d,for,d,in,dirs,if,not,any,(,p,.,search,(,os,.,path,.,join,(,root,",",d,),),for,p,in,patterns,),],# We want patterns defined in a parent folder's .airflowignore to,# apply to subdirs too,for,d,in,dirs,:,patterns_by_dir,[,os,.,path,.,join,(,root,",",d,),],=,patterns,for,f,in,files,:,try,:,file_path,=,os,.,path,.,join,(,root,",",f,),if,not,os,.,path,.,isfile,(,file_path,),:,continue,mod_name,",",file_ext,=,os,.,path,.,splitext,(,os,.,path,.,split,(,file_path,),[,-,1,],),if,file_ext,!=,'.py',and,not,zipfile,.,is_zipfile,(,file_path,),:,continue,if,any,(,[,re,.,findall,(,p,",",file_path,),for,p,in,patterns,],),:,continue,# Heuristic that guesses whether a Python file contains an,# Airflow DAG definition.,might_contain_dag,=,True,if,safe_mode,and,not,zipfile,.,is_zipfile,(,file_path,),:,with,open,(,file_path,",",'rb',),as,fp,:,content,=,fp,.,read,(,),might_contain_dag,=,all,(,[,s,in,content,for,s,in,(,b'DAG',",",b'airflow',),],),if,not,might_contain_dag,:,continue,file_paths,.,append,(,file_path,),except,Exception,:,log,=,LoggingMixin,(,),.,log,log,.,exception,(,"""Error while examining %s""",",",f,),if,include_examples,:,import,airflow,.,example_dags,example_dag_folder,=,airflow,.,example_dags,.,__path__,[,0,],file_paths,.,extend,(,list_py_file_paths,(,example_dag_folder,",",safe_mode,",",False,),),return,file_paths,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,SimpleTaskInstance.construct_task_instance,"def construct_task_instance(self, session=None, lock_for_update=False):
        """"""
        Construct a TaskInstance from the database based on the primary key

        :param session: DB session.
        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.
        """"""
        TI = airflow.models.TaskInstance

        qry = session.query(TI).filter(
            TI.dag_id == self._dag_id,
            TI.task_id == self._task_id,
            TI.execution_date == self._execution_date)

        if lock_for_update:
            ti = qry.with_for_update().first()
        else:
            ti = qry.first()
        return ti",python,"def construct_task_instance(self, session=None, lock_for_update=False):
        """"""
        Construct a TaskInstance from the database based on the primary key

        :param session: DB session.
        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.
        """"""
        TI = airflow.models.TaskInstance

        qry = session.query(TI).filter(
            TI.dag_id == self._dag_id,
            TI.task_id == self._task_id,
            TI.execution_date == self._execution_date)

        if lock_for_update:
            ti = qry.with_for_update().first()
        else:
            ti = qry.first()
        return ti",def,construct_task_instance,(,self,",",session,=,None,",",lock_for_update,=,False,),:,TI,=,airflow,.,models,.,TaskInstance,qry,=,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,==,self,.,_dag_id,",",TI,.,task_id,==,self,.,_task_id,",",TI,.,execution_date,==,"Construct a TaskInstance from the database based on the primary key

        :param session: DB session.
        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.",Construct,a,TaskInstance,from,the,database,based,on,the,primary,key,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L213-L233,test,self,.,_execution_date,),if,lock_for_update,:,ti,=,qry,.,with_for_update,(,),.,first,(,),else,:,ti,=,qry,.,first,(,),return,ti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorAgent.start,"def start(self):
        """"""
        Launch DagFileProcessorManager processor and start DAG parsing loop in manager.
        """"""
        self._process = self._launch_process(self._dag_directory,
                                             self._file_paths,
                                             self._max_runs,
                                             self._processor_factory,
                                             self._child_signal_conn,
                                             self._stat_queue,
                                             self._result_queue,
                                             self._async_mode)
        self.log.info(""Launched DagFileProcessorManager with pid: %s"", self._process.pid)",python,"def start(self):
        """"""
        Launch DagFileProcessorManager processor and start DAG parsing loop in manager.
        """"""
        self._process = self._launch_process(self._dag_directory,
                                             self._file_paths,
                                             self._max_runs,
                                             self._processor_factory,
                                             self._child_signal_conn,
                                             self._stat_queue,
                                             self._result_queue,
                                             self._async_mode)
        self.log.info(""Launched DagFileProcessorManager with pid: %s"", self._process.pid)",def,start,(,self,),:,self,.,_process,=,self,.,_launch_process,(,self,.,_dag_directory,",",self,.,_file_paths,",",self,.,_max_runs,",",self,.,_processor_factory,",",self,.,_child_signal_conn,",",self,.,_stat_queue,",",self,.,_result_queue,",",self,.,_async_mode,),self,.,log,.,info,(,Launch DagFileProcessorManager processor and start DAG parsing loop in manager.,Launch,DagFileProcessorManager,processor,and,start,DAG,parsing,loop,in,manager,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L512-L524,test,"""Launched DagFileProcessorManager with pid: %s""",",",self,.,_process,.,pid,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorAgent.terminate,"def terminate(self):
        """"""
        Send termination signal to DAG parsing processor manager
        and expect it to terminate all DAG file processors.
        """"""
        self.log.info(""Sending termination message to manager."")
        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)",python,"def terminate(self):
        """"""
        Send termination signal to DAG parsing processor manager
        and expect it to terminate all DAG file processors.
        """"""
        self.log.info(""Sending termination message to manager."")
        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)",def,terminate,(,self,),:,self,.,log,.,info,(,"""Sending termination message to manager.""",),self,.,_child_signal_conn,.,send,(,DagParsingSignal,.,TERMINATE_MANAGER,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Send termination signal to DAG parsing processor manager
        and expect it to terminate all DAG file processors.",Send,termination,signal,to,DAG,parsing,processor,manager,and,expect,it,to,terminate,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L637-L643,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,all,DAG,file,processors,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager._exit_gracefully,"def _exit_gracefully(self, signum, frame):
        """"""
        Helper method to clean up DAG file processors to avoid leaving orphan processes.
        """"""
        self.log.info(""Exiting gracefully upon receiving signal %s"", signum)
        self.terminate()
        self.end()
        self.log.debug(""Finished terminating DAG processors."")
        sys.exit(os.EX_OK)",python,"def _exit_gracefully(self, signum, frame):
        """"""
        Helper method to clean up DAG file processors to avoid leaving orphan processes.
        """"""
        self.log.info(""Exiting gracefully upon receiving signal %s"", signum)
        self.terminate()
        self.end()
        self.log.debug(""Finished terminating DAG processors."")
        sys.exit(os.EX_OK)",def,_exit_gracefully,(,self,",",signum,",",frame,),:,self,.,log,.,info,(,"""Exiting gracefully upon receiving signal %s""",",",signum,),self,.,terminate,(,),self,.,end,(,),self,.,log,.,debug,(,"""Finished terminating DAG processors.""",),sys,.,exit,(,os,.,EX_OK,),,,,,,,Helper method to clean up DAG file processors to avoid leaving orphan processes.,Helper,method,to,clean,up,DAG,file,processors,to,avoid,leaving,orphan,processes,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L779-L787,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.start,"def start(self):
        """"""
        Use multiple processes to parse and generate tasks for the
        DAGs in parallel. By processing them in separate processes,
        we can get parallelism and isolation from potentially harmful
        user code.
        """"""

        self.log.info(""Processing files using up to %s processes at a time "", self._parallelism)
        self.log.info(""Process each file at most once every %s seconds"", self._file_process_interval)
        self.log.info(
            ""Checking for new files in %s every %s seconds"", self._dag_directory, self.dag_dir_list_interval
        )

        if self._async_mode:
            self.log.debug(""Starting DagFileProcessorManager in async mode"")
            self.start_in_async()
        else:
            self.log.debug(""Starting DagFileProcessorManager in sync mode"")
            self.start_in_sync()",python,"def start(self):
        """"""
        Use multiple processes to parse and generate tasks for the
        DAGs in parallel. By processing them in separate processes,
        we can get parallelism and isolation from potentially harmful
        user code.
        """"""

        self.log.info(""Processing files using up to %s processes at a time "", self._parallelism)
        self.log.info(""Process each file at most once every %s seconds"", self._file_process_interval)
        self.log.info(
            ""Checking for new files in %s every %s seconds"", self._dag_directory, self.dag_dir_list_interval
        )

        if self._async_mode:
            self.log.debug(""Starting DagFileProcessorManager in async mode"")
            self.start_in_async()
        else:
            self.log.debug(""Starting DagFileProcessorManager in sync mode"")
            self.start_in_sync()",def,start,(,self,),:,self,.,log,.,info,(,"""Processing files using up to %s processes at a time """,",",self,.,_parallelism,),self,.,log,.,info,(,"""Process each file at most once every %s seconds""",",",self,.,_file_process_interval,),self,.,log,.,info,(,"""Checking for new files in %s every %s seconds""",",",self,.,_dag_directory,",",self,.,dag_dir_list_interval,),if,self,.,_async_mode,:,self,"Use multiple processes to parse and generate tasks for the
        DAGs in parallel. By processing them in separate processes,
        we can get parallelism and isolation from potentially harmful
        user code.",Use,multiple,processes,to,parse,and,generate,tasks,for,the,DAGs,in,parallel,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L789-L808,test,.,log,.,debug,(,"""Starting DagFileProcessorManager in async mode""",),self,.,start_in_async,(,),else,:,self,.,log,.,debug,(,"""Starting DagFileProcessorManager in sync mode""",),self,.,start_in_sync,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,By,processing,them,in,separate,processes,we,can,get,parallelism,and,isolation,from,potentially,harmful,user,code,.,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.start_in_async,"def start_in_async(self):
        """"""
        Parse DAG files repeatedly in a standalone loop.
        """"""
        while True:
            loop_start_time = time.time()

            if self._signal_conn.poll():
                agent_signal = self._signal_conn.recv()
                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:
                    self.terminate()
                    break
                elif agent_signal == DagParsingSignal.END_MANAGER:
                    self.end()
                    sys.exit(os.EX_OK)

            self._refresh_dag_dir()

            simple_dags = self.heartbeat()
            for simple_dag in simple_dags:
                self._result_queue.put(simple_dag)

            self._print_stat()

            all_files_processed = all(self.get_last_finish_time(x) is not None
                                      for x in self.file_paths)
            max_runs_reached = self.max_runs_reached()

            dag_parsing_stat = DagParsingStat(self._file_paths,
                                              self.get_all_pids(),
                                              max_runs_reached,
                                              all_files_processed,
                                              len(simple_dags))
            self._stat_queue.put(dag_parsing_stat)

            if max_runs_reached:
                self.log.info(""Exiting dag parsing loop as all files ""
                              ""have been processed %s times"", self._max_runs)
                break

            loop_duration = time.time() - loop_start_time
            if loop_duration < 1:
                sleep_length = 1 - loop_duration
                self.log.debug(""Sleeping for %.2f seconds to prevent excessive logging"", sleep_length)
                time.sleep(sleep_length)",python,"def start_in_async(self):
        """"""
        Parse DAG files repeatedly in a standalone loop.
        """"""
        while True:
            loop_start_time = time.time()

            if self._signal_conn.poll():
                agent_signal = self._signal_conn.recv()
                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:
                    self.terminate()
                    break
                elif agent_signal == DagParsingSignal.END_MANAGER:
                    self.end()
                    sys.exit(os.EX_OK)

            self._refresh_dag_dir()

            simple_dags = self.heartbeat()
            for simple_dag in simple_dags:
                self._result_queue.put(simple_dag)

            self._print_stat()

            all_files_processed = all(self.get_last_finish_time(x) is not None
                                      for x in self.file_paths)
            max_runs_reached = self.max_runs_reached()

            dag_parsing_stat = DagParsingStat(self._file_paths,
                                              self.get_all_pids(),
                                              max_runs_reached,
                                              all_files_processed,
                                              len(simple_dags))
            self._stat_queue.put(dag_parsing_stat)

            if max_runs_reached:
                self.log.info(""Exiting dag parsing loop as all files ""
                              ""have been processed %s times"", self._max_runs)
                break

            loop_duration = time.time() - loop_start_time
            if loop_duration < 1:
                sleep_length = 1 - loop_duration
                self.log.debug(""Sleeping for %.2f seconds to prevent excessive logging"", sleep_length)
                time.sleep(sleep_length)",def,start_in_async,(,self,),:,while,True,:,loop_start_time,=,time,.,time,(,),if,self,.,_signal_conn,.,poll,(,),:,agent_signal,=,self,.,_signal_conn,.,recv,(,),if,agent_signal,==,DagParsingSignal,.,TERMINATE_MANAGER,:,self,.,terminate,(,),break,elif,agent_signal,==,DagParsingSignal,.,Parse DAG files repeatedly in a standalone loop.,Parse,DAG,files,repeatedly,in,a,standalone,loop,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L810-L854,test,END_MANAGER,:,self,.,end,(,),sys,.,exit,(,os,.,EX_OK,),self,.,_refresh_dag_dir,(,),simple_dags,=,self,.,heartbeat,(,),for,simple_dag,in,simple_dags,:,self,.,_result_queue,.,put,(,simple_dag,),self,.,_print_stat,(,),all_files_processed,=,all,(,self,.,get_last_finish_time,(,x,),is,not,None,for,x,in,self,.,file_paths,),max_runs_reached,=,self,.,max_runs_reached,(,),dag_parsing_stat,=,DagParsingStat,(,self,.,_file_paths,",",self,.,get_all_pids,(,),",",max_runs_reached,",",all_files_processed,",",len,(,simple_dags,),),self,.,_stat_queue,.,put,(,dag_parsing_stat,),if,max_runs_reached,:,self,.,log,.,info,(,"""Exiting dag parsing loop as all files ""","""have been processed %s times""",",",self,.,_max_runs,),break,loop_duration,=,time,.,time,(,),-,loop_start_time,if,loop_duration,<,1,:,sleep_length,=,1,-,loop_duration,self,.,log,.,debug,(,"""Sleeping for %.2f seconds to prevent excessive logging""",",",sleep_length,),time,.,sleep,(,sleep_length,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.start_in_sync,"def start_in_sync(self):
        """"""
        Parse DAG files in a loop controlled by DagParsingSignal.
        Actual DAG parsing loop will run once upon receiving one
        agent heartbeat message and will report done when finished the loop.
        """"""
        while True:
            agent_signal = self._signal_conn.recv()
            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:
                self.terminate()
                break
            elif agent_signal == DagParsingSignal.END_MANAGER:
                self.end()
                sys.exit(os.EX_OK)
            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:

                self._refresh_dag_dir()

                simple_dags = self.heartbeat()
                for simple_dag in simple_dags:
                    self._result_queue.put(simple_dag)

                self._print_stat()

                all_files_processed = all(self.get_last_finish_time(x) is not None
                                          for x in self.file_paths)
                max_runs_reached = self.max_runs_reached()

                dag_parsing_stat = DagParsingStat(self._file_paths,
                                                  self.get_all_pids(),
                                                  self.max_runs_reached(),
                                                  all_files_processed,
                                                  len(simple_dags))
                self._stat_queue.put(dag_parsing_stat)

                self.wait_until_finished()
                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)

                if max_runs_reached:
                    self.log.info(""Exiting dag parsing loop as all files ""
                                  ""have been processed %s times"", self._max_runs)
                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)
                    break",python,"def start_in_sync(self):
        """"""
        Parse DAG files in a loop controlled by DagParsingSignal.
        Actual DAG parsing loop will run once upon receiving one
        agent heartbeat message and will report done when finished the loop.
        """"""
        while True:
            agent_signal = self._signal_conn.recv()
            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:
                self.terminate()
                break
            elif agent_signal == DagParsingSignal.END_MANAGER:
                self.end()
                sys.exit(os.EX_OK)
            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:

                self._refresh_dag_dir()

                simple_dags = self.heartbeat()
                for simple_dag in simple_dags:
                    self._result_queue.put(simple_dag)

                self._print_stat()

                all_files_processed = all(self.get_last_finish_time(x) is not None
                                          for x in self.file_paths)
                max_runs_reached = self.max_runs_reached()

                dag_parsing_stat = DagParsingStat(self._file_paths,
                                                  self.get_all_pids(),
                                                  self.max_runs_reached(),
                                                  all_files_processed,
                                                  len(simple_dags))
                self._stat_queue.put(dag_parsing_stat)

                self.wait_until_finished()
                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)

                if max_runs_reached:
                    self.log.info(""Exiting dag parsing loop as all files ""
                                  ""have been processed %s times"", self._max_runs)
                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)
                    break",def,start_in_sync,(,self,),:,while,True,:,agent_signal,=,self,.,_signal_conn,.,recv,(,),if,agent_signal,==,DagParsingSignal,.,TERMINATE_MANAGER,:,self,.,terminate,(,),break,elif,agent_signal,==,DagParsingSignal,.,END_MANAGER,:,self,.,end,(,),sys,.,exit,(,os,.,EX_OK,),elif,"Parse DAG files in a loop controlled by DagParsingSignal.
        Actual DAG parsing loop will run once upon receiving one
        agent heartbeat message and will report done when finished the loop.",Parse,DAG,files,in,a,loop,controlled,by,DagParsingSignal,.,Actual,DAG,parsing,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L856-L898,test,agent_signal,==,DagParsingSignal,.,AGENT_HEARTBEAT,:,self,.,_refresh_dag_dir,(,),simple_dags,=,self,.,heartbeat,(,),for,simple_dag,in,simple_dags,:,self,.,_result_queue,.,put,(,simple_dag,),self,.,_print_stat,(,),all_files_processed,=,all,(,self,.,get_last_finish_time,(,x,),is,not,None,for,x,in,self,.,file_paths,),max_runs_reached,=,self,.,max_runs_reached,(,),dag_parsing_stat,=,DagParsingStat,(,self,.,_file_paths,",",self,.,get_all_pids,(,),",",self,.,max_runs_reached,(,),",",all_files_processed,",",len,(,simple_dags,),),self,.,_stat_queue,.,put,(,dag_parsing_stat,),self,.,wait_until_finished,(,),self,.,_signal_conn,.,send,(,DagParsingSignal,.,MANAGER_DONE,),if,max_runs_reached,:,self,.,log,.,info,(,"""Exiting dag parsing loop as all files ""","""have been processed %s times""",",",self,.,_max_runs,),self,.,_signal_conn,.,send,(,DagParsingSignal,.,MANAGER_DONE,),break,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,loop,will,run,once,upon,receiving,one,agent,heartbeat,message,and,will,report,done,when,finished,the,loop,.,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager._refresh_dag_dir,"def _refresh_dag_dir(self):
        """"""
        Refresh file paths from dag dir if we haven't done it for too long.
        """"""
        elapsed_time_since_refresh = (timezone.utcnow() -
                                      self.last_dag_dir_refresh_time).total_seconds()
        if elapsed_time_since_refresh > self.dag_dir_list_interval:
            # Build up a list of Python files that could contain DAGs
            self.log.info(""Searching for files in %s"", self._dag_directory)
            self._file_paths = list_py_file_paths(self._dag_directory)
            self.last_dag_dir_refresh_time = timezone.utcnow()
            self.log.info(""There are %s files in %s"", len(self._file_paths), self._dag_directory)
            self.set_file_paths(self._file_paths)

            try:
                self.log.debug(""Removing old import errors"")
                self.clear_nonexistent_import_errors()
            except Exception:
                self.log.exception(""Error removing old import errors"")",python,"def _refresh_dag_dir(self):
        """"""
        Refresh file paths from dag dir if we haven't done it for too long.
        """"""
        elapsed_time_since_refresh = (timezone.utcnow() -
                                      self.last_dag_dir_refresh_time).total_seconds()
        if elapsed_time_since_refresh > self.dag_dir_list_interval:
            # Build up a list of Python files that could contain DAGs
            self.log.info(""Searching for files in %s"", self._dag_directory)
            self._file_paths = list_py_file_paths(self._dag_directory)
            self.last_dag_dir_refresh_time = timezone.utcnow()
            self.log.info(""There are %s files in %s"", len(self._file_paths), self._dag_directory)
            self.set_file_paths(self._file_paths)

            try:
                self.log.debug(""Removing old import errors"")
                self.clear_nonexistent_import_errors()
            except Exception:
                self.log.exception(""Error removing old import errors"")",def,_refresh_dag_dir,(,self,),:,elapsed_time_since_refresh,=,(,timezone,.,utcnow,(,),-,self,.,last_dag_dir_refresh_time,),.,total_seconds,(,),if,elapsed_time_since_refresh,>,self,.,dag_dir_list_interval,:,# Build up a list of Python files that could contain DAGs,self,.,log,.,info,(,"""Searching for files in %s""",",",self,.,_dag_directory,),self,.,_file_paths,=,list_py_file_paths,(,self,.,_dag_directory,Refresh file paths from dag dir if we haven't done it for too long.,Refresh,file,paths,from,dag,dir,if,we,haven,t,done,it,for,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L900-L918,test,),self,.,last_dag_dir_refresh_time,=,timezone,.,utcnow,(,),self,.,log,.,info,(,"""There are %s files in %s""",",",len,(,self,.,_file_paths,),",",self,.,_dag_directory,),self,.,set_file_paths,(,self,.,_file_paths,),try,:,self,.,log,.,debug,(,"""Removing old import errors""",),self,.,clear_nonexistent_import_errors,(,),except,Exception,:,self,.,log,.,exception,(,"""Error removing old import errors""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,too,long,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager._print_stat,"def _print_stat(self):
        """"""
        Occasionally print out stats about how fast the files are getting processed
        """"""
        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >
                self.print_stats_interval):
            if len(self._file_paths) > 0:
                self._log_file_processing_stats(self._file_paths)
            self.last_stat_print_time = timezone.utcnow()",python,"def _print_stat(self):
        """"""
        Occasionally print out stats about how fast the files are getting processed
        """"""
        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >
                self.print_stats_interval):
            if len(self._file_paths) > 0:
                self._log_file_processing_stats(self._file_paths)
            self.last_stat_print_time = timezone.utcnow()",def,_print_stat,(,self,),:,if,(,(,timezone,.,utcnow,(,),-,self,.,last_stat_print_time,),.,total_seconds,(,),>,self,.,print_stats_interval,),:,if,len,(,self,.,_file_paths,),>,0,:,self,.,_log_file_processing_stats,(,self,.,_file_paths,),self,.,last_stat_print_time,=,timezone,Occasionally print out stats about how fast the files are getting processed,Occasionally,print,out,stats,about,how,fast,the,files,are,getting,processed,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L920-L928,test,.,utcnow,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.clear_nonexistent_import_errors,"def clear_nonexistent_import_errors(self, session):
        """"""
        Clears import errors for files that no longer exist.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session
        """"""
        query = session.query(errors.ImportError)
        if self._file_paths:
            query = query.filter(
                ~errors.ImportError.filename.in_(self._file_paths)
            )
        query.delete(synchronize_session='fetch')
        session.commit()",python,"def clear_nonexistent_import_errors(self, session):
        """"""
        Clears import errors for files that no longer exist.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session
        """"""
        query = session.query(errors.ImportError)
        if self._file_paths:
            query = query.filter(
                ~errors.ImportError.filename.in_(self._file_paths)
            )
        query.delete(synchronize_session='fetch')
        session.commit()",def,clear_nonexistent_import_errors,(,self,",",session,),:,query,=,session,.,query,(,errors,.,ImportError,),if,self,.,_file_paths,:,query,=,query,.,filter,(,~,errors,.,ImportError,.,filename,.,in_,(,self,.,_file_paths,),),query,.,delete,(,synchronize_session,=,'fetch',),session,"Clears import errors for files that no longer exist.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session",Clears,import,errors,for,files,that,no,longer,exist,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L931-L944,test,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager._log_file_processing_stats,"def _log_file_processing_stats(self, known_file_paths):
        """"""
        Print out stats about how files are getting processed.

        :param known_file_paths: a list of file paths that may contain Airflow
            DAG definitions
        :type known_file_paths: list[unicode]
        :return: None
        """"""

        # File Path: Path to the file containing the DAG definition
        # PID: PID associated with the process that's processing the file. May
        # be empty.
        # Runtime: If the process is currently running, how long it's been
        # running for in seconds.
        # Last Runtime: If the process ran before, how long did it take to
        # finish in seconds
        # Last Run: When the file finished processing in the previous run.
        headers = [""File Path"",
                   ""PID"",
                   ""Runtime"",
                   ""Last Runtime"",
                   ""Last Run""]

        rows = []
        for file_path in known_file_paths:
            last_runtime = self.get_last_runtime(file_path)
            file_name = os.path.basename(file_path)
            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')
            if last_runtime:
                Stats.gauge(
                    'dag_processing.last_runtime.{}'.format(file_name),
                    last_runtime
                )

            processor_pid = self.get_pid(file_path)
            processor_start_time = self.get_start_time(file_path)
            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()
                       if processor_start_time else None)
            last_run = self.get_last_finish_time(file_path)
            if last_run:
                seconds_ago = (timezone.utcnow() - last_run).total_seconds()
                Stats.gauge(
                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),
                    seconds_ago
                )

            rows.append((file_path,
                         processor_pid,
                         runtime,
                         last_runtime,
                         last_run))

        # Sort by longest last runtime. (Can't sort None values in python3)
        rows = sorted(rows, key=lambda x: x[3] or 0.0)

        formatted_rows = []
        for file_path, pid, runtime, last_runtime, last_run in rows:
            formatted_rows.append((file_path,
                                   pid,
                                   ""{:.2f}s"".format(runtime)
                                   if runtime else None,
                                   ""{:.2f}s"".format(last_runtime)
                                   if last_runtime else None,
                                   last_run.strftime(""%Y-%m-%dT%H:%M:%S"")
                                   if last_run else None))
        log_str = (""\n"" +
                   ""="" * 80 +
                   ""\n"" +
                   ""DAG File Processing Stats\n\n"" +
                   tabulate(formatted_rows, headers=headers) +
                   ""\n"" +
                   ""="" * 80)

        self.log.info(log_str)",python,"def _log_file_processing_stats(self, known_file_paths):
        """"""
        Print out stats about how files are getting processed.

        :param known_file_paths: a list of file paths that may contain Airflow
            DAG definitions
        :type known_file_paths: list[unicode]
        :return: None
        """"""

        # File Path: Path to the file containing the DAG definition
        # PID: PID associated with the process that's processing the file. May
        # be empty.
        # Runtime: If the process is currently running, how long it's been
        # running for in seconds.
        # Last Runtime: If the process ran before, how long did it take to
        # finish in seconds
        # Last Run: When the file finished processing in the previous run.
        headers = [""File Path"",
                   ""PID"",
                   ""Runtime"",
                   ""Last Runtime"",
                   ""Last Run""]

        rows = []
        for file_path in known_file_paths:
            last_runtime = self.get_last_runtime(file_path)
            file_name = os.path.basename(file_path)
            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')
            if last_runtime:
                Stats.gauge(
                    'dag_processing.last_runtime.{}'.format(file_name),
                    last_runtime
                )

            processor_pid = self.get_pid(file_path)
            processor_start_time = self.get_start_time(file_path)
            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()
                       if processor_start_time else None)
            last_run = self.get_last_finish_time(file_path)
            if last_run:
                seconds_ago = (timezone.utcnow() - last_run).total_seconds()
                Stats.gauge(
                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),
                    seconds_ago
                )

            rows.append((file_path,
                         processor_pid,
                         runtime,
                         last_runtime,
                         last_run))

        # Sort by longest last runtime. (Can't sort None values in python3)
        rows = sorted(rows, key=lambda x: x[3] or 0.0)

        formatted_rows = []
        for file_path, pid, runtime, last_runtime, last_run in rows:
            formatted_rows.append((file_path,
                                   pid,
                                   ""{:.2f}s"".format(runtime)
                                   if runtime else None,
                                   ""{:.2f}s"".format(last_runtime)
                                   if last_runtime else None,
                                   last_run.strftime(""%Y-%m-%dT%H:%M:%S"")
                                   if last_run else None))
        log_str = (""\n"" +
                   ""="" * 80 +
                   ""\n"" +
                   ""DAG File Processing Stats\n\n"" +
                   tabulate(formatted_rows, headers=headers) +
                   ""\n"" +
                   ""="" * 80)

        self.log.info(log_str)",def,_log_file_processing_stats,(,self,",",known_file_paths,),:,# File Path: Path to the file containing the DAG definition,# PID: PID associated with the process that's processing the file. May,# be empty.,"# Runtime: If the process is currently running, how long it's been",# running for in seconds.,"# Last Runtime: If the process ran before, how long did it take to",# finish in seconds,# Last Run: When the file finished processing in the previous run.,headers,=,[,"""File Path""",",","""PID""",",","""Runtime""",",","""Last Runtime""",",","""Last Run""",],rows,=,[,],for,file_path,in,known_file_paths,:,last_runtime,=,self,.,get_last_runtime,(,file_path,),file_name,=,os,.,path,.,"Print out stats about how files are getting processed.

        :param known_file_paths: a list of file paths that may contain Airflow
            DAG definitions
        :type known_file_paths: list[unicode]
        :return: None",Print,out,stats,about,how,files,are,getting,processed,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L946-L1020,test,basename,(,file_path,),file_name,=,os,.,path,.,splitext,(,file_name,),[,0,],.,replace,(,os,.,sep,",",'.',),if,last_runtime,:,Stats,.,gauge,(,'dag_processing.last_runtime.{}',.,format,(,file_name,),",",last_runtime,),processor_pid,=,self,.,get_pid,(,file_path,),processor_start_time,=,self,.,get_start_time,(,file_path,),runtime,=,(,(,timezone,.,utcnow,(,),-,processor_start_time,),.,total_seconds,(,),if,processor_start_time,else,None,),last_run,=,self,.,get_last_finish_time,(,file_path,),if,last_run,:,seconds_ago,=,(,timezone,.,utcnow,(,),-,last_run,),.,total_seconds,(,),Stats,.,gauge,(,'dag_processing.last_run.seconds_ago.{}',.,format,(,file_name,),",",seconds_ago,),rows,.,append,(,(,file_path,",",processor_pid,",",runtime,",",last_runtime,",",last_run,),),# Sort by longest last runtime. (Can't sort None values in python3),rows,=,sorted,(,rows,",",key,=,lambda,x,:,x,[,3,],or,0.0,),formatted_rows,=,[,],for,file_path,",",pid,",",runtime,",",last_runtime,",",last_run,in,rows,:,formatted_rows,.,append,(,(,file_path,",",pid,",","""{:.2f}s""",.,format,(,runtime,),if,runtime,else,None,",","""{:.2f}s""",.,format,(,last_runtime,),if,last_runtime,else,None,",",last_run,.,strftime,(,"""%Y-%m-%dT%H:%M:%S""",),if,last_run,else,None,),),log_str,=,(,"""\n""",+,"""=""",*,80,+,"""\n""",+,"""DAG File Processing Stats\n\n""",+,tabulate,(,formatted_rows,",",headers,=,headers,),+,"""\n""",+,"""=""",*,80,),self,.,log,.,info,(,log_str,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.set_file_paths,"def set_file_paths(self, new_file_paths):
        """"""
        Update this with a new set of paths to DAG definition files.

        :param new_file_paths: list of paths to DAG definition files
        :type new_file_paths: list[unicode]
        :return: None
        """"""
        self._file_paths = new_file_paths
        self._file_path_queue = [x for x in self._file_path_queue
                                 if x in new_file_paths]
        # Stop processors that are working on deleted files
        filtered_processors = {}
        for file_path, processor in self._processors.items():
            if file_path in new_file_paths:
                filtered_processors[file_path] = processor
            else:
                self.log.warning(""Stopping processor for %s"", file_path)
                processor.terminate()
        self._processors = filtered_processors",python,"def set_file_paths(self, new_file_paths):
        """"""
        Update this with a new set of paths to DAG definition files.

        :param new_file_paths: list of paths to DAG definition files
        :type new_file_paths: list[unicode]
        :return: None
        """"""
        self._file_paths = new_file_paths
        self._file_path_queue = [x for x in self._file_path_queue
                                 if x in new_file_paths]
        # Stop processors that are working on deleted files
        filtered_processors = {}
        for file_path, processor in self._processors.items():
            if file_path in new_file_paths:
                filtered_processors[file_path] = processor
            else:
                self.log.warning(""Stopping processor for %s"", file_path)
                processor.terminate()
        self._processors = filtered_processors",def,set_file_paths,(,self,",",new_file_paths,),:,self,.,_file_paths,=,new_file_paths,self,.,_file_path_queue,=,[,x,for,x,in,self,.,_file_path_queue,if,x,in,new_file_paths,],# Stop processors that are working on deleted files,filtered_processors,=,{,},for,file_path,",",processor,in,self,.,_processors,.,items,(,),:,if,file_path,in,new_file_paths,"Update this with a new set of paths to DAG definition files.

        :param new_file_paths: list of paths to DAG definition files
        :type new_file_paths: list[unicode]
        :return: None",Update,this,with,a,new,set,of,paths,to,DAG,definition,files,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1090-L1109,test,:,filtered_processors,[,file_path,],=,processor,else,:,self,.,log,.,warning,(,"""Stopping processor for %s""",",",file_path,),processor,.,terminate,(,),self,.,_processors,=,filtered_processors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.wait_until_finished,"def wait_until_finished(self):
        """"""
        Sleeps until all the processors are done.
        """"""
        for file_path, processor in self._processors.items():
            while not processor.done:
                time.sleep(0.1)",python,"def wait_until_finished(self):
        """"""
        Sleeps until all the processors are done.
        """"""
        for file_path, processor in self._processors.items():
            while not processor.done:
                time.sleep(0.1)",def,wait_until_finished,(,self,),:,for,file_path,",",processor,in,self,.,_processors,.,items,(,),:,while,not,processor,.,done,:,time,.,sleep,(,0.1,),,,,,,,,,,,,,,,,,,,,,,Sleeps until all the processors are done.,Sleeps,until,all,the,processors,are,done,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1118-L1124,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.heartbeat,"def heartbeat(self):
        """"""
        This should be periodically called by the manager loop. This method will
        kick off new processes to process DAG definition files and read the
        results from the finished processors.

        :return: a list of SimpleDags that were produced by processors that
            have finished since the last time this was called
        :rtype: list[airflow.utils.dag_processing.SimpleDag]
        """"""
        finished_processors = {}
        """""":type : dict[unicode, AbstractDagFileProcessor]""""""
        running_processors = {}
        """""":type : dict[unicode, AbstractDagFileProcessor]""""""

        for file_path, processor in self._processors.items():
            if processor.done:
                self.log.debug(""Processor for %s finished"", file_path)
                now = timezone.utcnow()
                finished_processors[file_path] = processor
                self._last_runtime[file_path] = (now -
                                                 processor.start_time).total_seconds()
                self._last_finish_time[file_path] = now
                self._run_count[file_path] += 1
            else:
                running_processors[file_path] = processor
        self._processors = running_processors

        self.log.debug(""%s/%s DAG parsing processes running"",
                       len(self._processors), self._parallelism)

        self.log.debug(""%s file paths queued for processing"",
                       len(self._file_path_queue))

        # Collect all the DAGs that were found in the processed files
        simple_dags = []
        for file_path, processor in finished_processors.items():
            if processor.result is None:
                self.log.warning(
                    ""Processor for %s exited with return code %s."",
                    processor.file_path, processor.exit_code
                )
            else:
                for simple_dag in processor.result:
                    simple_dags.append(simple_dag)

        # Generate more file paths to process if we processed all the files
        # already.
        if len(self._file_path_queue) == 0:
            # If the file path is already being processed, or if a file was
            # processed recently, wait until the next batch
            file_paths_in_progress = self._processors.keys()
            now = timezone.utcnow()
            file_paths_recently_processed = []
            for file_path in self._file_paths:
                last_finish_time = self.get_last_finish_time(file_path)
                if (last_finish_time is not None and
                    (now - last_finish_time).total_seconds() <
                        self._file_process_interval):
                    file_paths_recently_processed.append(file_path)

            files_paths_at_run_limit = [file_path
                                        for file_path, num_runs in self._run_count.items()
                                        if num_runs == self._max_runs]

            files_paths_to_queue = list(set(self._file_paths) -
                                        set(file_paths_in_progress) -
                                        set(file_paths_recently_processed) -
                                        set(files_paths_at_run_limit))

            for file_path, processor in self._processors.items():
                self.log.debug(
                    ""File path %s is still being processed (started: %s)"",
                    processor.file_path, processor.start_time.isoformat()
                )

            self.log.debug(
                ""Queuing the following files for processing:\n\t%s"",
                ""\n\t"".join(files_paths_to_queue)
            )

            self._file_path_queue.extend(files_paths_to_queue)

        zombies = self._find_zombies()

        # Start more processors if we have enough slots and files to process
        while (self._parallelism - len(self._processors) > 0 and
               len(self._file_path_queue) > 0):
            file_path = self._file_path_queue.pop(0)
            processor = self._processor_factory(file_path, zombies)

            processor.start()
            self.log.debug(
                ""Started a process (PID: %s) to generate tasks for %s"",
                processor.pid, file_path
            )
            self._processors[file_path] = processor

        # Update heartbeat count.
        self._run_count[self._heart_beat_key] += 1

        return simple_dags",python,"def heartbeat(self):
        """"""
        This should be periodically called by the manager loop. This method will
        kick off new processes to process DAG definition files and read the
        results from the finished processors.

        :return: a list of SimpleDags that were produced by processors that
            have finished since the last time this was called
        :rtype: list[airflow.utils.dag_processing.SimpleDag]
        """"""
        finished_processors = {}
        """""":type : dict[unicode, AbstractDagFileProcessor]""""""
        running_processors = {}
        """""":type : dict[unicode, AbstractDagFileProcessor]""""""

        for file_path, processor in self._processors.items():
            if processor.done:
                self.log.debug(""Processor for %s finished"", file_path)
                now = timezone.utcnow()
                finished_processors[file_path] = processor
                self._last_runtime[file_path] = (now -
                                                 processor.start_time).total_seconds()
                self._last_finish_time[file_path] = now
                self._run_count[file_path] += 1
            else:
                running_processors[file_path] = processor
        self._processors = running_processors

        self.log.debug(""%s/%s DAG parsing processes running"",
                       len(self._processors), self._parallelism)

        self.log.debug(""%s file paths queued for processing"",
                       len(self._file_path_queue))

        # Collect all the DAGs that were found in the processed files
        simple_dags = []
        for file_path, processor in finished_processors.items():
            if processor.result is None:
                self.log.warning(
                    ""Processor for %s exited with return code %s."",
                    processor.file_path, processor.exit_code
                )
            else:
                for simple_dag in processor.result:
                    simple_dags.append(simple_dag)

        # Generate more file paths to process if we processed all the files
        # already.
        if len(self._file_path_queue) == 0:
            # If the file path is already being processed, or if a file was
            # processed recently, wait until the next batch
            file_paths_in_progress = self._processors.keys()
            now = timezone.utcnow()
            file_paths_recently_processed = []
            for file_path in self._file_paths:
                last_finish_time = self.get_last_finish_time(file_path)
                if (last_finish_time is not None and
                    (now - last_finish_time).total_seconds() <
                        self._file_process_interval):
                    file_paths_recently_processed.append(file_path)

            files_paths_at_run_limit = [file_path
                                        for file_path, num_runs in self._run_count.items()
                                        if num_runs == self._max_runs]

            files_paths_to_queue = list(set(self._file_paths) -
                                        set(file_paths_in_progress) -
                                        set(file_paths_recently_processed) -
                                        set(files_paths_at_run_limit))

            for file_path, processor in self._processors.items():
                self.log.debug(
                    ""File path %s is still being processed (started: %s)"",
                    processor.file_path, processor.start_time.isoformat()
                )

            self.log.debug(
                ""Queuing the following files for processing:\n\t%s"",
                ""\n\t"".join(files_paths_to_queue)
            )

            self._file_path_queue.extend(files_paths_to_queue)

        zombies = self._find_zombies()

        # Start more processors if we have enough slots and files to process
        while (self._parallelism - len(self._processors) > 0 and
               len(self._file_path_queue) > 0):
            file_path = self._file_path_queue.pop(0)
            processor = self._processor_factory(file_path, zombies)

            processor.start()
            self.log.debug(
                ""Started a process (PID: %s) to generate tasks for %s"",
                processor.pid, file_path
            )
            self._processors[file_path] = processor

        # Update heartbeat count.
        self._run_count[self._heart_beat_key] += 1

        return simple_dags",def,heartbeat,(,self,),:,finished_processors,=,{,},""""""":type : dict[unicode, AbstractDagFileProcessor]""""""",running_processors,=,{,},""""""":type : dict[unicode, AbstractDagFileProcessor]""""""",for,file_path,",",processor,in,self,.,_processors,.,items,(,),:,if,processor,.,done,:,self,.,log,.,debug,(,"""Processor for %s finished""",",",file_path,),now,=,timezone,.,utcnow,(,),finished_processors,"This should be periodically called by the manager loop. This method will
        kick off new processes to process DAG definition files and read the
        results from the finished processors.

        :return: a list of SimpleDags that were produced by processors that
            have finished since the last time this was called
        :rtype: list[airflow.utils.dag_processing.SimpleDag]",This,should,be,periodically,called,by,the,manager,loop,.,This,method,will,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1126-L1227,test,[,file_path,],=,processor,self,.,_last_runtime,[,file_path,],=,(,now,-,processor,.,start_time,),.,total_seconds,(,),self,.,_last_finish_time,[,file_path,],=,now,self,.,_run_count,[,file_path,],+=,1,else,:,running_processors,[,file_path,],=,processor,self,.,_processors,=,running_processors,self,.,log,.,debug,(,"""%s/%s DAG parsing processes running""",",",len,(,self,.,_processors,),",",self,.,_parallelism,),self,.,log,.,debug,(,"""%s file paths queued for processing""",",",len,(,self,.,_file_path_queue,),),# Collect all the DAGs that were found in the processed files,simple_dags,=,[,],for,file_path,",",processor,in,finished_processors,.,items,(,),:,if,processor,.,result,is,None,:,self,.,log,.,warning,(,"""Processor for %s exited with return code %s.""",",",processor,.,file_path,",",processor,.,exit_code,),else,:,for,simple_dag,in,processor,.,result,:,simple_dags,.,append,(,simple_dag,),# Generate more file paths to process if we processed all the files,# already.,if,len,(,self,.,_file_path_queue,),==,0,:,"# If the file path is already being processed, or if a file was","# processed recently, wait until the next batch",file_paths_in_progress,=,self,.,_processors,.,keys,(,),now,=,timezone,.,utcnow,(,),file_paths_recently_processed,=,[,],for,file_path,in,self,.,_file_paths,:,last_finish_time,=,self,.,get_last_finish_time,(,file_path,),if,(,last_finish_time,is,not,None,and,(,now,-,last_finish_time,),.,total_seconds,(,),<,self,.,_file_process_interval,),:,file_paths_recently_processed,.,append,(,file_path,),files_paths_at_run_limit,=,[,file_path,for,file_path,",",num_runs,in,self,.,_run_count,.,items,(,),if,num_runs,==,self,.,_max_runs,],files_paths_to_queue,=,list,(,set,(,self,.,_file_paths,),-,set,(,file_paths_in_progress,),-,set,(,file_paths_recently_processed,),-,set,(,files_paths_at_run_limit,),),for,file_path,",",processor,in,self,.,_processors,.,items,(,),:,self,.,log,.,debug,(,"""File path %s is still being processed (started: %s)""",",",processor,.,file_path,",",processor,.,start_time,.,isoformat,(,),),self,.,log,.,debug,(,"""Queuing the following files for processing:\n\t%s""",",","""\n\t""",.,join,(,files_paths_to_queue,),),self,.,_file_path_queue,.,extend,(,files_paths_to_queue,),zombies,=,self,.,_find_zombies,(,),# Start more processors if we have enough slots and files to process,while,(,self,.,_parallelism,-,len,(,self,.,_processors,),>,0,and,len,(,self,.,_file_path_queue,),>,0,),:,file_path,=,self,.,_file_path_queue,.,pop,(,0,),processor,=,self,.,_processor_factory,(,file_path,",",zombies,),processor,.,start,(,),self,.,log,.,debug,(,"""Started a process (PID: %s) to generate tasks for %s""",",",processor,.,pid,",",file_path,),self,.,_processors,[,file_path,],=,processor,# Update heartbeat count.,self,.,_run_count,[,self,.,_heart_beat_key,],+=,1,return,simple_dags,,,,,,,,,,,,,,,,,,,,,,,kick,off,new,processes,to,process,DAG,definition,files,and,read,the,results,from,the,finished,processors,.,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dag_processing.py,DagFileProcessorManager.end,"def end(self):
        """"""
        Kill all child processes on exit since we don't want to leave
        them as orphaned.
        """"""
        pids_to_kill = self.get_all_pids()
        if len(pids_to_kill) > 0:
            # First try SIGTERM
            this_process = psutil.Process(os.getpid())
            # Only check child processes to ensure that we don't have a case
            # where we kill the wrong process because a child process died
            # but the PID got reused.
            child_processes = [x for x in this_process.children(recursive=True)
                               if x.is_running() and x.pid in pids_to_kill]
            for child in child_processes:
                self.log.info(""Terminating child PID: %s"", child.pid)
                child.terminate()
            # TODO: Remove magic number
            timeout = 5
            self.log.info(""Waiting up to %s seconds for processes to exit..."", timeout)
            try:
                psutil.wait_procs(
                    child_processes, timeout=timeout,
                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))
            except psutil.TimeoutExpired:
                self.log.debug(""Ran out of time while waiting for processes to exit"")

            # Then SIGKILL
            child_processes = [x for x in this_process.children(recursive=True)
                               if x.is_running() and x.pid in pids_to_kill]
            if len(child_processes) > 0:
                self.log.info(""SIGKILL processes that did not terminate gracefully"")
                for child in child_processes:
                    self.log.info(""Killing child PID: %s"", child.pid)
                    child.kill()
                    child.wait()",python,"def end(self):
        """"""
        Kill all child processes on exit since we don't want to leave
        them as orphaned.
        """"""
        pids_to_kill = self.get_all_pids()
        if len(pids_to_kill) > 0:
            # First try SIGTERM
            this_process = psutil.Process(os.getpid())
            # Only check child processes to ensure that we don't have a case
            # where we kill the wrong process because a child process died
            # but the PID got reused.
            child_processes = [x for x in this_process.children(recursive=True)
                               if x.is_running() and x.pid in pids_to_kill]
            for child in child_processes:
                self.log.info(""Terminating child PID: %s"", child.pid)
                child.terminate()
            # TODO: Remove magic number
            timeout = 5
            self.log.info(""Waiting up to %s seconds for processes to exit..."", timeout)
            try:
                psutil.wait_procs(
                    child_processes, timeout=timeout,
                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))
            except psutil.TimeoutExpired:
                self.log.debug(""Ran out of time while waiting for processes to exit"")

            # Then SIGKILL
            child_processes = [x for x in this_process.children(recursive=True)
                               if x.is_running() and x.pid in pids_to_kill]
            if len(child_processes) > 0:
                self.log.info(""SIGKILL processes that did not terminate gracefully"")
                for child in child_processes:
                    self.log.info(""Killing child PID: %s"", child.pid)
                    child.kill()
                    child.wait()",def,end,(,self,),:,pids_to_kill,=,self,.,get_all_pids,(,),if,len,(,pids_to_kill,),>,0,:,# First try SIGTERM,this_process,=,psutil,.,Process,(,os,.,getpid,(,),),# Only check child processes to ensure that we don't have a case,# where we kill the wrong process because a child process died,# but the PID got reused.,child_processes,=,[,x,for,x,in,this_process,.,children,(,recursive,=,True,),"Kill all child processes on exit since we don't want to leave
        them as orphaned.",Kill,all,child,processes,on,exit,since,we,don,t,want,to,leave,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1285-L1320,test,if,x,.,is_running,(,),and,x,.,pid,in,pids_to_kill,],for,child,in,child_processes,:,self,.,log,.,info,(,"""Terminating child PID: %s""",",",child,.,pid,),child,.,terminate,(,),# TODO: Remove magic number,timeout,=,5,self,.,log,.,info,(,"""Waiting up to %s seconds for processes to exit...""",",",timeout,),try,:,psutil,.,wait_procs,(,child_processes,",",timeout,=,timeout,",",callback,=,lambda,x,:,self,.,log,.,info,(,'Terminated PID %s',",",x,.,pid,),),except,psutil,.,TimeoutExpired,:,self,.,log,.,debug,(,"""Ran out of time while waiting for processes to exit""",),# Then SIGKILL,child_processes,=,[,x,for,x,in,this_process,.,children,(,recursive,=,True,),if,x,.,is_running,(,),and,x,.,pid,in,pids_to_kill,],if,len,(,child_processes,),>,0,:,self,.,log,.,info,(,"""SIGKILL processes that did not terminate gracefully""",),for,child,in,child_processes,:,self,.,log,.,info,(,"""Killing child PID: %s""",",",child,.,pid,),child,.,kill,(,),child,.,wait,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,them,as,orphaned,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ssh_hook.py,SSHHook.get_conn,"def get_conn(self):
        """"""
        Opens a ssh connection to the remote host.

        :rtype: paramiko.client.SSHClient
        """"""

        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)
        client = paramiko.SSHClient()
        if not self.allow_host_key_change:
            self.log.warning('Remote Identification Change is not verified. '
                             'This wont protect against Man-In-The-Middle attacks')
            client.load_system_host_keys()
        if self.no_host_key_check:
            self.log.warning('No Host Key Verification. This wont protect '
                             'against Man-In-The-Middle attacks')
            # Default is RejectPolicy
            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        if self.password and self.password.strip():
            client.connect(hostname=self.remote_host,
                           username=self.username,
                           password=self.password,
                           key_filename=self.key_file,
                           timeout=self.timeout,
                           compress=self.compress,
                           port=self.port,
                           sock=self.host_proxy)
        else:
            client.connect(hostname=self.remote_host,
                           username=self.username,
                           key_filename=self.key_file,
                           timeout=self.timeout,
                           compress=self.compress,
                           port=self.port,
                           sock=self.host_proxy)

        if self.keepalive_interval:
            client.get_transport().set_keepalive(self.keepalive_interval)

        self.client = client
        return client",python,"def get_conn(self):
        """"""
        Opens a ssh connection to the remote host.

        :rtype: paramiko.client.SSHClient
        """"""

        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)
        client = paramiko.SSHClient()
        if not self.allow_host_key_change:
            self.log.warning('Remote Identification Change is not verified. '
                             'This wont protect against Man-In-The-Middle attacks')
            client.load_system_host_keys()
        if self.no_host_key_check:
            self.log.warning('No Host Key Verification. This wont protect '
                             'against Man-In-The-Middle attacks')
            # Default is RejectPolicy
            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        if self.password and self.password.strip():
            client.connect(hostname=self.remote_host,
                           username=self.username,
                           password=self.password,
                           key_filename=self.key_file,
                           timeout=self.timeout,
                           compress=self.compress,
                           port=self.port,
                           sock=self.host_proxy)
        else:
            client.connect(hostname=self.remote_host,
                           username=self.username,
                           key_filename=self.key_file,
                           timeout=self.timeout,
                           compress=self.compress,
                           port=self.port,
                           sock=self.host_proxy)

        if self.keepalive_interval:
            client.get_transport().set_keepalive(self.keepalive_interval)

        self.client = client
        return client",def,get_conn,(,self,),:,self,.,log,.,debug,(,'Creating SSH client for conn_id: %s',",",self,.,ssh_conn_id,),client,=,paramiko,.,SSHClient,(,),if,not,self,.,allow_host_key_change,:,self,.,log,.,warning,(,'Remote Identification Change is not verified. ','This wont protect against Man-In-The-Middle attacks',),client,.,load_system_host_keys,(,),if,self,.,no_host_key_check,:,self,.,"Opens a ssh connection to the remote host.

        :rtype: paramiko.client.SSHClient",Opens,a,ssh,connection,to,the,remote,host,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ssh_hook.py#L144-L185,test,log,.,warning,(,'No Host Key Verification. This wont protect ','against Man-In-The-Middle attacks',),# Default is RejectPolicy,client,.,set_missing_host_key_policy,(,paramiko,.,AutoAddPolicy,(,),),if,self,.,password,and,self,.,password,.,strip,(,),:,client,.,connect,(,hostname,=,self,.,remote_host,",",username,=,self,.,username,",",password,=,self,.,password,",",key_filename,=,self,.,key_file,",",timeout,=,self,.,timeout,",",compress,=,self,.,compress,",",port,=,self,.,port,",",sock,=,self,.,host_proxy,),else,:,client,.,connect,(,hostname,=,self,.,remote_host,",",username,=,self,.,username,",",key_filename,=,self,.,key_file,",",timeout,=,self,.,timeout,",",compress,=,self,.,compress,",",port,=,self,.,port,",",sock,=,self,.,host_proxy,),if,self,.,keepalive_interval,:,client,.,get_transport,(,),.,set_keepalive,(,self,.,keepalive_interval,),self,.,client,=,client,return,client,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.create_transfer_job,"def create_transfer_job(self, body):
        """"""
        Creates a transfer job that runs periodically.

        :param body: (Required) A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: transfer job.
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :rtype: dict
        """"""
        body = self._inject_project_id(body, BODY, PROJECT_ID)
        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)",python,"def create_transfer_job(self, body):
        """"""
        Creates a transfer job that runs periodically.

        :param body: (Required) A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: transfer job.
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :rtype: dict
        """"""
        body = self._inject_project_id(body, BODY, PROJECT_ID)
        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)",def,create_transfer_job,(,self,",",body,),:,body,=,self,.,_inject_project_id,(,body,",",BODY,",",PROJECT_ID,),return,self,.,get_conn,(,),.,transferJobs,(,),.,create,(,body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,"Creates a transfer job that runs periodically.

        :param body: (Required) A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: transfer job.
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :rtype: dict",Creates,a,transfer,job,that,runs,periodically,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L119-L132,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.get_transfer_job,"def get_transfer_job(self, job_name, project_id=None):
        """"""
        Gets the latest state of a long-running operation in Google Storage
        Transfer Service.

        :param job_name: (Required) Name of the job to be fetched
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :return: Transfer Job
        :rtype: dict
        """"""
        return (
            self.get_conn()
            .transferJobs()
            .get(jobName=job_name, projectId=project_id)
            .execute(num_retries=self.num_retries)
        )",python,"def get_transfer_job(self, job_name, project_id=None):
        """"""
        Gets the latest state of a long-running operation in Google Storage
        Transfer Service.

        :param job_name: (Required) Name of the job to be fetched
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :return: Transfer Job
        :rtype: dict
        """"""
        return (
            self.get_conn()
            .transferJobs()
            .get(jobName=job_name, projectId=project_id)
            .execute(num_retries=self.num_retries)
        )",def,get_transfer_job,(,self,",",job_name,",",project_id,=,None,),:,return,(,self,.,get_conn,(,),.,transferJobs,(,),.,get,(,jobName,=,job_name,",",projectId,=,project_id,),.,execute,(,num_retries,=,self,.,num_retries,),),,,,,,,,,"Gets the latest state of a long-running operation in Google Storage
        Transfer Service.

        :param job_name: (Required) Name of the job to be fetched
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :return: Transfer Job
        :rtype: dict",Gets,the,latest,state,of,a,long,-,running,operation,in,Google,Storage,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L136-L155,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Transfer,Service,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.list_transfer_job,"def list_transfer_job(self, filter):
        """"""
        Lists long-running operations in Google Storage Transfer
        Service that match the specified filter.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
        :type filter: dict
        :return: List of Transfer Jobs
        :rtype: list[dict]
        """"""
        conn = self.get_conn()
        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)
        request = conn.transferJobs().list(filter=json.dumps(filter))
        jobs = []

        while request is not None:
            response = request.execute(num_retries=self.num_retries)
            jobs.extend(response[TRANSFER_JOBS])

            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)

        return jobs",python,"def list_transfer_job(self, filter):
        """"""
        Lists long-running operations in Google Storage Transfer
        Service that match the specified filter.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
        :type filter: dict
        :return: List of Transfer Jobs
        :rtype: list[dict]
        """"""
        conn = self.get_conn()
        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)
        request = conn.transferJobs().list(filter=json.dumps(filter))
        jobs = []

        while request is not None:
            response = request.execute(num_retries=self.num_retries)
            jobs.extend(response[TRANSFER_JOBS])

            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)

        return jobs",def,list_transfer_job,(,self,",",filter,),:,conn,=,self,.,get_conn,(,),filter,=,self,.,_inject_project_id,(,filter,",",FILTER,",",FILTER_PROJECT_ID,),request,=,conn,.,transferJobs,(,),.,list,(,filter,=,json,.,dumps,(,filter,),),jobs,=,[,],while,request,"Lists long-running operations in Google Storage Transfer
        Service that match the specified filter.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
        :type filter: dict
        :return: List of Transfer Jobs
        :rtype: list[dict]",Lists,long,-,running,operations,in,Google,Storage,Transfer,Service,that,match,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L157-L179,test,is,not,None,:,response,=,request,.,execute,(,num_retries,=,self,.,num_retries,),jobs,.,extend,(,response,[,TRANSFER_JOBS,],),request,=,conn,.,transferJobs,(,),.,list_next,(,previous_request,=,request,",",previous_response,=,response,),return,jobs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,specified,filter,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.update_transfer_job,"def update_transfer_job(self, job_name, body):
        """"""
        Updates a transfer job that runs periodically.

        :param job_name: (Required) Name of the job to be updated
        :type job_name: str
        :param body: A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: If successful, TransferJob.
        :rtype: dict
        """"""
        body = self._inject_project_id(body, BODY, PROJECT_ID)
        return (
            self.get_conn()
            .transferJobs()
            .patch(jobName=job_name, body=body)
            .execute(num_retries=self.num_retries)
        )",python,"def update_transfer_job(self, job_name, body):
        """"""
        Updates a transfer job that runs periodically.

        :param job_name: (Required) Name of the job to be updated
        :type job_name: str
        :param body: A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: If successful, TransferJob.
        :rtype: dict
        """"""
        body = self._inject_project_id(body, BODY, PROJECT_ID)
        return (
            self.get_conn()
            .transferJobs()
            .patch(jobName=job_name, body=body)
            .execute(num_retries=self.num_retries)
        )",def,update_transfer_job,(,self,",",job_name,",",body,),:,body,=,self,.,_inject_project_id,(,body,",",BODY,",",PROJECT_ID,),return,(,self,.,get_conn,(,),.,transferJobs,(,),.,patch,(,jobName,=,job_name,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,"Updates a transfer job that runs periodically.

        :param job_name: (Required) Name of the job to be updated
        :type job_name: str
        :param body: A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: If successful, TransferJob.
        :rtype: dict",Updates,a,transfer,job,that,runs,periodically,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L182-L200,test,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.delete_transfer_job,"def delete_transfer_job(self, job_name, project_id):
        """"""
        Deletes a transfer job. This is a soft delete. After a transfer job is
        deleted, the job and all the transfer executions are subject to garbage
        collection. Transfer jobs become eligible for garbage collection
        30 days after soft delete.

        :param job_name: (Required) Name of the job to be deleted
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :rtype: None
        """"""

        return (
            self.get_conn()
            .transferJobs()
            .patch(
                jobName=job_name,
                body={
                    PROJECT_ID: project_id,
                    TRANSFER_JOB: {STATUS1: GcpTransferJobsStatus.DELETED},
                    TRANSFER_JOB_FIELD_MASK: STATUS1,
                },
            )
            .execute(num_retries=self.num_retries)
        )",python,"def delete_transfer_job(self, job_name, project_id):
        """"""
        Deletes a transfer job. This is a soft delete. After a transfer job is
        deleted, the job and all the transfer executions are subject to garbage
        collection. Transfer jobs become eligible for garbage collection
        30 days after soft delete.

        :param job_name: (Required) Name of the job to be deleted
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :rtype: None
        """"""

        return (
            self.get_conn()
            .transferJobs()
            .patch(
                jobName=job_name,
                body={
                    PROJECT_ID: project_id,
                    TRANSFER_JOB: {STATUS1: GcpTransferJobsStatus.DELETED},
                    TRANSFER_JOB_FIELD_MASK: STATUS1,
                },
            )
            .execute(num_retries=self.num_retries)
        )",def,delete_transfer_job,(,self,",",job_name,",",project_id,),:,return,(,self,.,get_conn,(,),.,transferJobs,(,),.,patch,(,jobName,=,job_name,",",body,=,{,PROJECT_ID,:,project_id,",",TRANSFER_JOB,:,{,STATUS1,:,GcpTransferJobsStatus,.,DELETED,},",",TRANSFER_JOB_FIELD_MASK,:,STATUS1,",",},",",),"Deletes a transfer job. This is a soft delete. After a transfer job is
        deleted, the job and all the transfer executions are subject to garbage
        collection. Transfer jobs become eligible for garbage collection
        30 days after soft delete.

        :param job_name: (Required) Name of the job to be deleted
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :rtype: None",Deletes,a,transfer,job,.,This,is,a,soft,delete,.,After,a,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L204-L232,test,.,execute,(,num_retries,=,self,.,num_retries,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,transfer,job,is,deleted,the,job,and,all,the,transfer,executions,are,subject,to,garbage,collection,.,Transfer,jobs,become,eligible,for,garbage,collection,30,days,after,soft,delete,.,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.cancel_transfer_operation,"def cancel_transfer_operation(self, operation_name):
        """"""
        Cancels an transfer operation in Google Storage Transfer Service.

        :param operation_name: Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)",python,"def cancel_transfer_operation(self, operation_name):
        """"""
        Cancels an transfer operation in Google Storage Transfer Service.

        :param operation_name: Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)",def,cancel_transfer_operation,(,self,",",operation_name,),:,self,.,get_conn,(,),.,transferOperations,(,),.,cancel,(,name,=,operation_name,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,,,,,,,,,,,,,,"Cancels an transfer operation in Google Storage Transfer Service.

        :param operation_name: Name of the transfer operation.
        :type operation_name: str
        :rtype: None",Cancels,an,transfer,operation,in,Google,Storage,Transfer,Service,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L235-L243,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.pause_transfer_operation,"def pause_transfer_operation(self, operation_name):
        """"""
        Pauses an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)",python,"def pause_transfer_operation(self, operation_name):
        """"""
        Pauses an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)",def,pause_transfer_operation,(,self,",",operation_name,),:,self,.,get_conn,(,),.,transferOperations,(,),.,pause,(,name,=,operation_name,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,,,,,,,,,,,,,,"Pauses an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None",Pauses,an,transfer,operation,in,Google,Storage,Transfer,Service,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L301-L309,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.resume_transfer_operation,"def resume_transfer_operation(self, operation_name):
        """"""
        Resumes an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)",python,"def resume_transfer_operation(self, operation_name):
        """"""
        Resumes an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)",def,resume_transfer_operation,(,self,",",operation_name,),:,self,.,get_conn,(,),.,transferOperations,(,),.,resume,(,name,=,operation_name,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,,,,,,,,,,,,,,"Resumes an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None",Resumes,an,transfer,operation,in,Google,Storage,Transfer,Service,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L312-L320,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,GCPTransferServiceHook.wait_for_transfer_job,"def wait_for_transfer_job(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):
        """"""
        Waits until the job reaches the expected state.

        :param job: Transfer job
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :type job: dict
        :param expected_statuses: State that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :param timeout:
        :type timeout: time in which the operation must end in seconds
        :rtype: None
        """"""
        while timeout > 0:
            operations = self.list_transfer_operations(
                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}
            )

            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)
            timeout -= TIME_TO_SLEEP_IN_SECONDS
        raise AirflowException(""Timeout. The operation could not be completed within the allotted time."")",python,"def wait_for_transfer_job(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):
        """"""
        Waits until the job reaches the expected state.

        :param job: Transfer job
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :type job: dict
        :param expected_statuses: State that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :param timeout:
        :type timeout: time in which the operation must end in seconds
        :rtype: None
        """"""
        while timeout > 0:
            operations = self.list_transfer_operations(
                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}
            )

            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)
            timeout -= TIME_TO_SLEEP_IN_SECONDS
        raise AirflowException(""Timeout. The operation could not be completed within the allotted time."")",def,wait_for_transfer_job,(,self,",",job,",",expected_statuses,=,(,GcpTransferOperationStatus,.,SUCCESS,",",),",",timeout,=,60,),:,while,timeout,>,0,:,operations,=,self,.,list_transfer_operations,(,filter,=,{,FILTER_PROJECT_ID,:,job,[,PROJECT_ID,],",",FILTER_JOB_NAMES,:,[,job,[,NAME,],],},),"Waits until the job reaches the expected state.

        :param job: Transfer job
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :type job: dict
        :param expected_statuses: State that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :param timeout:
        :type timeout: time in which the operation must end in seconds
        :rtype: None",Waits,until,the,job,reaches,the,expected,state,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L323-L348,test,if,GCPTransferServiceHook,.,operations_contain_expected_statuses,(,operations,",",expected_statuses,),:,return,time,.,sleep,(,TIME_TO_SLEEP_IN_SECONDS,),timeout,-=,TIME_TO_SLEEP_IN_SECONDS,raise,AirflowException,(,"""Timeout. The operation could not be completed within the allotted time.""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskreschedule.py,TaskReschedule.find_for_task_instance,"def find_for_task_instance(task_instance, session):
        """"""
        Returns all task reschedules for the task instance and try number,
        in ascending order.

        :param task_instance: the task instance to find task reschedules for
        :type task_instance: airflow.models.TaskInstance
        """"""
        TR = TaskReschedule
        return (
            session
            .query(TR)
            .filter(TR.dag_id == task_instance.dag_id,
                    TR.task_id == task_instance.task_id,
                    TR.execution_date == task_instance.execution_date,
                    TR.try_number == task_instance.try_number)
            .order_by(asc(TR.id))
            .all()
        )",python,"def find_for_task_instance(task_instance, session):
        """"""
        Returns all task reschedules for the task instance and try number,
        in ascending order.

        :param task_instance: the task instance to find task reschedules for
        :type task_instance: airflow.models.TaskInstance
        """"""
        TR = TaskReschedule
        return (
            session
            .query(TR)
            .filter(TR.dag_id == task_instance.dag_id,
                    TR.task_id == task_instance.task_id,
                    TR.execution_date == task_instance.execution_date,
                    TR.try_number == task_instance.try_number)
            .order_by(asc(TR.id))
            .all()
        )",def,find_for_task_instance,(,task_instance,",",session,),:,TR,=,TaskReschedule,return,(,session,.,query,(,TR,),.,filter,(,TR,.,dag_id,==,task_instance,.,dag_id,",",TR,.,task_id,==,task_instance,.,task_id,",",TR,.,execution_date,==,task_instance,.,execution_date,",",TR,.,try_number,==,task_instance,.,"Returns all task reschedules for the task instance and try number,
        in ascending order.

        :param task_instance: the task instance to find task reschedules for
        :type task_instance: airflow.models.TaskInstance",Returns,all,task,reschedules,for,the,task,instance,and,try,number,in,ascending,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskreschedule.py#L67-L85,test,try_number,),.,order_by,(,asc,(,TR,.,id,),),.,all,(,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,order,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/pool.py,Pool.open_slots,"def open_slots(self, session):
        """"""
        Returns the number of slots open at the moment
        """"""
        from airflow.models.taskinstance import \
            TaskInstance as TI  # Avoid circular import

        used_slots = session.query(func.count()).filter(TI.pool == self.pool).filter(
            TI.state.in_([State.RUNNING, State.QUEUED])).scalar()
        return self.slots - used_slots",python,"def open_slots(self, session):
        """"""
        Returns the number of slots open at the moment
        """"""
        from airflow.models.taskinstance import \
            TaskInstance as TI  # Avoid circular import

        used_slots = session.query(func.count()).filter(TI.pool == self.pool).filter(
            TI.state.in_([State.RUNNING, State.QUEUED])).scalar()
        return self.slots - used_slots",def,open_slots,(,self,",",session,),:,from,airflow,.,models,.,taskinstance,import,TaskInstance,as,TI,# Avoid circular import,used_slots,=,session,.,query,(,func,.,count,(,),),.,filter,(,TI,.,pool,==,self,.,pool,),.,filter,(,TI,.,state,.,in_,(,[,Returns the number of slots open at the moment,Returns,the,number,of,slots,open,at,the,moment,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/pool.py#L60-L69,test,State,.,RUNNING,",",State,.,QUEUED,],),),.,scalar,(,),return,self,.,slots,-,used_slots,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/configuration.py,run_command,"def run_command(command):
    """"""
    Runs command and returns stdout
    """"""
    process = subprocess.Popen(
        shlex.split(command),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        close_fds=True)
    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')
                      for stream in process.communicate()]

    if process.returncode != 0:
        raise AirflowConfigException(
            ""Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}""
            .format(command, process.returncode, output, stderr)
        )

    return output",python,"def run_command(command):
    """"""
    Runs command and returns stdout
    """"""
    process = subprocess.Popen(
        shlex.split(command),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        close_fds=True)
    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')
                      for stream in process.communicate()]

    if process.returncode != 0:
        raise AirflowConfigException(
            ""Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}""
            .format(command, process.returncode, output, stderr)
        )

    return output",def,run_command,(,command,),:,process,=,subprocess,.,Popen,(,shlex,.,split,(,command,),",",stdout,=,subprocess,.,PIPE,",",stderr,=,subprocess,.,PIPE,",",close_fds,=,True,),output,",",stderr,=,[,stream,.,decode,(,sys,.,getdefaultencoding,(,),",",'ignore',),Runs command and returns stdout,Runs,command,and,returns,stdout,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L75-L93,test,for,stream,in,process,.,communicate,(,),],if,process,.,returncode,!=,0,:,raise,AirflowConfigException,(,"""Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}""",.,format,(,command,",",process,.,returncode,",",output,",",stderr,),),return,output,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/configuration.py,AirflowConfigParser.remove_option,"def remove_option(self, section, option, remove_default=True):
        """"""
        Remove an option if it exists in config from a file or
        default config. If both of config have the same option, this removes
        the option in both configs unless remove_default=False.
        """"""
        if super().has_option(section, option):
            super().remove_option(section, option)

        if self.airflow_defaults.has_option(section, option) and remove_default:
            self.airflow_defaults.remove_option(section, option)",python,"def remove_option(self, section, option, remove_default=True):
        """"""
        Remove an option if it exists in config from a file or
        default config. If both of config have the same option, this removes
        the option in both configs unless remove_default=False.
        """"""
        if super().has_option(section, option):
            super().remove_option(section, option)

        if self.airflow_defaults.has_option(section, option) and remove_default:
            self.airflow_defaults.remove_option(section, option)",def,remove_option,(,self,",",section,",",option,",",remove_default,=,True,),:,if,super,(,),.,has_option,(,section,",",option,),:,super,(,),.,remove_option,(,section,",",option,),if,self,.,airflow_defaults,.,has_option,(,section,",",option,),and,remove_default,:,self,.,"Remove an option if it exists in config from a file or
        default config. If both of config have the same option, this removes
        the option in both configs unless remove_default=False.",Remove,an,option,if,it,exists,in,config,from,a,file,or,default,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L297-L307,test,airflow_defaults,.,remove_option,(,section,",",option,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,config,.,If,both,of,config,have,the,same,option,this,removes,the,option,in,both,configs,unless,remove_default,=,False,.,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/configuration.py,AirflowConfigParser.getsection,"def getsection(self, section):
        """"""
        Returns the section as a dict. Values are converted to int, float, bool
        as required.

        :param section: section from the config
        :rtype: dict
        """"""
        if (section not in self._sections and
                section not in self.airflow_defaults._sections):
            return None

        _section = copy.deepcopy(self.airflow_defaults._sections[section])

        if section in self._sections:
            _section.update(copy.deepcopy(self._sections[section]))

        section_prefix = 'AIRFLOW__{S}__'.format(S=section.upper())
        for env_var in sorted(os.environ.keys()):
            if env_var.startswith(section_prefix):
                key = env_var.replace(section_prefix, '').lower()
                _section[key] = self._get_env_var_option(section, key)

        for key, val in iteritems(_section):
            try:
                val = int(val)
            except ValueError:
                try:
                    val = float(val)
                except ValueError:
                    if val.lower() in ('t', 'true'):
                        val = True
                    elif val.lower() in ('f', 'false'):
                        val = False
            _section[key] = val
        return _section",python,"def getsection(self, section):
        """"""
        Returns the section as a dict. Values are converted to int, float, bool
        as required.

        :param section: section from the config
        :rtype: dict
        """"""
        if (section not in self._sections and
                section not in self.airflow_defaults._sections):
            return None

        _section = copy.deepcopy(self.airflow_defaults._sections[section])

        if section in self._sections:
            _section.update(copy.deepcopy(self._sections[section]))

        section_prefix = 'AIRFLOW__{S}__'.format(S=section.upper())
        for env_var in sorted(os.environ.keys()):
            if env_var.startswith(section_prefix):
                key = env_var.replace(section_prefix, '').lower()
                _section[key] = self._get_env_var_option(section, key)

        for key, val in iteritems(_section):
            try:
                val = int(val)
            except ValueError:
                try:
                    val = float(val)
                except ValueError:
                    if val.lower() in ('t', 'true'):
                        val = True
                    elif val.lower() in ('f', 'false'):
                        val = False
            _section[key] = val
        return _section",def,getsection,(,self,",",section,),:,if,(,section,not,in,self,.,_sections,and,section,not,in,self,.,airflow_defaults,.,_sections,),:,return,None,_section,=,copy,.,deepcopy,(,self,.,airflow_defaults,.,_sections,[,section,],),if,section,in,self,.,_sections,:,_section,"Returns the section as a dict. Values are converted to int, float, bool
        as required.

        :param section: section from the config
        :rtype: dict",Returns,the,section,as,a,dict,.,Values,are,converted,to,int,float,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L309-L344,test,.,update,(,copy,.,deepcopy,(,self,.,_sections,[,section,],),),section_prefix,=,'AIRFLOW__{S}__',.,format,(,S,=,section,.,upper,(,),),for,env_var,in,sorted,(,os,.,environ,.,keys,(,),),:,if,env_var,.,startswith,(,section_prefix,),:,key,=,env_var,.,replace,(,section_prefix,",",'',),.,lower,(,),_section,[,key,],=,self,.,_get_env_var_option,(,section,",",key,),for,key,",",val,in,iteritems,(,_section,),:,try,:,val,=,int,(,val,),except,ValueError,:,try,:,val,=,float,(,val,),except,ValueError,:,if,val,.,lower,(,),in,(,'t',",",'true',),:,val,=,True,elif,val,.,lower,(,),in,(,'f',",",'false',),:,val,=,False,_section,[,key,],=,val,return,_section,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bool,as,required,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.allocate_ids,"def allocate_ids(self, partial_keys):
        """"""
        Allocate IDs for incomplete keys.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds

        :param partial_keys: a list of partial keys.
        :type partial_keys: list
        :return: a list of full keys.
        :rtype: list
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})
                .execute(num_retries=self.num_retries))

        return resp['keys']",python,"def allocate_ids(self, partial_keys):
        """"""
        Allocate IDs for incomplete keys.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds

        :param partial_keys: a list of partial keys.
        :type partial_keys: list
        :return: a list of full keys.
        :rtype: list
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})
                .execute(num_retries=self.num_retries))

        return resp['keys']",def,allocate_ids,(,self,",",partial_keys,),:,conn,=,self,.,get_conn,(,),resp,=,(,conn,.,projects,(,),.,allocateIds,(,projectId,=,self,.,project_id,",",body,=,{,'keys',:,partial_keys,},),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,"Allocate IDs for incomplete keys.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds

        :param partial_keys: a list of partial keys.
        :type partial_keys: list
        :return: a list of full keys.
        :rtype: list",Allocate,IDs,for,incomplete,keys,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L62-L81,test,[,'keys',],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.begin_transaction,"def begin_transaction(self):
        """"""
        Begins a new transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction

        :return: a transaction handle.
        :rtype: str
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .beginTransaction(projectId=self.project_id, body={})
                .execute(num_retries=self.num_retries))

        return resp['transaction']",python,"def begin_transaction(self):
        """"""
        Begins a new transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction

        :return: a transaction handle.
        :rtype: str
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .beginTransaction(projectId=self.project_id, body={})
                .execute(num_retries=self.num_retries))

        return resp['transaction']",def,begin_transaction,(,self,),:,conn,=,self,.,get_conn,(,),resp,=,(,conn,.,projects,(,),.,beginTransaction,(,projectId,=,self,.,project_id,",",body,=,{,},),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,[,'transaction',],,,"Begins a new transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction

        :return: a transaction handle.
        :rtype: str",Begins,a,new,transaction,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L83-L100,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.commit,"def commit(self, body):
        """"""
        Commit a transaction, optionally creating, deleting or modifying some entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit

        :param body: the body of the commit request.
        :type body: dict
        :return: the response body of the commit request.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .commit(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",python,"def commit(self, body):
        """"""
        Commit a transaction, optionally creating, deleting or modifying some entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit

        :param body: the body of the commit request.
        :type body: dict
        :return: the response body of the commit request.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .commit(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",def,commit,(,self,",",body,),:,conn,=,self,.,get_conn,(,),resp,=,(,conn,.,projects,(,),.,commit,(,projectId,=,self,.,project_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,,,,,"Commit a transaction, optionally creating, deleting or modifying some entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit

        :param body: the body of the commit request.
        :type body: dict
        :return: the response body of the commit request.
        :rtype: dict",Commit,a,transaction,optionally,creating,deleting,or,modifying,some,entities,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L102-L121,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.lookup,"def lookup(self, keys, read_consistency=None, transaction=None):
        """"""
        Lookup some entities by key.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup

        :param keys: the keys to lookup.
        :type keys: list
        :param read_consistency: the read consistency to use. default, strong or eventual.
                                 Cannot be used with a transaction.
        :type read_consistency: str
        :param transaction: the transaction to use, if any.
        :type transaction: str
        :return: the response body of the lookup request.
        :rtype: dict
        """"""
        conn = self.get_conn()

        body = {'keys': keys}
        if read_consistency:
            body['readConsistency'] = read_consistency
        if transaction:
            body['transaction'] = transaction
        resp = (conn
                .projects()
                .lookup(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",python,"def lookup(self, keys, read_consistency=None, transaction=None):
        """"""
        Lookup some entities by key.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup

        :param keys: the keys to lookup.
        :type keys: list
        :param read_consistency: the read consistency to use. default, strong or eventual.
                                 Cannot be used with a transaction.
        :type read_consistency: str
        :param transaction: the transaction to use, if any.
        :type transaction: str
        :return: the response body of the lookup request.
        :rtype: dict
        """"""
        conn = self.get_conn()

        body = {'keys': keys}
        if read_consistency:
            body['readConsistency'] = read_consistency
        if transaction:
            body['transaction'] = transaction
        resp = (conn
                .projects()
                .lookup(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",def,lookup,(,self,",",keys,",",read_consistency,=,None,",",transaction,=,None,),:,conn,=,self,.,get_conn,(,),body,=,{,'keys',:,keys,},if,read_consistency,:,body,[,'readConsistency',],=,read_consistency,if,transaction,:,body,[,'transaction',],=,transaction,resp,=,(,conn,"Lookup some entities by key.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup

        :param keys: the keys to lookup.
        :type keys: list
        :param read_consistency: the read consistency to use. default, strong or eventual.
                                 Cannot be used with a transaction.
        :type read_consistency: str
        :param transaction: the transaction to use, if any.
        :type transaction: str
        :return: the response body of the lookup request.
        :rtype: dict",Lookup,some,entities,by,key,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L123-L152,test,.,projects,(,),.,lookup,(,projectId,=,self,.,project_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.rollback,"def rollback(self, transaction):
        """"""
        Roll back a transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback

        :param transaction: the transaction to roll back.
        :type transaction: str
        """"""
        conn = self.get_conn()

        conn.projects().rollback(
            projectId=self.project_id, body={'transaction': transaction}
        ).execute(num_retries=self.num_retries)",python,"def rollback(self, transaction):
        """"""
        Roll back a transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback

        :param transaction: the transaction to roll back.
        :type transaction: str
        """"""
        conn = self.get_conn()

        conn.projects().rollback(
            projectId=self.project_id, body={'transaction': transaction}
        ).execute(num_retries=self.num_retries)",def,rollback,(,self,",",transaction,),:,conn,=,self,.,get_conn,(,),conn,.,projects,(,),.,rollback,(,projectId,=,self,.,project_id,",",body,=,{,'transaction',:,transaction,},),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,"Roll back a transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback

        :param transaction: the transaction to roll back.
        :type transaction: str",Roll,back,a,transaction,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L154-L168,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.run_query,"def run_query(self, body):
        """"""
        Run a query for entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery

        :param body: the body of the query request.
        :type body: dict
        :return: the batch of query results.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .runQuery(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp['batch']",python,"def run_query(self, body):
        """"""
        Run a query for entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery

        :param body: the body of the query request.
        :type body: dict
        :return: the batch of query results.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .runQuery(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp['batch']",def,run_query,(,self,",",body,),:,conn,=,self,.,get_conn,(,),resp,=,(,conn,.,projects,(,),.,runQuery,(,projectId,=,self,.,project_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,[,'batch',],,"Run a query for entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery

        :param body: the body of the query request.
        :type body: dict
        :return: the batch of query results.
        :rtype: dict",Run,a,query,for,entities,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L170-L189,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.get_operation,"def get_operation(self, name):
        """"""
        Gets the latest state of a long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get

        :param name: the name of the operation resource.
        :type name: str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .operations()
                .get(name=name)
                .execute(num_retries=self.num_retries))

        return resp",python,"def get_operation(self, name):
        """"""
        Gets the latest state of a long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get

        :param name: the name of the operation resource.
        :type name: str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .operations()
                .get(name=name)
                .execute(num_retries=self.num_retries))

        return resp",def,get_operation,(,self,",",name,),:,conn,=,self,.,get_conn,(,),resp,=,(,conn,.,projects,(,),.,operations,(,),.,get,(,name,=,name,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,,,,,,,"Gets the latest state of a long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get

        :param name: the name of the operation resource.
        :type name: str
        :return: a resource operation instance.
        :rtype: dict",Gets,the,latest,state,of,a,long,-,running,operation,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L191-L211,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.delete_operation,"def delete_operation(self, name):
        """"""
        Deletes the long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete

        :param name: the name of the operation resource.
        :type name: str
        :return: none if successful.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .operations()
                .delete(name=name)
                .execute(num_retries=self.num_retries))

        return resp",python,"def delete_operation(self, name):
        """"""
        Deletes the long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete

        :param name: the name of the operation resource.
        :type name: str
        :return: none if successful.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .operations()
                .delete(name=name)
                .execute(num_retries=self.num_retries))

        return resp",def,delete_operation,(,self,",",name,),:,conn,=,self,.,get_conn,(,),resp,=,(,conn,.,projects,(,),.,operations,(,),.,delete,(,name,=,name,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,,,,,,,"Deletes the long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete

        :param name: the name of the operation resource.
        :type name: str
        :return: none if successful.
        :rtype: dict",Deletes,the,long,-,running,operation,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L213-L233,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.poll_operation_until_done,"def poll_operation_until_done(self, name, polling_interval_in_seconds):
        """"""
        Poll backup operation state until it's completed.

        :param name: the name of the operation resource
        :type name: str
        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.
        :type polling_interval_in_seconds: int
        :return: a resource operation instance.
        :rtype: dict
        """"""
        while True:
            result = self.get_operation(name)

            state = result['metadata']['common']['state']
            if state == 'PROCESSING':
                self.log.info('Operation is processing. Re-polling state in {} seconds'
                              .format(polling_interval_in_seconds))
                time.sleep(polling_interval_in_seconds)
            else:
                return result",python,"def poll_operation_until_done(self, name, polling_interval_in_seconds):
        """"""
        Poll backup operation state until it's completed.

        :param name: the name of the operation resource
        :type name: str
        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.
        :type polling_interval_in_seconds: int
        :return: a resource operation instance.
        :rtype: dict
        """"""
        while True:
            result = self.get_operation(name)

            state = result['metadata']['common']['state']
            if state == 'PROCESSING':
                self.log.info('Operation is processing. Re-polling state in {} seconds'
                              .format(polling_interval_in_seconds))
                time.sleep(polling_interval_in_seconds)
            else:
                return result",def,poll_operation_until_done,(,self,",",name,",",polling_interval_in_seconds,),:,while,True,:,result,=,self,.,get_operation,(,name,),state,=,result,[,'metadata',],[,'common',],[,'state',],if,state,==,'PROCESSING',:,self,.,log,.,info,(,'Operation is processing. Re-polling state in {} seconds',.,format,(,polling_interval_in_seconds,),),time,"Poll backup operation state until it's completed.

        :param name: the name of the operation resource
        :type name: str
        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.
        :type polling_interval_in_seconds: int
        :return: a resource operation instance.
        :rtype: dict",Poll,backup,operation,state,until,it,s,completed,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L235-L255,test,.,sleep,(,polling_interval_in_seconds,),else,:,return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.export_to_storage_bucket,"def export_to_storage_bucket(self, bucket, namespace=None, entity_filter=None, labels=None):
        """"""
        Export entities from Cloud Datastore to Cloud Storage for backup.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: Description of what data from the project is included in the export.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        admin_conn = self.get_conn()

        output_uri_prefix = 'gs://' + '/'.join(filter(None, [bucket, namespace]))
        if not entity_filter:
            entity_filter = {}
        if not labels:
            labels = {}
        body = {
            'outputUrlPrefix': output_uri_prefix,
            'entityFilter': entity_filter,
            'labels': labels,
        }
        resp = (admin_conn
                .projects()
                .export(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",python,"def export_to_storage_bucket(self, bucket, namespace=None, entity_filter=None, labels=None):
        """"""
        Export entities from Cloud Datastore to Cloud Storage for backup.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: Description of what data from the project is included in the export.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        admin_conn = self.get_conn()

        output_uri_prefix = 'gs://' + '/'.join(filter(None, [bucket, namespace]))
        if not entity_filter:
            entity_filter = {}
        if not labels:
            labels = {}
        body = {
            'outputUrlPrefix': output_uri_prefix,
            'entityFilter': entity_filter,
            'labels': labels,
        }
        resp = (admin_conn
                .projects()
                .export(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",def,export_to_storage_bucket,(,self,",",bucket,",",namespace,=,None,",",entity_filter,=,None,",",labels,=,None,),:,admin_conn,=,self,.,get_conn,(,),output_uri_prefix,=,'gs://',+,'/',.,join,(,filter,(,None,",",[,bucket,",",namespace,],),),if,not,entity_filter,:,entity_filter,=,"Export entities from Cloud Datastore to Cloud Storage for backup.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: Description of what data from the project is included in the export.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict",Export,entities,from,Cloud,Datastore,to,Cloud,Storage,for,backup,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L257-L295,test,{,},if,not,labels,:,labels,=,{,},body,=,{,'outputUrlPrefix',:,output_uri_prefix,",",'entityFilter',:,entity_filter,",",'labels',:,labels,",",},resp,=,(,admin_conn,.,projects,(,),.,export,(,projectId,=,self,.,project_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datastore_hook.py,DatastoreHook.import_from_storage_bucket,"def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):
        """"""
        Import a backup from Cloud Storage to Cloud Datastore.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param file: the metadata file written by the projects.export operation.
        :type file: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: specify which kinds/namespaces are to be imported.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        admin_conn = self.get_conn()

        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))
        if not entity_filter:
            entity_filter = {}
        if not labels:
            labels = {}
        body = {
            'inputUrl': input_url,
            'entityFilter': entity_filter,
            'labels': labels,
        }
        resp = (admin_conn
                .projects()
                .import_(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",python,"def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):
        """"""
        Import a backup from Cloud Storage to Cloud Datastore.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param file: the metadata file written by the projects.export operation.
        :type file: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: specify which kinds/namespaces are to be imported.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        admin_conn = self.get_conn()

        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))
        if not entity_filter:
            entity_filter = {}
        if not labels:
            labels = {}
        body = {
            'inputUrl': input_url,
            'entityFilter': entity_filter,
            'labels': labels,
        }
        resp = (admin_conn
                .projects()
                .import_(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp",def,import_from_storage_bucket,(,self,",",bucket,",",file,",",namespace,=,None,",",entity_filter,=,None,",",labels,=,None,),:,admin_conn,=,self,.,get_conn,(,),input_url,=,'gs://',+,'/',.,join,(,filter,(,None,",",[,bucket,",",namespace,",",file,],),),if,not,"Import a backup from Cloud Storage to Cloud Datastore.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param file: the metadata file written by the projects.export operation.
        :type file: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: specify which kinds/namespaces are to be imported.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict",Import,a,backup,from,Cloud,Storage,to,Cloud,Datastore,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L297-L337,test,entity_filter,:,entity_filter,=,{,},if,not,labels,:,labels,=,{,},body,=,{,'inputUrl',:,input_url,",",'entityFilter',:,entity_filter,",",'labels',:,labels,",",},resp,=,(,admin_conn,.,projects,(,),.,import_,(,projectId,=,self,.,project_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),),return,resp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_sns_hook.py,AwsSnsHook.publish_to_target,"def publish_to_target(self, target_arn, message):
        """"""
        Publish a message to a topic or an endpoint.

        :param target_arn: either a TopicArn or an EndpointArn
        :type target_arn: str
        :param message: the default message you want to send
        :param message: str
        """"""

        conn = self.get_conn()

        messages = {
            'default': message
        }

        return conn.publish(
            TargetArn=target_arn,
            Message=json.dumps(messages),
            MessageStructure='json'
        )",python,"def publish_to_target(self, target_arn, message):
        """"""
        Publish a message to a topic or an endpoint.

        :param target_arn: either a TopicArn or an EndpointArn
        :type target_arn: str
        :param message: the default message you want to send
        :param message: str
        """"""

        conn = self.get_conn()

        messages = {
            'default': message
        }

        return conn.publish(
            TargetArn=target_arn,
            Message=json.dumps(messages),
            MessageStructure='json'
        )",def,publish_to_target,(,self,",",target_arn,",",message,),:,conn,=,self,.,get_conn,(,),messages,=,{,'default',:,message,},return,conn,.,publish,(,TargetArn,=,target_arn,",",Message,=,json,.,dumps,(,messages,),",",MessageStructure,=,'json',),,,,,,,"Publish a message to a topic or an endpoint.

        :param target_arn: either a TopicArn or an EndpointArn
        :type target_arn: str
        :param message: the default message you want to send
        :param message: str",Publish,a,message,to,a,topic,or,an,endpoint,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_sns_hook.py#L40-L60,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/net.py,get_hostname,"def get_hostname():
    """"""
    Fetch the hostname using the callable from the config or using
    `socket.getfqdn` as a fallback.
    """"""
    # First we attempt to fetch the callable path from the config.
    try:
        callable_path = conf.get('core', 'hostname_callable')
    except AirflowConfigException:
        callable_path = None

    # Then we handle the case when the config is missing or empty. This is the
    # default behavior.
    if not callable_path:
        return socket.getfqdn()

    # Since we have a callable path, we try to import and run it next.
    module_path, attr_name = callable_path.split(':')
    module = importlib.import_module(module_path)
    callable = getattr(module, attr_name)
    return callable()",python,"def get_hostname():
    """"""
    Fetch the hostname using the callable from the config or using
    `socket.getfqdn` as a fallback.
    """"""
    # First we attempt to fetch the callable path from the config.
    try:
        callable_path = conf.get('core', 'hostname_callable')
    except AirflowConfigException:
        callable_path = None

    # Then we handle the case when the config is missing or empty. This is the
    # default behavior.
    if not callable_path:
        return socket.getfqdn()

    # Since we have a callable path, we try to import and run it next.
    module_path, attr_name = callable_path.split(':')
    module = importlib.import_module(module_path)
    callable = getattr(module, attr_name)
    return callable()",def,get_hostname,(,),:,# First we attempt to fetch the callable path from the config.,try,:,callable_path,=,conf,.,get,(,'core',",",'hostname_callable',),except,AirflowConfigException,:,callable_path,=,None,# Then we handle the case when the config is missing or empty. This is the,# default behavior.,if,not,callable_path,:,return,socket,.,getfqdn,(,),"# Since we have a callable path, we try to import and run it next.",module_path,",",attr_name,=,callable_path,.,split,(,':',),module,=,importlib,.,import_module,"Fetch the hostname using the callable from the config or using
    `socket.getfqdn` as a fallback.",Fetch,the,hostname,using,the,callable,from,the,config,or,using,socket,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/net.py#L25-L45,test,(,module_path,),callable,=,getattr,(,module,",",attr_name,),return,callable,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,getfqdn,as,a,fallback,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,CloudNaturalLanguageHook.get_conn,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Natural Language service.

        :return: Cloud Natural Language service object
        :rtype: google.cloud.language_v1.LanguageServiceClient
        """"""
        if not self._conn:
            self._conn = LanguageServiceClient(credentials=self._get_credentials())
        return self._conn",python,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Natural Language service.

        :return: Cloud Natural Language service object
        :rtype: google.cloud.language_v1.LanguageServiceClient
        """"""
        if not self._conn:
            self._conn = LanguageServiceClient(credentials=self._get_credentials())
        return self._conn",def,get_conn,(,self,),:,if,not,self,.,_conn,:,self,.,_conn,=,LanguageServiceClient,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_conn,,,,,,,,,,,,,,,,,,,,,,,"Retrieves connection to Cloud Natural Language service.

        :return: Cloud Natural Language service object
        :rtype: google.cloud.language_v1.LanguageServiceClient",Retrieves,connection,to,Cloud,Natural,Language,service,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L44-L53,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,CloudNaturalLanguageHook.analyze_entities,"def analyze_entities(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):
        """"""
        Finds named entities in the text along with entity types,
        salience, mentions for each entity, and other properties.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
        """"""
        client = self.get_conn()

        return client.analyze_entities(
            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata
        )",python,"def analyze_entities(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):
        """"""
        Finds named entities in the text along with entity types,
        salience, mentions for each entity, and other properties.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
        """"""
        client = self.get_conn()

        return client.analyze_entities(
            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata
        )",def,analyze_entities,(,self,",",document,",",encoding_type,=,None,",",retry,=,None,",",timeout,=,None,",",metadata,=,None,),:,client,=,self,.,get_conn,(,),return,client,.,analyze_entities,(,document,=,document,",",encoding_type,=,encoding_type,",",retry,=,retry,",",timeout,=,timeout,",","Finds named entities in the text along with entity types,
        salience, mentions for each entity, and other properties.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse",Finds,named,entities,in,the,text,along,with,entity,types,salience,mentions,for,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L56-L80,test,metadata,=,metadata,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,each,entity,and,other,properties,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,CloudNaturalLanguageHook.annotate_text,"def annotate_text(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):
        """"""
        A convenience method that provides all the features that analyzeSentiment,
        analyzeEntities, and analyzeSyntax provide in one call.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or google.cloud.language_v1.types.Document
        :param features: The enabled features.
            If a dict is provided, it must be of the same form as the protobuf message Features
        :type features: dict or google.cloud.language_v1.enums.Features
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnnotateTextResponse
        """"""
        client = self.get_conn()

        return client.annotate_text(
            document=document,
            features=features,
            encoding_type=encoding_type,
            retry=retry,
            timeout=timeout,
            metadata=metadata,
        )",python,"def annotate_text(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):
        """"""
        A convenience method that provides all the features that analyzeSentiment,
        analyzeEntities, and analyzeSyntax provide in one call.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or google.cloud.language_v1.types.Document
        :param features: The enabled features.
            If a dict is provided, it must be of the same form as the protobuf message Features
        :type features: dict or google.cloud.language_v1.enums.Features
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnnotateTextResponse
        """"""
        client = self.get_conn()

        return client.annotate_text(
            document=document,
            features=features,
            encoding_type=encoding_type,
            retry=retry,
            timeout=timeout,
            metadata=metadata,
        )",def,annotate_text,(,self,",",document,",",features,",",encoding_type,=,None,",",retry,=,None,",",timeout,=,None,",",metadata,=,None,),:,client,=,self,.,get_conn,(,),return,client,.,annotate_text,(,document,=,document,",",features,=,features,",",encoding_type,=,encoding_type,",",retry,=,"A convenience method that provides all the features that analyzeSentiment,
        analyzeEntities, and analyzeSyntax provide in one call.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or google.cloud.language_v1.types.Document
        :param features: The enabled features.
            If a dict is provided, it must be of the same form as the protobuf message Features
        :type features: dict or google.cloud.language_v1.enums.Features
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnnotateTextResponse",A,convenience,method,that,provides,all,the,features,that,analyzeSentiment,analyzeEntities,and,analyzeSyntax,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L163-L195,test,retry,",",timeout,=,timeout,",",metadata,=,metadata,",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,provide,in,one,call,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,CloudNaturalLanguageHook.classify_text,"def classify_text(self, document, retry=None, timeout=None, metadata=None):
        """"""
        Classifies a document into categories.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
        """"""
        client = self.get_conn()

        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)",python,"def classify_text(self, document, retry=None, timeout=None, metadata=None):
        """"""
        Classifies a document into categories.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
        """"""
        client = self.get_conn()

        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)",def,classify_text,(,self,",",document,",",retry,=,None,",",timeout,=,None,",",metadata,=,None,),:,client,=,self,.,get_conn,(,),return,client,.,classify_text,(,document,=,document,",",retry,=,retry,",",timeout,=,timeout,",",metadata,=,metadata,),,,,,"Classifies a document into categories.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse",Classifies,a,document,into,categories,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L198-L217,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,docs/exts/docroles.py,get_template_field,"def get_template_field(env, fullname):
    """"""
    Gets template fields for specific operator class.

    :param fullname: Full path to operator class.
        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``
    :return: List of template field
    :rtype: list[str]
    """"""
    modname, classname = fullname.rsplit(""."", 1)

    try:
        with mock(env.config.autodoc_mock_imports):
            mod = import_module(modname)
    except ImportError:
        raise RoleException(""Error loading %s module."" % (modname, ))

    clazz = getattr(mod, classname)
    if not clazz:
        raise RoleException(""Error finding %s class in %s module."" % (classname, modname))

    template_fields = getattr(clazz, ""template_fields"")

    if not template_fields:
        raise RoleException(
            ""Could not find the template fields for %s class in %s module."" % (classname, modname)
        )

    return list(template_fields)",python,"def get_template_field(env, fullname):
    """"""
    Gets template fields for specific operator class.

    :param fullname: Full path to operator class.
        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``
    :return: List of template field
    :rtype: list[str]
    """"""
    modname, classname = fullname.rsplit(""."", 1)

    try:
        with mock(env.config.autodoc_mock_imports):
            mod = import_module(modname)
    except ImportError:
        raise RoleException(""Error loading %s module."" % (modname, ))

    clazz = getattr(mod, classname)
    if not clazz:
        raise RoleException(""Error finding %s class in %s module."" % (classname, modname))

    template_fields = getattr(clazz, ""template_fields"")

    if not template_fields:
        raise RoleException(
            ""Could not find the template fields for %s class in %s module."" % (classname, modname)
        )

    return list(template_fields)",def,get_template_field,(,env,",",fullname,),:,modname,",",classname,=,fullname,.,rsplit,(,""".""",",",1,),try,:,with,mock,(,env,.,config,.,autodoc_mock_imports,),:,mod,=,import_module,(,modname,),except,ImportError,:,raise,RoleException,(,"""Error loading %s module.""",%,(,modname,",",),),clazz,"Gets template fields for specific operator class.

    :param fullname: Full path to operator class.
        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``
    :return: List of template field
    :rtype: list[str]",Gets,template,fields,for,specific,operator,class,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/docs/exts/docroles.py#L27-L55,test,=,getattr,(,mod,",",classname,),if,not,clazz,:,raise,RoleException,(,"""Error finding %s class in %s module.""",%,(,classname,",",modname,),),template_fields,=,getattr,(,clazz,",","""template_fields""",),if,not,template_fields,:,raise,RoleException,(,"""Could not find the template fields for %s class in %s module.""",%,(,classname,",",modname,),),return,list,(,template_fields,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,docs/exts/docroles.py,template_field_role,"def template_field_role(app, typ, rawtext, text, lineno, inliner, options={}, content=[]):
    """"""
    A role that allows you to include a list of template fields in the middle of the text. This is especially
    useful when writing guides describing how to use the operator.
    The result is a list of fields where each field is shorted in the literal block.

    Sample usage::

    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`

    For further information look at:

    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted
      Text Roles)
    """"""
    text = utils.unescape(text)

    try:
        template_fields = get_template_field(app.env, text)
    except RoleException as e:
        msg = inliner.reporter.error(""invalid class name %s \n%s"" % (text, e, ), line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]

    node = nodes.inline(rawtext=rawtext)
    for i, field in enumerate(template_fields):
        if i != 0:
            node += nodes.Text("", "")
        node += nodes.literal(field, """", nodes.Text(field))

    return [node], []",python,"def template_field_role(app, typ, rawtext, text, lineno, inliner, options={}, content=[]):
    """"""
    A role that allows you to include a list of template fields in the middle of the text. This is especially
    useful when writing guides describing how to use the operator.
    The result is a list of fields where each field is shorted in the literal block.

    Sample usage::

    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`

    For further information look at:

    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted
      Text Roles)
    """"""
    text = utils.unescape(text)

    try:
        template_fields = get_template_field(app.env, text)
    except RoleException as e:
        msg = inliner.reporter.error(""invalid class name %s \n%s"" % (text, e, ), line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]

    node = nodes.inline(rawtext=rawtext)
    for i, field in enumerate(template_fields):
        if i != 0:
            node += nodes.Text("", "")
        node += nodes.literal(field, """", nodes.Text(field))

    return [node], []",def,template_field_role,(,app,",",typ,",",rawtext,",",text,",",lineno,",",inliner,",",options,=,{,},",",content,=,[,],),:,text,=,utils,.,unescape,(,text,),try,:,template_fields,=,get_template_field,(,app,.,env,",",text,),except,RoleException,as,e,:,msg,"A role that allows you to include a list of template fields in the middle of the text. This is especially
    useful when writing guides describing how to use the operator.
    The result is a list of fields where each field is shorted in the literal block.

    Sample usage::

    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`

    For further information look at:

    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted
      Text Roles)",A,role,that,allows,you,to,include,a,list,of,template,fields,in,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/docs/exts/docroles.py#L58-L88,test,=,inliner,.,reporter,.,error,(,"""invalid class name %s \n%s""",%,(,text,",",e,",",),",",line,=,lineno,),prb,=,inliner,.,problematic,(,rawtext,",",rawtext,",",msg,),return,[,prb,],",",[,msg,],node,=,nodes,.,inline,(,rawtext,=,rawtext,),for,i,",",field,in,enumerate,(,template_fields,),:,if,i,!=,0,:,node,+=,nodes,.,Text,(,""", """,),node,+=,nodes,.,literal,(,field,",","""""",",",nodes,.,Text,(,field,),),return,[,node,],",",[,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,middle,of,the,text,.,This,is,especially,useful,when,writing,guides,describing,how,to,use,the,operator,.,The,result,is,a,list,of,fields,where,each,field,is,shorted,in,the,literal,block,.,,,
apache/airflow,airflow/settings.py,dispose_orm,"def dispose_orm():
    """""" Properly close pooled database connections """"""
    log.debug(""Disposing DB connection pool (PID %s)"", os.getpid())
    global engine
    global Session

    if Session:
        Session.remove()
        Session = None
    if engine:
        engine.dispose()
        engine = None",python,"def dispose_orm():
    """""" Properly close pooled database connections """"""
    log.debug(""Disposing DB connection pool (PID %s)"", os.getpid())
    global engine
    global Session

    if Session:
        Session.remove()
        Session = None
    if engine:
        engine.dispose()
        engine = None",def,dispose_orm,(,),:,log,.,debug,(,"""Disposing DB connection pool (PID %s)""",",",os,.,getpid,(,),),global,engine,global,Session,if,Session,:,Session,.,remove,(,),Session,=,None,if,engine,:,engine,.,dispose,(,),engine,=,None,,,,,,,,,,Properly close pooled database connections,Properly,close,pooled,database,connections,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/settings.py#L166-L177,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/settings.py,prepare_classpath,"def prepare_classpath():
    """"""
    Ensures that certain subfolders of AIRFLOW_HOME are on the classpath
    """"""

    if DAGS_FOLDER not in sys.path:
        sys.path.append(DAGS_FOLDER)

    # Add ./config/ for loading custom log parsers etc, or
    # airflow_local_settings etc.
    config_path = os.path.join(AIRFLOW_HOME, 'config')
    if config_path not in sys.path:
        sys.path.append(config_path)

    if PLUGINS_FOLDER not in sys.path:
        sys.path.append(PLUGINS_FOLDER)",python,"def prepare_classpath():
    """"""
    Ensures that certain subfolders of AIRFLOW_HOME are on the classpath
    """"""

    if DAGS_FOLDER not in sys.path:
        sys.path.append(DAGS_FOLDER)

    # Add ./config/ for loading custom log parsers etc, or
    # airflow_local_settings etc.
    config_path = os.path.join(AIRFLOW_HOME, 'config')
    if config_path not in sys.path:
        sys.path.append(config_path)

    if PLUGINS_FOLDER not in sys.path:
        sys.path.append(PLUGINS_FOLDER)",def,prepare_classpath,(,),:,if,DAGS_FOLDER,not,in,sys,.,path,:,sys,.,path,.,append,(,DAGS_FOLDER,),"# Add ./config/ for loading custom log parsers etc, or",# airflow_local_settings etc.,config_path,=,os,.,path,.,join,(,AIRFLOW_HOME,",",'config',),if,config_path,not,in,sys,.,path,:,sys,.,path,.,append,(,config_path,),if,Ensures that certain subfolders of AIRFLOW_HOME are on the classpath,Ensures,that,certain,subfolders,of,AIRFLOW_HOME,are,on,the,classpath,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/settings.py#L220-L235,test,PLUGINS_FOLDER,not,in,sys,.,path,:,sys,.,path,.,append,(,PLUGINS_FOLDER,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/celery_queue_sensor.py,CeleryQueueSensor._check_task_id,"def _check_task_id(self, context):
        """"""
        Gets the returned Celery result from the Airflow task
        ID provided to the sensor, and returns True if the
        celery result has been finished execution.

        :param context: Airflow's execution context
        :type context: dict
        :return: True if task has been executed, otherwise False
        :rtype: bool
        """"""
        ti = context['ti']
        celery_result = ti.xcom_pull(task_ids=self.target_task_id)
        return celery_result.ready()",python,"def _check_task_id(self, context):
        """"""
        Gets the returned Celery result from the Airflow task
        ID provided to the sensor, and returns True if the
        celery result has been finished execution.

        :param context: Airflow's execution context
        :type context: dict
        :return: True if task has been executed, otherwise False
        :rtype: bool
        """"""
        ti = context['ti']
        celery_result = ti.xcom_pull(task_ids=self.target_task_id)
        return celery_result.ready()",def,_check_task_id,(,self,",",context,),:,ti,=,context,[,'ti',],celery_result,=,ti,.,xcom_pull,(,task_ids,=,self,.,target_task_id,),return,celery_result,.,ready,(,),,,,,,,,,,,,,,,,,,,,,"Gets the returned Celery result from the Airflow task
        ID provided to the sensor, and returns True if the
        celery result has been finished execution.

        :param context: Airflow's execution context
        :type context: dict
        :return: True if task has been executed, otherwise False
        :rtype: bool",Gets,the,returned,Celery,result,from,the,Airflow,task,ID,provided,to,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/celery_queue_sensor.py#L49-L62,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,sensor,and,returns,True,if,the,celery,result,has,been,finished,execution,.,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/security/kerberos.py,detect_conf_var,"def detect_conf_var():
    """"""Return true if the ticket cache contains ""conf"" information as is found
    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the
    Sun Java Krb5LoginModule in Java6, so we need to take an action to work
    around it.
    """"""
    ticket_cache = configuration.conf.get('kerberos', 'ccache')

    with open(ticket_cache, 'rb') as f:
        # Note: this file is binary, so we check against a bytearray.
        return b'X-CACHECONF:' in f.read()",python,"def detect_conf_var():
    """"""Return true if the ticket cache contains ""conf"" information as is found
    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the
    Sun Java Krb5LoginModule in Java6, so we need to take an action to work
    around it.
    """"""
    ticket_cache = configuration.conf.get('kerberos', 'ccache')

    with open(ticket_cache, 'rb') as f:
        # Note: this file is binary, so we check against a bytearray.
        return b'X-CACHECONF:' in f.read()",def,detect_conf_var,(,),:,ticket_cache,=,configuration,.,conf,.,get,(,'kerberos',",",'ccache',),with,open,(,ticket_cache,",",'rb',),as,f,:,"# Note: this file is binary, so we check against a bytearray.",return,b'X-CACHECONF:',in,f,.,read,(,),,,,,,,,,,,,,,,,,"Return true if the ticket cache contains ""conf"" information as is found
    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the
    Sun Java Krb5LoginModule in Java6, so we need to take an action to work
    around it.",Return,true,if,the,ticket,cache,contains,conf,information,as,is,found,in,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/security/kerberos.py#L100-L110,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ticket,caches,of,Kerberos,1,.,8,.,1,or,later,.,This,is,incompatible,with,the,Sun,Java,Krb5LoginModule,in,Java6,so,we,need,to,take,an,action,to,work,around,it,.,,,,,,
apache/airflow,airflow/utils/helpers.py,alchemy_to_dict,"def alchemy_to_dict(obj):
    """"""
    Transforms a SQLAlchemy model instance into a dictionary
    """"""
    if not obj:
        return None
    d = {}
    for c in obj.__table__.columns:
        value = getattr(obj, c.name)
        if type(value) == datetime:
            value = value.isoformat()
        d[c.name] = value
    return d",python,"def alchemy_to_dict(obj):
    """"""
    Transforms a SQLAlchemy model instance into a dictionary
    """"""
    if not obj:
        return None
    d = {}
    for c in obj.__table__.columns:
        value = getattr(obj, c.name)
        if type(value) == datetime:
            value = value.isoformat()
        d[c.name] = value
    return d",def,alchemy_to_dict,(,obj,),:,if,not,obj,:,return,None,d,=,{,},for,c,in,obj,.,__table__,.,columns,:,value,=,getattr,(,obj,",",c,.,name,),if,type,(,value,),==,datetime,:,value,=,value,.,isoformat,(,),d,[,Transforms a SQLAlchemy model instance into a dictionary,Transforms,a,SQLAlchemy,model,instance,into,a,dictionary,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L60-L72,test,c,.,name,],=,value,return,d,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/helpers.py,chunks,"def chunks(items, chunk_size):
    """"""
    Yield successive chunks of a given size from a list of items
    """"""
    if chunk_size <= 0:
        raise ValueError('Chunk size must be a positive integer')
    for i in range(0, len(items), chunk_size):
        yield items[i:i + chunk_size]",python,"def chunks(items, chunk_size):
    """"""
    Yield successive chunks of a given size from a list of items
    """"""
    if chunk_size <= 0:
        raise ValueError('Chunk size must be a positive integer')
    for i in range(0, len(items), chunk_size):
        yield items[i:i + chunk_size]",def,chunks,(,items,",",chunk_size,),:,if,chunk_size,<=,0,:,raise,ValueError,(,'Chunk size must be a positive integer',),for,i,in,range,(,0,",",len,(,items,),",",chunk_size,),:,yield,items,[,i,:,i,+,chunk_size,],,,,,,,,,,,Yield successive chunks of a given size from a list of items,Yield,successive,chunks,of,a,given,size,from,a,list,of,items,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L121-L128,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/helpers.py,reduce_in_chunks,"def reduce_in_chunks(fn, iterable, initializer, chunk_size=0):
    """"""
    Reduce the given list of items by splitting it into chunks
    of the given size and passing each chunk through the reducer
    """"""
    if len(iterable) == 0:
        return initializer
    if chunk_size == 0:
        chunk_size = len(iterable)
    return reduce(fn, chunks(iterable, chunk_size), initializer)",python,"def reduce_in_chunks(fn, iterable, initializer, chunk_size=0):
    """"""
    Reduce the given list of items by splitting it into chunks
    of the given size and passing each chunk through the reducer
    """"""
    if len(iterable) == 0:
        return initializer
    if chunk_size == 0:
        chunk_size = len(iterable)
    return reduce(fn, chunks(iterable, chunk_size), initializer)",def,reduce_in_chunks,(,fn,",",iterable,",",initializer,",",chunk_size,=,0,),:,if,len,(,iterable,),==,0,:,return,initializer,if,chunk_size,==,0,:,chunk_size,=,len,(,iterable,),return,reduce,(,fn,",",chunks,(,iterable,",",chunk_size,),",",initializer,),,,,"Reduce the given list of items by splitting it into chunks
    of the given size and passing each chunk through the reducer",Reduce,the,given,list,of,items,by,splitting,it,into,chunks,of,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L131-L140,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,given,size,and,passing,each,chunk,through,the,reducer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/helpers.py,chain,"def chain(*tasks):
    """"""
    Given a number of tasks, builds a dependency chain.

    chain(task_1, task_2, task_3, task_4)

    is equivalent to

    task_1.set_downstream(task_2)
    task_2.set_downstream(task_3)
    task_3.set_downstream(task_4)
    """"""
    for up_task, down_task in zip(tasks[:-1], tasks[1:]):
        up_task.set_downstream(down_task)",python,"def chain(*tasks):
    """"""
    Given a number of tasks, builds a dependency chain.

    chain(task_1, task_2, task_3, task_4)

    is equivalent to

    task_1.set_downstream(task_2)
    task_2.set_downstream(task_3)
    task_3.set_downstream(task_4)
    """"""
    for up_task, down_task in zip(tasks[:-1], tasks[1:]):
        up_task.set_downstream(down_task)",def,chain,(,*,tasks,),:,for,up_task,",",down_task,in,zip,(,tasks,[,:,-,1,],",",tasks,[,1,:,],),:,up_task,.,set_downstream,(,down_task,),,,,,,,,,,,,,,,,,,,"Given a number of tasks, builds a dependency chain.

    chain(task_1, task_2, task_3, task_4)

    is equivalent to

    task_1.set_downstream(task_2)
    task_2.set_downstream(task_3)
    task_3.set_downstream(task_4)",Given,a,number,of,tasks,builds,a,dependency,chain,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L153-L166,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/helpers.py,pprinttable,"def pprinttable(rows):
    """"""Returns a pretty ascii table from tuples

    If namedtuple are used, the table will have headers
    """"""
    if not rows:
        return
    if hasattr(rows[0], '_fields'):  # if namedtuple
        headers = rows[0]._fields
    else:
        headers = [""col{}"".format(i) for i in range(len(rows[0]))]
    lens = [len(s) for s in headers]

    for row in rows:
        for i in range(len(rows[0])):
            slenght = len(""{}"".format(row[i]))
            if slenght > lens[i]:
                lens[i] = slenght
    formats = []
    hformats = []
    for i in range(len(rows[0])):
        if isinstance(rows[0][i], int):
            formats.append(""%%%dd"" % lens[i])
        else:
            formats.append(""%%-%ds"" % lens[i])
        hformats.append(""%%-%ds"" % lens[i])
    pattern = "" | "".join(formats)
    hpattern = "" | "".join(hformats)
    separator = ""-+-"".join(['-' * n for n in lens])
    s = """"
    s += separator + '\n'
    s += (hpattern % tuple(headers)) + '\n'
    s += separator + '\n'

    def f(t):
        return ""{}"".format(t) if isinstance(t, basestring) else t

    for line in rows:
        s += pattern % tuple(f(t) for t in line) + '\n'
    s += separator + '\n'
    return s",python,"def pprinttable(rows):
    """"""Returns a pretty ascii table from tuples

    If namedtuple are used, the table will have headers
    """"""
    if not rows:
        return
    if hasattr(rows[0], '_fields'):  # if namedtuple
        headers = rows[0]._fields
    else:
        headers = [""col{}"".format(i) for i in range(len(rows[0]))]
    lens = [len(s) for s in headers]

    for row in rows:
        for i in range(len(rows[0])):
            slenght = len(""{}"".format(row[i]))
            if slenght > lens[i]:
                lens[i] = slenght
    formats = []
    hformats = []
    for i in range(len(rows[0])):
        if isinstance(rows[0][i], int):
            formats.append(""%%%dd"" % lens[i])
        else:
            formats.append(""%%-%ds"" % lens[i])
        hformats.append(""%%-%ds"" % lens[i])
    pattern = "" | "".join(formats)
    hpattern = "" | "".join(hformats)
    separator = ""-+-"".join(['-' * n for n in lens])
    s = """"
    s += separator + '\n'
    s += (hpattern % tuple(headers)) + '\n'
    s += separator + '\n'

    def f(t):
        return ""{}"".format(t) if isinstance(t, basestring) else t

    for line in rows:
        s += pattern % tuple(f(t) for t in line) + '\n'
    s += separator + '\n'
    return s",def,pprinttable,(,rows,),:,if,not,rows,:,return,if,hasattr,(,rows,[,0,],",",'_fields',),:,# if namedtuple,headers,=,rows,[,0,],.,_fields,else,:,headers,=,[,"""col{}""",.,format,(,i,),for,i,in,range,(,len,(,rows,[,0,"Returns a pretty ascii table from tuples

    If namedtuple are used, the table will have headers",Returns,a,pretty,ascii,table,from,tuples,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L200-L240,test,],),),],lens,=,[,len,(,s,),for,s,in,headers,],for,row,in,rows,:,for,i,in,range,(,len,(,rows,[,0,],),),:,slenght,=,len,(,"""{}""",.,format,(,row,[,i,],),),if,slenght,>,lens,[,i,],:,lens,[,i,],=,slenght,formats,=,[,],hformats,=,[,],for,i,in,range,(,len,(,rows,[,0,],),),:,if,isinstance,(,rows,[,0,],[,i,],",",int,),:,formats,.,append,(,"""%%%dd""",%,lens,[,i,],),else,:,formats,.,append,(,"""%%-%ds""",%,lens,[,i,],),hformats,.,append,(,"""%%-%ds""",%,lens,[,i,],),pattern,=,""" | """,.,join,(,formats,),hpattern,=,""" | """,.,join,(,hformats,),separator,=,"""-+-""",.,join,(,[,'-',*,n,for,n,in,lens,],),s,=,"""""",s,+=,separator,+,'\n',s,+=,(,hpattern,%,tuple,(,headers,),),+,'\n',s,+=,separator,+,'\n',def,f,(,t,),:,return,"""{}""",.,format,(,t,),if,isinstance,(,t,",",basestring,),else,t,for,line,in,rows,:,s,+=,pattern,%,tuple,(,f,(,t,),for,t,in,line,),+,'\n',s,+=,separator,+,'\n',return,s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/helpers.py,render_log_filename,"def render_log_filename(ti, try_number, filename_template):
    """"""
    Given task instance, try_number, filename_template, return the rendered log
    filename

    :param ti: task instance
    :param try_number: try_number of the task
    :param filename_template: filename template, which can be jinja template or
        python string template
    """"""
    filename_template, filename_jinja_template = parse_template_string(filename_template)
    if filename_jinja_template:
        jinja_context = ti.get_template_context()
        jinja_context['try_number'] = try_number
        return filename_jinja_template.render(**jinja_context)

    return filename_template.format(dag_id=ti.dag_id,
                                    task_id=ti.task_id,
                                    execution_date=ti.execution_date.isoformat(),
                                    try_number=try_number)",python,"def render_log_filename(ti, try_number, filename_template):
    """"""
    Given task instance, try_number, filename_template, return the rendered log
    filename

    :param ti: task instance
    :param try_number: try_number of the task
    :param filename_template: filename template, which can be jinja template or
        python string template
    """"""
    filename_template, filename_jinja_template = parse_template_string(filename_template)
    if filename_jinja_template:
        jinja_context = ti.get_template_context()
        jinja_context['try_number'] = try_number
        return filename_jinja_template.render(**jinja_context)

    return filename_template.format(dag_id=ti.dag_id,
                                    task_id=ti.task_id,
                                    execution_date=ti.execution_date.isoformat(),
                                    try_number=try_number)",def,render_log_filename,(,ti,",",try_number,",",filename_template,),:,filename_template,",",filename_jinja_template,=,parse_template_string,(,filename_template,),if,filename_jinja_template,:,jinja_context,=,ti,.,get_template_context,(,),jinja_context,[,'try_number',],=,try_number,return,filename_jinja_template,.,render,(,*,*,jinja_context,),return,filename_template,.,format,(,dag_id,=,ti,.,"Given task instance, try_number, filename_template, return the rendered log
    filename

    :param ti: task instance
    :param try_number: try_number of the task
    :param filename_template: filename template, which can be jinja template or
        python string template",Given,task,instance,try_number,filename_template,return,the,rendered,log,filename,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L299-L318,test,dag_id,",",task_id,=,ti,.,task_id,",",execution_date,=,ti,.,execution_date,.,isoformat,(,),",",try_number,=,try_number,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_dataproc_hook.py,DataProcHook.get_conn,"def get_conn(self):
        """"""Returns a Google Cloud Dataproc service object.""""""
        http_authorized = self._authorize()
        return build(
            'dataproc', self.api_version, http=http_authorized,
            cache_discovery=False)",python,"def get_conn(self):
        """"""Returns a Google Cloud Dataproc service object.""""""
        http_authorized = self._authorize()
        return build(
            'dataproc', self.api_version, http=http_authorized,
            cache_discovery=False)",def,get_conn,(,self,),:,http_authorized,=,self,.,_authorize,(,),return,build,(,'dataproc',",",self,.,api_version,",",http,=,http_authorized,",",cache_discovery,=,False,),,,,,,,,,,,,,,,,,,,,,,,Returns a Google Cloud Dataproc service object.,Returns,a,Google,Cloud,Dataproc,service,object,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_dataproc_hook.py#L218-L223,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_dataproc_hook.py,DataProcHook.wait,"def wait(self, operation):
        """"""Awaits for Google Cloud Dataproc Operation to complete.""""""
        submitted = _DataProcOperation(self.get_conn(), operation,
                                       self.num_retries)
        submitted.wait_for_done()",python,"def wait(self, operation):
        """"""Awaits for Google Cloud Dataproc Operation to complete.""""""
        submitted = _DataProcOperation(self.get_conn(), operation,
                                       self.num_retries)
        submitted.wait_for_done()",def,wait,(,self,",",operation,),:,submitted,=,_DataProcOperation,(,self,.,get_conn,(,),",",operation,",",self,.,num_retries,),submitted,.,wait_for_done,(,),,,,,,,,,,,,,,,,,,,,,,,,Awaits for Google Cloud Dataproc Operation to complete.,Awaits,for,Google,Cloud,Dataproc,Operation,to,complete,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_dataproc_hook.py#L243-L247,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/databricks_operator.py,_deep_string_coerce,"def _deep_string_coerce(content, json_path='json'):
    """"""
    Coerces content or all values of content if it is a dict to a string. The
    function will throw if content contains non-string or non-numeric types.

    The reason why we have this function is because the ``self.json`` field must be a
    dict with only string values. This is because ``render_template`` will fail
    for numerical values.
    """"""
    c = _deep_string_coerce
    if isinstance(content, six.string_types):
        return content
    elif isinstance(content, six.integer_types + (float,)):
        # Databricks can tolerate either numeric or string types in the API backend.
        return str(content)
    elif isinstance(content, (list, tuple)):
        return [c(e, '{0}[{1}]'.format(json_path, i)) for i, e in enumerate(content)]
    elif isinstance(content, dict):
        return {k: c(v, '{0}[{1}]'.format(json_path, k))
                for k, v in list(content.items())}
    else:
        param_type = type(content)
        msg = 'Type {0} used for parameter {1} is not a number or a string' \
            .format(param_type, json_path)
        raise AirflowException(msg)",python,"def _deep_string_coerce(content, json_path='json'):
    """"""
    Coerces content or all values of content if it is a dict to a string. The
    function will throw if content contains non-string or non-numeric types.

    The reason why we have this function is because the ``self.json`` field must be a
    dict with only string values. This is because ``render_template`` will fail
    for numerical values.
    """"""
    c = _deep_string_coerce
    if isinstance(content, six.string_types):
        return content
    elif isinstance(content, six.integer_types + (float,)):
        # Databricks can tolerate either numeric or string types in the API backend.
        return str(content)
    elif isinstance(content, (list, tuple)):
        return [c(e, '{0}[{1}]'.format(json_path, i)) for i, e in enumerate(content)]
    elif isinstance(content, dict):
        return {k: c(v, '{0}[{1}]'.format(json_path, k))
                for k, v in list(content.items())}
    else:
        param_type = type(content)
        msg = 'Type {0} used for parameter {1} is not a number or a string' \
            .format(param_type, json_path)
        raise AirflowException(msg)",def,_deep_string_coerce,(,content,",",json_path,=,'json',),:,c,=,_deep_string_coerce,if,isinstance,(,content,",",six,.,string_types,),:,return,content,elif,isinstance,(,content,",",six,.,integer_types,+,(,float,",",),),:,# Databricks can tolerate either numeric or string types in the API backend.,return,str,(,content,),elif,isinstance,(,content,",",(,"Coerces content or all values of content if it is a dict to a string. The
    function will throw if content contains non-string or non-numeric types.

    The reason why we have this function is because the ``self.json`` field must be a
    dict with only string values. This is because ``render_template`` will fail
    for numerical values.",Coerces,content,or,all,values,of,content,if,it,is,a,dict,to,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/databricks_operator.py#L34-L58,test,list,",",tuple,),),:,return,[,c,(,e,",",'{0}[{1}]',.,format,(,json_path,",",i,),),for,i,",",e,in,enumerate,(,content,),],elif,isinstance,(,content,",",dict,),:,return,{,k,:,c,(,v,",",'{0}[{1}]',.,format,(,json_path,",",k,),),for,k,",",v,in,list,(,content,.,items,(,),),},else,:,param_type,=,type,(,content,),msg,=,'Type {0} used for parameter {1} is not a number or a string',.,format,(,param_type,",",json_path,),raise,AirflowException,(,msg,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,a,string,.,The,function,will,throw,if,content,contains,non,-,string,or,non,-,numeric,types,.,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/databricks_operator.py,_handle_databricks_operator_execution,"def _handle_databricks_operator_execution(operator, hook, log, context):
    """"""
    Handles the Airflow + Databricks lifecycle logic for a Databricks operator

    :param operator: Databricks operator being handled
    :param context: Airflow context
    """"""
    if operator.do_xcom_push:
        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)
    log.info('Run submitted with run_id: %s', operator.run_id)
    run_page_url = hook.get_run_page_url(operator.run_id)
    if operator.do_xcom_push:
        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)

    log.info('View run status, Spark UI, and logs at %s', run_page_url)
    while True:
        run_state = hook.get_run_state(operator.run_id)
        if run_state.is_terminal:
            if run_state.is_successful:
                log.info('%s completed successfully.', operator.task_id)
                log.info('View run status, Spark UI, and logs at %s', run_page_url)
                return
            else:
                error_message = '{t} failed with terminal state: {s}'.format(
                    t=operator.task_id,
                    s=run_state)
                raise AirflowException(error_message)
        else:
            log.info('%s in run state: %s', operator.task_id, run_state)
            log.info('View run status, Spark UI, and logs at %s', run_page_url)
            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)
            time.sleep(operator.polling_period_seconds)",python,"def _handle_databricks_operator_execution(operator, hook, log, context):
    """"""
    Handles the Airflow + Databricks lifecycle logic for a Databricks operator

    :param operator: Databricks operator being handled
    :param context: Airflow context
    """"""
    if operator.do_xcom_push:
        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)
    log.info('Run submitted with run_id: %s', operator.run_id)
    run_page_url = hook.get_run_page_url(operator.run_id)
    if operator.do_xcom_push:
        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)

    log.info('View run status, Spark UI, and logs at %s', run_page_url)
    while True:
        run_state = hook.get_run_state(operator.run_id)
        if run_state.is_terminal:
            if run_state.is_successful:
                log.info('%s completed successfully.', operator.task_id)
                log.info('View run status, Spark UI, and logs at %s', run_page_url)
                return
            else:
                error_message = '{t} failed with terminal state: {s}'.format(
                    t=operator.task_id,
                    s=run_state)
                raise AirflowException(error_message)
        else:
            log.info('%s in run state: %s', operator.task_id, run_state)
            log.info('View run status, Spark UI, and logs at %s', run_page_url)
            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)
            time.sleep(operator.polling_period_seconds)",def,_handle_databricks_operator_execution,(,operator,",",hook,",",log,",",context,),:,if,operator,.,do_xcom_push,:,context,[,'ti',],.,xcom_push,(,key,=,XCOM_RUN_ID_KEY,",",value,=,operator,.,run_id,),log,.,info,(,'Run submitted with run_id: %s',",",operator,.,run_id,),run_page_url,=,hook,.,get_run_page_url,(,operator,.,"Handles the Airflow + Databricks lifecycle logic for a Databricks operator

    :param operator: Databricks operator being handled
    :param context: Airflow context",Handles,the,Airflow,+,Databricks,lifecycle,logic,for,a,Databricks,operator,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/databricks_operator.py#L61-L92,test,run_id,),if,operator,.,do_xcom_push,:,context,[,'ti',],.,xcom_push,(,key,=,XCOM_RUN_PAGE_URL_KEY,",",value,=,run_page_url,),log,.,info,(,"'View run status, Spark UI, and logs at %s'",",",run_page_url,),while,True,:,run_state,=,hook,.,get_run_state,(,operator,.,run_id,),if,run_state,.,is_terminal,:,if,run_state,.,is_successful,:,log,.,info,(,'%s completed successfully.',",",operator,.,task_id,),log,.,info,(,"'View run status, Spark UI, and logs at %s'",",",run_page_url,),return,else,:,error_message,=,'{t} failed with terminal state: {s}',.,format,(,t,=,operator,.,task_id,",",s,=,run_state,),raise,AirflowException,(,error_message,),else,:,log,.,info,(,'%s in run state: %s',",",operator,.,task_id,",",run_state,),log,.,info,(,"'View run status, Spark UI, and logs at %s'",",",run_page_url,),log,.,info,(,'Sleeping for %s seconds.',",",operator,.,polling_period_seconds,),time,.,sleep,(,operator,.,polling_period_seconds,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/pig_hook.py,PigCliHook.run_cli,"def run_cli(self, pig, verbose=True):
        """"""
        Run an pig script using the pig cli

        >>> ph = PigCliHook()
        >>> result = ph.run_cli(""ls /;"")
        >>> (""hdfs://"" in result)
        True
        """"""

        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:
            with NamedTemporaryFile(dir=tmp_dir) as f:
                f.write(pig.encode('utf-8'))
                f.flush()
                fname = f.name
                pig_bin = 'pig'
                cmd_extra = []

                pig_cmd = [pig_bin, '-f', fname] + cmd_extra

                if self.pig_properties:
                    pig_properties_list = self.pig_properties.split()
                    pig_cmd.extend(pig_properties_list)
                if verbose:
                    self.log.info(""%s"", "" "".join(pig_cmd))
                sp = subprocess.Popen(
                    pig_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    cwd=tmp_dir,
                    close_fds=True)
                self.sp = sp
                stdout = ''
                for line in iter(sp.stdout.readline, b''):
                    stdout += line.decode('utf-8')
                    if verbose:
                        self.log.info(line.strip())
                sp.wait()

                if sp.returncode:
                    raise AirflowException(stdout)

                return stdout",python,"def run_cli(self, pig, verbose=True):
        """"""
        Run an pig script using the pig cli

        >>> ph = PigCliHook()
        >>> result = ph.run_cli(""ls /;"")
        >>> (""hdfs://"" in result)
        True
        """"""

        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:
            with NamedTemporaryFile(dir=tmp_dir) as f:
                f.write(pig.encode('utf-8'))
                f.flush()
                fname = f.name
                pig_bin = 'pig'
                cmd_extra = []

                pig_cmd = [pig_bin, '-f', fname] + cmd_extra

                if self.pig_properties:
                    pig_properties_list = self.pig_properties.split()
                    pig_cmd.extend(pig_properties_list)
                if verbose:
                    self.log.info(""%s"", "" "".join(pig_cmd))
                sp = subprocess.Popen(
                    pig_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    cwd=tmp_dir,
                    close_fds=True)
                self.sp = sp
                stdout = ''
                for line in iter(sp.stdout.readline, b''):
                    stdout += line.decode('utf-8')
                    if verbose:
                        self.log.info(line.strip())
                sp.wait()

                if sp.returncode:
                    raise AirflowException(stdout)

                return stdout",def,run_cli,(,self,",",pig,",",verbose,=,True,),:,with,TemporaryDirectory,(,prefix,=,'airflow_pigop_',),as,tmp_dir,:,with,NamedTemporaryFile,(,dir,=,tmp_dir,),as,f,:,f,.,write,(,pig,.,encode,(,'utf-8',),),f,.,flush,(,),fname,=,f,.,"Run an pig script using the pig cli

        >>> ph = PigCliHook()
        >>> result = ph.run_cli(""ls /;"")
        >>> (""hdfs://"" in result)
        True",Run,an,pig,script,using,the,pig,cli,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/pig_hook.py#L45-L87,test,name,pig_bin,=,'pig',cmd_extra,=,[,],pig_cmd,=,[,pig_bin,",",'-f',",",fname,],+,cmd_extra,if,self,.,pig_properties,:,pig_properties_list,=,self,.,pig_properties,.,split,(,),pig_cmd,.,extend,(,pig_properties_list,),if,verbose,:,self,.,log,.,info,(,"""%s""",",",""" """,.,join,(,pig_cmd,),),sp,=,subprocess,.,Popen,(,pig_cmd,",",stdout,=,subprocess,.,PIPE,",",stderr,=,subprocess,.,STDOUT,",",cwd,=,tmp_dir,",",close_fds,=,True,),self,.,sp,=,sp,stdout,=,'',for,line,in,iter,(,sp,.,stdout,.,readline,",",b'',),:,stdout,+=,line,.,decode,(,'utf-8',),if,verbose,:,self,.,log,.,info,(,line,.,strip,(,),),sp,.,wait,(,),if,sp,.,returncode,:,raise,AirflowException,(,stdout,),return,stdout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/celery_executor.py,fetch_celery_task_state,"def fetch_celery_task_state(celery_task):
    """"""
    Fetch and return the state of the given celery task. The scope of this function is
    global so that it can be called by subprocesses in the pool.

    :param celery_task: a tuple of the Celery task key and the async Celery object used
        to fetch the task's state
    :type celery_task: tuple(str, celery.result.AsyncResult)
    :return: a tuple of the Celery task key and the Celery state of the task
    :rtype: tuple[str, str]
    """"""

    try:
        with timeout(seconds=2):
            # Accessing state property of celery task will make actual network request
            # to get the current state of the task.
            res = (celery_task[0], celery_task[1].state)
    except Exception as e:
        exception_traceback = ""Celery Task ID: {}\n{}"".format(celery_task[0],
                                                              traceback.format_exc())
        res = ExceptionWithTraceback(e, exception_traceback)
    return res",python,"def fetch_celery_task_state(celery_task):
    """"""
    Fetch and return the state of the given celery task. The scope of this function is
    global so that it can be called by subprocesses in the pool.

    :param celery_task: a tuple of the Celery task key and the async Celery object used
        to fetch the task's state
    :type celery_task: tuple(str, celery.result.AsyncResult)
    :return: a tuple of the Celery task key and the Celery state of the task
    :rtype: tuple[str, str]
    """"""

    try:
        with timeout(seconds=2):
            # Accessing state property of celery task will make actual network request
            # to get the current state of the task.
            res = (celery_task[0], celery_task[1].state)
    except Exception as e:
        exception_traceback = ""Celery Task ID: {}\n{}"".format(celery_task[0],
                                                              traceback.format_exc())
        res = ExceptionWithTraceback(e, exception_traceback)
    return res",def,fetch_celery_task_state,(,celery_task,),:,try,:,with,timeout,(,seconds,=,2,),:,# Accessing state property of celery task will make actual network request,# to get the current state of the task.,res,=,(,celery_task,[,0,],",",celery_task,[,1,],.,state,),except,Exception,as,e,:,exception_traceback,=,"""Celery Task ID: {}\n{}""",.,format,(,celery_task,[,0,],",",traceback,.,format_exc,"Fetch and return the state of the given celery task. The scope of this function is
    global so that it can be called by subprocesses in the pool.

    :param celery_task: a tuple of the Celery task key and the async Celery object used
        to fetch the task's state
    :type celery_task: tuple(str, celery.result.AsyncResult)
    :return: a tuple of the Celery task key and the Celery state of the task
    :rtype: tuple[str, str]",Fetch,and,return,the,state,of,the,given,celery,task,.,The,scope,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/celery_executor.py#L90-L111,test,(,),),res,=,ExceptionWithTraceback,(,e,",",exception_traceback,),return,res,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,of,this,function,is,global,so,that,it,can,be,called,by,subprocesses,in,the,pool,.,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/celery_executor.py,CeleryExecutor._num_tasks_per_send_process,"def _num_tasks_per_send_process(self, to_send_count):
        """"""
        How many Celery tasks should each worker process send.

        :return: Number of tasks that should be sent per process
        :rtype: int
        """"""
        return max(1,
                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))",python,"def _num_tasks_per_send_process(self, to_send_count):
        """"""
        How many Celery tasks should each worker process send.

        :return: Number of tasks that should be sent per process
        :rtype: int
        """"""
        return max(1,
                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))",def,_num_tasks_per_send_process,(,self,",",to_send_count,),:,return,max,(,1,",",int,(,math,.,ceil,(,1.0,*,to_send_count,/,self,.,_sync_parallelism,),),),,,,,,,,,,,,,,,,,,,,,,,,"How many Celery tasks should each worker process send.

        :return: Number of tasks that should be sent per process
        :rtype: int",How,many,Celery,tasks,should,each,worker,process,send,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/celery_executor.py#L158-L166,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/celery_executor.py,CeleryExecutor._num_tasks_per_fetch_process,"def _num_tasks_per_fetch_process(self):
        """"""
        How many Celery tasks should be sent to each worker process.

        :return: Number of tasks that should be used per process
        :rtype: int
        """"""
        return max(1,
                   int(math.ceil(1.0 * len(self.tasks) / self._sync_parallelism)))",python,"def _num_tasks_per_fetch_process(self):
        """"""
        How many Celery tasks should be sent to each worker process.

        :return: Number of tasks that should be used per process
        :rtype: int
        """"""
        return max(1,
                   int(math.ceil(1.0 * len(self.tasks) / self._sync_parallelism)))",def,_num_tasks_per_fetch_process,(,self,),:,return,max,(,1,",",int,(,math,.,ceil,(,1.0,*,len,(,self,.,tasks,),/,self,.,_sync_parallelism,),),),,,,,,,,,,,,,,,,,,,,,"How many Celery tasks should be sent to each worker process.

        :return: Number of tasks that should be used per process
        :rtype: int",How,many,Celery,tasks,should,be,sent,to,each,worker,process,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/celery_executor.py#L168-L176,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/variable.py,Variable.setdefault,"def setdefault(cls, key, default, deserialize_json=False):
        """"""
        Like a Python builtin dict object, setdefault returns the current value
        for a key, and if it isn't there, stores the default value and returns it.

        :param key: Dict key for this Variable
        :type key: str
        :param default: Default value to set and return if the variable
            isn't already in the DB
        :type default: Mixed
        :param deserialize_json: Store this as a JSON encoded value in the DB
            and un-encode it when retrieving a value
        :return: Mixed
        """"""
        obj = Variable.get(key, default_var=None,
                           deserialize_json=deserialize_json)
        if obj is None:
            if default is not None:
                Variable.set(key, default, serialize_json=deserialize_json)
                return default
            else:
                raise ValueError('Default Value must be set')
        else:
            return obj",python,"def setdefault(cls, key, default, deserialize_json=False):
        """"""
        Like a Python builtin dict object, setdefault returns the current value
        for a key, and if it isn't there, stores the default value and returns it.

        :param key: Dict key for this Variable
        :type key: str
        :param default: Default value to set and return if the variable
            isn't already in the DB
        :type default: Mixed
        :param deserialize_json: Store this as a JSON encoded value in the DB
            and un-encode it when retrieving a value
        :return: Mixed
        """"""
        obj = Variable.get(key, default_var=None,
                           deserialize_json=deserialize_json)
        if obj is None:
            if default is not None:
                Variable.set(key, default, serialize_json=deserialize_json)
                return default
            else:
                raise ValueError('Default Value must be set')
        else:
            return obj",def,setdefault,(,cls,",",key,",",default,",",deserialize_json,=,False,),:,obj,=,Variable,.,get,(,key,",",default_var,=,None,",",deserialize_json,=,deserialize_json,),if,obj,is,None,:,if,default,is,not,None,:,Variable,.,set,(,key,",",default,",",serialize_json,=,deserialize_json,"Like a Python builtin dict object, setdefault returns the current value
        for a key, and if it isn't there, stores the default value and returns it.

        :param key: Dict key for this Variable
        :type key: str
        :param default: Default value to set and return if the variable
            isn't already in the DB
        :type default: Mixed
        :param deserialize_json: Store this as a JSON encoded value in the DB
            and un-encode it when retrieving a value
        :return: Mixed",Like,a,Python,builtin,dict,object,setdefault,returns,the,current,value,for,a,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/variable.py#L76-L99,test,),return,default,else,:,raise,ValueError,(,'Default Value must be set',),else,:,return,obj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,key,and,if,it,isn,t,there,stores,the,default,value,and,returns,it,.,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.get_conn,"def get_conn(self):
        """"""
        Returns a Google MLEngine service object.
        """"""
        authed_http = self._authorize()
        return build('ml', 'v1', http=authed_http, cache_discovery=False)",python,"def get_conn(self):
        """"""
        Returns a Google MLEngine service object.
        """"""
        authed_http = self._authorize()
        return build('ml', 'v1', http=authed_http, cache_discovery=False)",def,get_conn,(,self,),:,authed_http,=,self,.,_authorize,(,),return,build,(,'ml',",",'v1',",",http,=,authed_http,",",cache_discovery,=,False,),,,,,,,,,,,,,,,,,,,,,,,,,Returns a Google MLEngine service object.,Returns,a,Google,MLEngine,service,object,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L53-L58,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.create_job,"def create_job(self, project_id, job, use_existing_job_fn=None):
        """"""
        Launches a MLEngine job and wait for it to reach a terminal state.

        :param project_id: The Google Cloud project id within which MLEngine
            job will be launched.
        :type project_id: str

        :param job: MLEngine Job object that should be provided to the MLEngine
            API, such as: ::

                {
                  'jobId': 'my_job_id',
                  'trainingInput': {
                    'scaleTier': 'STANDARD_1',
                    ...
                  }
                }

        :type job: dict

        :param use_existing_job_fn: In case that a MLEngine job with the same
            job_id already exist, this method (if provided) will decide whether
            we should use this existing job, continue waiting for it to finish
            and returning the job object. It should accepts a MLEngine job
            object, and returns a boolean value indicating whether it is OK to
            reuse the existing job. If 'use_existing_job_fn' is not provided,
            we by default reuse the existing MLEngine job.
        :type use_existing_job_fn: function

        :return: The MLEngine job object if the job successfully reach a
            terminal state (which might be FAILED or CANCELLED state).
        :rtype: dict
        """"""
        request = self._mlengine.projects().jobs().create(
            parent='projects/{}'.format(project_id),
            body=job)
        job_id = job['jobId']

        try:
            request.execute()
        except HttpError as e:
            # 409 means there is an existing job with the same job ID.
            if e.resp.status == 409:
                if use_existing_job_fn is not None:
                    existing_job = self._get_job(project_id, job_id)
                    if not use_existing_job_fn(existing_job):
                        self.log.error(
                            'Job with job_id %s already exist, but it does '
                            'not match our expectation: %s',
                            job_id, existing_job
                        )
                        raise
                self.log.info(
                    'Job with job_id %s already exist. Will waiting for it to finish',
                    job_id
                )
            else:
                self.log.error('Failed to create MLEngine job: {}'.format(e))
                raise

        return self._wait_for_job_done(project_id, job_id)",python,"def create_job(self, project_id, job, use_existing_job_fn=None):
        """"""
        Launches a MLEngine job and wait for it to reach a terminal state.

        :param project_id: The Google Cloud project id within which MLEngine
            job will be launched.
        :type project_id: str

        :param job: MLEngine Job object that should be provided to the MLEngine
            API, such as: ::

                {
                  'jobId': 'my_job_id',
                  'trainingInput': {
                    'scaleTier': 'STANDARD_1',
                    ...
                  }
                }

        :type job: dict

        :param use_existing_job_fn: In case that a MLEngine job with the same
            job_id already exist, this method (if provided) will decide whether
            we should use this existing job, continue waiting for it to finish
            and returning the job object. It should accepts a MLEngine job
            object, and returns a boolean value indicating whether it is OK to
            reuse the existing job. If 'use_existing_job_fn' is not provided,
            we by default reuse the existing MLEngine job.
        :type use_existing_job_fn: function

        :return: The MLEngine job object if the job successfully reach a
            terminal state (which might be FAILED or CANCELLED state).
        :rtype: dict
        """"""
        request = self._mlengine.projects().jobs().create(
            parent='projects/{}'.format(project_id),
            body=job)
        job_id = job['jobId']

        try:
            request.execute()
        except HttpError as e:
            # 409 means there is an existing job with the same job ID.
            if e.resp.status == 409:
                if use_existing_job_fn is not None:
                    existing_job = self._get_job(project_id, job_id)
                    if not use_existing_job_fn(existing_job):
                        self.log.error(
                            'Job with job_id %s already exist, but it does '
                            'not match our expectation: %s',
                            job_id, existing_job
                        )
                        raise
                self.log.info(
                    'Job with job_id %s already exist. Will waiting for it to finish',
                    job_id
                )
            else:
                self.log.error('Failed to create MLEngine job: {}'.format(e))
                raise

        return self._wait_for_job_done(project_id, job_id)",def,create_job,(,self,",",project_id,",",job,",",use_existing_job_fn,=,None,),:,request,=,self,.,_mlengine,.,projects,(,),.,jobs,(,),.,create,(,parent,=,'projects/{}',.,format,(,project_id,),",",body,=,job,),job_id,=,job,[,'jobId',],try,:,request,"Launches a MLEngine job and wait for it to reach a terminal state.

        :param project_id: The Google Cloud project id within which MLEngine
            job will be launched.
        :type project_id: str

        :param job: MLEngine Job object that should be provided to the MLEngine
            API, such as: ::

                {
                  'jobId': 'my_job_id',
                  'trainingInput': {
                    'scaleTier': 'STANDARD_1',
                    ...
                  }
                }

        :type job: dict

        :param use_existing_job_fn: In case that a MLEngine job with the same
            job_id already exist, this method (if provided) will decide whether
            we should use this existing job, continue waiting for it to finish
            and returning the job object. It should accepts a MLEngine job
            object, and returns a boolean value indicating whether it is OK to
            reuse the existing job. If 'use_existing_job_fn' is not provided,
            we by default reuse the existing MLEngine job.
        :type use_existing_job_fn: function

        :return: The MLEngine job object if the job successfully reach a
            terminal state (which might be FAILED or CANCELLED state).
        :rtype: dict",Launches,a,MLEngine,job,and,wait,for,it,to,reach,a,terminal,state,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L60-L121,test,.,execute,(,),except,HttpError,as,e,:,# 409 means there is an existing job with the same job ID.,if,e,.,resp,.,status,==,409,:,if,use_existing_job_fn,is,not,None,:,existing_job,=,self,.,_get_job,(,project_id,",",job_id,),if,not,use_existing_job_fn,(,existing_job,),:,self,.,log,.,error,(,"'Job with job_id %s already exist, but it does '",'not match our expectation: %s',",",job_id,",",existing_job,),raise,self,.,log,.,info,(,'Job with job_id %s already exist. Will waiting for it to finish',",",job_id,),else,:,self,.,log,.,error,(,'Failed to create MLEngine job: {}',.,format,(,e,),),raise,return,self,.,_wait_for_job_done,(,project_id,",",job_id,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook._get_job,"def _get_job(self, project_id, job_id):
        """"""
        Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server
        """"""
        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)
        request = self._mlengine.projects().jobs().get(name=job_name)
        while True:
            try:
                return request.execute()
            except HttpError as e:
                if e.resp.status == 429:
                    # polling after 30 seconds when quota failure occurs
                    time.sleep(30)
                else:
                    self.log.error('Failed to get MLEngine job: {}'.format(e))
                    raise",python,"def _get_job(self, project_id, job_id):
        """"""
        Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server
        """"""
        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)
        request = self._mlengine.projects().jobs().get(name=job_name)
        while True:
            try:
                return request.execute()
            except HttpError as e:
                if e.resp.status == 429:
                    # polling after 30 seconds when quota failure occurs
                    time.sleep(30)
                else:
                    self.log.error('Failed to get MLEngine job: {}'.format(e))
                    raise",def,_get_job,(,self,",",project_id,",",job_id,),:,job_name,=,'projects/{}/jobs/{}',.,format,(,project_id,",",job_id,),request,=,self,.,_mlengine,.,projects,(,),.,jobs,(,),.,get,(,name,=,job_name,),while,True,:,try,:,return,request,.,execute,(,),except,"Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server",Gets,a,MLEngine,job,based,on,the,job,name,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L123-L144,test,HttpError,as,e,:,if,e,.,resp,.,status,==,429,:,# polling after 30 seconds when quota failure occurs,time,.,sleep,(,30,),else,:,self,.,log,.,error,(,'Failed to get MLEngine job: {}',.,format,(,e,),),raise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook._wait_for_job_done,"def _wait_for_job_done(self, project_id, job_id, interval=30):
        """"""
        Waits for the Job to reach a terminal state.

        This method will periodically check the job state until the job reach
        a terminal state.

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned when getting
            the job
        """"""
        if interval <= 0:
            raise ValueError(""Interval must be > 0"")
        while True:
            job = self._get_job(project_id, job_id)
            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                return job
            time.sleep(interval)",python,"def _wait_for_job_done(self, project_id, job_id, interval=30):
        """"""
        Waits for the Job to reach a terminal state.

        This method will periodically check the job state until the job reach
        a terminal state.

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned when getting
            the job
        """"""
        if interval <= 0:
            raise ValueError(""Interval must be > 0"")
        while True:
            job = self._get_job(project_id, job_id)
            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                return job
            time.sleep(interval)",def,_wait_for_job_done,(,self,",",project_id,",",job_id,",",interval,=,30,),:,if,interval,<=,0,:,raise,ValueError,(,"""Interval must be > 0""",),while,True,:,job,=,self,.,_get_job,(,project_id,",",job_id,),if,job,[,'state',],in,[,'SUCCEEDED',",",'FAILED',",",'CANCELLED',],:,return,"Waits for the Job to reach a terminal state.

        This method will periodically check the job state until the job reach
        a terminal state.

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned when getting
            the job",Waits,for,the,Job,to,reach,a,terminal,state,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L146-L163,test,job,time,.,sleep,(,interval,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.create_version,"def create_version(self, project_id, model_name, version_spec):
        """"""
        Creates the Version on Google Cloud ML Engine.

        Returns the operation if the version was created successfully and
        raises an error otherwise.
        """"""
        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)
        create_request = self._mlengine.projects().models().versions().create(
            parent=parent_name, body=version_spec)
        response = create_request.execute()
        get_request = self._mlengine.projects().operations().get(
            name=response['name'])

        return _poll_with_exponential_delay(
            request=get_request,
            max_n=9,
            is_done_func=lambda resp: resp.get('done', False),
            is_error_func=lambda resp: resp.get('error', None) is not None)",python,"def create_version(self, project_id, model_name, version_spec):
        """"""
        Creates the Version on Google Cloud ML Engine.

        Returns the operation if the version was created successfully and
        raises an error otherwise.
        """"""
        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)
        create_request = self._mlengine.projects().models().versions().create(
            parent=parent_name, body=version_spec)
        response = create_request.execute()
        get_request = self._mlengine.projects().operations().get(
            name=response['name'])

        return _poll_with_exponential_delay(
            request=get_request,
            max_n=9,
            is_done_func=lambda resp: resp.get('done', False),
            is_error_func=lambda resp: resp.get('error', None) is not None)",def,create_version,(,self,",",project_id,",",model_name,",",version_spec,),:,parent_name,=,'projects/{}/models/{}',.,format,(,project_id,",",model_name,),create_request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,versions,(,),.,create,(,parent,=,parent_name,",",body,=,version_spec,),response,=,"Creates the Version on Google Cloud ML Engine.

        Returns the operation if the version was created successfully and
        raises an error otherwise.",Creates,the,Version,on,Google,Cloud,ML,Engine,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L165-L183,test,create_request,.,execute,(,),get_request,=,self,.,_mlengine,.,projects,(,),.,operations,(,),.,get,(,name,=,response,[,'name',],),return,_poll_with_exponential_delay,(,request,=,get_request,",",max_n,=,9,",",is_done_func,=,lambda,resp,:,resp,.,get,(,'done',",",False,),",",is_error_func,=,lambda,resp,:,resp,.,get,(,'error',",",None,),is,not,None,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.set_default_version,"def set_default_version(self, project_id, model_name, version_name):
        """"""
        Sets a version to be the default. Blocks until finished.
        """"""
        full_version_name = 'projects/{}/models/{}/versions/{}'.format(
            project_id, model_name, version_name)
        request = self._mlengine.projects().models().versions().setDefault(
            name=full_version_name, body={})

        try:
            response = request.execute()
            self.log.info('Successfully set version: %s to default', response)
            return response
        except HttpError as e:
            self.log.error('Something went wrong: %s', e)
            raise",python,"def set_default_version(self, project_id, model_name, version_name):
        """"""
        Sets a version to be the default. Blocks until finished.
        """"""
        full_version_name = 'projects/{}/models/{}/versions/{}'.format(
            project_id, model_name, version_name)
        request = self._mlengine.projects().models().versions().setDefault(
            name=full_version_name, body={})

        try:
            response = request.execute()
            self.log.info('Successfully set version: %s to default', response)
            return response
        except HttpError as e:
            self.log.error('Something went wrong: %s', e)
            raise",def,set_default_version,(,self,",",project_id,",",model_name,",",version_name,),:,full_version_name,=,'projects/{}/models/{}/versions/{}',.,format,(,project_id,",",model_name,",",version_name,),request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,versions,(,),.,setDefault,(,name,=,full_version_name,",",body,=,{,},Sets a version to be the default. Blocks until finished.,Sets,a,version,to,be,the,default,.,Blocks,until,finished,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L185-L200,test,),try,:,response,=,request,.,execute,(,),self,.,log,.,info,(,'Successfully set version: %s to default',",",response,),return,response,except,HttpError,as,e,:,self,.,log,.,error,(,'Something went wrong: %s',",",e,),raise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.list_versions,"def list_versions(self, project_id, model_name):
        """"""
        Lists all available versions of a model. Blocks until finished.
        """"""
        result = []
        full_parent_name = 'projects/{}/models/{}'.format(
            project_id, model_name)
        request = self._mlengine.projects().models().versions().list(
            parent=full_parent_name, pageSize=100)

        response = request.execute()
        next_page_token = response.get('nextPageToken', None)
        result.extend(response.get('versions', []))
        while next_page_token is not None:
            next_request = self._mlengine.projects().models().versions().list(
                parent=full_parent_name,
                pageToken=next_page_token,
                pageSize=100)
            response = next_request.execute()
            next_page_token = response.get('nextPageToken', None)
            result.extend(response.get('versions', []))
            time.sleep(5)
        return result",python,"def list_versions(self, project_id, model_name):
        """"""
        Lists all available versions of a model. Blocks until finished.
        """"""
        result = []
        full_parent_name = 'projects/{}/models/{}'.format(
            project_id, model_name)
        request = self._mlengine.projects().models().versions().list(
            parent=full_parent_name, pageSize=100)

        response = request.execute()
        next_page_token = response.get('nextPageToken', None)
        result.extend(response.get('versions', []))
        while next_page_token is not None:
            next_request = self._mlengine.projects().models().versions().list(
                parent=full_parent_name,
                pageToken=next_page_token,
                pageSize=100)
            response = next_request.execute()
            next_page_token = response.get('nextPageToken', None)
            result.extend(response.get('versions', []))
            time.sleep(5)
        return result",def,list_versions,(,self,",",project_id,",",model_name,),:,result,=,[,],full_parent_name,=,'projects/{}/models/{}',.,format,(,project_id,",",model_name,),request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,versions,(,),.,list,(,parent,=,full_parent_name,",",pageSize,=,100,),Lists all available versions of a model. Blocks until finished.,Lists,all,available,versions,of,a,model,.,Blocks,until,finished,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L202-L224,test,response,=,request,.,execute,(,),next_page_token,=,response,.,get,(,'nextPageToken',",",None,),result,.,extend,(,response,.,get,(,'versions',",",[,],),),while,next_page_token,is,not,None,:,next_request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,versions,(,),.,list,(,parent,=,full_parent_name,",",pageToken,=,next_page_token,",",pageSize,=,100,),response,=,next_request,.,execute,(,),next_page_token,=,response,.,get,(,'nextPageToken',",",None,),result,.,extend,(,response,.,get,(,'versions',",",[,],),),time,.,sleep,(,5,),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.delete_version,"def delete_version(self, project_id, model_name, version_name):
        """"""
        Deletes the given version of a model. Blocks until finished.
        """"""
        full_name = 'projects/{}/models/{}/versions/{}'.format(
            project_id, model_name, version_name)
        delete_request = self._mlengine.projects().models().versions().delete(
            name=full_name)
        response = delete_request.execute()
        get_request = self._mlengine.projects().operations().get(
            name=response['name'])

        return _poll_with_exponential_delay(
            request=get_request,
            max_n=9,
            is_done_func=lambda resp: resp.get('done', False),
            is_error_func=lambda resp: resp.get('error', None) is not None)",python,"def delete_version(self, project_id, model_name, version_name):
        """"""
        Deletes the given version of a model. Blocks until finished.
        """"""
        full_name = 'projects/{}/models/{}/versions/{}'.format(
            project_id, model_name, version_name)
        delete_request = self._mlengine.projects().models().versions().delete(
            name=full_name)
        response = delete_request.execute()
        get_request = self._mlengine.projects().operations().get(
            name=response['name'])

        return _poll_with_exponential_delay(
            request=get_request,
            max_n=9,
            is_done_func=lambda resp: resp.get('done', False),
            is_error_func=lambda resp: resp.get('error', None) is not None)",def,delete_version,(,self,",",project_id,",",model_name,",",version_name,),:,full_name,=,'projects/{}/models/{}/versions/{}',.,format,(,project_id,",",model_name,",",version_name,),delete_request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,versions,(,),.,delete,(,name,=,full_name,),response,=,delete_request,.,Deletes the given version of a model. Blocks until finished.,Deletes,the,given,version,of,a,model,.,Blocks,until,finished,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L226-L242,test,execute,(,),get_request,=,self,.,_mlengine,.,projects,(,),.,operations,(,),.,get,(,name,=,response,[,'name',],),return,_poll_with_exponential_delay,(,request,=,get_request,",",max_n,=,9,",",is_done_func,=,lambda,resp,:,resp,.,get,(,'done',",",False,),",",is_error_func,=,lambda,resp,:,resp,.,get,(,'error',",",None,),is,not,None,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.create_model,"def create_model(self, project_id, model):
        """"""
        Create a Model. Blocks until finished.
        """"""
        if not model['name']:
            raise ValueError(""Model name must be provided and ""
                             ""could not be an empty string"")
        project = 'projects/{}'.format(project_id)

        request = self._mlengine.projects().models().create(
            parent=project, body=model)
        return request.execute()",python,"def create_model(self, project_id, model):
        """"""
        Create a Model. Blocks until finished.
        """"""
        if not model['name']:
            raise ValueError(""Model name must be provided and ""
                             ""could not be an empty string"")
        project = 'projects/{}'.format(project_id)

        request = self._mlengine.projects().models().create(
            parent=project, body=model)
        return request.execute()",def,create_model,(,self,",",project_id,",",model,),:,if,not,model,[,'name',],:,raise,ValueError,(,"""Model name must be provided and ""","""could not be an empty string""",),project,=,'projects/{}',.,format,(,project_id,),request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,create,(,parent,=,project,",",body,Create a Model. Blocks until finished.,Create,a,Model,.,Blocks,until,finished,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L244-L255,test,=,model,),return,request,.,execute,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,MLEngineHook.get_model,"def get_model(self, project_id, model_name):
        """"""
        Gets a Model. Blocks until finished.
        """"""
        if not model_name:
            raise ValueError(""Model name must be provided and ""
                             ""it could not be an empty string"")
        full_model_name = 'projects/{}/models/{}'.format(
            project_id, model_name)
        request = self._mlengine.projects().models().get(name=full_model_name)
        try:
            return request.execute()
        except HttpError as e:
            if e.resp.status == 404:
                self.log.error('Model was not found: %s', e)
                return None
            raise",python,"def get_model(self, project_id, model_name):
        """"""
        Gets a Model. Blocks until finished.
        """"""
        if not model_name:
            raise ValueError(""Model name must be provided and ""
                             ""it could not be an empty string"")
        full_model_name = 'projects/{}/models/{}'.format(
            project_id, model_name)
        request = self._mlengine.projects().models().get(name=full_model_name)
        try:
            return request.execute()
        except HttpError as e:
            if e.resp.status == 404:
                self.log.error('Model was not found: %s', e)
                return None
            raise",def,get_model,(,self,",",project_id,",",model_name,),:,if,not,model_name,:,raise,ValueError,(,"""Model name must be provided and ""","""it could not be an empty string""",),full_model_name,=,'projects/{}/models/{}',.,format,(,project_id,",",model_name,),request,=,self,.,_mlengine,.,projects,(,),.,models,(,),.,get,(,name,=,full_model_name,),try,:,Gets a Model. Blocks until finished.,Gets,a,Model,.,Blocks,until,finished,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L257-L273,test,return,request,.,execute,(,),except,HttpError,as,e,:,if,e,.,resp,.,status,==,404,:,self,.,log,.,error,(,'Model was not found: %s',",",e,),return,None,raise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_dynamodb_hook.py,AwsDynamoDBHook.write_batch_data,"def write_batch_data(self, items):
        """"""
        Write batch items to dynamodb table with provisioned throughout capacity.
        """"""

        dynamodb_conn = self.get_conn()

        try:
            table = dynamodb_conn.Table(self.table_name)

            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:
                for item in items:
                    batch.put_item(Item=item)
            return True
        except Exception as general_error:
            raise AirflowException(
                'Failed to insert items in dynamodb, error: {error}'.format(
                    error=str(general_error)
                )
            )",python,"def write_batch_data(self, items):
        """"""
        Write batch items to dynamodb table with provisioned throughout capacity.
        """"""

        dynamodb_conn = self.get_conn()

        try:
            table = dynamodb_conn.Table(self.table_name)

            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:
                for item in items:
                    batch.put_item(Item=item)
            return True
        except Exception as general_error:
            raise AirflowException(
                'Failed to insert items in dynamodb, error: {error}'.format(
                    error=str(general_error)
                )
            )",def,write_batch_data,(,self,",",items,),:,dynamodb_conn,=,self,.,get_conn,(,),try,:,table,=,dynamodb_conn,.,Table,(,self,.,table_name,),with,table,.,batch_writer,(,overwrite_by_pkeys,=,self,.,table_keys,),as,batch,:,for,item,in,items,:,batch,.,put_item,(,Item,=,Write batch items to dynamodb table with provisioned throughout capacity.,Write,batch,items,to,dynamodb,table,with,provisioned,throughout,capacity,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_dynamodb_hook.py#L50-L69,test,item,),return,True,except,Exception,as,general_error,:,raise,AirflowException,(,"'Failed to insert items in dynamodb, error: {error}'",.,format,(,error,=,str,(,general_error,),),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/__init__.py,_integrate_plugins,"def _integrate_plugins():
    """"""Integrate plugins to the context.""""""
    from airflow.plugins_manager import executors_modules
    for executors_module in executors_modules:
        sys.modules[executors_module.__name__] = executors_module
        globals()[executors_module._name] = executors_module",python,"def _integrate_plugins():
    """"""Integrate plugins to the context.""""""
    from airflow.plugins_manager import executors_modules
    for executors_module in executors_modules:
        sys.modules[executors_module.__name__] = executors_module
        globals()[executors_module._name] = executors_module",def,_integrate_plugins,(,),:,from,airflow,.,plugins_manager,import,executors_modules,for,executors_module,in,executors_modules,:,sys,.,modules,[,executors_module,.,__name__,],=,executors_module,globals,(,),[,executors_module,.,_name,],=,executors_module,,,,,,,,,,,,,,,,,Integrate plugins to the context.,Integrate,plugins,to,the,context,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/__init__.py#L31-L36,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/__init__.py,get_default_executor,"def get_default_executor():
    """"""Creates a new instance of the configured executor if none exists and returns it""""""
    global DEFAULT_EXECUTOR

    if DEFAULT_EXECUTOR is not None:
        return DEFAULT_EXECUTOR

    executor_name = configuration.conf.get('core', 'EXECUTOR')

    DEFAULT_EXECUTOR = _get_executor(executor_name)

    log = LoggingMixin().log
    log.info(""Using executor %s"", executor_name)

    return DEFAULT_EXECUTOR",python,"def get_default_executor():
    """"""Creates a new instance of the configured executor if none exists and returns it""""""
    global DEFAULT_EXECUTOR

    if DEFAULT_EXECUTOR is not None:
        return DEFAULT_EXECUTOR

    executor_name = configuration.conf.get('core', 'EXECUTOR')

    DEFAULT_EXECUTOR = _get_executor(executor_name)

    log = LoggingMixin().log
    log.info(""Using executor %s"", executor_name)

    return DEFAULT_EXECUTOR",def,get_default_executor,(,),:,global,DEFAULT_EXECUTOR,if,DEFAULT_EXECUTOR,is,not,None,:,return,DEFAULT_EXECUTOR,executor_name,=,configuration,.,conf,.,get,(,'core',",",'EXECUTOR',),DEFAULT_EXECUTOR,=,_get_executor,(,executor_name,),log,=,LoggingMixin,(,),.,log,log,.,info,(,"""Using executor %s""",",",executor_name,),return,DEFAULT_EXECUTOR,,,Creates a new instance of the configured executor if none exists and returns it,Creates,a,new,instance,of,the,configured,executor,if,none,exists,and,returns,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/__init__.py#L39-L53,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/executors/__init__.py,_get_executor,"def _get_executor(executor_name):
    """"""
    Creates a new instance of the named executor.
    In case the executor name is not know in airflow,
    look for it in the plugins
    """"""
    if executor_name == Executors.LocalExecutor:
        return LocalExecutor()
    elif executor_name == Executors.SequentialExecutor:
        return SequentialExecutor()
    elif executor_name == Executors.CeleryExecutor:
        from airflow.executors.celery_executor import CeleryExecutor
        return CeleryExecutor()
    elif executor_name == Executors.DaskExecutor:
        from airflow.executors.dask_executor import DaskExecutor
        return DaskExecutor()
    elif executor_name == Executors.KubernetesExecutor:
        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor
        return KubernetesExecutor()
    else:
        # Loading plugins
        _integrate_plugins()
        executor_path = executor_name.split('.')
        if len(executor_path) != 2:
            raise AirflowException(
                ""Executor {0} not supported: ""
                ""please specify in format plugin_module.executor"".format(executor_name))

        if executor_path[0] in globals():
            return globals()[executor_path[0]].__dict__[executor_path[1]]()
        else:
            raise AirflowException(""Executor {0} not supported."".format(executor_name))",python,"def _get_executor(executor_name):
    """"""
    Creates a new instance of the named executor.
    In case the executor name is not know in airflow,
    look for it in the plugins
    """"""
    if executor_name == Executors.LocalExecutor:
        return LocalExecutor()
    elif executor_name == Executors.SequentialExecutor:
        return SequentialExecutor()
    elif executor_name == Executors.CeleryExecutor:
        from airflow.executors.celery_executor import CeleryExecutor
        return CeleryExecutor()
    elif executor_name == Executors.DaskExecutor:
        from airflow.executors.dask_executor import DaskExecutor
        return DaskExecutor()
    elif executor_name == Executors.KubernetesExecutor:
        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor
        return KubernetesExecutor()
    else:
        # Loading plugins
        _integrate_plugins()
        executor_path = executor_name.split('.')
        if len(executor_path) != 2:
            raise AirflowException(
                ""Executor {0} not supported: ""
                ""please specify in format plugin_module.executor"".format(executor_name))

        if executor_path[0] in globals():
            return globals()[executor_path[0]].__dict__[executor_path[1]]()
        else:
            raise AirflowException(""Executor {0} not supported."".format(executor_name))",def,_get_executor,(,executor_name,),:,if,executor_name,==,Executors,.,LocalExecutor,:,return,LocalExecutor,(,),elif,executor_name,==,Executors,.,SequentialExecutor,:,return,SequentialExecutor,(,),elif,executor_name,==,Executors,.,CeleryExecutor,:,from,airflow,.,executors,.,celery_executor,import,CeleryExecutor,return,CeleryExecutor,(,),elif,executor_name,==,Executors,.,"Creates a new instance of the named executor.
    In case the executor name is not know in airflow,
    look for it in the plugins",Creates,a,new,instance,of,the,named,executor,.,In,case,the,executor,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/__init__.py#L64-L95,test,DaskExecutor,:,from,airflow,.,executors,.,dask_executor,import,DaskExecutor,return,DaskExecutor,(,),elif,executor_name,==,Executors,.,KubernetesExecutor,:,from,airflow,.,contrib,.,executors,.,kubernetes_executor,import,KubernetesExecutor,return,KubernetesExecutor,(,),else,:,# Loading plugins,_integrate_plugins,(,),executor_path,=,executor_name,.,split,(,'.',),if,len,(,executor_path,),!=,2,:,raise,AirflowException,(,"""Executor {0} not supported: ""","""please specify in format plugin_module.executor""",.,format,(,executor_name,),),if,executor_path,[,0,],in,globals,(,),:,return,globals,(,),[,executor_path,[,0,],],.,__dict__,[,executor_path,[,1,],],(,),else,:,raise,AirflowException,(,"""Executor {0} not supported.""",.,format,(,executor_name,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,name,is,not,know,in,airflow,look,for,it,in,the,plugins,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/segment_hook.py,SegmentHook.on_error,"def on_error(self, error, items):
        """"""
        Handles error callbacks when using Segment with segment_debug_mode set to True
        """"""
        self.log.error('Encountered Segment error: {segment_error} with '
                       'items: {with_items}'.format(segment_error=error,
                                                    with_items=items))
        raise AirflowException('Segment error: {}'.format(error))",python,"def on_error(self, error, items):
        """"""
        Handles error callbacks when using Segment with segment_debug_mode set to True
        """"""
        self.log.error('Encountered Segment error: {segment_error} with '
                       'items: {with_items}'.format(segment_error=error,
                                                    with_items=items))
        raise AirflowException('Segment error: {}'.format(error))",def,on_error,(,self,",",error,",",items,),:,self,.,log,.,error,(,'Encountered Segment error: {segment_error} with ','items: {with_items}',.,format,(,segment_error,=,error,",",with_items,=,items,),),raise,AirflowException,(,'Segment error: {}',.,format,(,error,),),,,,,,,,,,,,,Handles error callbacks when using Segment with segment_debug_mode set to True,Handles,error,callbacks,when,using,Segment,with,segment_debug_mode,set,to,True,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/segment_hook.py#L83-L90,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/mssql_hook.py,MsSqlHook.get_conn,"def get_conn(self):
        """"""
        Returns a mssql connection object
        """"""
        conn = self.get_connection(self.mssql_conn_id)
        conn = pymssql.connect(
            server=conn.host,
            user=conn.login,
            password=conn.password,
            database=self.schema or conn.schema,
            port=conn.port)
        return conn",python,"def get_conn(self):
        """"""
        Returns a mssql connection object
        """"""
        conn = self.get_connection(self.mssql_conn_id)
        conn = pymssql.connect(
            server=conn.host,
            user=conn.login,
            password=conn.password,
            database=self.schema or conn.schema,
            port=conn.port)
        return conn",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,mssql_conn_id,),conn,=,pymssql,.,connect,(,server,=,conn,.,host,",",user,=,conn,.,login,",",password,=,conn,.,password,",",database,=,self,.,schema,or,conn,.,schema,",",port,=,Returns a mssql connection object,Returns,a,mssql,connection,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49,test,conn,.,port,),return,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/api/experimental/endpoints.py,trigger_dag,"def trigger_dag(dag_id):
    """"""
    Trigger a new dag run for a Dag with an execution date of now unless
    specified in the data.
    """"""
    data = request.get_json(force=True)

    run_id = None
    if 'run_id' in data:
        run_id = data['run_id']

    conf = None
    if 'conf' in data:
        conf = data['conf']

    execution_date = None
    if 'execution_date' in data and data['execution_date'] is not None:
        execution_date = data['execution_date']

        # Convert string datetime into actual datetime
        try:
            execution_date = timezone.parse(execution_date)
        except ValueError:
            error_message = (
                'Given execution date, {}, could not be identified '
                'as a date. Example date format: 2015-11-16T14:34:15+00:00'
                .format(execution_date))
            _log.info(error_message)
            response = jsonify({'error': error_message})
            response.status_code = 400

            return response

    try:
        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    if getattr(g, 'user', None):
        _log.info(""User %s created %s"", g.user, dr)

    response = jsonify(message=""Created {}"".format(dr))
    return response",python,"def trigger_dag(dag_id):
    """"""
    Trigger a new dag run for a Dag with an execution date of now unless
    specified in the data.
    """"""
    data = request.get_json(force=True)

    run_id = None
    if 'run_id' in data:
        run_id = data['run_id']

    conf = None
    if 'conf' in data:
        conf = data['conf']

    execution_date = None
    if 'execution_date' in data and data['execution_date'] is not None:
        execution_date = data['execution_date']

        # Convert string datetime into actual datetime
        try:
            execution_date = timezone.parse(execution_date)
        except ValueError:
            error_message = (
                'Given execution date, {}, could not be identified '
                'as a date. Example date format: 2015-11-16T14:34:15+00:00'
                .format(execution_date))
            _log.info(error_message)
            response = jsonify({'error': error_message})
            response.status_code = 400

            return response

    try:
        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    if getattr(g, 'user', None):
        _log.info(""User %s created %s"", g.user, dr)

    response = jsonify(message=""Created {}"".format(dr))
    return response",def,trigger_dag,(,dag_id,),:,data,=,request,.,get_json,(,force,=,True,),run_id,=,None,if,'run_id',in,data,:,run_id,=,data,[,'run_id',],conf,=,None,if,'conf',in,data,:,conf,=,data,[,'conf',],execution_date,=,None,if,'execution_date',in,data,and,"Trigger a new dag run for a Dag with an execution date of now unless
    specified in the data.",Trigger,a,new,dag,run,for,a,Dag,with,an,execution,date,of,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L47-L92,test,data,[,'execution_date',],is,not,None,:,execution_date,=,data,[,'execution_date',],# Convert string datetime into actual datetime,try,:,execution_date,=,timezone,.,parse,(,execution_date,),except,ValueError,:,error_message,=,(,"'Given execution date, {}, could not be identified '",'as a date. Example date format: 2015-11-16T14:34:15+00:00',.,format,(,execution_date,),),_log,.,info,(,error_message,),response,=,jsonify,(,{,'error',:,error_message,},),response,.,status_code,=,400,return,response,try,:,dr,=,trigger,.,trigger_dag,(,dag_id,",",run_id,",",conf,",",execution_date,),except,AirflowException,as,err,:,_log,.,error,(,err,),response,=,jsonify,(,error,=,"""{}""",.,format,(,err,),),response,.,status_code,=,err,.,status_code,return,response,if,getattr,(,g,",",'user',",",None,),:,_log,.,info,(,"""User %s created %s""",",",g,.,user,",",dr,),response,=,jsonify,(,message,=,"""Created {}""",.,format,(,dr,),),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,now,unless,specified,in,the,data,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/api/experimental/endpoints.py,delete_dag,"def delete_dag(dag_id):
    """"""
    Delete all DB records related to the specified Dag.
    """"""
    try:
        count = delete.delete_dag(dag_id)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    return jsonify(message=""Removed {} record(s)"".format(count), count=count)",python,"def delete_dag(dag_id):
    """"""
    Delete all DB records related to the specified Dag.
    """"""
    try:
        count = delete.delete_dag(dag_id)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    return jsonify(message=""Removed {} record(s)"".format(count), count=count)",def,delete_dag,(,dag_id,),:,try,:,count,=,delete,.,delete_dag,(,dag_id,),except,AirflowException,as,err,:,_log,.,error,(,err,),response,=,jsonify,(,error,=,"""{}""",.,format,(,err,),),response,.,status_code,=,err,.,status_code,return,response,return,jsonify,(,Delete all DB records related to the specified Dag.,Delete,all,DB,records,related,to,the,specified,Dag,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L98-L109,test,message,=,"""Removed {} record(s)""",.,format,(,count,),",",count,=,count,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/api/experimental/endpoints.py,task_info,"def task_info(dag_id, task_id):
    """"""Returns a JSON with a task's public instance variables. """"""
    try:
        info = get_task(dag_id, task_id)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    # JSONify and return.
    fields = {k: str(v)
              for k, v in vars(info).items()
              if not k.startswith('_')}
    return jsonify(fields)",python,"def task_info(dag_id, task_id):
    """"""Returns a JSON with a task's public instance variables. """"""
    try:
        info = get_task(dag_id, task_id)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    # JSONify and return.
    fields = {k: str(v)
              for k, v in vars(info).items()
              if not k.startswith('_')}
    return jsonify(fields)",def,task_info,(,dag_id,",",task_id,),:,try,:,info,=,get_task,(,dag_id,",",task_id,),except,AirflowException,as,err,:,_log,.,info,(,err,),response,=,jsonify,(,error,=,"""{}""",.,format,(,err,),),response,.,status_code,=,err,.,status_code,return,response,# JSONify and return.,Returns a JSON with a task's public instance variables.,Returns,a,JSON,with,a,task,s,public,instance,variables,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L155-L169,test,fields,=,{,k,:,str,(,v,),for,k,",",v,in,vars,(,info,),.,items,(,),if,not,k,.,startswith,(,'_',),},return,jsonify,(,fields,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/api/experimental/endpoints.py,get_pools,"def get_pools():
    """"""Get all pools.""""""
    try:
        pools = pool_api.get_pools()
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify([p.to_json() for p in pools])",python,"def get_pools():
    """"""Get all pools.""""""
    try:
        pools = pool_api.get_pools()
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify([p.to_json() for p in pools])",def,get_pools,(,),:,try,:,pools,=,pool_api,.,get_pools,(,),except,AirflowException,as,err,:,_log,.,error,(,err,),response,=,jsonify,(,error,=,"""{}""",.,format,(,err,),),response,.,status_code,=,err,.,status_code,return,response,else,:,return,jsonify,(,Get all pools.,Get,all,pools,.,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L309-L319,test,[,p,.,to_json,(,),for,p,in,pools,],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/api/experimental/endpoints.py,create_pool,"def create_pool():
    """"""Create a pool.""""""
    params = request.get_json(force=True)
    try:
        pool = pool_api.create_pool(**params)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify(pool.to_json())",python,"def create_pool():
    """"""Create a pool.""""""
    params = request.get_json(force=True)
    try:
        pool = pool_api.create_pool(**params)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify(pool.to_json())",def,create_pool,(,),:,params,=,request,.,get_json,(,force,=,True,),try,:,pool,=,pool_api,.,create_pool,(,*,*,params,),except,AirflowException,as,err,:,_log,.,error,(,err,),response,=,jsonify,(,error,=,"""{}""",.,format,(,err,),),response,Create a pool.,Create,a,pool,.,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L325-L336,test,.,status_code,=,err,.,status_code,return,response,else,:,return,jsonify,(,pool,.,to_json,(,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/api/experimental/endpoints.py,delete_pool,"def delete_pool(name):
    """"""Delete pool.""""""
    try:
        pool = pool_api.delete_pool(name=name)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify(pool.to_json())",python,"def delete_pool(name):
    """"""Delete pool.""""""
    try:
        pool = pool_api.delete_pool(name=name)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify(pool.to_json())",def,delete_pool,(,name,),:,try,:,pool,=,pool_api,.,delete_pool,(,name,=,name,),except,AirflowException,as,err,:,_log,.,error,(,err,),response,=,jsonify,(,error,=,"""{}""",.,format,(,err,),),response,.,status_code,=,err,.,status_code,return,response,else,Delete pool.,Delete,pool,.,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L342-L352,test,:,return,jsonify,(,pool,.,to_json,(,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,AzureContainerInstanceHook.create_or_update,"def create_or_update(self, resource_group, name, container_group):
        """"""
        Create a new container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param container_group: the properties of the container group
        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup
        """"""
        self.connection.container_groups.create_or_update(resource_group,
                                                          name,
                                                          container_group)",python,"def create_or_update(self, resource_group, name, container_group):
        """"""
        Create a new container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param container_group: the properties of the container group
        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup
        """"""
        self.connection.container_groups.create_or_update(resource_group,
                                                          name,
                                                          container_group)",def,create_or_update,(,self,",",resource_group,",",name,",",container_group,),:,self,.,connection,.,container_groups,.,create_or_update,(,resource_group,",",name,",",container_group,),,,,,,,,,,,,,,,,,,,,,,,,,,,"Create a new container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param container_group: the properties of the container group
        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup",Create,a,new,container,group,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L80-L93,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,AzureContainerInstanceHook.get_state_exitcode_details,"def get_state_exitcode_details(self, resource_group, name):
        """"""
        Get the state and exitcode of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A tuple with the state, exitcode, and details.
            If the exitcode is unknown 0 is returned.
        :rtype: tuple(state,exitcode,details)
        """"""
        current_state = self._get_instance_view(resource_group, name).current_state
        return (current_state.state,
                current_state.exit_code,
                current_state.detail_status)",python,"def get_state_exitcode_details(self, resource_group, name):
        """"""
        Get the state and exitcode of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A tuple with the state, exitcode, and details.
            If the exitcode is unknown 0 is returned.
        :rtype: tuple(state,exitcode,details)
        """"""
        current_state = self._get_instance_view(resource_group, name).current_state
        return (current_state.state,
                current_state.exit_code,
                current_state.detail_status)",def,get_state_exitcode_details,(,self,",",resource_group,",",name,),:,current_state,=,self,.,_get_instance_view,(,resource_group,",",name,),.,current_state,return,(,current_state,.,state,",",current_state,.,exit_code,",",current_state,.,detail_status,),,,,,,,,,,,,,,,,,"Get the state and exitcode of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A tuple with the state, exitcode, and details.
            If the exitcode is unknown 0 is returned.
        :rtype: tuple(state,exitcode,details)",Get,the,state,and,exitcode,of,a,container,group,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L95-L110,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,AzureContainerInstanceHook.get_messages,"def get_messages(self, resource_group, name):
        """"""
        Get the messages of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A list of the event messages
        :rtype: list[str]
        """"""
        instance_view = self._get_instance_view(resource_group, name)

        return [event.message for event in instance_view.events]",python,"def get_messages(self, resource_group, name):
        """"""
        Get the messages of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A list of the event messages
        :rtype: list[str]
        """"""
        instance_view = self._get_instance_view(resource_group, name)

        return [event.message for event in instance_view.events]",def,get_messages,(,self,",",resource_group,",",name,),:,instance_view,=,self,.,_get_instance_view,(,resource_group,",",name,),return,[,event,.,message,for,event,in,instance_view,.,events,],,,,,,,,,,,,,,,,,,,,,"Get the messages of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A list of the event messages
        :rtype: list[str]",Get,the,messages,of,a,container,group,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L118-L131,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,AzureContainerInstanceHook.get_logs,"def get_logs(self, resource_group, name, tail=1000):
        """"""
        Get the tail from logs of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param tail: the size of the tail
        :type tail: int
        :return: A list of log messages
        :rtype: list[str]
        """"""
        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)
        return logs.content.splitlines(True)",python,"def get_logs(self, resource_group, name, tail=1000):
        """"""
        Get the tail from logs of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param tail: the size of the tail
        :type tail: int
        :return: A list of log messages
        :rtype: list[str]
        """"""
        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)
        return logs.content.splitlines(True)",def,get_logs,(,self,",",resource_group,",",name,",",tail,=,1000,),:,logs,=,self,.,connection,.,container,.,list_logs,(,resource_group,",",name,",",name,",",tail,=,tail,),return,logs,.,content,.,splitlines,(,True,),,,,,,,,,,"Get the tail from logs of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param tail: the size of the tail
        :type tail: int
        :return: A list of log messages
        :rtype: list[str]",Get,the,tail,from,logs,of,a,container,group,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L133-L147,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,AzureContainerInstanceHook.delete,"def delete(self, resource_group, name):
        """"""
        Delete a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        """"""
        self.connection.container_groups.delete(resource_group, name)",python,"def delete(self, resource_group, name):
        """"""
        Delete a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        """"""
        self.connection.container_groups.delete(resource_group, name)",def,delete,(,self,",",resource_group,",",name,),:,self,.,connection,.,container_groups,.,delete,(,resource_group,",",name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Delete a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str",Delete,a,container,group,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L149-L158,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,AzureContainerInstanceHook.exists,"def exists(self, resource_group, name):
        """"""
        Test if a container group exists

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        """"""
        for container in self.connection.container_groups.list_by_resource_group(resource_group):
            if container.name == name:
                return True
        return False",python,"def exists(self, resource_group, name):
        """"""
        Test if a container group exists

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        """"""
        for container in self.connection.container_groups.list_by_resource_group(resource_group):
            if container.name == name:
                return True
        return False",def,exists,(,self,",",resource_group,",",name,),:,for,container,in,self,.,connection,.,container_groups,.,list_by_resource_group,(,resource_group,),:,if,container,.,name,==,name,:,return,True,return,False,,,,,,,,,,,,,,,,,,"Test if a container group exists

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str",Test,if,a,container,group,exists,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L160-L172,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/decorators.py,apply_defaults,"def apply_defaults(func):
    """"""
    Function decorator that Looks for an argument named ""default_args"", and
    fills the unspecified arguments from it.

    Since python2.* isn't clear about which arguments are missing when
    calling a function, and that this can be quite confusing with multi-level
    inheritance and argument defaults, this decorator also alerts with
    specific information about the missing arguments.
    """"""

    # Cache inspect.signature for the wrapper closure to avoid calling it
    # at every decorated invocation. This is separate sig_cache created
    # per decoration, i.e. each function decorated using apply_defaults will
    # have a different sig_cache.
    sig_cache = signature(func)
    non_optional_args = {
        name for (name, param) in sig_cache.parameters.items()
        if param.default == param.empty and
        param.name != 'self' and
        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}

    @wraps(func)
    def wrapper(*args, **kwargs):
        if len(args) > 1:
            raise AirflowException(
                ""Use keyword arguments when initializing operators"")
        dag_args = {}
        dag_params = {}

        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG
        if dag:
            dag_args = copy(dag.default_args) or {}
            dag_params = copy(dag.params) or {}

        params = {}
        if 'params' in kwargs:
            params = kwargs['params']
        dag_params.update(params)

        default_args = {}
        if 'default_args' in kwargs:
            default_args = kwargs['default_args']
            if 'params' in default_args:
                dag_params.update(default_args['params'])
                del default_args['params']

        dag_args.update(default_args)
        default_args = dag_args

        for arg in sig_cache.parameters:
            if arg not in kwargs and arg in default_args:
                kwargs[arg] = default_args[arg]
        missing_args = list(non_optional_args - set(kwargs))
        if missing_args:
            msg = ""Argument {0} is required"".format(missing_args)
            raise AirflowException(msg)

        kwargs['params'] = dag_params

        result = func(*args, **kwargs)
        return result
    return wrapper",python,"def apply_defaults(func):
    """"""
    Function decorator that Looks for an argument named ""default_args"", and
    fills the unspecified arguments from it.

    Since python2.* isn't clear about which arguments are missing when
    calling a function, and that this can be quite confusing with multi-level
    inheritance and argument defaults, this decorator also alerts with
    specific information about the missing arguments.
    """"""

    # Cache inspect.signature for the wrapper closure to avoid calling it
    # at every decorated invocation. This is separate sig_cache created
    # per decoration, i.e. each function decorated using apply_defaults will
    # have a different sig_cache.
    sig_cache = signature(func)
    non_optional_args = {
        name for (name, param) in sig_cache.parameters.items()
        if param.default == param.empty and
        param.name != 'self' and
        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}

    @wraps(func)
    def wrapper(*args, **kwargs):
        if len(args) > 1:
            raise AirflowException(
                ""Use keyword arguments when initializing operators"")
        dag_args = {}
        dag_params = {}

        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG
        if dag:
            dag_args = copy(dag.default_args) or {}
            dag_params = copy(dag.params) or {}

        params = {}
        if 'params' in kwargs:
            params = kwargs['params']
        dag_params.update(params)

        default_args = {}
        if 'default_args' in kwargs:
            default_args = kwargs['default_args']
            if 'params' in default_args:
                dag_params.update(default_args['params'])
                del default_args['params']

        dag_args.update(default_args)
        default_args = dag_args

        for arg in sig_cache.parameters:
            if arg not in kwargs and arg in default_args:
                kwargs[arg] = default_args[arg]
        missing_args = list(non_optional_args - set(kwargs))
        if missing_args:
            msg = ""Argument {0} is required"".format(missing_args)
            raise AirflowException(msg)

        kwargs['params'] = dag_params

        result = func(*args, **kwargs)
        return result
    return wrapper",def,apply_defaults,(,func,),:,# Cache inspect.signature for the wrapper closure to avoid calling it,# at every decorated invocation. This is separate sig_cache created,"# per decoration, i.e. each function decorated using apply_defaults will",# have a different sig_cache.,sig_cache,=,signature,(,func,),non_optional_args,=,{,name,for,(,name,",",param,),in,sig_cache,.,parameters,.,items,(,),if,param,.,default,==,param,.,empty,and,param,.,name,!=,'self',and,param,.,kind,"Function decorator that Looks for an argument named ""default_args"", and
    fills the unspecified arguments from it.

    Since python2.* isn't clear about which arguments are missing when
    calling a function, and that this can be quite confusing with multi-level
    inheritance and argument defaults, this decorator also alerts with
    specific information about the missing arguments.",Function,decorator,that,Looks,for,an,argument,named,default_args,and,fills,the,unspecified,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/decorators.py#L38-L100,test,not,in,(,param,.,VAR_POSITIONAL,",",param,.,VAR_KEYWORD,),},@,wraps,(,func,),def,wrapper,(,*,args,",",*,*,kwargs,),:,if,len,(,args,),>,1,:,raise,AirflowException,(,"""Use keyword arguments when initializing operators""",),dag_args,=,{,},dag_params,=,{,},dag,=,kwargs,.,get,(,'dag',",",None,),or,settings,.,CONTEXT_MANAGER_DAG,if,dag,:,dag_args,=,copy,(,dag,.,default_args,),or,{,},dag_params,=,copy,(,dag,.,params,),or,{,},params,=,{,},if,'params',in,kwargs,:,params,=,kwargs,[,'params',],dag_params,.,update,(,params,),default_args,=,{,},if,'default_args',in,kwargs,:,default_args,=,kwargs,[,'default_args',],if,'params',in,default_args,:,dag_params,.,update,(,default_args,[,'params',],),del,default_args,[,'params',],dag_args,.,update,(,default_args,),default_args,=,dag_args,for,arg,in,sig_cache,.,parameters,:,if,arg,not,in,kwargs,and,arg,in,default_args,:,kwargs,[,arg,],=,default_args,[,arg,],missing_args,=,list,(,non_optional_args,-,set,(,kwargs,),),if,missing_args,:,msg,=,"""Argument {0} is required""",.,format,(,missing_args,),raise,AirflowException,(,msg,),kwargs,[,'params',],=,dag_params,result,=,func,(,*,args,",",*,*,kwargs,),return,result,return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arguments,from,it,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/operators/hive_to_druid.py,HiveToDruidTransfer.construct_ingest_query,"def construct_ingest_query(self, static_path, columns):
        """"""
        Builds an ingest query for an HDFS TSV load.

        :param static_path: The path on hdfs where the data is
        :type static_path: str
        :param columns: List of all the columns that are available
        :type columns: list
        """"""

        # backward compatibility for num_shards,
        # but target_partition_size is the default setting
        # and overwrites the num_shards
        num_shards = self.num_shards
        target_partition_size = self.target_partition_size
        if self.target_partition_size == -1:
            if self.num_shards == -1:
                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE
        else:
            num_shards = -1

        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']

        # Take all the columns, which are not the time dimension
        # or a metric, as the dimension columns
        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]

        ingest_query_dict = {
            ""type"": ""index_hadoop"",
            ""spec"": {
                ""dataSchema"": {
                    ""metricsSpec"": self.metric_spec,
                    ""granularitySpec"": {
                        ""queryGranularity"": self.query_granularity,
                        ""intervals"": self.intervals,
                        ""type"": ""uniform"",
                        ""segmentGranularity"": self.segment_granularity,
                    },
                    ""parser"": {
                        ""type"": ""string"",
                        ""parseSpec"": {
                            ""columns"": columns,
                            ""dimensionsSpec"": {
                                ""dimensionExclusions"": [],
                                ""dimensions"": dimensions,  # list of names
                                ""spatialDimensions"": []
                            },
                            ""timestampSpec"": {
                                ""column"": self.ts_dim,
                                ""format"": ""auto""
                            },
                            ""format"": ""tsv""
                        }
                    },
                    ""dataSource"": self.druid_datasource
                },
                ""tuningConfig"": {
                    ""type"": ""hadoop"",
                    ""jobProperties"": {
                        ""mapreduce.job.user.classpath.first"": ""false"",
                        ""mapreduce.map.output.compress"": ""false"",
                        ""mapreduce.output.fileoutputformat.compress"": ""false"",
                    },
                    ""partitionsSpec"": {
                        ""type"": ""hashed"",
                        ""targetPartitionSize"": target_partition_size,
                        ""numShards"": num_shards,
                    },
                },
                ""ioConfig"": {
                    ""inputSpec"": {
                        ""paths"": static_path,
                        ""type"": ""static""
                    },
                    ""type"": ""hadoop""
                }
            }
        }

        if self.job_properties:
            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \
                .update(self.job_properties)

        if self.hadoop_dependency_coordinates:
            ingest_query_dict['hadoopDependencyCoordinates'] \
                = self.hadoop_dependency_coordinates

        return ingest_query_dict",python,"def construct_ingest_query(self, static_path, columns):
        """"""
        Builds an ingest query for an HDFS TSV load.

        :param static_path: The path on hdfs where the data is
        :type static_path: str
        :param columns: List of all the columns that are available
        :type columns: list
        """"""

        # backward compatibility for num_shards,
        # but target_partition_size is the default setting
        # and overwrites the num_shards
        num_shards = self.num_shards
        target_partition_size = self.target_partition_size
        if self.target_partition_size == -1:
            if self.num_shards == -1:
                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE
        else:
            num_shards = -1

        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']

        # Take all the columns, which are not the time dimension
        # or a metric, as the dimension columns
        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]

        ingest_query_dict = {
            ""type"": ""index_hadoop"",
            ""spec"": {
                ""dataSchema"": {
                    ""metricsSpec"": self.metric_spec,
                    ""granularitySpec"": {
                        ""queryGranularity"": self.query_granularity,
                        ""intervals"": self.intervals,
                        ""type"": ""uniform"",
                        ""segmentGranularity"": self.segment_granularity,
                    },
                    ""parser"": {
                        ""type"": ""string"",
                        ""parseSpec"": {
                            ""columns"": columns,
                            ""dimensionsSpec"": {
                                ""dimensionExclusions"": [],
                                ""dimensions"": dimensions,  # list of names
                                ""spatialDimensions"": []
                            },
                            ""timestampSpec"": {
                                ""column"": self.ts_dim,
                                ""format"": ""auto""
                            },
                            ""format"": ""tsv""
                        }
                    },
                    ""dataSource"": self.druid_datasource
                },
                ""tuningConfig"": {
                    ""type"": ""hadoop"",
                    ""jobProperties"": {
                        ""mapreduce.job.user.classpath.first"": ""false"",
                        ""mapreduce.map.output.compress"": ""false"",
                        ""mapreduce.output.fileoutputformat.compress"": ""false"",
                    },
                    ""partitionsSpec"": {
                        ""type"": ""hashed"",
                        ""targetPartitionSize"": target_partition_size,
                        ""numShards"": num_shards,
                    },
                },
                ""ioConfig"": {
                    ""inputSpec"": {
                        ""paths"": static_path,
                        ""type"": ""static""
                    },
                    ""type"": ""hadoop""
                }
            }
        }

        if self.job_properties:
            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \
                .update(self.job_properties)

        if self.hadoop_dependency_coordinates:
            ingest_query_dict['hadoopDependencyCoordinates'] \
                = self.hadoop_dependency_coordinates

        return ingest_query_dict",def,construct_ingest_query,(,self,",",static_path,",",columns,),:,"# backward compatibility for num_shards,",# but target_partition_size is the default setting,# and overwrites the num_shards,num_shards,=,self,.,num_shards,target_partition_size,=,self,.,target_partition_size,if,self,.,target_partition_size,==,-,1,:,if,self,.,num_shards,==,-,1,:,target_partition_size,=,DEFAULT_TARGET_PARTITION_SIZE,else,:,num_shards,=,-,1,metric_names,=,[,m,"Builds an ingest query for an HDFS TSV load.

        :param static_path: The path on hdfs where the data is
        :type static_path: str
        :param columns: List of all the columns that are available
        :type columns: list",Builds,an,ingest,query,for,an,HDFS,TSV,load,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/hive_to_druid.py#L157-L244,test,[,'fieldName',],for,m,in,self,.,metric_spec,if,m,[,'type',],!=,'count',],"# Take all the columns, which are not the time dimension","# or a metric, as the dimension columns",dimensions,=,[,c,for,c,in,columns,if,c,not,in,metric_names,and,c,!=,self,.,ts_dim,],ingest_query_dict,=,{,"""type""",:,"""index_hadoop""",",","""spec""",:,{,"""dataSchema""",:,{,"""metricsSpec""",:,self,.,metric_spec,",","""granularitySpec""",:,{,"""queryGranularity""",:,self,.,query_granularity,",","""intervals""",:,self,.,intervals,",","""type""",:,"""uniform""",",","""segmentGranularity""",:,self,.,segment_granularity,",",},",","""parser""",:,{,"""type""",:,"""string""",",","""parseSpec""",:,{,"""columns""",:,columns,",","""dimensionsSpec""",:,{,"""dimensionExclusions""",:,[,],",","""dimensions""",:,dimensions,",",# list of names,"""spatialDimensions""",:,[,],},",","""timestampSpec""",:,{,"""column""",:,self,.,ts_dim,",","""format""",:,"""auto""",},",","""format""",:,"""tsv""",},},",","""dataSource""",:,self,.,druid_datasource,},",","""tuningConfig""",:,{,"""type""",:,"""hadoop""",",","""jobProperties""",:,{,"""mapreduce.job.user.classpath.first""",:,"""false""",",","""mapreduce.map.output.compress""",:,"""false""",",","""mapreduce.output.fileoutputformat.compress""",:,"""false""",",",},",","""partitionsSpec""",:,{,"""type""",:,"""hashed""",",","""targetPartitionSize""",:,target_partition_size,",","""numShards""",:,num_shards,",",},",",},",","""ioConfig""",:,{,"""inputSpec""",:,{,"""paths""",:,static_path,",","""type""",:,"""static""",},",","""type""",:,"""hadoop""",},},},if,self,.,job_properties,:,ingest_query_dict,[,'spec',],[,'tuningConfig',],[,'jobProperties',],.,update,(,self,.,job_properties,),if,self,.,hadoop_dependency_coordinates,:,ingest_query_dict,[,'hadoopDependencyCoordinates',],=,self,.,hadoop_dependency_coordinates,return,ingest_query_dict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/redis_pub_sub_sensor.py,RedisPubSubSensor.poke,"def poke(self, context):
        """"""
        Check for message on subscribed channels and write to xcom the message with key ``message``

        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message (with type 'message') is available or ``False`` if not
        """"""
        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)

        message = self.pubsub.get_message()
        self.log.info('Message %s from channel %s', message, self.channels)

        # Process only message types
        if message and message['type'] == 'message':

            context['ti'].xcom_push(key='message', value=message)
            self.pubsub.unsubscribe(self.channels)

            return True

        return False",python,"def poke(self, context):
        """"""
        Check for message on subscribed channels and write to xcom the message with key ``message``

        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message (with type 'message') is available or ``False`` if not
        """"""
        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)

        message = self.pubsub.get_message()
        self.log.info('Message %s from channel %s', message, self.channels)

        # Process only message types
        if message and message['type'] == 'message':

            context['ti'].xcom_push(key='message', value=message)
            self.pubsub.unsubscribe(self.channels)

            return True

        return False",def,poke,(,self,",",context,),:,self,.,log,.,info,(,'RedisPubSubSensor checking for message on channels: %s',",",self,.,channels,),message,=,self,.,pubsub,.,get_message,(,),self,.,log,.,info,(,'Message %s from channel %s',",",message,",",self,.,channels,),# Process only message types,if,message,and,message,[,'type',],==,"Check for message on subscribed channels and write to xcom the message with key ``message``

        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message (with type 'message') is available or ``False`` if not",Check,for,message,on,subscribed,channels,and,write,to,xcom,the,message,with,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/redis_pub_sub_sensor.py#L50-L73,test,'message',:,context,[,'ti',],.,xcom_push,(,key,=,'message',",",value,=,message,),self,.,pubsub,.,unsubscribe,(,self,.,channels,),return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,key,message,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.find,"def find(dag_id=None, run_id=None, execution_date=None,
             state=None, external_trigger=None, no_backfills=False,
             session=None):
        """"""
        Returns a set of dag runs for the given search criteria.

        :param dag_id: the dag_id to find dag runs for
        :type dag_id: int, list
        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param no_backfills: return no backfills (True), return all (False).
            Defaults to False
        :type no_backfills: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        """"""
        DR = DagRun

        qry = session.query(DR)
        if dag_id:
            qry = qry.filter(DR.dag_id == dag_id)
        if run_id:
            qry = qry.filter(DR.run_id == run_id)
        if execution_date:
            if isinstance(execution_date, list):
                qry = qry.filter(DR.execution_date.in_(execution_date))
            else:
                qry = qry.filter(DR.execution_date == execution_date)
        if state:
            qry = qry.filter(DR.state == state)
        if external_trigger is not None:
            qry = qry.filter(DR.external_trigger == external_trigger)
        if no_backfills:
            # in order to prevent a circular dependency
            from airflow.jobs import BackfillJob
            qry = qry.filter(DR.run_id.notlike(BackfillJob.ID_PREFIX + '%'))

        dr = qry.order_by(DR.execution_date).all()

        return dr",python,"def find(dag_id=None, run_id=None, execution_date=None,
             state=None, external_trigger=None, no_backfills=False,
             session=None):
        """"""
        Returns a set of dag runs for the given search criteria.

        :param dag_id: the dag_id to find dag runs for
        :type dag_id: int, list
        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param no_backfills: return no backfills (True), return all (False).
            Defaults to False
        :type no_backfills: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        """"""
        DR = DagRun

        qry = session.query(DR)
        if dag_id:
            qry = qry.filter(DR.dag_id == dag_id)
        if run_id:
            qry = qry.filter(DR.run_id == run_id)
        if execution_date:
            if isinstance(execution_date, list):
                qry = qry.filter(DR.execution_date.in_(execution_date))
            else:
                qry = qry.filter(DR.execution_date == execution_date)
        if state:
            qry = qry.filter(DR.state == state)
        if external_trigger is not None:
            qry = qry.filter(DR.external_trigger == external_trigger)
        if no_backfills:
            # in order to prevent a circular dependency
            from airflow.jobs import BackfillJob
            qry = qry.filter(DR.run_id.notlike(BackfillJob.ID_PREFIX + '%'))

        dr = qry.order_by(DR.execution_date).all()

        return dr",def,find,(,dag_id,=,None,",",run_id,=,None,",",execution_date,=,None,",",state,=,None,",",external_trigger,=,None,",",no_backfills,=,False,",",session,=,None,),:,DR,=,DagRun,qry,=,session,.,query,(,DR,),if,dag_id,:,qry,=,qry,.,filter,(,"Returns a set of dag runs for the given search criteria.

        :param dag_id: the dag_id to find dag runs for
        :type dag_id: int, list
        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param no_backfills: return no backfills (True), return all (False).
            Defaults to False
        :type no_backfills: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session",Returns,a,set,of,dag,runs,for,the,given,search,criteria,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L115-L160,test,DR,.,dag_id,==,dag_id,),if,run_id,:,qry,=,qry,.,filter,(,DR,.,run_id,==,run_id,),if,execution_date,:,if,isinstance,(,execution_date,",",list,),:,qry,=,qry,.,filter,(,DR,.,execution_date,.,in_,(,execution_date,),),else,:,qry,=,qry,.,filter,(,DR,.,execution_date,==,execution_date,),if,state,:,qry,=,qry,.,filter,(,DR,.,state,==,state,),if,external_trigger,is,not,None,:,qry,=,qry,.,filter,(,DR,.,external_trigger,==,external_trigger,),if,no_backfills,:,# in order to prevent a circular dependency,from,airflow,.,jobs,import,BackfillJob,qry,=,qry,.,filter,(,DR,.,run_id,.,notlike,(,BackfillJob,.,ID_PREFIX,+,'%',),),dr,=,qry,.,order_by,(,DR,.,execution_date,),.,all,(,),return,dr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.get_task_instances,"def get_task_instances(self, state=None, session=None):
        """"""
        Returns the task instances for this dag run
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import
        tis = session.query(TaskInstance).filter(
            TaskInstance.dag_id == self.dag_id,
            TaskInstance.execution_date == self.execution_date,
        )
        if state:
            if isinstance(state, six.string_types):
                tis = tis.filter(TaskInstance.state == state)
            else:
                # this is required to deal with NULL values
                if None in state:
                    tis = tis.filter(
                        or_(TaskInstance.state.in_(state),
                            TaskInstance.state.is_(None))
                    )
                else:
                    tis = tis.filter(TaskInstance.state.in_(state))

        if self.dag and self.dag.partial:
            tis = tis.filter(TaskInstance.task_id.in_(self.dag.task_ids))

        return tis.all()",python,"def get_task_instances(self, state=None, session=None):
        """"""
        Returns the task instances for this dag run
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import
        tis = session.query(TaskInstance).filter(
            TaskInstance.dag_id == self.dag_id,
            TaskInstance.execution_date == self.execution_date,
        )
        if state:
            if isinstance(state, six.string_types):
                tis = tis.filter(TaskInstance.state == state)
            else:
                # this is required to deal with NULL values
                if None in state:
                    tis = tis.filter(
                        or_(TaskInstance.state.in_(state),
                            TaskInstance.state.is_(None))
                    )
                else:
                    tis = tis.filter(TaskInstance.state.in_(state))

        if self.dag and self.dag.partial:
            tis = tis.filter(TaskInstance.task_id.in_(self.dag.task_ids))

        return tis.all()",def,get_task_instances,(,self,",",state,=,None,",",session,=,None,),:,from,airflow,.,models,.,taskinstance,import,TaskInstance,# Avoid circular import,tis,=,session,.,query,(,TaskInstance,),.,filter,(,TaskInstance,.,dag_id,==,self,.,dag_id,",",TaskInstance,.,execution_date,==,self,.,execution_date,",",),if,Returns the task instances for this dag run,Returns,the,task,instances,for,this,dag,run,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L163-L188,test,state,:,if,isinstance,(,state,",",six,.,string_types,),:,tis,=,tis,.,filter,(,TaskInstance,.,state,==,state,),else,:,# this is required to deal with NULL values,if,None,in,state,:,tis,=,tis,.,filter,(,or_,(,TaskInstance,.,state,.,in_,(,state,),",",TaskInstance,.,state,.,is_,(,None,),),),else,:,tis,=,tis,.,filter,(,TaskInstance,.,state,.,in_,(,state,),),if,self,.,dag,and,self,.,dag,.,partial,:,tis,=,tis,.,filter,(,TaskInstance,.,task_id,.,in_,(,self,.,dag,.,task_ids,),),return,tis,.,all,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.get_task_instance,"def get_task_instance(self, task_id, session=None):
        """"""
        Returns the task instance specified by task_id for this dag run

        :param task_id: the task id
        """"""

        from airflow.models.taskinstance import TaskInstance  # Avoid circular import
        TI = TaskInstance
        ti = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.execution_date == self.execution_date,
            TI.task_id == task_id
        ).first()

        return ti",python,"def get_task_instance(self, task_id, session=None):
        """"""
        Returns the task instance specified by task_id for this dag run

        :param task_id: the task id
        """"""

        from airflow.models.taskinstance import TaskInstance  # Avoid circular import
        TI = TaskInstance
        ti = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.execution_date == self.execution_date,
            TI.task_id == task_id
        ).first()

        return ti",def,get_task_instance,(,self,",",task_id,",",session,=,None,),:,from,airflow,.,models,.,taskinstance,import,TaskInstance,# Avoid circular import,TI,=,TaskInstance,ti,=,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,==,self,.,dag_id,",",TI,.,execution_date,==,self,.,execution_date,",",TI,"Returns the task instance specified by task_id for this dag run

        :param task_id: the task id",Returns,the,task,instance,specified,by,task_id,for,this,dag,run,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L191-L206,test,.,task_id,==,task_id,),.,first,(,),return,ti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.get_previous_dagrun,"def get_previous_dagrun(self, session=None):
        """"""The previous DagRun, if there is one""""""

        return session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date < self.execution_date
        ).order_by(
            DagRun.execution_date.desc()
        ).first()",python,"def get_previous_dagrun(self, session=None):
        """"""The previous DagRun, if there is one""""""

        return session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date < self.execution_date
        ).order_by(
            DagRun.execution_date.desc()
        ).first()",def,get_previous_dagrun,(,self,",",session,=,None,),:,return,session,.,query,(,DagRun,),.,filter,(,DagRun,.,dag_id,==,self,.,dag_id,",",DagRun,.,execution_date,<,self,.,execution_date,),.,order_by,(,DagRun,.,execution_date,.,desc,(,),),.,first,(,),,"The previous DagRun, if there is one",The,previous,DagRun,if,there,is,one,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L221-L229,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.get_previous_scheduled_dagrun,"def get_previous_scheduled_dagrun(self, session=None):
        """"""The previous, SCHEDULED DagRun, if there is one""""""
        dag = self.get_dag()

        return session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date == dag.previous_schedule(self.execution_date)
        ).first()",python,"def get_previous_scheduled_dagrun(self, session=None):
        """"""The previous, SCHEDULED DagRun, if there is one""""""
        dag = self.get_dag()

        return session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date == dag.previous_schedule(self.execution_date)
        ).first()",def,get_previous_scheduled_dagrun,(,self,",",session,=,None,),:,dag,=,self,.,get_dag,(,),return,session,.,query,(,DagRun,),.,filter,(,DagRun,.,dag_id,==,self,.,dag_id,",",DagRun,.,execution_date,==,dag,.,previous_schedule,(,self,.,execution_date,),),.,first,(,),"The previous, SCHEDULED DagRun, if there is one",The,previous,SCHEDULED,DagRun,if,there,is,one,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L232-L239,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.update_state,"def update_state(self, session=None):
        """"""
        Determines the overall state of the DagRun based on the state
        of its TaskInstances.

        :return: State
        """"""

        dag = self.get_dag()

        tis = self.get_task_instances(session=session)
        self.log.debug(""Updating state for %s considering %s task(s)"", self, len(tis))

        for ti in list(tis):
            # skip in db?
            if ti.state == State.REMOVED:
                tis.remove(ti)
            else:
                ti.task = dag.get_task(ti.task_id)

        # pre-calculate
        # db is faster
        start_dttm = timezone.utcnow()
        unfinished_tasks = self.get_task_instances(
            state=State.unfinished(),
            session=session
        )
        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)
        none_task_concurrency = all(t.task.task_concurrency is None
                                    for t in unfinished_tasks)
        # small speed up
        if unfinished_tasks and none_depends_on_past and none_task_concurrency:
            # todo: this can actually get pretty slow: one task costs between 0.01-015s
            no_dependencies_met = True
            for ut in unfinished_tasks:
                # We need to flag upstream and check for changes because upstream
                # failures/re-schedules can result in deadlock false positives
                old_state = ut.state
                deps_met = ut.are_dependencies_met(
                    dep_context=DepContext(
                        flag_upstream_failed=True,
                        ignore_in_retry_period=True,
                        ignore_in_reschedule_period=True),
                    session=session)
                if deps_met or old_state != ut.current_state(session=session):
                    no_dependencies_met = False
                    break

        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000
        Stats.timing(""dagrun.dependency-check.{}"".format(self.dag_id), duration)

        root_ids = [t.task_id for t in dag.roots]
        roots = [t for t in tis if t.task_id in root_ids]

        # if all roots finished and at least one failed, the run failed
        if (not unfinished_tasks and
                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):
            self.log.info('Marking run %s failed', self)
            self.set_state(State.FAILED)
            dag.handle_callback(self, success=False, reason='task_failure',
                                session=session)

        # if all roots succeeded and no unfinished tasks, the run succeeded
        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)
                                          for r in roots):
            self.log.info('Marking run %s successful', self)
            self.set_state(State.SUCCESS)
            dag.handle_callback(self, success=True, reason='success', session=session)

        # if *all tasks* are deadlocked, the run failed
        elif (unfinished_tasks and none_depends_on_past and
              none_task_concurrency and no_dependencies_met):
            self.log.info('Deadlock; marking run %s failed', self)
            self.set_state(State.FAILED)
            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',
                                session=session)

        # finally, if the roots aren't done, the dag is still running
        else:
            self.set_state(State.RUNNING)

        self._emit_duration_stats_for_finished_state()

        # todo: determine we want to use with_for_update to make sure to lock the run
        session.merge(self)
        session.commit()

        return self.state",python,"def update_state(self, session=None):
        """"""
        Determines the overall state of the DagRun based on the state
        of its TaskInstances.

        :return: State
        """"""

        dag = self.get_dag()

        tis = self.get_task_instances(session=session)
        self.log.debug(""Updating state for %s considering %s task(s)"", self, len(tis))

        for ti in list(tis):
            # skip in db?
            if ti.state == State.REMOVED:
                tis.remove(ti)
            else:
                ti.task = dag.get_task(ti.task_id)

        # pre-calculate
        # db is faster
        start_dttm = timezone.utcnow()
        unfinished_tasks = self.get_task_instances(
            state=State.unfinished(),
            session=session
        )
        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)
        none_task_concurrency = all(t.task.task_concurrency is None
                                    for t in unfinished_tasks)
        # small speed up
        if unfinished_tasks and none_depends_on_past and none_task_concurrency:
            # todo: this can actually get pretty slow: one task costs between 0.01-015s
            no_dependencies_met = True
            for ut in unfinished_tasks:
                # We need to flag upstream and check for changes because upstream
                # failures/re-schedules can result in deadlock false positives
                old_state = ut.state
                deps_met = ut.are_dependencies_met(
                    dep_context=DepContext(
                        flag_upstream_failed=True,
                        ignore_in_retry_period=True,
                        ignore_in_reschedule_period=True),
                    session=session)
                if deps_met or old_state != ut.current_state(session=session):
                    no_dependencies_met = False
                    break

        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000
        Stats.timing(""dagrun.dependency-check.{}"".format(self.dag_id), duration)

        root_ids = [t.task_id for t in dag.roots]
        roots = [t for t in tis if t.task_id in root_ids]

        # if all roots finished and at least one failed, the run failed
        if (not unfinished_tasks and
                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):
            self.log.info('Marking run %s failed', self)
            self.set_state(State.FAILED)
            dag.handle_callback(self, success=False, reason='task_failure',
                                session=session)

        # if all roots succeeded and no unfinished tasks, the run succeeded
        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)
                                          for r in roots):
            self.log.info('Marking run %s successful', self)
            self.set_state(State.SUCCESS)
            dag.handle_callback(self, success=True, reason='success', session=session)

        # if *all tasks* are deadlocked, the run failed
        elif (unfinished_tasks and none_depends_on_past and
              none_task_concurrency and no_dependencies_met):
            self.log.info('Deadlock; marking run %s failed', self)
            self.set_state(State.FAILED)
            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',
                                session=session)

        # finally, if the roots aren't done, the dag is still running
        else:
            self.set_state(State.RUNNING)

        self._emit_duration_stats_for_finished_state()

        # todo: determine we want to use with_for_update to make sure to lock the run
        session.merge(self)
        session.commit()

        return self.state",def,update_state,(,self,",",session,=,None,),:,dag,=,self,.,get_dag,(,),tis,=,self,.,get_task_instances,(,session,=,session,),self,.,log,.,debug,(,"""Updating state for %s considering %s task(s)""",",",self,",",len,(,tis,),),for,ti,in,list,(,tis,),:,# skip in db?,if,"Determines the overall state of the DagRun based on the state
        of its TaskInstances.

        :return: State",Determines,the,overall,state,of,the,DagRun,based,on,the,state,of,its,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L242-L329,test,ti,.,state,==,State,.,REMOVED,:,tis,.,remove,(,ti,),else,:,ti,.,task,=,dag,.,get_task,(,ti,.,task_id,),# pre-calculate,# db is faster,start_dttm,=,timezone,.,utcnow,(,),unfinished_tasks,=,self,.,get_task_instances,(,state,=,State,.,unfinished,(,),",",session,=,session,),none_depends_on_past,=,all,(,not,t,.,task,.,depends_on_past,for,t,in,unfinished_tasks,),none_task_concurrency,=,all,(,t,.,task,.,task_concurrency,is,None,for,t,in,unfinished_tasks,),# small speed up,if,unfinished_tasks,and,none_depends_on_past,and,none_task_concurrency,:,# todo: this can actually get pretty slow: one task costs between 0.01-015s,no_dependencies_met,=,True,for,ut,in,unfinished_tasks,:,# We need to flag upstream and check for changes because upstream,# failures/re-schedules can result in deadlock false positives,old_state,=,ut,.,state,deps_met,=,ut,.,are_dependencies_met,(,dep_context,=,DepContext,(,flag_upstream_failed,=,True,",",ignore_in_retry_period,=,True,",",ignore_in_reschedule_period,=,True,),",",session,=,session,),if,deps_met,or,old_state,!=,ut,.,current_state,(,session,=,session,),:,no_dependencies_met,=,False,break,duration,=,(,timezone,.,utcnow,(,),-,start_dttm,),.,total_seconds,(,),*,1000,Stats,.,timing,(,"""dagrun.dependency-check.{}""",.,format,(,self,.,dag_id,),",",duration,),root_ids,=,[,t,.,task_id,for,t,in,dag,.,roots,],roots,=,[,t,for,t,in,tis,if,t,.,task_id,in,root_ids,],"# if all roots finished and at least one failed, the run failed",if,(,not,unfinished_tasks,and,any,(,r,.,state,in,(,State,.,FAILED,",",State,.,UPSTREAM_FAILED,),for,r,in,roots,),),:,self,.,log,.,info,(,'Marking run %s failed',",",self,),self,.,set_state,(,State,.,FAILED,),dag,.,handle_callback,(,self,",",success,=,False,",",reason,=,'task_failure',",",session,=,session,),"# if all roots succeeded and no unfinished tasks, the run succeeded",elif,not,unfinished_tasks,and,all,(,r,.,state,in,(,State,.,SUCCESS,",",State,.,SKIPPED,),for,r,in,roots,),:,self,.,log,.,info,(,'Marking run %s successful',",",self,),self,.,set_state,(,State,.,SUCCESS,),dag,.,handle_callback,(,self,",",success,=,True,",",reason,=,'success',",",session,=,session,),"# if *all tasks* are deadlocked, the run failed",elif,(,unfinished_tasks,and,none_depends_on_past,and,none_task_concurrency,and,no_dependencies_met,),:,self,.,log,.,info,(,'Deadlock; marking run %s failed',",",self,),self,.,set_state,(,State,.,FAILED,),dag,.,handle_callback,(,self,",",success,=,False,",",reason,=,'all_tasks_deadlocked',",",session,=,session,),"# finally, if the roots aren't done, the dag is still running",else,:,self,.,set_state,(,State,.,RUNNING,),self,.,_emit_duration_stats_for_finished_state,(,),# todo: determine we want to use with_for_update to make sure to lock the run,session,.,merge,(,self,),session,.,commit,(,),return,self,.,state,,,,,,,,,,,,,,,,,TaskInstances,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagrun.py,DagRun.verify_integrity,"def verify_integrity(self, session=None):
        """"""
        Verifies the DagRun by checking for removed tasks or tasks that are not in the
        database yet. It will set state to removed or add the task if required.
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        dag = self.get_dag()
        tis = self.get_task_instances(session=session)

        # check for removed or restored tasks
        task_ids = []
        for ti in tis:
            task_ids.append(ti.task_id)
            task = None
            try:
                task = dag.get_task(ti.task_id)
            except AirflowException:
                if ti.state == State.REMOVED:
                    pass  # ti has already been removed, just ignore it
                elif self.state is not State.RUNNING and not dag.partial:
                    self.log.warning(""Failed to get task '{}' for dag '{}'. ""
                                     ""Marking it as removed."".format(ti, dag))
                    Stats.incr(
                        ""task_removed_from_dag.{}"".format(dag.dag_id), 1, 1)
                    ti.state = State.REMOVED

            is_task_in_dag = task is not None
            should_restore_task = is_task_in_dag and ti.state == State.REMOVED
            if should_restore_task:
                self.log.info(""Restoring task '{}' which was previously ""
                              ""removed from DAG '{}'"".format(ti, dag))
                Stats.incr(""task_restored_to_dag.{}"".format(dag.dag_id), 1, 1)
                ti.state = State.NONE

        # check for missing tasks
        for task in six.itervalues(dag.task_dict):
            if task.start_date > self.execution_date and not self.is_backfill:
                continue

            if task.task_id not in task_ids:
                Stats.incr(
                    ""task_instance_created-{}"".format(task.__class__.__name__),
                    1, 1)
                ti = TaskInstance(task, self.execution_date)
                session.add(ti)

        session.commit()",python,"def verify_integrity(self, session=None):
        """"""
        Verifies the DagRun by checking for removed tasks or tasks that are not in the
        database yet. It will set state to removed or add the task if required.
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        dag = self.get_dag()
        tis = self.get_task_instances(session=session)

        # check for removed or restored tasks
        task_ids = []
        for ti in tis:
            task_ids.append(ti.task_id)
            task = None
            try:
                task = dag.get_task(ti.task_id)
            except AirflowException:
                if ti.state == State.REMOVED:
                    pass  # ti has already been removed, just ignore it
                elif self.state is not State.RUNNING and not dag.partial:
                    self.log.warning(""Failed to get task '{}' for dag '{}'. ""
                                     ""Marking it as removed."".format(ti, dag))
                    Stats.incr(
                        ""task_removed_from_dag.{}"".format(dag.dag_id), 1, 1)
                    ti.state = State.REMOVED

            is_task_in_dag = task is not None
            should_restore_task = is_task_in_dag and ti.state == State.REMOVED
            if should_restore_task:
                self.log.info(""Restoring task '{}' which was previously ""
                              ""removed from DAG '{}'"".format(ti, dag))
                Stats.incr(""task_restored_to_dag.{}"".format(dag.dag_id), 1, 1)
                ti.state = State.NONE

        # check for missing tasks
        for task in six.itervalues(dag.task_dict):
            if task.start_date > self.execution_date and not self.is_backfill:
                continue

            if task.task_id not in task_ids:
                Stats.incr(
                    ""task_instance_created-{}"".format(task.__class__.__name__),
                    1, 1)
                ti = TaskInstance(task, self.execution_date)
                session.add(ti)

        session.commit()",def,verify_integrity,(,self,",",session,=,None,),:,from,airflow,.,models,.,taskinstance,import,TaskInstance,# Avoid circular import,dag,=,self,.,get_dag,(,),tis,=,self,.,get_task_instances,(,session,=,session,),# check for removed or restored tasks,task_ids,=,[,],for,ti,in,tis,:,task_ids,.,append,(,ti,.,"Verifies the DagRun by checking for removed tasks or tasks that are not in the
        database yet. It will set state to removed or add the task if required.",Verifies,the,DagRun,by,checking,for,removed,tasks,or,tasks,that,are,not,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L342-L389,test,task_id,),task,=,None,try,:,task,=,dag,.,get_task,(,ti,.,task_id,),except,AirflowException,:,if,ti,.,state,==,State,.,REMOVED,:,pass,"# ti has already been removed, just ignore it",elif,self,.,state,is,not,State,.,RUNNING,and,not,dag,.,partial,:,self,.,log,.,warning,(,"""Failed to get task '{}' for dag '{}'. ""","""Marking it as removed.""",.,format,(,ti,",",dag,),),Stats,.,incr,(,"""task_removed_from_dag.{}""",.,format,(,dag,.,dag_id,),",",1,",",1,),ti,.,state,=,State,.,REMOVED,is_task_in_dag,=,task,is,not,None,should_restore_task,=,is_task_in_dag,and,ti,.,state,==,State,.,REMOVED,if,should_restore_task,:,self,.,log,.,info,(,"""Restoring task '{}' which was previously ""","""removed from DAG '{}'""",.,format,(,ti,",",dag,),),Stats,.,incr,(,"""task_restored_to_dag.{}""",.,format,(,dag,.,dag_id,),",",1,",",1,),ti,.,state,=,State,.,NONE,# check for missing tasks,for,task,in,six,.,itervalues,(,dag,.,task_dict,),:,if,task,.,start_date,>,self,.,execution_date,and,not,self,.,is_backfill,:,continue,if,task,.,task_id,not,in,task_ids,:,Stats,.,incr,(,"""task_instance_created-{}""",.,format,(,task,.,__class__,.,__name__,),",",1,",",1,),ti,=,TaskInstance,(,task,",",self,.,execution_date,),session,.,add,(,ti,),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,in,the,database,yet,.,It,will,set,state,to,removed,or,add,the,task,if,required,.,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/jenkins_job_trigger_operator.py,jenkins_request_with_headers,"def jenkins_request_with_headers(jenkins_server, req):
    """"""
    We need to get the headers in addition to the body answer
    to get the location from them
    This function uses jenkins_request method from python-jenkins library
    with just the return call changed

    :param jenkins_server: The server to query
    :param req: The request to execute
    :return: Dict containing the response body (key body)
        and the headers coming along (headers)
    """"""
    try:
        response = jenkins_server.jenkins_request(req)
        response_body = response.content
        response_headers = response.headers
        if response_body is None:
            raise jenkins.EmptyResponseException(
                ""Error communicating with server[%s]: ""
                ""empty response"" % jenkins_server.server)
        return {'body': response_body.decode('utf-8'), 'headers': response_headers}
    except HTTPError as e:
        # Jenkins's funky authentication means its nigh impossible to
        # distinguish errors.
        if e.code in [401, 403, 500]:
            # six.moves.urllib.error.HTTPError provides a 'reason'
            # attribute for all python version except for ver 2.6
            # Falling back to HTTPError.msg since it contains the
            # same info as reason
            raise JenkinsException(
                'Error in request. ' +
                'Possibly authentication failed [%s]: %s' % (
                    e.code, e.msg)
            )
        elif e.code == 404:
            raise jenkins.NotFoundException('Requested item could not be found')
        else:
            raise
    except socket.timeout as e:
        raise jenkins.TimeoutException('Error in request: %s' % e)
    except URLError as e:
        # python 2.6 compatibility to ensure same exception raised
        # since URLError wraps a socket timeout on python 2.6.
        if str(e.reason) == ""timed out"":
            raise jenkins.TimeoutException('Error in request: %s' % e.reason)
        raise JenkinsException('Error in request: %s' % e.reason)",python,"def jenkins_request_with_headers(jenkins_server, req):
    """"""
    We need to get the headers in addition to the body answer
    to get the location from them
    This function uses jenkins_request method from python-jenkins library
    with just the return call changed

    :param jenkins_server: The server to query
    :param req: The request to execute
    :return: Dict containing the response body (key body)
        and the headers coming along (headers)
    """"""
    try:
        response = jenkins_server.jenkins_request(req)
        response_body = response.content
        response_headers = response.headers
        if response_body is None:
            raise jenkins.EmptyResponseException(
                ""Error communicating with server[%s]: ""
                ""empty response"" % jenkins_server.server)
        return {'body': response_body.decode('utf-8'), 'headers': response_headers}
    except HTTPError as e:
        # Jenkins's funky authentication means its nigh impossible to
        # distinguish errors.
        if e.code in [401, 403, 500]:
            # six.moves.urllib.error.HTTPError provides a 'reason'
            # attribute for all python version except for ver 2.6
            # Falling back to HTTPError.msg since it contains the
            # same info as reason
            raise JenkinsException(
                'Error in request. ' +
                'Possibly authentication failed [%s]: %s' % (
                    e.code, e.msg)
            )
        elif e.code == 404:
            raise jenkins.NotFoundException('Requested item could not be found')
        else:
            raise
    except socket.timeout as e:
        raise jenkins.TimeoutException('Error in request: %s' % e)
    except URLError as e:
        # python 2.6 compatibility to ensure same exception raised
        # since URLError wraps a socket timeout on python 2.6.
        if str(e.reason) == ""timed out"":
            raise jenkins.TimeoutException('Error in request: %s' % e.reason)
        raise JenkinsException('Error in request: %s' % e.reason)",def,jenkins_request_with_headers,(,jenkins_server,",",req,),:,try,:,response,=,jenkins_server,.,jenkins_request,(,req,),response_body,=,response,.,content,response_headers,=,response,.,headers,if,response_body,is,None,:,raise,jenkins,.,EmptyResponseException,(,"""Error communicating with server[%s]: ""","""empty response""",%,jenkins_server,.,server,),return,{,'body',:,response_body,.,decode,"We need to get the headers in addition to the body answer
    to get the location from them
    This function uses jenkins_request method from python-jenkins library
    with just the return call changed

    :param jenkins_server: The server to query
    :param req: The request to execute
    :return: Dict containing the response body (key body)
        and the headers coming along (headers)",We,need,to,get,the,headers,in,addition,to,the,body,answer,to,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/jenkins_job_trigger_operator.py#L34-L79,test,(,'utf-8',),",",'headers',:,response_headers,},except,HTTPError,as,e,:,# Jenkins's funky authentication means its nigh impossible to,# distinguish errors.,if,e,.,code,in,[,401,",",403,",",500,],:,# six.moves.urllib.error.HTTPError provides a 'reason',# attribute for all python version except for ver 2.6,# Falling back to HTTPError.msg since it contains the,# same info as reason,raise,JenkinsException,(,'Error in request. ',+,'Possibly authentication failed [%s]: %s',%,(,e,.,code,",",e,.,msg,),),elif,e,.,code,==,404,:,raise,jenkins,.,NotFoundException,(,'Requested item could not be found',),else,:,raise,except,socket,.,timeout,as,e,:,raise,jenkins,.,TimeoutException,(,'Error in request: %s',%,e,),except,URLError,as,e,:,# python 2.6 compatibility to ensure same exception raised,# since URLError wraps a socket timeout on python 2.6.,if,str,(,e,.,reason,),==,"""timed out""",:,raise,jenkins,.,TimeoutException,(,'Error in request: %s',%,e,.,reason,),raise,JenkinsException,(,'Error in request: %s',%,e,.,reason,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get,the,location,from,them,This,function,uses,jenkins_request,method,from,python,-,jenkins,library,with,just,the,return,call,changed,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/operator_helpers.py,context_to_airflow_vars,"def context_to_airflow_vars(context, in_env_var_format=False):
    """"""
    Given a context, this function provides a dictionary of values that can be used to
    externally reconstruct relations between dags, dag_runs, tasks and task_instances.
    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if
    in_env_var_format is set to True.

    :param context: The context for the task_instance of interest.
    :type context: dict
    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.
    :type in_env_var_format: bool
    :return: task_instance context as dict.
    """"""
    params = dict()
    if in_env_var_format:
        name_format = 'env_var_format'
    else:
        name_format = 'default'
    task_instance = context.get('task_instance')
    if task_instance and task_instance.dag_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][
            name_format]] = task_instance.dag_id
    if task_instance and task_instance.task_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][
            name_format]] = task_instance.task_id
    if task_instance and task_instance.execution_date:
        params[
            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][
                name_format]] = task_instance.execution_date.isoformat()
    dag_run = context.get('dag_run')
    if dag_run and dag_run.run_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][
            name_format]] = dag_run.run_id
    return params",python,"def context_to_airflow_vars(context, in_env_var_format=False):
    """"""
    Given a context, this function provides a dictionary of values that can be used to
    externally reconstruct relations between dags, dag_runs, tasks and task_instances.
    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if
    in_env_var_format is set to True.

    :param context: The context for the task_instance of interest.
    :type context: dict
    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.
    :type in_env_var_format: bool
    :return: task_instance context as dict.
    """"""
    params = dict()
    if in_env_var_format:
        name_format = 'env_var_format'
    else:
        name_format = 'default'
    task_instance = context.get('task_instance')
    if task_instance and task_instance.dag_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][
            name_format]] = task_instance.dag_id
    if task_instance and task_instance.task_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][
            name_format]] = task_instance.task_id
    if task_instance and task_instance.execution_date:
        params[
            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][
                name_format]] = task_instance.execution_date.isoformat()
    dag_run = context.get('dag_run')
    if dag_run and dag_run.run_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][
            name_format]] = dag_run.run_id
    return params",def,context_to_airflow_vars,(,context,",",in_env_var_format,=,False,),:,params,=,dict,(,),if,in_env_var_format,:,name_format,=,'env_var_format',else,:,name_format,=,'default',task_instance,=,context,.,get,(,'task_instance',),if,task_instance,and,task_instance,.,dag_id,:,params,[,AIRFLOW_VAR_NAME_FORMAT_MAPPING,[,'AIRFLOW_CONTEXT_DAG_ID',],[,name_format,],],=,"Given a context, this function provides a dictionary of values that can be used to
    externally reconstruct relations between dags, dag_runs, tasks and task_instances.
    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if
    in_env_var_format is set to True.

    :param context: The context for the task_instance of interest.
    :type context: dict
    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.
    :type in_env_var_format: bool
    :return: task_instance context as dict.",Given,a,context,this,function,provides,a,dictionary,of,values,that,can,be,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/operator_helpers.py#L33-L66,test,task_instance,.,dag_id,if,task_instance,and,task_instance,.,task_id,:,params,[,AIRFLOW_VAR_NAME_FORMAT_MAPPING,[,'AIRFLOW_CONTEXT_TASK_ID',],[,name_format,],],=,task_instance,.,task_id,if,task_instance,and,task_instance,.,execution_date,:,params,[,AIRFLOW_VAR_NAME_FORMAT_MAPPING,[,'AIRFLOW_CONTEXT_EXECUTION_DATE',],[,name_format,],],=,task_instance,.,execution_date,.,isoformat,(,),dag_run,=,context,.,get,(,'dag_run',),if,dag_run,and,dag_run,.,run_id,:,params,[,AIRFLOW_VAR_NAME_FORMAT_MAPPING,[,'AIRFLOW_CONTEXT_DAG_RUN_ID',],[,name_format,],],=,dag_run,.,run_id,return,params,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,used,to,externally,reconstruct,relations,between,dags,dag_runs,tasks,and,task_instances,.,Default,to,abc,.,def,.,ghi,format,and,can,be,made,to,ABC_DEF_GHI,format,if,in_env_var_format,is,set,to,True,.,,,,,,
apache/airflow,airflow/example_dags/example_trigger_controller_dag.py,conditionally_trigger,"def conditionally_trigger(context, dag_run_obj):
    """"""This function decides whether or not to Trigger the remote DAG""""""
    c_p = context['params']['condition_param']
    print(""Controller DAG : conditionally_trigger = {}"".format(c_p))
    if context['params']['condition_param']:
        dag_run_obj.payload = {'message': context['params']['message']}
        pp.pprint(dag_run_obj.payload)
        return dag_run_obj",python,"def conditionally_trigger(context, dag_run_obj):
    """"""This function decides whether or not to Trigger the remote DAG""""""
    c_p = context['params']['condition_param']
    print(""Controller DAG : conditionally_trigger = {}"".format(c_p))
    if context['params']['condition_param']:
        dag_run_obj.payload = {'message': context['params']['message']}
        pp.pprint(dag_run_obj.payload)
        return dag_run_obj",def,conditionally_trigger,(,context,",",dag_run_obj,),:,c_p,=,context,[,'params',],[,'condition_param',],print,(,"""Controller DAG : conditionally_trigger = {}""",.,format,(,c_p,),),if,context,[,'params',],[,'condition_param',],:,dag_run_obj,.,payload,=,{,'message',:,context,[,'params',],[,'message',],},pp,.,This function decides whether or not to Trigger the remote DAG,This,function,decides,whether,or,not,to,Trigger,the,remote,DAG,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/example_dags/example_trigger_controller_dag.py#L45-L52,test,pprint,(,dag_run_obj,.,payload,),return,dag_run_obj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datadog_hook.py,DatadogHook.send_metric,"def send_metric(self, metric_name, datapoint, tags=None, type_=None, interval=None):
        """"""
        Sends a single datapoint metric to DataDog

        :param metric_name: The name of the metric
        :type metric_name: str
        :param datapoint: A single integer or float related to the metric
        :type datapoint: int or float
        :param tags: A list of tags associated with the metric
        :type tags: list
        :param type_: Type of your metric: gauge, rate, or count
        :type type_: str
        :param interval: If the type of the metric is rate or count, define the corresponding interval
        :type interval: int
        """"""
        response = api.Metric.send(
            metric=metric_name,
            points=datapoint,
            host=self.host,
            tags=tags,
            type=type_,
            interval=interval)

        self.validate_response(response)
        return response",python,"def send_metric(self, metric_name, datapoint, tags=None, type_=None, interval=None):
        """"""
        Sends a single datapoint metric to DataDog

        :param metric_name: The name of the metric
        :type metric_name: str
        :param datapoint: A single integer or float related to the metric
        :type datapoint: int or float
        :param tags: A list of tags associated with the metric
        :type tags: list
        :param type_: Type of your metric: gauge, rate, or count
        :type type_: str
        :param interval: If the type of the metric is rate or count, define the corresponding interval
        :type interval: int
        """"""
        response = api.Metric.send(
            metric=metric_name,
            points=datapoint,
            host=self.host,
            tags=tags,
            type=type_,
            interval=interval)

        self.validate_response(response)
        return response",def,send_metric,(,self,",",metric_name,",",datapoint,",",tags,=,None,",",type_,=,None,",",interval,=,None,),:,response,=,api,.,Metric,.,send,(,metric,=,metric_name,",",points,=,datapoint,",",host,=,self,.,host,",",tags,=,tags,",",type,=,type_,",","Sends a single datapoint metric to DataDog

        :param metric_name: The name of the metric
        :type metric_name: str
        :param datapoint: A single integer or float related to the metric
        :type datapoint: int or float
        :param tags: A list of tags associated with the metric
        :type tags: list
        :param type_: Type of your metric: gauge, rate, or count
        :type type_: str
        :param interval: If the type of the metric is rate or count, define the corresponding interval
        :type interval: int",Sends,a,single,datapoint,metric,to,DataDog,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datadog_hook.py#L62-L86,test,interval,=,interval,),self,.,validate_response,(,response,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/datadog_hook.py,DatadogHook.query_metric,"def query_metric(self,
                     query,
                     from_seconds_ago,
                     to_seconds_ago):
        """"""
        Queries datadog for a specific metric, potentially with some
        function applied to it and returns the results.

        :param query: The datadog query to execute (see datadog docs)
        :type query: str
        :param from_seconds_ago: How many seconds ago to start querying for.
        :type from_seconds_ago: int
        :param to_seconds_ago: Up to how many seconds ago to query for.
        :type to_seconds_ago: int
        """"""
        now = int(time.time())

        response = api.Metric.query(
            start=now - from_seconds_ago,
            end=now - to_seconds_ago,
            query=query)

        self.validate_response(response)
        return response",python,"def query_metric(self,
                     query,
                     from_seconds_ago,
                     to_seconds_ago):
        """"""
        Queries datadog for a specific metric, potentially with some
        function applied to it and returns the results.

        :param query: The datadog query to execute (see datadog docs)
        :type query: str
        :param from_seconds_ago: How many seconds ago to start querying for.
        :type from_seconds_ago: int
        :param to_seconds_ago: Up to how many seconds ago to query for.
        :type to_seconds_ago: int
        """"""
        now = int(time.time())

        response = api.Metric.query(
            start=now - from_seconds_ago,
            end=now - to_seconds_ago,
            query=query)

        self.validate_response(response)
        return response",def,query_metric,(,self,",",query,",",from_seconds_ago,",",to_seconds_ago,),:,now,=,int,(,time,.,time,(,),),response,=,api,.,Metric,.,query,(,start,=,now,-,from_seconds_ago,",",end,=,now,-,to_seconds_ago,",",query,=,query,),self,.,validate_response,(,response,),"Queries datadog for a specific metric, potentially with some
        function applied to it and returns the results.

        :param query: The datadog query to execute (see datadog docs)
        :type query: str
        :param from_seconds_ago: How many seconds ago to start querying for.
        :type from_seconds_ago: int
        :param to_seconds_ago: Up to how many seconds ago to query for.
        :type to_seconds_ago: int",Queries,datadog,for,a,specific,metric,potentially,with,some,function,applied,to,it,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datadog_hook.py#L88-L111,test,return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,and,returns,the,results,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagbag.py,DagBag.get_dag,"def get_dag(self, dag_id):
        """"""
        Gets the DAG out of the dictionary, and refreshes it if expired
        """"""
        from airflow.models.dag import DagModel  # Avoid circular import

        # If asking for a known subdag, we want to refresh the parent
        root_dag_id = dag_id
        if dag_id in self.dags:
            dag = self.dags[dag_id]
            if dag.is_subdag:
                root_dag_id = dag.parent_dag.dag_id

        # If the dag corresponding to root_dag_id is absent or expired
        orm_dag = DagModel.get_current(root_dag_id)
        if orm_dag and (
                root_dag_id not in self.dags or
                (
                    orm_dag.last_expired and
                    dag.last_loaded < orm_dag.last_expired
                )
        ):
            # Reprocess source file
            found_dags = self.process_file(
                filepath=orm_dag.fileloc, only_if_updated=False)

            # If the source file no longer exports `dag_id`, delete it from self.dags
            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:
                return self.dags[dag_id]
            elif dag_id in self.dags:
                del self.dags[dag_id]
        return self.dags.get(dag_id)",python,"def get_dag(self, dag_id):
        """"""
        Gets the DAG out of the dictionary, and refreshes it if expired
        """"""
        from airflow.models.dag import DagModel  # Avoid circular import

        # If asking for a known subdag, we want to refresh the parent
        root_dag_id = dag_id
        if dag_id in self.dags:
            dag = self.dags[dag_id]
            if dag.is_subdag:
                root_dag_id = dag.parent_dag.dag_id

        # If the dag corresponding to root_dag_id is absent or expired
        orm_dag = DagModel.get_current(root_dag_id)
        if orm_dag and (
                root_dag_id not in self.dags or
                (
                    orm_dag.last_expired and
                    dag.last_loaded < orm_dag.last_expired
                )
        ):
            # Reprocess source file
            found_dags = self.process_file(
                filepath=orm_dag.fileloc, only_if_updated=False)

            # If the source file no longer exports `dag_id`, delete it from self.dags
            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:
                return self.dags[dag_id]
            elif dag_id in self.dags:
                del self.dags[dag_id]
        return self.dags.get(dag_id)",def,get_dag,(,self,",",dag_id,),:,from,airflow,.,models,.,dag,import,DagModel,# Avoid circular import,"# If asking for a known subdag, we want to refresh the parent",root_dag_id,=,dag_id,if,dag_id,in,self,.,dags,:,dag,=,self,.,dags,[,dag_id,],if,dag,.,is_subdag,:,root_dag_id,=,dag,.,parent_dag,.,dag_id,# If the dag corresponding to root_dag_id is absent or expired,orm_dag,=,DagModel,"Gets the DAG out of the dictionary, and refreshes it if expired",Gets,the,DAG,out,of,the,dictionary,and,refreshes,it,if,expired,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L112-L143,test,.,get_current,(,root_dag_id,),if,orm_dag,and,(,root_dag_id,not,in,self,.,dags,or,(,orm_dag,.,last_expired,and,dag,.,last_loaded,<,orm_dag,.,last_expired,),),:,# Reprocess source file,found_dags,=,self,.,process_file,(,filepath,=,orm_dag,.,fileloc,",",only_if_updated,=,False,),"# If the source file no longer exports `dag_id`, delete it from self.dags",if,found_dags,and,dag_id,in,[,found_dag,.,dag_id,for,found_dag,in,found_dags,],:,return,self,.,dags,[,dag_id,],elif,dag_id,in,self,.,dags,:,del,self,.,dags,[,dag_id,],return,self,.,dags,.,get,(,dag_id,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagbag.py,DagBag.kill_zombies,"def kill_zombies(self, zombies, session=None):
        """"""
        Fail given zombie tasks, which are tasks that haven't
        had a heartbeat for too long, in the current DagBag.

        :param zombies: zombie task instances to kill.
        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance
        :param session: DB session.
        :type session: sqlalchemy.orm.session.Session
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        for zombie in zombies:
            if zombie.dag_id in self.dags:
                dag = self.dags[zombie.dag_id]
                if zombie.task_id in dag.task_ids:
                    task = dag.get_task(zombie.task_id)
                    ti = TaskInstance(task, zombie.execution_date)
                    # Get properties needed for failure handling from SimpleTaskInstance.
                    ti.start_date = zombie.start_date
                    ti.end_date = zombie.end_date
                    ti.try_number = zombie.try_number
                    ti.state = zombie.state
                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')
                    ti.handle_failure(""{} detected as zombie"".format(ti),
                                      ti.test_mode, ti.get_template_context())
                    self.log.info(
                        'Marked zombie job %s as %s', ti, ti.state)
                    Stats.incr('zombies_killed')
        session.commit()",python,"def kill_zombies(self, zombies, session=None):
        """"""
        Fail given zombie tasks, which are tasks that haven't
        had a heartbeat for too long, in the current DagBag.

        :param zombies: zombie task instances to kill.
        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance
        :param session: DB session.
        :type session: sqlalchemy.orm.session.Session
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        for zombie in zombies:
            if zombie.dag_id in self.dags:
                dag = self.dags[zombie.dag_id]
                if zombie.task_id in dag.task_ids:
                    task = dag.get_task(zombie.task_id)
                    ti = TaskInstance(task, zombie.execution_date)
                    # Get properties needed for failure handling from SimpleTaskInstance.
                    ti.start_date = zombie.start_date
                    ti.end_date = zombie.end_date
                    ti.try_number = zombie.try_number
                    ti.state = zombie.state
                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')
                    ti.handle_failure(""{} detected as zombie"".format(ti),
                                      ti.test_mode, ti.get_template_context())
                    self.log.info(
                        'Marked zombie job %s as %s', ti, ti.state)
                    Stats.incr('zombies_killed')
        session.commit()",def,kill_zombies,(,self,",",zombies,",",session,=,None,),:,from,airflow,.,models,.,taskinstance,import,TaskInstance,# Avoid circular import,for,zombie,in,zombies,:,if,zombie,.,dag_id,in,self,.,dags,:,dag,=,self,.,dags,[,zombie,.,dag_id,],if,zombie,.,task_id,in,dag,.,"Fail given zombie tasks, which are tasks that haven't
        had a heartbeat for too long, in the current DagBag.

        :param zombies: zombie task instances to kill.
        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance
        :param session: DB session.
        :type session: sqlalchemy.orm.session.Session",Fail,given,zombie,tasks,which,are,tasks,that,haven,t,had,a,heartbeat,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L274-L303,test,task_ids,:,task,=,dag,.,get_task,(,zombie,.,task_id,),ti,=,TaskInstance,(,task,",",zombie,.,execution_date,),# Get properties needed for failure handling from SimpleTaskInstance.,ti,.,start_date,=,zombie,.,start_date,ti,.,end_date,=,zombie,.,end_date,ti,.,try_number,=,zombie,.,try_number,ti,.,state,=,zombie,.,state,ti,.,test_mode,=,configuration,.,getboolean,(,'core',",",'unit_test_mode',),ti,.,handle_failure,(,"""{} detected as zombie""",.,format,(,ti,),",",ti,.,test_mode,",",ti,.,get_template_context,(,),),self,.,log,.,info,(,'Marked zombie job %s as %s',",",ti,",",ti,.,state,),Stats,.,incr,(,'zombies_killed',),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,for,too,long,in,the,current,DagBag,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagbag.py,DagBag.bag_dag,"def bag_dag(self, dag, parent_dag, root_dag):
        """"""
        Adds the DAG into the bag, recurses into sub dags.
        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags
        """"""

        dag.test_cycle()  # throws if a task cycle is found

        dag.resolve_template_files()
        dag.last_loaded = timezone.utcnow()

        for task in dag.tasks:
            settings.policy(task)

        subdags = dag.subdags

        try:
            for subdag in subdags:
                subdag.full_filepath = dag.full_filepath
                subdag.parent_dag = dag
                subdag.is_subdag = True
                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)

            self.dags[dag.dag_id] = dag
            self.log.debug('Loaded DAG %s', dag)
        except AirflowDagCycleException as cycle_exception:
            # There was an error in bagging the dag. Remove it from the list of dags
            self.log.exception('Exception bagging dag: %s', dag.dag_id)
            # Only necessary at the root level since DAG.subdags automatically
            # performs DFS to search through all subdags
            if dag == root_dag:
                for subdag in subdags:
                    if subdag.dag_id in self.dags:
                        del self.dags[subdag.dag_id]
            raise cycle_exception",python,"def bag_dag(self, dag, parent_dag, root_dag):
        """"""
        Adds the DAG into the bag, recurses into sub dags.
        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags
        """"""

        dag.test_cycle()  # throws if a task cycle is found

        dag.resolve_template_files()
        dag.last_loaded = timezone.utcnow()

        for task in dag.tasks:
            settings.policy(task)

        subdags = dag.subdags

        try:
            for subdag in subdags:
                subdag.full_filepath = dag.full_filepath
                subdag.parent_dag = dag
                subdag.is_subdag = True
                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)

            self.dags[dag.dag_id] = dag
            self.log.debug('Loaded DAG %s', dag)
        except AirflowDagCycleException as cycle_exception:
            # There was an error in bagging the dag. Remove it from the list of dags
            self.log.exception('Exception bagging dag: %s', dag.dag_id)
            # Only necessary at the root level since DAG.subdags automatically
            # performs DFS to search through all subdags
            if dag == root_dag:
                for subdag in subdags:
                    if subdag.dag_id in self.dags:
                        del self.dags[subdag.dag_id]
            raise cycle_exception",def,bag_dag,(,self,",",dag,",",parent_dag,",",root_dag,),:,dag,.,test_cycle,(,),# throws if a task cycle is found,dag,.,resolve_template_files,(,),dag,.,last_loaded,=,timezone,.,utcnow,(,),for,task,in,dag,.,tasks,:,settings,.,policy,(,task,),subdags,=,dag,.,subdags,try,:,"Adds the DAG into the bag, recurses into sub dags.
        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags",Adds,the,DAG,into,the,bag,recurses,into,sub,dags,.,Throws,AirflowDagCycleException,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L305-L339,test,for,subdag,in,subdags,:,subdag,.,full_filepath,=,dag,.,full_filepath,subdag,.,parent_dag,=,dag,subdag,.,is_subdag,=,True,self,.,bag_dag,(,subdag,",",parent_dag,=,dag,",",root_dag,=,root_dag,),self,.,dags,[,dag,.,dag_id,],=,dag,self,.,log,.,debug,(,'Loaded DAG %s',",",dag,),except,AirflowDagCycleException,as,cycle_exception,:,# There was an error in bagging the dag. Remove it from the list of dags,self,.,log,.,exception,(,'Exception bagging dag: %s',",",dag,.,dag_id,),# Only necessary at the root level since DAG.subdags automatically,# performs DFS to search through all subdags,if,dag,==,root_dag,:,for,subdag,in,subdags,:,if,subdag,.,dag_id,in,self,.,dags,:,del,self,.,dags,[,subdag,.,dag_id,],raise,cycle_exception,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,if,a,cycle,is,detected,in,this,dag,or,its,subdags,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagbag.py,DagBag.collect_dags,"def collect_dags(
            self,
            dag_folder=None,
            only_if_updated=True,
            include_examples=configuration.conf.getboolean('core', 'LOAD_EXAMPLES'),
            safe_mode=configuration.conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE')):
        """"""
        Given a file path or a folder, this method looks for python modules,
        imports them and adds them to the dagbag collection.

        Note that if a ``.airflowignore`` file is found while processing
        the directory, it will behave much like a ``.gitignore``,
        ignoring files that match any of the regex patterns specified
        in the file.

        **Note**: The patterns in .airflowignore are treated as
        un-anchored regexes, not shell-like glob patterns.
        """"""
        start_dttm = timezone.utcnow()
        dag_folder = dag_folder or self.dag_folder

        # Used to store stats around DagBag processing
        stats = []
        FileLoadStat = namedtuple(
            'FileLoadStat', ""file duration dag_num task_num dags"")

        dag_folder = correct_maybe_zipped(dag_folder)

        for filepath in list_py_file_paths(dag_folder, safe_mode=safe_mode,
                                           include_examples=include_examples):
            try:
                ts = timezone.utcnow()
                found_dags = self.process_file(
                    filepath, only_if_updated=only_if_updated,
                    safe_mode=safe_mode)

                td = timezone.utcnow() - ts
                td = td.total_seconds() + (
                    float(td.microseconds) / 1000000)
                stats.append(FileLoadStat(
                    filepath.replace(dag_folder, ''),
                    td,
                    len(found_dags),
                    sum([len(dag.tasks) for dag in found_dags]),
                    str([dag.dag_id for dag in found_dags]),
                ))
            except Exception as e:
                self.log.exception(e)
        Stats.gauge(
            'collect_dags', (timezone.utcnow() - start_dttm).total_seconds(), 1)
        Stats.gauge(
            'dagbag_size', len(self.dags), 1)
        Stats.gauge(
            'dagbag_import_errors', len(self.import_errors), 1)
        self.dagbag_stats = sorted(
            stats, key=lambda x: x.duration, reverse=True)",python,"def collect_dags(
            self,
            dag_folder=None,
            only_if_updated=True,
            include_examples=configuration.conf.getboolean('core', 'LOAD_EXAMPLES'),
            safe_mode=configuration.conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE')):
        """"""
        Given a file path or a folder, this method looks for python modules,
        imports them and adds them to the dagbag collection.

        Note that if a ``.airflowignore`` file is found while processing
        the directory, it will behave much like a ``.gitignore``,
        ignoring files that match any of the regex patterns specified
        in the file.

        **Note**: The patterns in .airflowignore are treated as
        un-anchored regexes, not shell-like glob patterns.
        """"""
        start_dttm = timezone.utcnow()
        dag_folder = dag_folder or self.dag_folder

        # Used to store stats around DagBag processing
        stats = []
        FileLoadStat = namedtuple(
            'FileLoadStat', ""file duration dag_num task_num dags"")

        dag_folder = correct_maybe_zipped(dag_folder)

        for filepath in list_py_file_paths(dag_folder, safe_mode=safe_mode,
                                           include_examples=include_examples):
            try:
                ts = timezone.utcnow()
                found_dags = self.process_file(
                    filepath, only_if_updated=only_if_updated,
                    safe_mode=safe_mode)

                td = timezone.utcnow() - ts
                td = td.total_seconds() + (
                    float(td.microseconds) / 1000000)
                stats.append(FileLoadStat(
                    filepath.replace(dag_folder, ''),
                    td,
                    len(found_dags),
                    sum([len(dag.tasks) for dag in found_dags]),
                    str([dag.dag_id for dag in found_dags]),
                ))
            except Exception as e:
                self.log.exception(e)
        Stats.gauge(
            'collect_dags', (timezone.utcnow() - start_dttm).total_seconds(), 1)
        Stats.gauge(
            'dagbag_size', len(self.dags), 1)
        Stats.gauge(
            'dagbag_import_errors', len(self.import_errors), 1)
        self.dagbag_stats = sorted(
            stats, key=lambda x: x.duration, reverse=True)",def,collect_dags,(,self,",",dag_folder,=,None,",",only_if_updated,=,True,",",include_examples,=,configuration,.,conf,.,getboolean,(,'core',",",'LOAD_EXAMPLES',),",",safe_mode,=,configuration,.,conf,.,getboolean,(,'core',",",'DAG_DISCOVERY_SAFE_MODE',),),:,start_dttm,=,timezone,.,utcnow,(,),dag_folder,=,dag_folder,or,self,"Given a file path or a folder, this method looks for python modules,
        imports them and adds them to the dagbag collection.

        Note that if a ``.airflowignore`` file is found while processing
        the directory, it will behave much like a ``.gitignore``,
        ignoring files that match any of the regex patterns specified
        in the file.

        **Note**: The patterns in .airflowignore are treated as
        un-anchored regexes, not shell-like glob patterns.",Given,a,file,path,or,a,folder,this,method,looks,for,python,modules,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L341-L396,test,.,dag_folder,# Used to store stats around DagBag processing,stats,=,[,],FileLoadStat,=,namedtuple,(,'FileLoadStat',",","""file duration dag_num task_num dags""",),dag_folder,=,correct_maybe_zipped,(,dag_folder,),for,filepath,in,list_py_file_paths,(,dag_folder,",",safe_mode,=,safe_mode,",",include_examples,=,include_examples,),:,try,:,ts,=,timezone,.,utcnow,(,),found_dags,=,self,.,process_file,(,filepath,",",only_if_updated,=,only_if_updated,",",safe_mode,=,safe_mode,),td,=,timezone,.,utcnow,(,),-,ts,td,=,td,.,total_seconds,(,),+,(,float,(,td,.,microseconds,),/,1000000,),stats,.,append,(,FileLoadStat,(,filepath,.,replace,(,dag_folder,",",'',),",",td,",",len,(,found_dags,),",",sum,(,[,len,(,dag,.,tasks,),for,dag,in,found_dags,],),",",str,(,[,dag,.,dag_id,for,dag,in,found_dags,],),",",),),except,Exception,as,e,:,self,.,log,.,exception,(,e,),Stats,.,gauge,(,'collect_dags',",",(,timezone,.,utcnow,(,),-,start_dttm,),.,total_seconds,(,),",",1,),Stats,.,gauge,(,'dagbag_size',",",len,(,self,.,dags,),",",1,),Stats,.,gauge,(,'dagbag_import_errors',",",len,(,self,.,import_errors,),",",1,),self,.,dagbag_stats,=,sorted,(,stats,",",key,=,lambda,x,:,x,.,duration,",",reverse,=,True,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,imports,them,and,adds,them,to,the,dagbag,collection,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dagbag.py,DagBag.dagbag_report,"def dagbag_report(self):
        """"""Prints a report around DagBag loading stats""""""
        report = textwrap.dedent(""""""\n
        -------------------------------------------------------------------
        DagBag loading stats for {dag_folder}
        -------------------------------------------------------------------
        Number of DAGs: {dag_num}
        Total task number: {task_num}
        DagBag parsing time: {duration}
        {table}
        """""")
        stats = self.dagbag_stats
        return report.format(
            dag_folder=self.dag_folder,
            duration=sum([o.duration for o in stats]),
            dag_num=sum([o.dag_num for o in stats]),
            task_num=sum([o.task_num for o in stats]),
            table=pprinttable(stats),
        )",python,"def dagbag_report(self):
        """"""Prints a report around DagBag loading stats""""""
        report = textwrap.dedent(""""""\n
        -------------------------------------------------------------------
        DagBag loading stats for {dag_folder}
        -------------------------------------------------------------------
        Number of DAGs: {dag_num}
        Total task number: {task_num}
        DagBag parsing time: {duration}
        {table}
        """""")
        stats = self.dagbag_stats
        return report.format(
            dag_folder=self.dag_folder,
            duration=sum([o.duration for o in stats]),
            dag_num=sum([o.dag_num for o in stats]),
            task_num=sum([o.task_num for o in stats]),
            table=pprinttable(stats),
        )",def,dagbag_report,(,self,),:,report,=,textwrap,.,dedent,(,"""""""\n
        -------------------------------------------------------------------
        DagBag loading stats for {dag_folder}
        -------------------------------------------------------------------
        Number of DAGs: {dag_num}
        Total task number: {task_num}
        DagBag parsing time: {duration}
        {table}
        """"""",),stats,=,self,.,dagbag_stats,return,report,.,format,(,dag_folder,=,self,.,dag_folder,",",duration,=,sum,(,[,o,.,duration,for,o,in,stats,],),",",dag_num,=,sum,(,[,o,.,Prints a report around DagBag loading stats,Prints,a,report,around,DagBag,loading,stats,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L398-L416,test,dag_num,for,o,in,stats,],),",",task_num,=,sum,(,[,o,.,task_num,for,o,in,stats,],),",",table,=,pprinttable,(,stats,),",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/macros/__init__.py,ds_add,"def ds_add(ds, days):
    """"""
    Add or subtract days from a YYYY-MM-DD

    :param ds: anchor date in ``YYYY-MM-DD`` format to add to
    :type ds: str
    :param days: number of days to add to the ds, you can use negative values
    :type days: int

    >>> ds_add('2015-01-01', 5)
    '2015-01-06'
    >>> ds_add('2015-01-06', -5)
    '2015-01-01'
    """"""

    ds = datetime.strptime(ds, '%Y-%m-%d')
    if days:
        ds = ds + timedelta(days)
    return ds.isoformat()[:10]",python,"def ds_add(ds, days):
    """"""
    Add or subtract days from a YYYY-MM-DD

    :param ds: anchor date in ``YYYY-MM-DD`` format to add to
    :type ds: str
    :param days: number of days to add to the ds, you can use negative values
    :type days: int

    >>> ds_add('2015-01-01', 5)
    '2015-01-06'
    >>> ds_add('2015-01-06', -5)
    '2015-01-01'
    """"""

    ds = datetime.strptime(ds, '%Y-%m-%d')
    if days:
        ds = ds + timedelta(days)
    return ds.isoformat()[:10]",def,ds_add,(,ds,",",days,),:,ds,=,datetime,.,strptime,(,ds,",",'%Y-%m-%d',),if,days,:,ds,=,ds,+,timedelta,(,days,),return,ds,.,isoformat,(,),[,:,10,],,,,,,,,,,,,,,"Add or subtract days from a YYYY-MM-DD

    :param ds: anchor date in ``YYYY-MM-DD`` format to add to
    :type ds: str
    :param days: number of days to add to the ds, you can use negative values
    :type days: int

    >>> ds_add('2015-01-01', 5)
    '2015-01-06'
    >>> ds_add('2015-01-06', -5)
    '2015-01-01'",Add,or,subtract,days,from,a,YYYY,-,MM,-,DD,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/__init__.py#L28-L46,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/macros/__init__.py,ds_format,"def ds_format(ds, input_format, output_format):
    """"""
    Takes an input string and outputs another string
    as specified in the output format

    :param ds: input string which contains a date
    :type ds: str
    :param input_format: input string format. E.g. %Y-%m-%d
    :type input_format: str
    :param output_format: output string format  E.g. %Y-%m-%d
    :type output_format: str

    >>> ds_format('2015-01-01', ""%Y-%m-%d"", ""%m-%d-%y"")
    '01-01-15'
    >>> ds_format('1/5/2015', ""%m/%d/%Y"",  ""%Y-%m-%d"")
    '2015-01-05'
    """"""
    return datetime.strptime(ds, input_format).strftime(output_format)",python,"def ds_format(ds, input_format, output_format):
    """"""
    Takes an input string and outputs another string
    as specified in the output format

    :param ds: input string which contains a date
    :type ds: str
    :param input_format: input string format. E.g. %Y-%m-%d
    :type input_format: str
    :param output_format: output string format  E.g. %Y-%m-%d
    :type output_format: str

    >>> ds_format('2015-01-01', ""%Y-%m-%d"", ""%m-%d-%y"")
    '01-01-15'
    >>> ds_format('1/5/2015', ""%m/%d/%Y"",  ""%Y-%m-%d"")
    '2015-01-05'
    """"""
    return datetime.strptime(ds, input_format).strftime(output_format)",def,ds_format,(,ds,",",input_format,",",output_format,),:,return,datetime,.,strptime,(,ds,",",input_format,),.,strftime,(,output_format,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Takes an input string and outputs another string
    as specified in the output format

    :param ds: input string which contains a date
    :type ds: str
    :param input_format: input string format. E.g. %Y-%m-%d
    :type input_format: str
    :param output_format: output string format  E.g. %Y-%m-%d
    :type output_format: str

    >>> ds_format('2015-01-01', ""%Y-%m-%d"", ""%m-%d-%y"")
    '01-01-15'
    >>> ds_format('1/5/2015', ""%m/%d/%Y"",  ""%Y-%m-%d"")
    '2015-01-05'",Takes,an,input,string,and,outputs,another,string,as,specified,in,the,output,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/__init__.py#L49-L66,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,format,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/hdfs_sensor.py,HdfsSensorRegex.poke,"def poke(self, context):
        """"""
        poke matching files in a directory with self.regex

        :return: Bool depending on the search criteria
        """"""
        sb = self.hook(self.hdfs_conn_id).get_conn()
        self.log.info(
            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern
        )
        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if
                  f['file_type'] == 'f' and
                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        return bool(result)",python,"def poke(self, context):
        """"""
        poke matching files in a directory with self.regex

        :return: Bool depending on the search criteria
        """"""
        sb = self.hook(self.hdfs_conn_id).get_conn()
        self.log.info(
            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern
        )
        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if
                  f['file_type'] == 'f' and
                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        return bool(result)",def,poke,(,self,",",context,),:,sb,=,self,.,hook,(,self,.,hdfs_conn_id,),.,get_conn,(,),self,.,log,.,info,(,'Poking for %s to be a directory with files matching %s',",",self,.,filepath,",",self,.,regex,.,pattern,),result,=,[,f,for,f,in,sb,.,ls,(,[,"poke matching files in a directory with self.regex

        :return: Bool depending on the search criteria",poke,matching,files,in,a,directory,with,self,.,regex,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/hdfs_sensor.py#L30-L46,test,self,.,filepath,],",",include_toplevel,=,False,),if,f,[,'file_type',],==,'f',and,self,.,regex,.,match,(,f,[,'path',],.,replace,(,'%s/',%,self,.,filepath,",",'',),),],result,=,self,.,filter_for_ignored_ext,(,result,",",self,.,ignored_ext,",",self,.,ignore_copying,),result,=,self,.,filter_for_filesize,(,result,",",self,.,file_size,),return,bool,(,result,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/hdfs_sensor.py,HdfsSensorFolder.poke,"def poke(self, context):
        """"""
        poke for a non empty directory

        :return: Bool depending on the search criteria
        """"""
        sb = self.hook(self.hdfs_conn_id).get_conn()
        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        if self.be_empty:
            self.log.info('Poking for filepath %s to a empty directory', self.filepath)
            return len(result) == 1 and result[0]['path'] == self.filepath
        else:
            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)
            result.pop(0)
            return bool(result) and result[0]['file_type'] == 'f'",python,"def poke(self, context):
        """"""
        poke for a non empty directory

        :return: Bool depending on the search criteria
        """"""
        sb = self.hook(self.hdfs_conn_id).get_conn()
        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        if self.be_empty:
            self.log.info('Poking for filepath %s to a empty directory', self.filepath)
            return len(result) == 1 and result[0]['path'] == self.filepath
        else:
            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)
            result.pop(0)
            return bool(result) and result[0]['file_type'] == 'f'",def,poke,(,self,",",context,),:,sb,=,self,.,hook,(,self,.,hdfs_conn_id,),.,get_conn,(,),result,=,[,f,for,f,in,sb,.,ls,(,[,self,.,filepath,],",",include_toplevel,=,True,),],result,=,self,.,filter_for_ignored_ext,(,result,",","poke for a non empty directory

        :return: Bool depending on the search criteria",poke,for,a,non,empty,directory,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/hdfs_sensor.py#L57-L74,test,self,.,ignored_ext,",",self,.,ignore_copying,),result,=,self,.,filter_for_filesize,(,result,",",self,.,file_size,),if,self,.,be_empty,:,self,.,log,.,info,(,'Poking for filepath %s to a empty directory',",",self,.,filepath,),return,len,(,result,),==,1,and,result,[,0,],[,'path',],==,self,.,filepath,else,:,self,.,log,.,info,(,'Poking for filepath %s to a non empty directory',",",self,.,filepath,),result,.,pop,(,0,),return,bool,(,result,),and,result,[,0,],[,'file_type',],==,'f',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,clear_task_instances,"def clear_task_instances(tis,
                         session,
                         activate_dag_runs=True,
                         dag=None,
                         ):
    """"""
    Clears a set of task instances, but makes sure the running ones
    get killed.

    :param tis: a list of task instances
    :param session: current session
    :param activate_dag_runs: flag to check for active dag run
    :param dag: DAG object
    """"""
    job_ids = []
    for ti in tis:
        if ti.state == State.RUNNING:
            if ti.job_id:
                ti.state = State.SHUTDOWN
                job_ids.append(ti.job_id)
        else:
            task_id = ti.task_id
            if dag and dag.has_task(task_id):
                task = dag.get_task(task_id)
                task_retries = task.retries
                ti.max_tries = ti.try_number + task_retries - 1
            else:
                # Ignore errors when updating max_tries if dag is None or
                # task not found in dag since database records could be
                # outdated. We make max_tries the maximum value of its
                # original max_tries or the current task try number.
                ti.max_tries = max(ti.max_tries, ti.try_number - 1)
            ti.state = State.NONE
            session.merge(ti)

    if job_ids:
        from airflow.jobs import BaseJob as BJ
        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():
            job.state = State.SHUTDOWN

    if activate_dag_runs and tis:
        from airflow.models.dagrun import DagRun  # Avoid circular import
        drs = session.query(DagRun).filter(
            DagRun.dag_id.in_({ti.dag_id for ti in tis}),
            DagRun.execution_date.in_({ti.execution_date for ti in tis}),
        ).all()
        for dr in drs:
            dr.state = State.RUNNING
            dr.start_date = timezone.utcnow()",python,"def clear_task_instances(tis,
                         session,
                         activate_dag_runs=True,
                         dag=None,
                         ):
    """"""
    Clears a set of task instances, but makes sure the running ones
    get killed.

    :param tis: a list of task instances
    :param session: current session
    :param activate_dag_runs: flag to check for active dag run
    :param dag: DAG object
    """"""
    job_ids = []
    for ti in tis:
        if ti.state == State.RUNNING:
            if ti.job_id:
                ti.state = State.SHUTDOWN
                job_ids.append(ti.job_id)
        else:
            task_id = ti.task_id
            if dag and dag.has_task(task_id):
                task = dag.get_task(task_id)
                task_retries = task.retries
                ti.max_tries = ti.try_number + task_retries - 1
            else:
                # Ignore errors when updating max_tries if dag is None or
                # task not found in dag since database records could be
                # outdated. We make max_tries the maximum value of its
                # original max_tries or the current task try number.
                ti.max_tries = max(ti.max_tries, ti.try_number - 1)
            ti.state = State.NONE
            session.merge(ti)

    if job_ids:
        from airflow.jobs import BaseJob as BJ
        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():
            job.state = State.SHUTDOWN

    if activate_dag_runs and tis:
        from airflow.models.dagrun import DagRun  # Avoid circular import
        drs = session.query(DagRun).filter(
            DagRun.dag_id.in_({ti.dag_id for ti in tis}),
            DagRun.execution_date.in_({ti.execution_date for ti in tis}),
        ).all()
        for dr in drs:
            dr.state = State.RUNNING
            dr.start_date = timezone.utcnow()",def,clear_task_instances,(,tis,",",session,",",activate_dag_runs,=,True,",",dag,=,None,",",),:,job_ids,=,[,],for,ti,in,tis,:,if,ti,.,state,==,State,.,RUNNING,:,if,ti,.,job_id,:,ti,.,state,=,State,.,SHUTDOWN,job_ids,.,append,(,ti,"Clears a set of task instances, but makes sure the running ones
    get killed.

    :param tis: a list of task instances
    :param session: current session
    :param activate_dag_runs: flag to check for active dag run
    :param dag: DAG object",Clears,a,set,of,task,instances,but,makes,sure,the,running,ones,get,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L59-L107,test,.,job_id,),else,:,task_id,=,ti,.,task_id,if,dag,and,dag,.,has_task,(,task_id,),:,task,=,dag,.,get_task,(,task_id,),task_retries,=,task,.,retries,ti,.,max_tries,=,ti,.,try_number,+,task_retries,-,1,else,:,# Ignore errors when updating max_tries if dag is None or,# task not found in dag since database records could be,# outdated. We make max_tries the maximum value of its,# original max_tries or the current task try number.,ti,.,max_tries,=,max,(,ti,.,max_tries,",",ti,.,try_number,-,1,),ti,.,state,=,State,.,NONE,session,.,merge,(,ti,),if,job_ids,:,from,airflow,.,jobs,import,BaseJob,as,BJ,for,job,in,session,.,query,(,BJ,),.,filter,(,BJ,.,id,.,in_,(,job_ids,),),.,all,(,),:,job,.,state,=,State,.,SHUTDOWN,if,activate_dag_runs,and,tis,:,from,airflow,.,models,.,dagrun,import,DagRun,# Avoid circular import,drs,=,session,.,query,(,DagRun,),.,filter,(,DagRun,.,dag_id,.,in_,(,{,ti,.,dag_id,for,ti,in,tis,},),",",DagRun,.,execution_date,.,in_,(,{,ti,.,execution_date,for,ti,in,tis,},),",",),.,all,(,),for,dr,in,drs,:,dr,.,state,=,State,.,RUNNING,dr,.,start_date,=,timezone,.,utcnow,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,killed,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.try_number,"def try_number(self):
        """"""
        Return the try number that this task number will be when it is actually
        run.

        If the TI is currently running, this will match the column in the
        databse, in all othercases this will be incremenetd
        """"""
        # This is designed so that task logs end up in the right file.
        if self.state == State.RUNNING:
            return self._try_number
        return self._try_number + 1",python,"def try_number(self):
        """"""
        Return the try number that this task number will be when it is actually
        run.

        If the TI is currently running, this will match the column in the
        databse, in all othercases this will be incremenetd
        """"""
        # This is designed so that task logs end up in the right file.
        if self.state == State.RUNNING:
            return self._try_number
        return self._try_number + 1",def,try_number,(,self,),:,# This is designed so that task logs end up in the right file.,if,self,.,state,==,State,.,RUNNING,:,return,self,.,_try_number,return,self,.,_try_number,+,1,,,,,,,,,,,,,,,,,,,,,,,,,,,"Return the try number that this task number will be when it is actually
        run.

        If the TI is currently running, this will match the column in the
        databse, in all othercases this will be incremenetd",Return,the,try,number,that,this,task,number,will,be,when,it,is,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L197-L208,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,actually,run,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.generate_command,"def generate_command(dag_id,
                         task_id,
                         execution_date,
                         mark_success=False,
                         ignore_all_deps=False,
                         ignore_depends_on_past=False,
                         ignore_task_deps=False,
                         ignore_ti_state=False,
                         local=False,
                         pickle_id=None,
                         file_path=None,
                         raw=False,
                         job_id=None,
                         pool=None,
                         cfg_path=None
                         ):
        """"""
        Generates the shell command required to execute this task instance.

        :param dag_id: DAG ID
        :type dag_id: unicode
        :param task_id: Task ID
        :type task_id: unicode
        :param execution_date: Execution date for the task
        :type execution_date: datetime
        :param mark_success: Whether to mark the task as successful
        :type mark_success: bool
        :param ignore_all_deps: Ignore all ignorable dependencies.
            Overrides the other ignore_* parameters.
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs
            (e.g. for Backfills)
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past
            and trigger rule
        :type ignore_task_deps: bool
        :param ignore_ti_state: Ignore the task instance's previous failure/success
        :type ignore_ti_state: bool
        :param local: Whether to run the task locally
        :type local: bool
        :param pickle_id: If the DAG was serialized to the DB, the ID
            associated with the pickled DAG
        :type pickle_id: unicode
        :param file_path: path to the file containing the DAG definition
        :param raw: raw mode (needs more details)
        :param job_id: job ID (needs more details)
        :param pool: the Airflow pool that the task should run in
        :type pool: unicode
        :param cfg_path: the Path to the configuration file
        :type cfg_path: basestring
        :return: shell command that can be used to run the task instance
        """"""
        iso = execution_date.isoformat()
        cmd = [""airflow"", ""run"", str(dag_id), str(task_id), str(iso)]
        cmd.extend([""--mark_success""]) if mark_success else None
        cmd.extend([""--pickle"", str(pickle_id)]) if pickle_id else None
        cmd.extend([""--job_id"", str(job_id)]) if job_id else None
        cmd.extend([""-A""]) if ignore_all_deps else None
        cmd.extend([""-i""]) if ignore_task_deps else None
        cmd.extend([""-I""]) if ignore_depends_on_past else None
        cmd.extend([""--force""]) if ignore_ti_state else None
        cmd.extend([""--local""]) if local else None
        cmd.extend([""--pool"", pool]) if pool else None
        cmd.extend([""--raw""]) if raw else None
        cmd.extend([""-sd"", file_path]) if file_path else None
        cmd.extend([""--cfg_path"", cfg_path]) if cfg_path else None
        return cmd",python,"def generate_command(dag_id,
                         task_id,
                         execution_date,
                         mark_success=False,
                         ignore_all_deps=False,
                         ignore_depends_on_past=False,
                         ignore_task_deps=False,
                         ignore_ti_state=False,
                         local=False,
                         pickle_id=None,
                         file_path=None,
                         raw=False,
                         job_id=None,
                         pool=None,
                         cfg_path=None
                         ):
        """"""
        Generates the shell command required to execute this task instance.

        :param dag_id: DAG ID
        :type dag_id: unicode
        :param task_id: Task ID
        :type task_id: unicode
        :param execution_date: Execution date for the task
        :type execution_date: datetime
        :param mark_success: Whether to mark the task as successful
        :type mark_success: bool
        :param ignore_all_deps: Ignore all ignorable dependencies.
            Overrides the other ignore_* parameters.
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs
            (e.g. for Backfills)
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past
            and trigger rule
        :type ignore_task_deps: bool
        :param ignore_ti_state: Ignore the task instance's previous failure/success
        :type ignore_ti_state: bool
        :param local: Whether to run the task locally
        :type local: bool
        :param pickle_id: If the DAG was serialized to the DB, the ID
            associated with the pickled DAG
        :type pickle_id: unicode
        :param file_path: path to the file containing the DAG definition
        :param raw: raw mode (needs more details)
        :param job_id: job ID (needs more details)
        :param pool: the Airflow pool that the task should run in
        :type pool: unicode
        :param cfg_path: the Path to the configuration file
        :type cfg_path: basestring
        :return: shell command that can be used to run the task instance
        """"""
        iso = execution_date.isoformat()
        cmd = [""airflow"", ""run"", str(dag_id), str(task_id), str(iso)]
        cmd.extend([""--mark_success""]) if mark_success else None
        cmd.extend([""--pickle"", str(pickle_id)]) if pickle_id else None
        cmd.extend([""--job_id"", str(job_id)]) if job_id else None
        cmd.extend([""-A""]) if ignore_all_deps else None
        cmd.extend([""-i""]) if ignore_task_deps else None
        cmd.extend([""-I""]) if ignore_depends_on_past else None
        cmd.extend([""--force""]) if ignore_ti_state else None
        cmd.extend([""--local""]) if local else None
        cmd.extend([""--pool"", pool]) if pool else None
        cmd.extend([""--raw""]) if raw else None
        cmd.extend([""-sd"", file_path]) if file_path else None
        cmd.extend([""--cfg_path"", cfg_path]) if cfg_path else None
        return cmd",def,generate_command,(,dag_id,",",task_id,",",execution_date,",",mark_success,=,False,",",ignore_all_deps,=,False,",",ignore_depends_on_past,=,False,",",ignore_task_deps,=,False,",",ignore_ti_state,=,False,",",local,=,False,",",pickle_id,=,None,",",file_path,=,None,",",raw,=,False,",",job_id,=,None,",",pool,=,None,"Generates the shell command required to execute this task instance.

        :param dag_id: DAG ID
        :type dag_id: unicode
        :param task_id: Task ID
        :type task_id: unicode
        :param execution_date: Execution date for the task
        :type execution_date: datetime
        :param mark_success: Whether to mark the task as successful
        :type mark_success: bool
        :param ignore_all_deps: Ignore all ignorable dependencies.
            Overrides the other ignore_* parameters.
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs
            (e.g. for Backfills)
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past
            and trigger rule
        :type ignore_task_deps: bool
        :param ignore_ti_state: Ignore the task instance's previous failure/success
        :type ignore_ti_state: bool
        :param local: Whether to run the task locally
        :type local: bool
        :param pickle_id: If the DAG was serialized to the DB, the ID
            associated with the pickled DAG
        :type pickle_id: unicode
        :param file_path: path to the file containing the DAG definition
        :param raw: raw mode (needs more details)
        :param job_id: job ID (needs more details)
        :param pool: the Airflow pool that the task should run in
        :type pool: unicode
        :param cfg_path: the Path to the configuration file
        :type cfg_path: basestring
        :return: shell command that can be used to run the task instance",Generates,the,shell,command,required,to,execute,this,task,instance,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L295-L361,test,",",cfg_path,=,None,),:,iso,=,execution_date,.,isoformat,(,),cmd,=,[,"""airflow""",",","""run""",",",str,(,dag_id,),",",str,(,task_id,),",",str,(,iso,),],cmd,.,extend,(,[,"""--mark_success""",],),if,mark_success,else,None,cmd,.,extend,(,[,"""--pickle""",",",str,(,pickle_id,),],),if,pickle_id,else,None,cmd,.,extend,(,[,"""--job_id""",",",str,(,job_id,),],),if,job_id,else,None,cmd,.,extend,(,[,"""-A""",],),if,ignore_all_deps,else,None,cmd,.,extend,(,[,"""-i""",],),if,ignore_task_deps,else,None,cmd,.,extend,(,[,"""-I""",],),if,ignore_depends_on_past,else,None,cmd,.,extend,(,[,"""--force""",],),if,ignore_ti_state,else,None,cmd,.,extend,(,[,"""--local""",],),if,local,else,None,cmd,.,extend,(,[,"""--pool""",",",pool,],),if,pool,else,None,cmd,.,extend,(,[,"""--raw""",],),if,raw,else,None,cmd,.,extend,(,[,"""-sd""",",",file_path,],),if,file_path,else,None,cmd,.,extend,(,[,"""--cfg_path""",",",cfg_path,],),if,cfg_path,else,None,return,cmd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.current_state,"def current_state(self, session=None):
        """"""
        Get the very latest state from the database, if a session is passed,
        we use and looking up the state becomes part of the session, otherwise
        a new session is used.
        """"""
        TI = TaskInstance
        ti = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.task_id == self.task_id,
            TI.execution_date == self.execution_date,
        ).all()
        if ti:
            state = ti[0].state
        else:
            state = None
        return state",python,"def current_state(self, session=None):
        """"""
        Get the very latest state from the database, if a session is passed,
        we use and looking up the state becomes part of the session, otherwise
        a new session is used.
        """"""
        TI = TaskInstance
        ti = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.task_id == self.task_id,
            TI.execution_date == self.execution_date,
        ).all()
        if ti:
            state = ti[0].state
        else:
            state = None
        return state",def,current_state,(,self,",",session,=,None,),:,TI,=,TaskInstance,ti,=,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,==,self,.,dag_id,",",TI,.,task_id,==,self,.,task_id,",",TI,.,execution_date,==,self,.,execution_date,",",),.,all,(,"Get the very latest state from the database, if a session is passed,
        we use and looking up the state becomes part of the session, otherwise
        a new session is used.",Get,the,very,latest,state,from,the,database,if,a,session,is,passed,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L395-L411,test,),if,ti,:,state,=,ti,[,0,],.,state,else,:,state,=,None,return,state,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,we,use,and,looking,up,the,state,becomes,part,of,the,session,otherwise,a,new,session,is,used,.,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.error,"def error(self, session=None):
        """"""
        Forces the task instance's state to FAILED in the database.
        """"""
        self.log.error(""Recording the task instance as FAILED"")
        self.state = State.FAILED
        session.merge(self)
        session.commit()",python,"def error(self, session=None):
        """"""
        Forces the task instance's state to FAILED in the database.
        """"""
        self.log.error(""Recording the task instance as FAILED"")
        self.state = State.FAILED
        session.merge(self)
        session.commit()",def,error,(,self,",",session,=,None,),:,self,.,log,.,error,(,"""Recording the task instance as FAILED""",),self,.,state,=,State,.,FAILED,session,.,merge,(,self,),session,.,commit,(,),,,,,,,,,,,,,,,,,Forces the task instance's state to FAILED in the database.,Forces,the,task,instance,s,state,to,FAILED,in,the,database,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L414-L421,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.refresh_from_db,"def refresh_from_db(self, session=None, lock_for_update=False):
        """"""
        Refreshes the task instance from the database based on the primary key

        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.
        """"""
        TI = TaskInstance

        qry = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.task_id == self.task_id,
            TI.execution_date == self.execution_date)

        if lock_for_update:
            ti = qry.with_for_update().first()
        else:
            ti = qry.first()
        if ti:
            self.state = ti.state
            self.start_date = ti.start_date
            self.end_date = ti.end_date
            # Get the raw value of try_number column, don't read through the
            # accessor here otherwise it will be incremeneted by one already.
            self.try_number = ti._try_number
            self.max_tries = ti.max_tries
            self.hostname = ti.hostname
            self.pid = ti.pid
            self.executor_config = ti.executor_config
        else:
            self.state = None",python,"def refresh_from_db(self, session=None, lock_for_update=False):
        """"""
        Refreshes the task instance from the database based on the primary key

        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.
        """"""
        TI = TaskInstance

        qry = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.task_id == self.task_id,
            TI.execution_date == self.execution_date)

        if lock_for_update:
            ti = qry.with_for_update().first()
        else:
            ti = qry.first()
        if ti:
            self.state = ti.state
            self.start_date = ti.start_date
            self.end_date = ti.end_date
            # Get the raw value of try_number column, don't read through the
            # accessor here otherwise it will be incremeneted by one already.
            self.try_number = ti._try_number
            self.max_tries = ti.max_tries
            self.hostname = ti.hostname
            self.pid = ti.pid
            self.executor_config = ti.executor_config
        else:
            self.state = None",def,refresh_from_db,(,self,",",session,=,None,",",lock_for_update,=,False,),:,TI,=,TaskInstance,qry,=,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,==,self,.,dag_id,",",TI,.,task_id,==,self,.,task_id,",",TI,.,execution_date,==,self,.,execution_date,),"Refreshes the task instance from the database based on the primary key

        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.",Refreshes,the,task,instance,from,the,database,based,on,the,primary,key,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L424-L455,test,if,lock_for_update,:,ti,=,qry,.,with_for_update,(,),.,first,(,),else,:,ti,=,qry,.,first,(,),if,ti,:,self,.,state,=,ti,.,state,self,.,start_date,=,ti,.,start_date,self,.,end_date,=,ti,.,end_date,"# Get the raw value of try_number column, don't read through the",# accessor here otherwise it will be incremeneted by one already.,self,.,try_number,=,ti,.,_try_number,self,.,max_tries,=,ti,.,max_tries,self,.,hostname,=,ti,.,hostname,self,.,pid,=,ti,.,pid,self,.,executor_config,=,ti,.,executor_config,else,:,self,.,state,=,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.clear_xcom_data,"def clear_xcom_data(self, session=None):
        """"""
        Clears all XCom data from the database for the task instance
        """"""
        session.query(XCom).filter(
            XCom.dag_id == self.dag_id,
            XCom.task_id == self.task_id,
            XCom.execution_date == self.execution_date
        ).delete()
        session.commit()",python,"def clear_xcom_data(self, session=None):
        """"""
        Clears all XCom data from the database for the task instance
        """"""
        session.query(XCom).filter(
            XCom.dag_id == self.dag_id,
            XCom.task_id == self.task_id,
            XCom.execution_date == self.execution_date
        ).delete()
        session.commit()",def,clear_xcom_data,(,self,",",session,=,None,),:,session,.,query,(,XCom,),.,filter,(,XCom,.,dag_id,==,self,.,dag_id,",",XCom,.,task_id,==,self,.,task_id,",",XCom,.,execution_date,==,self,.,execution_date,),.,delete,(,),session,.,commit,(,),Clears all XCom data from the database for the task instance,Clears,all,XCom,data,from,the,database,for,the,task,instance,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L458-L467,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.key,"def key(self):
        """"""
        Returns a tuple that identifies the task instance uniquely
        """"""
        return self.dag_id, self.task_id, self.execution_date, self.try_number",python,"def key(self):
        """"""
        Returns a tuple that identifies the task instance uniquely
        """"""
        return self.dag_id, self.task_id, self.execution_date, self.try_number",def,key,(,self,),:,return,self,.,dag_id,",",self,.,task_id,",",self,.,execution_date,",",self,.,try_number,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Returns a tuple that identifies the task instance uniquely,Returns,a,tuple,that,identifies,the,task,instance,uniquely,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L470-L474,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.are_dependents_done,"def are_dependents_done(self, session=None):
        """"""
        Checks whether the dependents of this task instance have all succeeded.
        This is meant to be used by wait_for_downstream.

        This is useful when you do not want to start processing the next
        schedule of a task until the dependents are done. For instance,
        if the task DROPs and recreates a table.
        """"""
        task = self.task

        if not task.downstream_task_ids:
            return True

        ti = session.query(func.count(TaskInstance.task_id)).filter(
            TaskInstance.dag_id == self.dag_id,
            TaskInstance.task_id.in_(task.downstream_task_ids),
            TaskInstance.execution_date == self.execution_date,
            TaskInstance.state == State.SUCCESS,
        )
        count = ti[0][0]
        return count == len(task.downstream_task_ids)",python,"def are_dependents_done(self, session=None):
        """"""
        Checks whether the dependents of this task instance have all succeeded.
        This is meant to be used by wait_for_downstream.

        This is useful when you do not want to start processing the next
        schedule of a task until the dependents are done. For instance,
        if the task DROPs and recreates a table.
        """"""
        task = self.task

        if not task.downstream_task_ids:
            return True

        ti = session.query(func.count(TaskInstance.task_id)).filter(
            TaskInstance.dag_id == self.dag_id,
            TaskInstance.task_id.in_(task.downstream_task_ids),
            TaskInstance.execution_date == self.execution_date,
            TaskInstance.state == State.SUCCESS,
        )
        count = ti[0][0]
        return count == len(task.downstream_task_ids)",def,are_dependents_done,(,self,",",session,=,None,),:,task,=,self,.,task,if,not,task,.,downstream_task_ids,:,return,True,ti,=,session,.,query,(,func,.,count,(,TaskInstance,.,task_id,),),.,filter,(,TaskInstance,.,dag_id,==,self,.,dag_id,",",TaskInstance,.,task_id,"Checks whether the dependents of this task instance have all succeeded.
        This is meant to be used by wait_for_downstream.

        This is useful when you do not want to start processing the next
        schedule of a task until the dependents are done. For instance,
        if the task DROPs and recreates a table.",Checks,whether,the,dependents,of,this,task,instance,have,all,succeeded,.,This,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L494-L515,test,.,in_,(,task,.,downstream_task_ids,),",",TaskInstance,.,execution_date,==,self,.,execution_date,",",TaskInstance,.,state,==,State,.,SUCCESS,",",),count,=,ti,[,0,],[,0,],return,count,==,len,(,task,.,downstream_task_ids,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,is,meant,to,be,used,by,wait_for_downstream,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.next_retry_datetime,"def next_retry_datetime(self):
        """"""
        Get datetime of the next retry if the task instance fails. For exponential
        backoff, retry_delay is used as base and will be converted to seconds.
        """"""
        delay = self.task.retry_delay
        if self.task.retry_exponential_backoff:
            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))
            # deterministic per task instance
            hash = int(hashlib.sha1(""{}#{}#{}#{}"".format(self.dag_id,
                                                         self.task_id,
                                                         self.execution_date,
                                                         self.try_number)
                                    .encode('utf-8')).hexdigest(), 16)
            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)
            modded_hash = min_backoff + hash % min_backoff
            # timedelta has a maximum representable value. The exponentiation
            # here means this value can be exceeded after a certain number
            # of tries (around 50 if the initial delay is 1s, even fewer if
            # the delay is larger). Cap the value here before creating a
            # timedelta object so the operation doesn't fail.
            delay_backoff_in_seconds = min(
                modded_hash,
                timedelta.max.total_seconds() - 1
            )
            delay = timedelta(seconds=delay_backoff_in_seconds)
            if self.task.max_retry_delay:
                delay = min(self.task.max_retry_delay, delay)
        return self.end_date + delay",python,"def next_retry_datetime(self):
        """"""
        Get datetime of the next retry if the task instance fails. For exponential
        backoff, retry_delay is used as base and will be converted to seconds.
        """"""
        delay = self.task.retry_delay
        if self.task.retry_exponential_backoff:
            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))
            # deterministic per task instance
            hash = int(hashlib.sha1(""{}#{}#{}#{}"".format(self.dag_id,
                                                         self.task_id,
                                                         self.execution_date,
                                                         self.try_number)
                                    .encode('utf-8')).hexdigest(), 16)
            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)
            modded_hash = min_backoff + hash % min_backoff
            # timedelta has a maximum representable value. The exponentiation
            # here means this value can be exceeded after a certain number
            # of tries (around 50 if the initial delay is 1s, even fewer if
            # the delay is larger). Cap the value here before creating a
            # timedelta object so the operation doesn't fail.
            delay_backoff_in_seconds = min(
                modded_hash,
                timedelta.max.total_seconds() - 1
            )
            delay = timedelta(seconds=delay_backoff_in_seconds)
            if self.task.max_retry_delay:
                delay = min(self.task.max_retry_delay, delay)
        return self.end_date + delay",def,next_retry_datetime,(,self,),:,delay,=,self,.,task,.,retry_delay,if,self,.,task,.,retry_exponential_backoff,:,min_backoff,=,int,(,delay,.,total_seconds,(,),*,(,2,**,(,self,.,try_number,-,2,),),),# deterministic per task instance,hash,=,int,(,hashlib,.,sha1,(,"""{}#{}#{}#{}""","Get datetime of the next retry if the task instance fails. For exponential
        backoff, retry_delay is used as base and will be converted to seconds.",Get,datetime,of,the,next,retry,if,the,task,instance,fails,.,For,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L614-L642,test,.,format,(,self,.,dag_id,",",self,.,task_id,",",self,.,execution_date,",",self,.,try_number,),.,encode,(,'utf-8',),),.,hexdigest,(,),",",16,),# between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number),modded_hash,=,min_backoff,+,hash,%,min_backoff,# timedelta has a maximum representable value. The exponentiation,# here means this value can be exceeded after a certain number,"# of tries (around 50 if the initial delay is 1s, even fewer if",# the delay is larger). Cap the value here before creating a,# timedelta object so the operation doesn't fail.,delay_backoff_in_seconds,=,min,(,modded_hash,",",timedelta,.,max,.,total_seconds,(,),-,1,),delay,=,timedelta,(,seconds,=,delay_backoff_in_seconds,),if,self,.,task,.,max_retry_delay,:,delay,=,min,(,self,.,task,.,max_retry_delay,",",delay,),return,self,.,end_date,+,delay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,exponential,backoff,retry_delay,is,used,as,base,and,will,be,converted,to,seconds,.,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.ready_for_retry,"def ready_for_retry(self):
        """"""
        Checks on whether the task instance is in the right state and timeframe
        to be retried.
        """"""
        return (self.state == State.UP_FOR_RETRY and
                self.next_retry_datetime() < timezone.utcnow())",python,"def ready_for_retry(self):
        """"""
        Checks on whether the task instance is in the right state and timeframe
        to be retried.
        """"""
        return (self.state == State.UP_FOR_RETRY and
                self.next_retry_datetime() < timezone.utcnow())",def,ready_for_retry,(,self,),:,return,(,self,.,state,==,State,.,UP_FOR_RETRY,and,self,.,next_retry_datetime,(,),<,timezone,.,utcnow,(,),),,,,,,,,,,,,,,,,,,,,,,,,,"Checks on whether the task instance is in the right state and timeframe
        to be retried.",Checks,on,whether,the,task,instance,is,in,the,right,state,and,timeframe,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L644-L650,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,be,retried,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.pool_full,"def pool_full(self, session):
        """"""
        Returns a boolean as to whether the slot pool has room for this
        task to run
        """"""
        if not self.task.pool:
            return False

        pool = (
            session
            .query(Pool)
            .filter(Pool.pool == self.task.pool)
            .first()
        )
        if not pool:
            return False
        open_slots = pool.open_slots(session=session)

        return open_slots <= 0",python,"def pool_full(self, session):
        """"""
        Returns a boolean as to whether the slot pool has room for this
        task to run
        """"""
        if not self.task.pool:
            return False

        pool = (
            session
            .query(Pool)
            .filter(Pool.pool == self.task.pool)
            .first()
        )
        if not pool:
            return False
        open_slots = pool.open_slots(session=session)

        return open_slots <= 0",def,pool_full,(,self,",",session,),:,if,not,self,.,task,.,pool,:,return,False,pool,=,(,session,.,query,(,Pool,),.,filter,(,Pool,.,pool,==,self,.,task,.,pool,),.,first,(,),),if,not,pool,:,return,False,open_slots,"Returns a boolean as to whether the slot pool has room for this
        task to run",Returns,a,boolean,as,to,whether,the,slot,pool,has,room,for,this,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L653-L671,test,=,pool,.,open_slots,(,session,=,session,),return,open_slots,<=,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,task,to,run,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.get_dagrun,"def get_dagrun(self, session):
        """"""
        Returns the DagRun for this TaskInstance

        :param session:
        :return: DagRun
        """"""
        from airflow.models.dagrun import DagRun  # Avoid circular import
        dr = session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date == self.execution_date
        ).first()

        return dr",python,"def get_dagrun(self, session):
        """"""
        Returns the DagRun for this TaskInstance

        :param session:
        :return: DagRun
        """"""
        from airflow.models.dagrun import DagRun  # Avoid circular import
        dr = session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date == self.execution_date
        ).first()

        return dr",def,get_dagrun,(,self,",",session,),:,from,airflow,.,models,.,dagrun,import,DagRun,# Avoid circular import,dr,=,session,.,query,(,DagRun,),.,filter,(,DagRun,.,dag_id,==,self,.,dag_id,",",DagRun,.,execution_date,==,self,.,execution_date,),.,first,(,),return,dr,,,"Returns the DagRun for this TaskInstance

        :param session:
        :return: DagRun",Returns,the,DagRun,for,this,TaskInstance,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L674-L687,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.xcom_push,"def xcom_push(
            self,
            key,
            value,
            execution_date=None):
        """"""
        Make an XCom available for tasks to pull.

        :param key: A key for the XCom
        :type key: str
        :param value: A value for the XCom. The value is pickled and stored
            in the database.
        :type value: any pickleable object
        :param execution_date: if provided, the XCom will not be visible until
            this date. This can be used, for example, to send a message to a
            task on a future date without it being immediately visible.
        :type execution_date: datetime
        """"""

        if execution_date and execution_date < self.execution_date:
            raise ValueError(
                'execution_date can not be in the past (current '
                'execution_date is {}; received {})'.format(
                    self.execution_date, execution_date))

        XCom.set(
            key=key,
            value=value,
            task_id=self.task_id,
            dag_id=self.dag_id,
            execution_date=execution_date or self.execution_date)",python,"def xcom_push(
            self,
            key,
            value,
            execution_date=None):
        """"""
        Make an XCom available for tasks to pull.

        :param key: A key for the XCom
        :type key: str
        :param value: A value for the XCom. The value is pickled and stored
            in the database.
        :type value: any pickleable object
        :param execution_date: if provided, the XCom will not be visible until
            this date. This can be used, for example, to send a message to a
            task on a future date without it being immediately visible.
        :type execution_date: datetime
        """"""

        if execution_date and execution_date < self.execution_date:
            raise ValueError(
                'execution_date can not be in the past (current '
                'execution_date is {}; received {})'.format(
                    self.execution_date, execution_date))

        XCom.set(
            key=key,
            value=value,
            task_id=self.task_id,
            dag_id=self.dag_id,
            execution_date=execution_date or self.execution_date)",def,xcom_push,(,self,",",key,",",value,",",execution_date,=,None,),:,if,execution_date,and,execution_date,<,self,.,execution_date,:,raise,ValueError,(,'execution_date can not be in the past (current ','execution_date is {}; received {})',.,format,(,self,.,execution_date,",",execution_date,),),XCom,.,set,(,key,=,key,",",value,=,value,",",task_id,=,"Make an XCom available for tasks to pull.

        :param key: A key for the XCom
        :type key: str
        :param value: A value for the XCom. The value is pickled and stored
            in the database.
        :type value: any pickleable object
        :param execution_date: if provided, the XCom will not be visible until
            this date. This can be used, for example, to send a message to a
            task on a future date without it being immediately visible.
        :type execution_date: datetime",Make,an,XCom,available,for,tasks,to,pull,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1271-L1301,test,self,.,task_id,",",dag_id,=,self,.,dag_id,",",execution_date,=,execution_date,or,self,.,execution_date,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.xcom_pull,"def xcom_pull(
            self,
            task_ids=None,
            dag_id=None,
            key=XCOM_RETURN_KEY,
            include_prior_dates=False):
        """"""
        Pull XComs that optionally meet certain criteria.

        The default value for `key` limits the search to XComs
        that were returned by other tasks (as opposed to those that were pushed
        manually). To remove this filter, pass key=None (or any desired value).

        If a single task_id string is provided, the result is the value of the
        most recent matching XCom from that task_id. If multiple task_ids are
        provided, a tuple of matching values is returned. None is returned
        whenever no matches are found.

        :param key: A key for the XCom. If provided, only XComs with matching
            keys will be returned. The default key is 'return_value', also
            available as a constant XCOM_RETURN_KEY. This key is automatically
            given to XComs returned by tasks (as opposed to being pushed
            manually). To remove the filter, pass key=None.
        :type key: str
        :param task_ids: Only XComs from tasks with matching ids will be
            pulled. Can pass None to remove the filter.
        :type task_ids: str or iterable of strings (representing task_ids)
        :param dag_id: If provided, only pulls XComs from this DAG.
            If None (default), the DAG of the calling task is used.
        :type dag_id: str
        :param include_prior_dates: If False, only XComs from the current
            execution_date are returned. If True, XComs from previous dates
            are returned as well.
        :type include_prior_dates: bool
        """"""

        if dag_id is None:
            dag_id = self.dag_id

        pull_fn = functools.partial(
            XCom.get_one,
            execution_date=self.execution_date,
            key=key,
            dag_id=dag_id,
            include_prior_dates=include_prior_dates)

        if is_container(task_ids):
            return tuple(pull_fn(task_id=t) for t in task_ids)
        else:
            return pull_fn(task_id=task_ids)",python,"def xcom_pull(
            self,
            task_ids=None,
            dag_id=None,
            key=XCOM_RETURN_KEY,
            include_prior_dates=False):
        """"""
        Pull XComs that optionally meet certain criteria.

        The default value for `key` limits the search to XComs
        that were returned by other tasks (as opposed to those that were pushed
        manually). To remove this filter, pass key=None (or any desired value).

        If a single task_id string is provided, the result is the value of the
        most recent matching XCom from that task_id. If multiple task_ids are
        provided, a tuple of matching values is returned. None is returned
        whenever no matches are found.

        :param key: A key for the XCom. If provided, only XComs with matching
            keys will be returned. The default key is 'return_value', also
            available as a constant XCOM_RETURN_KEY. This key is automatically
            given to XComs returned by tasks (as opposed to being pushed
            manually). To remove the filter, pass key=None.
        :type key: str
        :param task_ids: Only XComs from tasks with matching ids will be
            pulled. Can pass None to remove the filter.
        :type task_ids: str or iterable of strings (representing task_ids)
        :param dag_id: If provided, only pulls XComs from this DAG.
            If None (default), the DAG of the calling task is used.
        :type dag_id: str
        :param include_prior_dates: If False, only XComs from the current
            execution_date are returned. If True, XComs from previous dates
            are returned as well.
        :type include_prior_dates: bool
        """"""

        if dag_id is None:
            dag_id = self.dag_id

        pull_fn = functools.partial(
            XCom.get_one,
            execution_date=self.execution_date,
            key=key,
            dag_id=dag_id,
            include_prior_dates=include_prior_dates)

        if is_container(task_ids):
            return tuple(pull_fn(task_id=t) for t in task_ids)
        else:
            return pull_fn(task_id=task_ids)",def,xcom_pull,(,self,",",task_ids,=,None,",",dag_id,=,None,",",key,=,XCOM_RETURN_KEY,",",include_prior_dates,=,False,),:,if,dag_id,is,None,:,dag_id,=,self,.,dag_id,pull_fn,=,functools,.,partial,(,XCom,.,get_one,",",execution_date,=,self,.,execution_date,",",key,=,key,",","Pull XComs that optionally meet certain criteria.

        The default value for `key` limits the search to XComs
        that were returned by other tasks (as opposed to those that were pushed
        manually). To remove this filter, pass key=None (or any desired value).

        If a single task_id string is provided, the result is the value of the
        most recent matching XCom from that task_id. If multiple task_ids are
        provided, a tuple of matching values is returned. None is returned
        whenever no matches are found.

        :param key: A key for the XCom. If provided, only XComs with matching
            keys will be returned. The default key is 'return_value', also
            available as a constant XCOM_RETURN_KEY. This key is automatically
            given to XComs returned by tasks (as opposed to being pushed
            manually). To remove the filter, pass key=None.
        :type key: str
        :param task_ids: Only XComs from tasks with matching ids will be
            pulled. Can pass None to remove the filter.
        :type task_ids: str or iterable of strings (representing task_ids)
        :param dag_id: If provided, only pulls XComs from this DAG.
            If None (default), the DAG of the calling task is used.
        :type dag_id: str
        :param include_prior_dates: If False, only XComs from the current
            execution_date are returned. If True, XComs from previous dates
            are returned as well.
        :type include_prior_dates: bool",Pull,XComs,that,optionally,meet,certain,criteria,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1303-L1352,test,dag_id,=,dag_id,",",include_prior_dates,=,include_prior_dates,),if,is_container,(,task_ids,),:,return,tuple,(,pull_fn,(,task_id,=,t,),for,t,in,task_ids,),else,:,return,pull_fn,(,task_id,=,task_ids,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/taskinstance.py,TaskInstance.init_run_context,"def init_run_context(self, raw=False):
        """"""
        Sets the log context.
        """"""
        self.raw = raw
        self._set_context(self)",python,"def init_run_context(self, raw=False):
        """"""
        Sets the log context.
        """"""
        self.raw = raw
        self._set_context(self)",def,init_run_context,(,self,",",raw,=,False,),:,self,.,raw,=,raw,self,.,_set_context,(,self,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sets the log context.,Sets,the,log,context,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1363-L1368,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/log/wasb_task_handler.py,WasbTaskHandler.close,"def close(self):
        """"""
        Close and upload local log file to remote storage Wasb.
        """"""
        # When application exit, system shuts down all handlers by
        # calling close method. Here we check if logger is already
        # closed to prevent uploading the log to remote storage multiple
        # times when `logging.shutdown` is called.
        if self.closed:
            return

        super().close()

        if not self.upload_on_close:
            return

        local_loc = os.path.join(self.local_base, self.log_relative_path)
        remote_loc = os.path.join(self.remote_base, self.log_relative_path)
        if os.path.exists(local_loc):
            # read log and remove old logs to get just the latest additions
            with open(local_loc, 'r') as logfile:
                log = logfile.read()
            self.wasb_write(log, remote_loc, append=True)

            if self.delete_local_copy:
                shutil.rmtree(os.path.dirname(local_loc))
        # Mark closed so we don't double write if close is called twice
        self.closed = True",python,"def close(self):
        """"""
        Close and upload local log file to remote storage Wasb.
        """"""
        # When application exit, system shuts down all handlers by
        # calling close method. Here we check if logger is already
        # closed to prevent uploading the log to remote storage multiple
        # times when `logging.shutdown` is called.
        if self.closed:
            return

        super().close()

        if not self.upload_on_close:
            return

        local_loc = os.path.join(self.local_base, self.log_relative_path)
        remote_loc = os.path.join(self.remote_base, self.log_relative_path)
        if os.path.exists(local_loc):
            # read log and remove old logs to get just the latest additions
            with open(local_loc, 'r') as logfile:
                log = logfile.read()
            self.wasb_write(log, remote_loc, append=True)

            if self.delete_local_copy:
                shutil.rmtree(os.path.dirname(local_loc))
        # Mark closed so we don't double write if close is called twice
        self.closed = True",def,close,(,self,),:,"# When application exit, system shuts down all handlers by",# calling close method. Here we check if logger is already,# closed to prevent uploading the log to remote storage multiple,# times when `logging.shutdown` is called.,if,self,.,closed,:,return,super,(,),.,close,(,),if,not,self,.,upload_on_close,:,return,local_loc,=,os,.,path,.,join,(,self,.,local_base,",",self,.,log_relative_path,),remote_loc,=,os,.,path,.,Close and upload local log file to remote storage Wasb.,Close,and,upload,local,log,file,to,remote,storage,Wasb,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L68-L95,test,join,(,self,.,remote_base,",",self,.,log_relative_path,),if,os,.,path,.,exists,(,local_loc,),:,# read log and remove old logs to get just the latest additions,with,open,(,local_loc,",",'r',),as,logfile,:,log,=,logfile,.,read,(,),self,.,wasb_write,(,log,",",remote_loc,",",append,=,True,),if,self,.,delete_local_copy,:,shutil,.,rmtree,(,os,.,path,.,dirname,(,local_loc,),),# Mark closed so we don't double write if close is called twice,self,.,closed,=,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.get_conn,"def get_conn(self):
        """"""
        Retrieves connection to Google Compute Engine.

        :return: Google Compute Engine services object
        :rtype: dict
        """"""
        if not self._conn:
            http_authorized = self._authorize()
            self._conn = build('compute', self.api_version,
                               http=http_authorized, cache_discovery=False)
        return self._conn",python,"def get_conn(self):
        """"""
        Retrieves connection to Google Compute Engine.

        :return: Google Compute Engine services object
        :rtype: dict
        """"""
        if not self._conn:
            http_authorized = self._authorize()
            self._conn = build('compute', self.api_version,
                               http=http_authorized, cache_discovery=False)
        return self._conn",def,get_conn,(,self,),:,if,not,self,.,_conn,:,http_authorized,=,self,.,_authorize,(,),self,.,_conn,=,build,(,'compute',",",self,.,api_version,",",http,=,http_authorized,",",cache_discovery,=,False,),return,self,.,_conn,,,,,,,,,,"Retrieves connection to Google Compute Engine.

        :return: Google Compute Engine services object
        :rtype: dict",Retrieves,connection,to,Google,Compute,Engine,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L55-L66,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.start_instance,"def start_instance(self, zone, resource_id, project_id=None):
        """"""
        Starts an existing instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().start(
            project=project_id,
            zone=zone,
            instance=resource_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)",python,"def start_instance(self, zone, resource_id, project_id=None):
        """"""
        Starts an existing instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().start(
            project=project_id,
            zone=zone,
            instance=resource_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)",def,start_instance,(,self,",",zone,",",resource_id,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instances,(,),.,start,(,project,=,project_id,",",zone,=,zone,",",instance,=,resource_id,),.,execute,(,num_retries,=,self,.,num_retries,),try,:,operation_name,"Starts an existing instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Starts,an,existing,instance,defined,by,project_id,zone,and,resource_id,.,Must,be,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L69-L97,test,=,response,[,"""name""",],except,KeyError,:,raise,AirflowException,(,"""Wrong response '{}' returned - it should contain ""","""'name' field""",.,format,(,response,),),self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,",",zone,=,zone,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,called,with,keyword,arguments,rather,than,positional,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.set_machine_type,"def set_machine_type(self, zone, resource_id, body, project_id=None):
        """"""
        Sets machine type of an instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists.
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param body: Body required by the Compute Engine setMachineType API,
            as described in
            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType
        :type body: dict
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self._execute_set_machine_type(zone, resource_id, body, project_id)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)",python,"def set_machine_type(self, zone, resource_id, body, project_id=None):
        """"""
        Sets machine type of an instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists.
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param body: Body required by the Compute Engine setMachineType API,
            as described in
            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType
        :type body: dict
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self._execute_set_machine_type(zone, resource_id, body, project_id)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)",def,set_machine_type,(,self,",",zone,",",resource_id,",",body,",",project_id,=,None,),:,response,=,self,.,_execute_set_machine_type,(,zone,",",resource_id,",",body,",",project_id,),try,:,operation_name,=,response,[,"""name""",],except,KeyError,:,raise,AirflowException,(,"""Wrong response '{}' returned - it should contain ""","""'name' field""",.,format,(,response,),),"Sets machine type of an instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists.
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param body: Body required by the Compute Engine setMachineType API,
            as described in
            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType
        :type body: dict
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Sets,machine,type,of,an,instance,defined,by,project_id,zone,and,resource_id,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L131-L159,test,self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,",",zone,=,zone,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Must,be,called,with,keyword,arguments,rather,than,positional,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.get_instance_template,"def get_instance_template(self, resource_id, project_id=None):
        """"""
        Retrieves instance template by project_id and resource_id.
        Must be called with keyword arguments rather than positional.

        :param resource_id: Name of the instance template
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :rtype: dict
        """"""
        response = self.get_conn().instanceTemplates().get(
            project=project_id,
            instanceTemplate=resource_id
        ).execute(num_retries=self.num_retries)
        return response",python,"def get_instance_template(self, resource_id, project_id=None):
        """"""
        Retrieves instance template by project_id and resource_id.
        Must be called with keyword arguments rather than positional.

        :param resource_id: Name of the instance template
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :rtype: dict
        """"""
        response = self.get_conn().instanceTemplates().get(
            project=project_id,
            instanceTemplate=resource_id
        ).execute(num_retries=self.num_retries)
        return response",def,get_instance_template,(,self,",",resource_id,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instanceTemplates,(,),.,get,(,project,=,project_id,",",instanceTemplate,=,resource_id,),.,execute,(,num_retries,=,self,.,num_retries,),return,response,,,,,,,,"Retrieves instance template by project_id and resource_id.
        Must be called with keyword arguments rather than positional.

        :param resource_id: Name of the instance template
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :rtype: dict",Retrieves,instance,template,by,project_id,and,resource_id,.,Must,be,called,with,keyword,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L167-L186,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arguments,rather,than,positional,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.insert_instance_template,"def insert_instance_template(self, body, request_id=None, project_id=None):
        """"""
        Inserts instance template using body specified
        Must be called with keyword arguments rather than positional.

        :param body: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again)
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instanceTemplates().insert(
            project=project_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",python,"def insert_instance_template(self, body, request_id=None, project_id=None):
        """"""
        Inserts instance template using body specified
        Must be called with keyword arguments rather than positional.

        :param body: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again)
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instanceTemplates().insert(
            project=project_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)",def,insert_instance_template,(,self,",",body,",",request_id,=,None,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instanceTemplates,(,),.,insert,(,project,=,project_id,",",body,=,body,",",requestId,=,request_id,),.,execute,(,num_retries,=,self,.,num_retries,),try,"Inserts instance template using body specified
        Must be called with keyword arguments rather than positional.

        :param body: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again)
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Inserts,instance,template,using,body,specified,Must,be,called,with,keyword,arguments,rather,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L189-L220,test,:,operation_name,=,response,[,"""name""",],except,KeyError,:,raise,AirflowException,(,"""Wrong response '{}' returned - it should contain ""","""'name' field""",.,format,(,response,),),self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,than,positional,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.get_instance_group_manager,"def get_instance_group_manager(self, zone, resource_id, project_id=None):
        """"""
        Retrieves Instance Group Manager by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance group manager representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers
        :rtype: dict
        """"""
        response = self.get_conn().instanceGroupManagers().get(
            project=project_id,
            zone=zone,
            instanceGroupManager=resource_id
        ).execute(num_retries=self.num_retries)
        return response",python,"def get_instance_group_manager(self, zone, resource_id, project_id=None):
        """"""
        Retrieves Instance Group Manager by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance group manager representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers
        :rtype: dict
        """"""
        response = self.get_conn().instanceGroupManagers().get(
            project=project_id,
            zone=zone,
            instanceGroupManager=resource_id
        ).execute(num_retries=self.num_retries)
        return response",def,get_instance_group_manager,(,self,",",zone,",",resource_id,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instanceGroupManagers,(,),.,get,(,project,=,project_id,",",zone,=,zone,",",instanceGroupManager,=,resource_id,),.,execute,(,num_retries,=,self,.,num_retries,),return,response,,"Retrieves Instance Group Manager by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance group manager representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers
        :rtype: dict",Retrieves,Instance,Group,Manager,by,project_id,zone,and,resource_id,.,Must,be,called,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L223-L245,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,with,keyword,arguments,rather,than,positional,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook.patch_instance_group_manager,"def patch_instance_group_manager(self, zone, resource_id,
                                     body, request_id=None, project_id=None):
        """"""
        Patches Instance Group Manager with the specified body.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param body: Instance Group Manager representation as json-merge-patch object
            according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again).
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instanceGroupManagers().patch(
            project=project_id,
            zone=zone,
            instanceGroupManager=resource_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)",python,"def patch_instance_group_manager(self, zone, resource_id,
                                     body, request_id=None, project_id=None):
        """"""
        Patches Instance Group Manager with the specified body.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param body: Instance Group Manager representation as json-merge-patch object
            according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again).
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instanceGroupManagers().patch(
            project=project_id,
            zone=zone,
            instanceGroupManager=resource_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)",def,patch_instance_group_manager,(,self,",",zone,",",resource_id,",",body,",",request_id,=,None,",",project_id,=,None,),:,response,=,self,.,get_conn,(,),.,instanceGroupManagers,(,),.,patch,(,project,=,project_id,",",zone,=,zone,",",instanceGroupManager,=,resource_id,",",body,=,body,",",requestId,=,"Patches Instance Group Manager with the specified body.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param body: Instance Group Manager representation as json-merge-patch object
            according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again).
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Patches,Instance,Group,Manager,with,the,specified,body,.,Must,be,called,with,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L248-L288,test,request_id,),.,execute,(,num_retries,=,self,.,num_retries,),try,:,operation_name,=,response,[,"""name""",],except,KeyError,:,raise,AirflowException,(,"""Wrong response '{}' returned - it should contain ""","""'name' field""",.,format,(,response,),),self,.,_wait_for_operation_to_complete,(,project_id,=,project_id,",",operation_name,=,operation_name,",",zone,=,zone,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,keyword,arguments,rather,than,positional,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,GceHook._wait_for_operation_to_complete,"def _wait_for_operation_to_complete(self, project_id, operation_name, zone=None):
        """"""
        Waits for the named operation to complete - checks status of the async call.

        :param operation_name: name of the operation
        :type operation_name: str
        :param zone: optional region of the request (might be None for global operations)
        :type zone: str
        :return: None
        """"""
        service = self.get_conn()
        while True:
            if zone is None:
                # noinspection PyTypeChecker
                operation_response = self._check_global_operation_status(
                    service, operation_name, project_id)
            else:
                # noinspection PyTypeChecker
                operation_response = self._check_zone_operation_status(
                    service, operation_name, project_id, zone, self.num_retries)
            if operation_response.get(""status"") == GceOperationStatus.DONE:
                error = operation_response.get(""error"")
                if error:
                    code = operation_response.get(""httpErrorStatusCode"")
                    msg = operation_response.get(""httpErrorMessage"")
                    # Extracting the errors list as string and trimming square braces
                    error_msg = str(error.get(""errors""))[1:-1]
                    raise AirflowException(""{} {}: "".format(code, msg) + error_msg)
                # No meaningful info to return from the response in case of success
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)",python,"def _wait_for_operation_to_complete(self, project_id, operation_name, zone=None):
        """"""
        Waits for the named operation to complete - checks status of the async call.

        :param operation_name: name of the operation
        :type operation_name: str
        :param zone: optional region of the request (might be None for global operations)
        :type zone: str
        :return: None
        """"""
        service = self.get_conn()
        while True:
            if zone is None:
                # noinspection PyTypeChecker
                operation_response = self._check_global_operation_status(
                    service, operation_name, project_id)
            else:
                # noinspection PyTypeChecker
                operation_response = self._check_zone_operation_status(
                    service, operation_name, project_id, zone, self.num_retries)
            if operation_response.get(""status"") == GceOperationStatus.DONE:
                error = operation_response.get(""error"")
                if error:
                    code = operation_response.get(""httpErrorStatusCode"")
                    msg = operation_response.get(""httpErrorMessage"")
                    # Extracting the errors list as string and trimming square braces
                    error_msg = str(error.get(""errors""))[1:-1]
                    raise AirflowException(""{} {}: "".format(code, msg) + error_msg)
                # No meaningful info to return from the response in case of success
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)",def,_wait_for_operation_to_complete,(,self,",",project_id,",",operation_name,",",zone,=,None,),:,service,=,self,.,get_conn,(,),while,True,:,if,zone,is,None,:,# noinspection PyTypeChecker,operation_response,=,self,.,_check_global_operation_status,(,service,",",operation_name,",",project_id,),else,:,# noinspection PyTypeChecker,operation_response,=,self,.,_check_zone_operation_status,(,service,"Waits for the named operation to complete - checks status of the async call.

        :param operation_name: name of the operation
        :type operation_name: str
        :param zone: optional region of the request (might be None for global operations)
        :type zone: str
        :return: None",Waits,for,the,named,operation,to,complete,-,checks,status,of,the,async,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L290-L320,test,",",operation_name,",",project_id,",",zone,",",self,.,num_retries,),if,operation_response,.,get,(,"""status""",),==,GceOperationStatus,.,DONE,:,error,=,operation_response,.,get,(,"""error""",),if,error,:,code,=,operation_response,.,get,(,"""httpErrorStatusCode""",),msg,=,operation_response,.,get,(,"""httpErrorMessage""",),# Extracting the errors list as string and trimming square braces,error_msg,=,str,(,error,.,get,(,"""errors""",),),[,1,:,-,1,],raise,AirflowException,(,"""{} {}: """,.,format,(,code,",",msg,),+,error_msg,),# No meaningful info to return from the response in case of success,return,time,.,sleep,(,TIME_TO_SLEEP_IN_SECONDS,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,call,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.check_for_bucket,"def check_for_bucket(self, bucket_name):
        """"""
        Check if bucket_name exists.

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        try:
            self.get_conn().head_bucket(Bucket=bucket_name)
            return True
        except ClientError as e:
            self.log.info(e.response[""Error""][""Message""])
            return False",python,"def check_for_bucket(self, bucket_name):
        """"""
        Check if bucket_name exists.

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        try:
            self.get_conn().head_bucket(Bucket=bucket_name)
            return True
        except ClientError as e:
            self.log.info(e.response[""Error""][""Message""])
            return False",def,check_for_bucket,(,self,",",bucket_name,),:,try,:,self,.,get_conn,(,),.,head_bucket,(,Bucket,=,bucket_name,),return,True,except,ClientError,as,e,:,self,.,log,.,info,(,e,.,response,[,"""Error""",],[,"""Message""",],),return,False,,,,,,"Check if bucket_name exists.

        :param bucket_name: the name of the bucket
        :type bucket_name: str",Check,if,bucket_name,exists,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L48-L60,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.create_bucket,"def create_bucket(self, bucket_name, region_name=None):
        """"""
        Creates an Amazon S3 bucket.

        :param bucket_name: The name of the bucket
        :type bucket_name: str
        :param region_name: The name of the aws region in which to create the bucket.
        :type region_name: str
        """"""
        s3_conn = self.get_conn()
        if not region_name:
            region_name = s3_conn.meta.region_name
        if region_name == 'us-east-1':
            self.get_conn().create_bucket(Bucket=bucket_name)
        else:
            self.get_conn().create_bucket(Bucket=bucket_name,
                                          CreateBucketConfiguration={
                                              'LocationConstraint': region_name
                                          })",python,"def create_bucket(self, bucket_name, region_name=None):
        """"""
        Creates an Amazon S3 bucket.

        :param bucket_name: The name of the bucket
        :type bucket_name: str
        :param region_name: The name of the aws region in which to create the bucket.
        :type region_name: str
        """"""
        s3_conn = self.get_conn()
        if not region_name:
            region_name = s3_conn.meta.region_name
        if region_name == 'us-east-1':
            self.get_conn().create_bucket(Bucket=bucket_name)
        else:
            self.get_conn().create_bucket(Bucket=bucket_name,
                                          CreateBucketConfiguration={
                                              'LocationConstraint': region_name
                                          })",def,create_bucket,(,self,",",bucket_name,",",region_name,=,None,),:,s3_conn,=,self,.,get_conn,(,),if,not,region_name,:,region_name,=,s3_conn,.,meta,.,region_name,if,region_name,==,'us-east-1',:,self,.,get_conn,(,),.,create_bucket,(,Bucket,=,bucket_name,),else,:,self,.,get_conn,"Creates an Amazon S3 bucket.

        :param bucket_name: The name of the bucket
        :type bucket_name: str
        :param region_name: The name of the aws region in which to create the bucket.
        :type region_name: str",Creates,an,Amazon,S3,bucket,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L72-L90,test,(,),.,create_bucket,(,Bucket,=,bucket_name,",",CreateBucketConfiguration,=,{,'LocationConstraint',:,region_name,},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.check_for_prefix,"def check_for_prefix(self, bucket_name, prefix, delimiter):
        """"""
        Checks that a prefix exists in a bucket

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        """"""
        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix
        prefix_split = re.split(r'(\w+[{d}])$'.format(d=delimiter), prefix, 1)
        previous_level = prefix_split[0]
        plist = self.list_prefixes(bucket_name, previous_level, delimiter)
        return False if plist is None else prefix in plist",python,"def check_for_prefix(self, bucket_name, prefix, delimiter):
        """"""
        Checks that a prefix exists in a bucket

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        """"""
        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix
        prefix_split = re.split(r'(\w+[{d}])$'.format(d=delimiter), prefix, 1)
        previous_level = prefix_split[0]
        plist = self.list_prefixes(bucket_name, previous_level, delimiter)
        return False if plist is None else prefix in plist",def,check_for_prefix,(,self,",",bucket_name,",",prefix,",",delimiter,),:,prefix,=,prefix,+,delimiter,if,prefix,[,-,1,],!=,delimiter,else,prefix,prefix_split,=,re,.,split,(,r'(\w+[{d}])$',.,format,(,d,=,delimiter,),",",prefix,",",1,),previous_level,=,prefix_split,[,0,],"Checks that a prefix exists in a bucket

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str",Checks,that,a,prefix,exists,in,a,bucket,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L92-L107,test,plist,=,self,.,list_prefixes,(,bucket_name,",",previous_level,",",delimiter,),return,False,if,plist,is,None,else,prefix,in,plist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.list_prefixes,"def list_prefixes(self, bucket_name, prefix='', delimiter='',
                      page_size=None, max_items=None):
        """"""
        Lists prefixes in a bucket under prefix

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('list_objects_v2')
        response = paginator.paginate(Bucket=bucket_name,
                                      Prefix=prefix,
                                      Delimiter=delimiter,
                                      PaginationConfig=config)

        has_results = False
        prefixes = []
        for page in response:
            if 'CommonPrefixes' in page:
                has_results = True
                for p in page['CommonPrefixes']:
                    prefixes.append(p['Prefix'])

        if has_results:
            return prefixes",python,"def list_prefixes(self, bucket_name, prefix='', delimiter='',
                      page_size=None, max_items=None):
        """"""
        Lists prefixes in a bucket under prefix

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('list_objects_v2')
        response = paginator.paginate(Bucket=bucket_name,
                                      Prefix=prefix,
                                      Delimiter=delimiter,
                                      PaginationConfig=config)

        has_results = False
        prefixes = []
        for page in response:
            if 'CommonPrefixes' in page:
                has_results = True
                for p in page['CommonPrefixes']:
                    prefixes.append(p['Prefix'])

        if has_results:
            return prefixes",def,list_prefixes,(,self,",",bucket_name,",",prefix,=,'',",",delimiter,=,'',",",page_size,=,None,",",max_items,=,None,),:,config,=,{,'PageSize',:,page_size,",",'MaxItems',:,max_items,",",},paginator,=,self,.,get_conn,(,),.,get_paginator,(,'list_objects_v2',),response,=,paginator,.,"Lists prefixes in a bucket under prefix

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int",Lists,prefixes,in,a,bucket,under,prefix,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L109-L145,test,paginate,(,Bucket,=,bucket_name,",",Prefix,=,prefix,",",Delimiter,=,delimiter,",",PaginationConfig,=,config,),has_results,=,False,prefixes,=,[,],for,page,in,response,:,if,'CommonPrefixes',in,page,:,has_results,=,True,for,p,in,page,[,'CommonPrefixes',],:,prefixes,.,append,(,p,[,'Prefix',],),if,has_results,:,return,prefixes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.list_keys,"def list_keys(self, bucket_name, prefix='', delimiter='',
                  page_size=None, max_items=None):
        """"""
        Lists keys in a bucket under prefix and not containing delimiter

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('list_objects_v2')
        response = paginator.paginate(Bucket=bucket_name,
                                      Prefix=prefix,
                                      Delimiter=delimiter,
                                      PaginationConfig=config)

        has_results = False
        keys = []
        for page in response:
            if 'Contents' in page:
                has_results = True
                for k in page['Contents']:
                    keys.append(k['Key'])

        if has_results:
            return keys",python,"def list_keys(self, bucket_name, prefix='', delimiter='',
                  page_size=None, max_items=None):
        """"""
        Lists keys in a bucket under prefix and not containing delimiter

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('list_objects_v2')
        response = paginator.paginate(Bucket=bucket_name,
                                      Prefix=prefix,
                                      Delimiter=delimiter,
                                      PaginationConfig=config)

        has_results = False
        keys = []
        for page in response:
            if 'Contents' in page:
                has_results = True
                for k in page['Contents']:
                    keys.append(k['Key'])

        if has_results:
            return keys",def,list_keys,(,self,",",bucket_name,",",prefix,=,'',",",delimiter,=,'',",",page_size,=,None,",",max_items,=,None,),:,config,=,{,'PageSize',:,page_size,",",'MaxItems',:,max_items,",",},paginator,=,self,.,get_conn,(,),.,get_paginator,(,'list_objects_v2',),response,=,paginator,.,"Lists keys in a bucket under prefix and not containing delimiter

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int",Lists,keys,in,a,bucket,under,prefix,and,not,containing,delimiter,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L147-L183,test,paginate,(,Bucket,=,bucket_name,",",Prefix,=,prefix,",",Delimiter,=,delimiter,",",PaginationConfig,=,config,),has_results,=,False,keys,=,[,],for,page,in,response,:,if,'Contents',in,page,:,has_results,=,True,for,k,in,page,[,'Contents',],:,keys,.,append,(,k,[,'Key',],),if,has_results,:,return,keys,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.check_for_key,"def check_for_key(self, key, bucket_name=None):
        """"""
        Checks if a key exists in a bucket

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        try:
            self.get_conn().head_object(Bucket=bucket_name, Key=key)
            return True
        except ClientError as e:
            self.log.info(e.response[""Error""][""Message""])
            return False",python,"def check_for_key(self, key, bucket_name=None):
        """"""
        Checks if a key exists in a bucket

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        try:
            self.get_conn().head_object(Bucket=bucket_name, Key=key)
            return True
        except ClientError as e:
            self.log.info(e.response[""Error""][""Message""])
            return False",def,check_for_key,(,self,",",key,",",bucket_name,=,None,),:,if,not,bucket_name,:,(,bucket_name,",",key,),=,self,.,parse_s3_url,(,key,),try,:,self,.,get_conn,(,),.,head_object,(,Bucket,=,bucket_name,",",Key,=,key,),return,True,except,ClientError,as,e,"Checks if a key exists in a bucket

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str",Checks,if,a,key,exists,in,a,bucket,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L185-L202,test,:,self,.,log,.,info,(,e,.,response,[,"""Error""",],[,"""Message""",],),return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.get_key,"def get_key(self, key, bucket_name=None):
        """"""
        Returns a boto3.s3.Object

        :param key: the path to the key
        :type key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        obj = self.get_resource_type('s3').Object(bucket_name, key)
        obj.load()
        return obj",python,"def get_key(self, key, bucket_name=None):
        """"""
        Returns a boto3.s3.Object

        :param key: the path to the key
        :type key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        obj = self.get_resource_type('s3').Object(bucket_name, key)
        obj.load()
        return obj",def,get_key,(,self,",",key,",",bucket_name,=,None,),:,if,not,bucket_name,:,(,bucket_name,",",key,),=,self,.,parse_s3_url,(,key,),obj,=,self,.,get_resource_type,(,'s3',),.,Object,(,bucket_name,",",key,),obj,.,load,(,),return,obj,,,"Returns a boto3.s3.Object

        :param key: the path to the key
        :type key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str",Returns,a,boto3,.,s3,.,Object,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L204-L218,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.read_key,"def read_key(self, key, bucket_name=None):
        """"""
        Reads a key from S3

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """"""

        obj = self.get_key(key, bucket_name)
        return obj.get()['Body'].read().decode('utf-8')",python,"def read_key(self, key, bucket_name=None):
        """"""
        Reads a key from S3

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """"""

        obj = self.get_key(key, bucket_name)
        return obj.get()['Body'].read().decode('utf-8')",def,read_key,(,self,",",key,",",bucket_name,=,None,),:,obj,=,self,.,get_key,(,key,",",bucket_name,),return,obj,.,get,(,),[,'Body',],.,read,(,),.,decode,(,'utf-8',),,,,,,,,,,,,,"Reads a key from S3

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str",Reads,a,key,from,S3,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L220-L231,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.select_key,"def select_key(self, key, bucket_name=None,
                   expression='SELECT * FROM S3Object',
                   expression_type='SQL',
                   input_serialization=None,
                   output_serialization=None):
        """"""
        Reads a key with S3 Select.

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        :param expression: S3 Select expression
        :type expression: str
        :param expression_type: S3 Select expression type
        :type expression_type: str
        :param input_serialization: S3 Select input data serialization format
        :type input_serialization: dict
        :param output_serialization: S3 Select output data serialization format
        :type output_serialization: dict
        :return: retrieved subset of original data by S3 Select
        :rtype: str

        .. seealso::
            For more details about S3 Select parameters:
            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content
        """"""
        if input_serialization is None:
            input_serialization = {'CSV': {}}
        if output_serialization is None:
            output_serialization = {'CSV': {}}
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        response = self.get_conn().select_object_content(
            Bucket=bucket_name,
            Key=key,
            Expression=expression,
            ExpressionType=expression_type,
            InputSerialization=input_serialization,
            OutputSerialization=output_serialization)

        return ''.join(event['Records']['Payload'].decode('utf-8')
                       for event in response['Payload']
                       if 'Records' in event)",python,"def select_key(self, key, bucket_name=None,
                   expression='SELECT * FROM S3Object',
                   expression_type='SQL',
                   input_serialization=None,
                   output_serialization=None):
        """"""
        Reads a key with S3 Select.

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        :param expression: S3 Select expression
        :type expression: str
        :param expression_type: S3 Select expression type
        :type expression_type: str
        :param input_serialization: S3 Select input data serialization format
        :type input_serialization: dict
        :param output_serialization: S3 Select output data serialization format
        :type output_serialization: dict
        :return: retrieved subset of original data by S3 Select
        :rtype: str

        .. seealso::
            For more details about S3 Select parameters:
            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content
        """"""
        if input_serialization is None:
            input_serialization = {'CSV': {}}
        if output_serialization is None:
            output_serialization = {'CSV': {}}
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        response = self.get_conn().select_object_content(
            Bucket=bucket_name,
            Key=key,
            Expression=expression,
            ExpressionType=expression_type,
            InputSerialization=input_serialization,
            OutputSerialization=output_serialization)

        return ''.join(event['Records']['Payload'].decode('utf-8')
                       for event in response['Payload']
                       if 'Records' in event)",def,select_key,(,self,",",key,",",bucket_name,=,None,",",expression,=,'SELECT * FROM S3Object',",",expression_type,=,'SQL',",",input_serialization,=,None,",",output_serialization,=,None,),:,if,input_serialization,is,None,:,input_serialization,=,{,'CSV',:,{,},},if,output_serialization,is,None,:,output_serialization,=,{,'CSV',:,{,"Reads a key with S3 Select.

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        :param expression: S3 Select expression
        :type expression: str
        :param expression_type: S3 Select expression type
        :type expression_type: str
        :param input_serialization: S3 Select input data serialization format
        :type input_serialization: dict
        :param output_serialization: S3 Select output data serialization format
        :type output_serialization: dict
        :return: retrieved subset of original data by S3 Select
        :rtype: str

        .. seealso::
            For more details about S3 Select parameters:
            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content",Reads,a,key,with,S3,Select,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L233-L277,test,},},if,not,bucket_name,:,(,bucket_name,",",key,),=,self,.,parse_s3_url,(,key,),response,=,self,.,get_conn,(,),.,select_object_content,(,Bucket,=,bucket_name,",",Key,=,key,",",Expression,=,expression,",",ExpressionType,=,expression_type,",",InputSerialization,=,input_serialization,",",OutputSerialization,=,output_serialization,),return,'',.,join,(,event,[,'Records',],[,'Payload',],.,decode,(,'utf-8',),for,event,in,response,[,'Payload',],if,'Records',in,event,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.check_for_wildcard_key,"def check_for_wildcard_key(self,
                               wildcard_key, bucket_name=None, delimiter=''):
        """"""
        Checks that a key matching a wildcard expression exists in a bucket

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str
        """"""
        return self.get_wildcard_key(wildcard_key=wildcard_key,
                                     bucket_name=bucket_name,
                                     delimiter=delimiter) is not None",python,"def check_for_wildcard_key(self,
                               wildcard_key, bucket_name=None, delimiter=''):
        """"""
        Checks that a key matching a wildcard expression exists in a bucket

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str
        """"""
        return self.get_wildcard_key(wildcard_key=wildcard_key,
                                     bucket_name=bucket_name,
                                     delimiter=delimiter) is not None",def,check_for_wildcard_key,(,self,",",wildcard_key,",",bucket_name,=,None,",",delimiter,=,'',),:,return,self,.,get_wildcard_key,(,wildcard_key,=,wildcard_key,",",bucket_name,=,bucket_name,",",delimiter,=,delimiter,),is,not,None,,,,,,,,,,,,,,,,,"Checks that a key matching a wildcard expression exists in a bucket

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str",Checks,that,a,key,matching,a,wildcard,expression,exists,in,a,bucket,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L279-L293,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.get_wildcard_key,"def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):
        """"""
        Returns a boto3.s3.Object object matching the wildcard expression

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str
        """"""
        if not bucket_name:
            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)

        prefix = re.split(r'[*]', wildcard_key, 1)[0]
        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)
        if klist:
            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]
            if key_matches:
                return self.get_key(key_matches[0], bucket_name)",python,"def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):
        """"""
        Returns a boto3.s3.Object object matching the wildcard expression

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str
        """"""
        if not bucket_name:
            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)

        prefix = re.split(r'[*]', wildcard_key, 1)[0]
        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)
        if klist:
            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]
            if key_matches:
                return self.get_key(key_matches[0], bucket_name)",def,get_wildcard_key,(,self,",",wildcard_key,",",bucket_name,=,None,",",delimiter,=,'',),:,if,not,bucket_name,:,(,bucket_name,",",wildcard_key,),=,self,.,parse_s3_url,(,wildcard_key,),prefix,=,re,.,split,(,r'[*]',",",wildcard_key,",",1,),[,0,],klist,=,self,.,list_keys,"Returns a boto3.s3.Object object matching the wildcard expression

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str",Returns,a,boto3,.,s3,.,Object,object,matching,the,wildcard,expression,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L295-L314,test,(,bucket_name,",",prefix,=,prefix,",",delimiter,=,delimiter,),if,klist,:,key_matches,=,[,k,for,k,in,klist,if,fnmatch,.,fnmatch,(,k,",",wildcard_key,),],if,key_matches,:,return,self,.,get_key,(,key_matches,[,0,],",",bucket_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.load_file,"def load_file(self,
                  filename,
                  key,
                  bucket_name=None,
                  replace=False,
                  encrypt=False):
        """"""
        Loads a local file to S3

        :param filename: name of the file to load.
        :type filename: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists. If replace is False and the key exists, an
            error will be raised.
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)",python,"def load_file(self,
                  filename,
                  key,
                  bucket_name=None,
                  replace=False,
                  encrypt=False):
        """"""
        Loads a local file to S3

        :param filename: name of the file to load.
        :type filename: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists. If replace is False and the key exists, an
            error will be raised.
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)",def,load_file,(,self,",",filename,",",key,",",bucket_name,=,None,",",replace,=,False,",",encrypt,=,False,),:,if,not,bucket_name,:,(,bucket_name,",",key,),=,self,.,parse_s3_url,(,key,),if,not,replace,and,self,.,check_for_key,(,key,",",bucket_name,),:,raise,"Loads a local file to S3

        :param filename: name of the file to load.
        :type filename: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists. If replace is False and the key exists, an
            error will be raised.
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool",Loads,a,local,file,to,S3,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L316-L350,test,ValueError,(,"""The key {key} already exists.""",.,format,(,key,=,key,),),extra_args,=,{,},if,encrypt,:,extra_args,[,'ServerSideEncryption',],=,"""AES256""",client,=,self,.,get_conn,(,),client,.,upload_file,(,filename,",",bucket_name,",",key,",",ExtraArgs,=,extra_args,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.load_string,"def load_string(self,
                    string_data,
                    key,
                    bucket_name=None,
                    replace=False,
                    encrypt=False,
                    encoding='utf-8'):
        """"""
        Loads a string to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param string_data: str to set as content for the key.
        :type string_data: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        self.load_bytes(string_data.encode(encoding),
                        key=key,
                        bucket_name=bucket_name,
                        replace=replace,
                        encrypt=encrypt)",python,"def load_string(self,
                    string_data,
                    key,
                    bucket_name=None,
                    replace=False,
                    encrypt=False,
                    encoding='utf-8'):
        """"""
        Loads a string to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param string_data: str to set as content for the key.
        :type string_data: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        self.load_bytes(string_data.encode(encoding),
                        key=key,
                        bucket_name=bucket_name,
                        replace=replace,
                        encrypt=encrypt)",def,load_string,(,self,",",string_data,",",key,",",bucket_name,=,None,",",replace,=,False,",",encrypt,=,False,",",encoding,=,'utf-8',),:,self,.,load_bytes,(,string_data,.,encode,(,encoding,),",",key,=,key,",",bucket_name,=,bucket_name,",",replace,=,replace,",",encrypt,=,encrypt,"Loads a string to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param string_data: str to set as content for the key.
        :type string_data: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool",Loads,a,string,to,S3,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L352-L382,test,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.load_bytes,"def load_bytes(self,
                   bytes_data,
                   key,
                   bucket_name=None,
                   replace=False,
                   encrypt=False):
        """"""
        Loads bytes to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param bytes_data: bytes to set as content for the key.
        :type bytes_data: bytes
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        filelike_buffer = BytesIO(bytes_data)

        client = self.get_conn()
        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)",python,"def load_bytes(self,
                   bytes_data,
                   key,
                   bucket_name=None,
                   replace=False,
                   encrypt=False):
        """"""
        Loads bytes to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param bytes_data: bytes to set as content for the key.
        :type bytes_data: bytes
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        filelike_buffer = BytesIO(bytes_data)

        client = self.get_conn()
        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)",def,load_bytes,(,self,",",bytes_data,",",key,",",bucket_name,=,None,",",replace,=,False,",",encrypt,=,False,),:,if,not,bucket_name,:,(,bucket_name,",",key,),=,self,.,parse_s3_url,(,key,),if,not,replace,and,self,.,check_for_key,(,key,",",bucket_name,),:,raise,"Loads bytes to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param bytes_data: bytes to set as content for the key.
        :type bytes_data: bytes
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool",Loads,bytes,to,S3,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L384-L422,test,ValueError,(,"""The key {key} already exists.""",.,format,(,key,=,key,),),extra_args,=,{,},if,encrypt,:,extra_args,[,'ServerSideEncryption',],=,"""AES256""",filelike_buffer,=,BytesIO,(,bytes_data,),client,=,self,.,get_conn,(,),client,.,upload_fileobj,(,filelike_buffer,",",bucket_name,",",key,",",ExtraArgs,=,extra_args,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.load_file_obj,"def load_file_obj(self,
                      file_obj,
                      key,
                      bucket_name=None,
                      replace=False,
                      encrypt=False):
        """"""
        Loads a file object to S3

        :param file_obj: The file-like object to set as the content for the S3 key.
        :type file_obj: file-like object
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag that indicates whether to overwrite the key
            if it already exists.
        :type replace: bool
        :param encrypt: If True, S3 encrypts the file on the server,
            and the file is stored in encrypted form at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)",python,"def load_file_obj(self,
                      file_obj,
                      key,
                      bucket_name=None,
                      replace=False,
                      encrypt=False):
        """"""
        Loads a file object to S3

        :param file_obj: The file-like object to set as the content for the S3 key.
        :type file_obj: file-like object
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag that indicates whether to overwrite the key
            if it already exists.
        :type replace: bool
        :param encrypt: If True, S3 encrypts the file on the server,
            and the file is stored in encrypted form at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)",def,load_file_obj,(,self,",",file_obj,",",key,",",bucket_name,=,None,",",replace,=,False,",",encrypt,=,False,),:,if,not,bucket_name,:,(,bucket_name,",",key,),=,self,.,parse_s3_url,(,key,),if,not,replace,and,self,.,check_for_key,(,key,",",bucket_name,),:,raise,"Loads a file object to S3

        :param file_obj: The file-like object to set as the content for the S3 key.
        :type file_obj: file-like object
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag that indicates whether to overwrite the key
            if it already exists.
        :type replace: bool
        :param encrypt: If True, S3 encrypts the file on the server,
            and the file is stored in encrypted form at rest in S3.
        :type encrypt: bool",Loads,a,file,object,to,S3,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L424-L457,test,ValueError,(,"""The key {key} already exists.""",.,format,(,key,=,key,),),extra_args,=,{,},if,encrypt,:,extra_args,[,'ServerSideEncryption',],=,"""AES256""",client,=,self,.,get_conn,(,),client,.,upload_fileobj,(,file_obj,",",bucket_name,",",key,",",ExtraArgs,=,extra_args,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/S3_hook.py,S3Hook.copy_object,"def copy_object(self,
                    source_bucket_key,
                    dest_bucket_key,
                    source_bucket_name=None,
                    dest_bucket_name=None,
                    source_version_id=None):
        """"""
        Creates a copy of an object that is already stored in S3.

        Note: the S3 connection used here needs to have access to both
        source and destination bucket/key.

        :param source_bucket_key: The key of the source object.

            It can be either full s3:// style url or relative path from root level.

            When it's specified as a full s3:// url, please omit source_bucket_name.
        :type source_bucket_key: str
        :param dest_bucket_key: The key of the object to copy to.

            The convention to specify `dest_bucket_key` is the same
            as `source_bucket_key`.
        :type dest_bucket_key: str
        :param source_bucket_name: Name of the S3 bucket where the source object is in.

            It should be omitted when `source_bucket_key` is provided as a full s3:// url.
        :type source_bucket_name: str
        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.

            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.
        :type dest_bucket_name: str
        :param source_version_id: Version ID of the source object (OPTIONAL)
        :type source_version_id: str
        """"""

        if dest_bucket_name is None:
            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)
        else:
            parsed_url = urlparse(dest_bucket_key)
            if parsed_url.scheme != '' or parsed_url.netloc != '':
                raise AirflowException('If dest_bucket_name is provided, ' +
                                       'dest_bucket_key should be relative path ' +
                                       'from root level, rather than a full s3:// url')

        if source_bucket_name is None:
            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)
        else:
            parsed_url = urlparse(source_bucket_key)
            if parsed_url.scheme != '' or parsed_url.netloc != '':
                raise AirflowException('If source_bucket_name is provided, ' +
                                       'source_bucket_key should be relative path ' +
                                       'from root level, rather than a full s3:// url')

        CopySource = {'Bucket': source_bucket_name,
                      'Key': source_bucket_key,
                      'VersionId': source_version_id}
        response = self.get_conn().copy_object(Bucket=dest_bucket_name,
                                               Key=dest_bucket_key,
                                               CopySource=CopySource)
        return response",python,"def copy_object(self,
                    source_bucket_key,
                    dest_bucket_key,
                    source_bucket_name=None,
                    dest_bucket_name=None,
                    source_version_id=None):
        """"""
        Creates a copy of an object that is already stored in S3.

        Note: the S3 connection used here needs to have access to both
        source and destination bucket/key.

        :param source_bucket_key: The key of the source object.

            It can be either full s3:// style url or relative path from root level.

            When it's specified as a full s3:// url, please omit source_bucket_name.
        :type source_bucket_key: str
        :param dest_bucket_key: The key of the object to copy to.

            The convention to specify `dest_bucket_key` is the same
            as `source_bucket_key`.
        :type dest_bucket_key: str
        :param source_bucket_name: Name of the S3 bucket where the source object is in.

            It should be omitted when `source_bucket_key` is provided as a full s3:// url.
        :type source_bucket_name: str
        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.

            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.
        :type dest_bucket_name: str
        :param source_version_id: Version ID of the source object (OPTIONAL)
        :type source_version_id: str
        """"""

        if dest_bucket_name is None:
            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)
        else:
            parsed_url = urlparse(dest_bucket_key)
            if parsed_url.scheme != '' or parsed_url.netloc != '':
                raise AirflowException('If dest_bucket_name is provided, ' +
                                       'dest_bucket_key should be relative path ' +
                                       'from root level, rather than a full s3:// url')

        if source_bucket_name is None:
            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)
        else:
            parsed_url = urlparse(source_bucket_key)
            if parsed_url.scheme != '' or parsed_url.netloc != '':
                raise AirflowException('If source_bucket_name is provided, ' +
                                       'source_bucket_key should be relative path ' +
                                       'from root level, rather than a full s3:// url')

        CopySource = {'Bucket': source_bucket_name,
                      'Key': source_bucket_key,
                      'VersionId': source_version_id}
        response = self.get_conn().copy_object(Bucket=dest_bucket_name,
                                               Key=dest_bucket_key,
                                               CopySource=CopySource)
        return response",def,copy_object,(,self,",",source_bucket_key,",",dest_bucket_key,",",source_bucket_name,=,None,",",dest_bucket_name,=,None,",",source_version_id,=,None,),:,if,dest_bucket_name,is,None,:,dest_bucket_name,",",dest_bucket_key,=,self,.,parse_s3_url,(,dest_bucket_key,),else,:,parsed_url,=,urlparse,(,dest_bucket_key,),if,parsed_url,.,scheme,!=,'',or,"Creates a copy of an object that is already stored in S3.

        Note: the S3 connection used here needs to have access to both
        source and destination bucket/key.

        :param source_bucket_key: The key of the source object.

            It can be either full s3:// style url or relative path from root level.

            When it's specified as a full s3:// url, please omit source_bucket_name.
        :type source_bucket_key: str
        :param dest_bucket_key: The key of the object to copy to.

            The convention to specify `dest_bucket_key` is the same
            as `source_bucket_key`.
        :type dest_bucket_key: str
        :param source_bucket_name: Name of the S3 bucket where the source object is in.

            It should be omitted when `source_bucket_key` is provided as a full s3:// url.
        :type source_bucket_name: str
        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.

            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.
        :type dest_bucket_name: str
        :param source_version_id: Version ID of the source object (OPTIONAL)
        :type source_version_id: str",Creates,a,copy,of,an,object,that,is,already,stored,in,S3,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L459-L518,test,parsed_url,.,netloc,!=,'',:,raise,AirflowException,(,"'If dest_bucket_name is provided, '",+,'dest_bucket_key should be relative path ',+,"'from root level, rather than a full s3:// url'",),if,source_bucket_name,is,None,:,source_bucket_name,",",source_bucket_key,=,self,.,parse_s3_url,(,source_bucket_key,),else,:,parsed_url,=,urlparse,(,source_bucket_key,),if,parsed_url,.,scheme,!=,'',or,parsed_url,.,netloc,!=,'',:,raise,AirflowException,(,"'If source_bucket_name is provided, '",+,'source_bucket_key should be relative path ',+,"'from root level, rather than a full s3:// url'",),CopySource,=,{,'Bucket',:,source_bucket_name,",",'Key',:,source_bucket_key,",",'VersionId',:,source_version_id,},response,=,self,.,get_conn,(,),.,copy_object,(,Bucket,=,dest_bucket_name,",",Key,=,dest_bucket_key,",",CopySource,=,CopySource,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,CassandraToGoogleCloudStorageOperator._query_cassandra,"def _query_cassandra(self):
        """"""
        Queries cassandra and returns a cursor to the results.
        """"""
        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)
        session = self.hook.get_conn()
        cursor = session.execute(self.cql)
        return cursor",python,"def _query_cassandra(self):
        """"""
        Queries cassandra and returns a cursor to the results.
        """"""
        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)
        session = self.hook.get_conn()
        cursor = session.execute(self.cql)
        return cursor",def,_query_cassandra,(,self,),:,self,.,hook,=,CassandraHook,(,cassandra_conn_id,=,self,.,cassandra_conn_id,),session,=,self,.,hook,.,get_conn,(,),cursor,=,session,.,execute,(,self,.,cql,),return,cursor,,,,,,,,,,,,,,Queries cassandra and returns a cursor to the results.,Queries,cassandra,and,returns,a,cursor,to,the,results,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L147-L154,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,CassandraToGoogleCloudStorageOperator.convert_user_type,"def convert_user_type(cls, name, value):
        """"""
        Converts a user type to RECORD that contains n fields, where n is the
        number of attributes. Each element in the user type class will be converted to its
        corresponding data type in BQ.
        """"""
        names = value._fields
        values = [cls.convert_value(name, getattr(value, name)) for name in names]
        return cls.generate_data_dict(names, values)",python,"def convert_user_type(cls, name, value):
        """"""
        Converts a user type to RECORD that contains n fields, where n is the
        number of attributes. Each element in the user type class will be converted to its
        corresponding data type in BQ.
        """"""
        names = value._fields
        values = [cls.convert_value(name, getattr(value, name)) for name in names]
        return cls.generate_data_dict(names, values)",def,convert_user_type,(,cls,",",name,",",value,),:,names,=,value,.,_fields,values,=,[,cls,.,convert_value,(,name,",",getattr,(,value,",",name,),),for,name,in,names,],return,cls,.,generate_data_dict,(,names,",",values,),,,,,,,,"Converts a user type to RECORD that contains n fields, where n is the
        number of attributes. Each element in the user type class will be converted to its
        corresponding data type in BQ.",Converts,a,user,type,to,RECORD,that,contains,n,fields,where,n,is,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L247-L255,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,number,of,attributes,.,Each,element,in,the,user,type,class,will,be,converted,to,its,corresponding,data,type,in,BQ,.,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/utils/sendgrid.py,send_email,"def send_email(to, subject, html_content, files=None, dryrun=False, cc=None,
               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):
    """"""
    Send an email with html content using sendgrid.

    To use this plugin:
    0. include sendgrid subpackage as part of your Airflow installation, e.g.,
    pip install 'apache-airflow[sendgrid]'
    1. update [email] backend in airflow.cfg, i.e.,
    [email]
    email_backend = airflow.contrib.utils.sendgrid.send_email
    2. configure Sendgrid specific environment variables at all Airflow instances:
    SENDGRID_MAIL_FROM={your-mail-from}
    SENDGRID_API_KEY={your-sendgrid-api-key}.
    """"""
    if files is None:
        files = []

    mail = Mail()
    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')
    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')
    mail.from_email = Email(from_email, from_name)
    mail.subject = subject
    mail.mail_settings = MailSettings()

    if sandbox_mode:
        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)

    # Add the recipient list of to emails.
    personalization = Personalization()
    to = get_email_address_list(to)
    for to_address in to:
        personalization.add_to(Email(to_address))
    if cc:
        cc = get_email_address_list(cc)
        for cc_address in cc:
            personalization.add_cc(Email(cc_address))
    if bcc:
        bcc = get_email_address_list(bcc)
        for bcc_address in bcc:
            personalization.add_bcc(Email(bcc_address))

    # Add custom_args to personalization if present
    pers_custom_args = kwargs.get('personalization_custom_args', None)
    if isinstance(pers_custom_args, dict):
        for key in pers_custom_args.keys():
            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))

    mail.add_personalization(personalization)
    mail.add_content(Content('text/html', html_content))

    categories = kwargs.get('categories', [])
    for cat in categories:
        mail.add_category(Category(cat))

    # Add email attachment.
    for fname in files:
        basename = os.path.basename(fname)

        attachment = Attachment()
        attachment.type = mimetypes.guess_type(basename)[0]
        attachment.filename = basename
        attachment.disposition = ""attachment""
        attachment.content_id = '<{0}>'.format(basename)

        with open(fname, ""rb"") as f:
            attachment.content = base64.b64encode(f.read()).decode('utf-8')

        mail.add_attachment(attachment)
    _post_sendgrid_mail(mail.get())",python,"def send_email(to, subject, html_content, files=None, dryrun=False, cc=None,
               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):
    """"""
    Send an email with html content using sendgrid.

    To use this plugin:
    0. include sendgrid subpackage as part of your Airflow installation, e.g.,
    pip install 'apache-airflow[sendgrid]'
    1. update [email] backend in airflow.cfg, i.e.,
    [email]
    email_backend = airflow.contrib.utils.sendgrid.send_email
    2. configure Sendgrid specific environment variables at all Airflow instances:
    SENDGRID_MAIL_FROM={your-mail-from}
    SENDGRID_API_KEY={your-sendgrid-api-key}.
    """"""
    if files is None:
        files = []

    mail = Mail()
    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')
    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')
    mail.from_email = Email(from_email, from_name)
    mail.subject = subject
    mail.mail_settings = MailSettings()

    if sandbox_mode:
        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)

    # Add the recipient list of to emails.
    personalization = Personalization()
    to = get_email_address_list(to)
    for to_address in to:
        personalization.add_to(Email(to_address))
    if cc:
        cc = get_email_address_list(cc)
        for cc_address in cc:
            personalization.add_cc(Email(cc_address))
    if bcc:
        bcc = get_email_address_list(bcc)
        for bcc_address in bcc:
            personalization.add_bcc(Email(bcc_address))

    # Add custom_args to personalization if present
    pers_custom_args = kwargs.get('personalization_custom_args', None)
    if isinstance(pers_custom_args, dict):
        for key in pers_custom_args.keys():
            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))

    mail.add_personalization(personalization)
    mail.add_content(Content('text/html', html_content))

    categories = kwargs.get('categories', [])
    for cat in categories:
        mail.add_category(Category(cat))

    # Add email attachment.
    for fname in files:
        basename = os.path.basename(fname)

        attachment = Attachment()
        attachment.type = mimetypes.guess_type(basename)[0]
        attachment.filename = basename
        attachment.disposition = ""attachment""
        attachment.content_id = '<{0}>'.format(basename)

        with open(fname, ""rb"") as f:
            attachment.content = base64.b64encode(f.read()).decode('utf-8')

        mail.add_attachment(attachment)
    _post_sendgrid_mail(mail.get())",def,send_email,(,to,",",subject,",",html_content,",",files,=,None,",",dryrun,=,False,",",cc,=,None,",",bcc,=,None,",",mime_subtype,=,'mixed',",",sandbox_mode,=,False,",",*,*,kwargs,),:,if,files,is,None,:,files,=,[,],mail,=,Mail,(,),"Send an email with html content using sendgrid.

    To use this plugin:
    0. include sendgrid subpackage as part of your Airflow installation, e.g.,
    pip install 'apache-airflow[sendgrid]'
    1. update [email] backend in airflow.cfg, i.e.,
    [email]
    email_backend = airflow.contrib.utils.sendgrid.send_email
    2. configure Sendgrid specific environment variables at all Airflow instances:
    SENDGRID_MAIL_FROM={your-mail-from}
    SENDGRID_API_KEY={your-sendgrid-api-key}.",Send,an,email,with,html,content,using,sendgrid,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/sendgrid.py#L33-L102,test,from_email,=,kwargs,.,get,(,'from_email',),or,os,.,environ,.,get,(,'SENDGRID_MAIL_FROM',),from_name,=,kwargs,.,get,(,'from_name',),or,os,.,environ,.,get,(,'SENDGRID_MAIL_SENDER',),mail,.,from_email,=,Email,(,from_email,",",from_name,),mail,.,subject,=,subject,mail,.,mail_settings,=,MailSettings,(,),if,sandbox_mode,:,mail,.,mail_settings,.,sandbox_mode,=,SandBoxMode,(,enable,=,True,),# Add the recipient list of to emails.,personalization,=,Personalization,(,),to,=,get_email_address_list,(,to,),for,to_address,in,to,:,personalization,.,add_to,(,Email,(,to_address,),),if,cc,:,cc,=,get_email_address_list,(,cc,),for,cc_address,in,cc,:,personalization,.,add_cc,(,Email,(,cc_address,),),if,bcc,:,bcc,=,get_email_address_list,(,bcc,),for,bcc_address,in,bcc,:,personalization,.,add_bcc,(,Email,(,bcc_address,),),# Add custom_args to personalization if present,pers_custom_args,=,kwargs,.,get,(,'personalization_custom_args',",",None,),if,isinstance,(,pers_custom_args,",",dict,),:,for,key,in,pers_custom_args,.,keys,(,),:,personalization,.,add_custom_arg,(,CustomArg,(,key,",",pers_custom_args,[,key,],),),mail,.,add_personalization,(,personalization,),mail,.,add_content,(,Content,(,'text/html',",",html_content,),),categories,=,kwargs,.,get,(,'categories',",",[,],),for,cat,in,categories,:,mail,.,add_category,(,Category,(,cat,),),# Add email attachment.,for,fname,in,files,:,basename,=,os,.,path,.,basename,(,fname,),attachment,=,Attachment,(,),attachment,.,type,=,mimetypes,.,guess_type,(,basename,),[,0,],attachment,.,filename,=,basename,attachment,.,disposition,=,"""attachment""",attachment,.,content_id,=,'<{0}>',.,format,(,basename,),with,open,(,fname,",","""rb""",),as,f,:,attachment,.,content,=,base64,.,b64encode,(,f,.,read,(,),),.,decode,(,'utf-8',),mail,.,add_attachment,(,attachment,),_post_sendgrid_mail,(,mail,.,get,(,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_speech_to_text_hook.py,GCPSpeechToTextHook.get_conn,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Speech.

        :return: Google Cloud Speech client object.
        :rtype: google.cloud.speech_v1.SpeechClient
        """"""
        if not self._client:
            self._client = SpeechClient(credentials=self._get_credentials())
        return self._client",python,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Speech.

        :return: Google Cloud Speech client object.
        :rtype: google.cloud.speech_v1.SpeechClient
        """"""
        if not self._client:
            self._client = SpeechClient(credentials=self._get_credentials())
        return self._client",def,get_conn,(,self,),:,if,not,self,.,_client,:,self,.,_client,=,SpeechClient,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_client,,,,,,,,,,,,,,,,,,,,,,,"Retrieves connection to Cloud Speech.

        :return: Google Cloud Speech client object.
        :rtype: google.cloud.speech_v1.SpeechClient",Retrieves,connection,to,Cloud,Speech,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_speech_to_text_hook.py#L42-L51,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_speech_to_text_hook.py,GCPSpeechToTextHook.recognize_speech,"def recognize_speech(self, config, audio, retry=None, timeout=None):
        """"""
        Recognizes audio input

        :param config: information to the recognizer that specifies how to process the request.
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig
        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig
        :param audio: audio data to be recognized
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio
        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        """"""
        client = self.get_conn()
        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)
        self.log.info(""Recognised speech: %s"" % response)
        return response",python,"def recognize_speech(self, config, audio, retry=None, timeout=None):
        """"""
        Recognizes audio input

        :param config: information to the recognizer that specifies how to process the request.
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig
        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig
        :param audio: audio data to be recognized
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio
        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        """"""
        client = self.get_conn()
        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)
        self.log.info(""Recognised speech: %s"" % response)
        return response",def,recognize_speech,(,self,",",config,",",audio,",",retry,=,None,",",timeout,=,None,),:,client,=,self,.,get_conn,(,),response,=,client,.,recognize,(,config,=,config,",",audio,=,audio,",",retry,=,retry,",",timeout,=,timeout,),self,.,log,.,info,"Recognizes audio input

        :param config: information to the recognizer that specifies how to process the request.
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig
        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig
        :param audio: audio data to be recognized
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio
        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float",Recognizes,audio,input,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_speech_to_text_hook.py#L53-L73,test,(,"""Recognised speech: %s""",%,response,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/spark_sql_operator.py,SparkSqlOperator.execute,"def execute(self, context):
        """"""
        Call the SparkSqlHook to run the provided sql query
        """"""
        self._hook = SparkSqlHook(sql=self._sql,
                                  conf=self._conf,
                                  conn_id=self._conn_id,
                                  total_executor_cores=self._total_executor_cores,
                                  executor_cores=self._executor_cores,
                                  executor_memory=self._executor_memory,
                                  keytab=self._keytab,
                                  principal=self._principal,
                                  name=self._name,
                                  num_executors=self._num_executors,
                                  master=self._master,
                                  yarn_queue=self._yarn_queue
                                  )
        self._hook.run_query()",python,"def execute(self, context):
        """"""
        Call the SparkSqlHook to run the provided sql query
        """"""
        self._hook = SparkSqlHook(sql=self._sql,
                                  conf=self._conf,
                                  conn_id=self._conn_id,
                                  total_executor_cores=self._total_executor_cores,
                                  executor_cores=self._executor_cores,
                                  executor_memory=self._executor_memory,
                                  keytab=self._keytab,
                                  principal=self._principal,
                                  name=self._name,
                                  num_executors=self._num_executors,
                                  master=self._master,
                                  yarn_queue=self._yarn_queue
                                  )
        self._hook.run_query()",def,execute,(,self,",",context,),:,self,.,_hook,=,SparkSqlHook,(,sql,=,self,.,_sql,",",conf,=,self,.,_conf,",",conn_id,=,self,.,_conn_id,",",total_executor_cores,=,self,.,_total_executor_cores,",",executor_cores,=,self,.,_executor_cores,",",executor_memory,=,self,.,_executor_memory,",",keytab,=,Call the SparkSqlHook to run the provided sql query,Call,the,SparkSqlHook,to,run,the,provided,sql,query,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/spark_sql_operator.py#L91-L108,test,self,.,_keytab,",",principal,=,self,.,_principal,",",name,=,self,.,_name,",",num_executors,=,self,.,_num_executors,",",master,=,self,.,_master,",",yarn_queue,=,self,.,_yarn_queue,),self,.,_hook,.,run_query,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/plugins_manager.py,load_entrypoint_plugins,"def load_entrypoint_plugins(entry_points, airflow_plugins):
    """"""
    Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]
    :rtype: list[airflow.plugins_manager.AirflowPlugin]
    """"""
    for entry_point in entry_points:
        log.debug('Importing entry_point plugin %s', entry_point.name)
        plugin_obj = entry_point.load()
        if is_valid_plugin(plugin_obj, airflow_plugins):
            if callable(getattr(plugin_obj, 'on_load', None)):
                plugin_obj.on_load()
                airflow_plugins.append(plugin_obj)
    return airflow_plugins",python,"def load_entrypoint_plugins(entry_points, airflow_plugins):
    """"""
    Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]
    :rtype: list[airflow.plugins_manager.AirflowPlugin]
    """"""
    for entry_point in entry_points:
        log.debug('Importing entry_point plugin %s', entry_point.name)
        plugin_obj = entry_point.load()
        if is_valid_plugin(plugin_obj, airflow_plugins):
            if callable(getattr(plugin_obj, 'on_load', None)):
                plugin_obj.on_load()
                airflow_plugins.append(plugin_obj)
    return airflow_plugins",def,load_entrypoint_plugins,(,entry_points,",",airflow_plugins,),:,for,entry_point,in,entry_points,:,log,.,debug,(,'Importing entry_point plugin %s',",",entry_point,.,name,),plugin_obj,=,entry_point,.,load,(,),if,is_valid_plugin,(,plugin_obj,",",airflow_plugins,),:,if,callable,(,getattr,(,plugin_obj,",",'on_load',",",None,),),:,plugin_obj,"Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]
    :rtype: list[airflow.plugins_manager.AirflowPlugin]",Load,AirflowPlugin,subclasses,from,the,entrypoints,provided,.,The,entry_point,group,should,be,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/plugins_manager.py#L79-L98,test,.,on_load,(,),airflow_plugins,.,append,(,plugin_obj,),return,airflow_plugins,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,airflow,.,plugins,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/plugins_manager.py,is_valid_plugin,"def is_valid_plugin(plugin_obj, existing_plugins):
    """"""
    Check whether a potential object is a subclass of
    the AirflowPlugin class.

    :param plugin_obj: potential subclass of AirflowPlugin
    :param existing_plugins: Existing list of AirflowPlugin subclasses
    :return: Whether or not the obj is a valid subclass of
        AirflowPlugin
    """"""
    if (
        inspect.isclass(plugin_obj) and
        issubclass(plugin_obj, AirflowPlugin) and
        (plugin_obj is not AirflowPlugin)
    ):
        plugin_obj.validate()
        return plugin_obj not in existing_plugins
    return False",python,"def is_valid_plugin(plugin_obj, existing_plugins):
    """"""
    Check whether a potential object is a subclass of
    the AirflowPlugin class.

    :param plugin_obj: potential subclass of AirflowPlugin
    :param existing_plugins: Existing list of AirflowPlugin subclasses
    :return: Whether or not the obj is a valid subclass of
        AirflowPlugin
    """"""
    if (
        inspect.isclass(plugin_obj) and
        issubclass(plugin_obj, AirflowPlugin) and
        (plugin_obj is not AirflowPlugin)
    ):
        plugin_obj.validate()
        return plugin_obj not in existing_plugins
    return False",def,is_valid_plugin,(,plugin_obj,",",existing_plugins,),:,if,(,inspect,.,isclass,(,plugin_obj,),and,issubclass,(,plugin_obj,",",AirflowPlugin,),and,(,plugin_obj,is,not,AirflowPlugin,),),:,plugin_obj,.,validate,(,),return,plugin_obj,not,in,existing_plugins,return,False,,,,,,,,,"Check whether a potential object is a subclass of
    the AirflowPlugin class.

    :param plugin_obj: potential subclass of AirflowPlugin
    :param existing_plugins: Existing list of AirflowPlugin subclasses
    :return: Whether or not the obj is a valid subclass of
        AirflowPlugin",Check,whether,a,potential,object,is,a,subclass,of,the,AirflowPlugin,class,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/plugins_manager.py#L101-L118,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/skipmixin.py,SkipMixin.skip,"def skip(self, dag_run, execution_date, tasks, session=None):
        """"""
        Sets tasks instances to skipped from the same dag run.

        :param dag_run: the DagRun for which to set the tasks to skipped
        :param execution_date: execution_date
        :param tasks: tasks to skip (not task_ids)
        :param session: db session to use
        """"""
        if not tasks:
            return

        task_ids = [d.task_id for d in tasks]
        now = timezone.utcnow()

        if dag_run:
            session.query(TaskInstance).filter(
                TaskInstance.dag_id == dag_run.dag_id,
                TaskInstance.execution_date == dag_run.execution_date,
                TaskInstance.task_id.in_(task_ids)
            ).update({TaskInstance.state: State.SKIPPED,
                      TaskInstance.start_date: now,
                      TaskInstance.end_date: now},
                     synchronize_session=False)
            session.commit()
        else:
            assert execution_date is not None, ""Execution date is None and no dag run""

            self.log.warning(""No DAG RUN present this should not happen"")
            # this is defensive against dag runs that are not complete
            for task in tasks:
                ti = TaskInstance(task, execution_date=execution_date)
                ti.state = State.SKIPPED
                ti.start_date = now
                ti.end_date = now
                session.merge(ti)

            session.commit()",python,"def skip(self, dag_run, execution_date, tasks, session=None):
        """"""
        Sets tasks instances to skipped from the same dag run.

        :param dag_run: the DagRun for which to set the tasks to skipped
        :param execution_date: execution_date
        :param tasks: tasks to skip (not task_ids)
        :param session: db session to use
        """"""
        if not tasks:
            return

        task_ids = [d.task_id for d in tasks]
        now = timezone.utcnow()

        if dag_run:
            session.query(TaskInstance).filter(
                TaskInstance.dag_id == dag_run.dag_id,
                TaskInstance.execution_date == dag_run.execution_date,
                TaskInstance.task_id.in_(task_ids)
            ).update({TaskInstance.state: State.SKIPPED,
                      TaskInstance.start_date: now,
                      TaskInstance.end_date: now},
                     synchronize_session=False)
            session.commit()
        else:
            assert execution_date is not None, ""Execution date is None and no dag run""

            self.log.warning(""No DAG RUN present this should not happen"")
            # this is defensive against dag runs that are not complete
            for task in tasks:
                ti = TaskInstance(task, execution_date=execution_date)
                ti.state = State.SKIPPED
                ti.start_date = now
                ti.end_date = now
                session.merge(ti)

            session.commit()",def,skip,(,self,",",dag_run,",",execution_date,",",tasks,",",session,=,None,),:,if,not,tasks,:,return,task_ids,=,[,d,.,task_id,for,d,in,tasks,],now,=,timezone,.,utcnow,(,),if,dag_run,:,session,.,query,(,TaskInstance,),.,filter,(,TaskInstance,"Sets tasks instances to skipped from the same dag run.

        :param dag_run: the DagRun for which to set the tasks to skipped
        :param execution_date: execution_date
        :param tasks: tasks to skip (not task_ids)
        :param session: db session to use",Sets,tasks,instances,to,skipped,from,the,same,dag,run,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/skipmixin.py#L29-L66,test,.,dag_id,==,dag_run,.,dag_id,",",TaskInstance,.,execution_date,==,dag_run,.,execution_date,",",TaskInstance,.,task_id,.,in_,(,task_ids,),),.,update,(,{,TaskInstance,.,state,:,State,.,SKIPPED,",",TaskInstance,.,start_date,:,now,",",TaskInstance,.,end_date,:,now,},",",synchronize_session,=,False,),session,.,commit,(,),else,:,assert,execution_date,is,not,None,",","""Execution date is None and no dag run""",self,.,log,.,warning,(,"""No DAG RUN present this should not happen""",),# this is defensive against dag runs that are not complete,for,task,in,tasks,:,ti,=,TaskInstance,(,task,",",execution_date,=,execution_date,),ti,.,state,=,State,.,SKIPPED,ti,.,start_date,=,now,ti,.,end_date,=,now,session,.,merge,(,ti,),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,AzureDataLakeHook.get_conn,"def get_conn(self):
        """"""Return a AzureDLFileSystem object.""""""
        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        self.account_name = service_options.get('account_name')

        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),
                            client_secret=conn.password,
                            client_id=conn.login)
        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,
                                                      store_name=self.account_name)
        adlsFileSystemClient.connect()
        return adlsFileSystemClient",python,"def get_conn(self):
        """"""Return a AzureDLFileSystem object.""""""
        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        self.account_name = service_options.get('account_name')

        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),
                            client_secret=conn.password,
                            client_id=conn.login)
        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,
                                                      store_name=self.account_name)
        adlsFileSystemClient.connect()
        return adlsFileSystemClient",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,conn_id,),service_options,=,conn,.,extra_dejson,self,.,account_name,=,service_options,.,get,(,'account_name',),adlCreds,=,lib,.,auth,(,tenant_id,=,service_options,.,get,(,'tenant',),",",client_secret,=,conn,.,password,",",Return a AzureDLFileSystem object.,Return,a,AzureDLFileSystem,object,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L41-L53,test,client_id,=,conn,.,login,),adlsFileSystemClient,=,core,.,AzureDLFileSystem,(,adlCreds,",",store_name,=,self,.,account_name,),adlsFileSystemClient,.,connect,(,),return,adlsFileSystemClient,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,AzureDataLakeHook.check_for_file,"def check_for_file(self, file_path):
        """"""
        Check if a file exists on Azure Data Lake.

        :param file_path: Path and name of the file.
        :type file_path: str
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        try:
            files = self.connection.glob(file_path, details=False, invalidate_cache=True)
            return len(files) == 1
        except FileNotFoundError:
            return False",python,"def check_for_file(self, file_path):
        """"""
        Check if a file exists on Azure Data Lake.

        :param file_path: Path and name of the file.
        :type file_path: str
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        try:
            files = self.connection.glob(file_path, details=False, invalidate_cache=True)
            return len(files) == 1
        except FileNotFoundError:
            return False",def,check_for_file,(,self,",",file_path,),:,try,:,files,=,self,.,connection,.,glob,(,file_path,",",details,=,False,",",invalidate_cache,=,True,),return,len,(,files,),==,1,except,FileNotFoundError,:,return,False,,,,,,,,,,,,,"Check if a file exists on Azure Data Lake.

        :param file_path: Path and name of the file.
        :type file_path: str
        :return: True if the file exists, False otherwise.
        :rtype: bool",Check,if,a,file,exists,on,Azure,Data,Lake,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L55-L68,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,AzureDataLakeHook.upload_file,"def upload_file(self, local_path, remote_path, nthreads=64, overwrite=True,
                    buffersize=4194304, blocksize=4194304):
        """"""
        Upload a file to Azure Data Lake.

        :param local_path: local path. Can be single file, directory (in which case,
            upload recursively) or glob pattern. Recursive glob patterns using `**`
            are not supported.
        :type local_path: str
        :param remote_path: Remote path to upload to; if multiple files, this is the
            directory root to write within.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int
        """"""
        multithread.ADLUploader(self.connection,
                                lpath=local_path,
                                rpath=remote_path,
                                nthreads=nthreads,
                                overwrite=overwrite,
                                buffersize=buffersize,
                                blocksize=blocksize)",python,"def upload_file(self, local_path, remote_path, nthreads=64, overwrite=True,
                    buffersize=4194304, blocksize=4194304):
        """"""
        Upload a file to Azure Data Lake.

        :param local_path: local path. Can be single file, directory (in which case,
            upload recursively) or glob pattern. Recursive glob patterns using `**`
            are not supported.
        :type local_path: str
        :param remote_path: Remote path to upload to; if multiple files, this is the
            directory root to write within.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int
        """"""
        multithread.ADLUploader(self.connection,
                                lpath=local_path,
                                rpath=remote_path,
                                nthreads=nthreads,
                                overwrite=overwrite,
                                buffersize=buffersize,
                                blocksize=blocksize)",def,upload_file,(,self,",",local_path,",",remote_path,",",nthreads,=,64,",",overwrite,=,True,",",buffersize,=,4194304,",",blocksize,=,4194304,),:,multithread,.,ADLUploader,(,self,.,connection,",",lpath,=,local_path,",",rpath,=,remote_path,",",nthreads,=,nthreads,",",overwrite,=,overwrite,",",buffersize,=,"Upload a file to Azure Data Lake.

        :param local_path: local path. Can be single file, directory (in which case,
            upload recursively) or glob pattern. Recursive glob patterns using `**`
            are not supported.
        :type local_path: str
        :param remote_path: Remote path to upload to; if multiple files, this is the
            directory root to write within.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int",Upload,a,file,to,Azure,Data,Lake,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L70-L104,test,buffersize,",",blocksize,=,blocksize,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,AzureDataLakeHook.list,"def list(self, path):
        """"""
        List files in Azure Data Lake Storage

        :param path: full path/globstring to use to list files in ADLS
        :type path: str
        """"""
        if ""*"" in path:
            return self.connection.glob(path)
        else:
            return self.connection.walk(path)",python,"def list(self, path):
        """"""
        List files in Azure Data Lake Storage

        :param path: full path/globstring to use to list files in ADLS
        :type path: str
        """"""
        if ""*"" in path:
            return self.connection.glob(path)
        else:
            return self.connection.walk(path)",def,list,(,self,",",path,),:,if,"""*""",in,path,:,return,self,.,connection,.,glob,(,path,),else,:,return,self,.,connection,.,walk,(,path,),,,,,,,,,,,,,,,,,,,,"List files in Azure Data Lake Storage

        :param path: full path/globstring to use to list files in ADLS
        :type path: str",List,files,in,Azure,Data,Lake,Storage,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L143-L153,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/aws_athena_operator.py,AWSAthenaOperator.execute,"def execute(self, context):
        """"""
        Run Presto Query on Athena
        """"""
        self.hook = self.get_hook()
        self.hook.get_conn()

        self.query_execution_context['Database'] = self.database
        self.result_configuration['OutputLocation'] = self.output_location
        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,
                                                      self.result_configuration, self.client_request_token)
        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)

        if query_status in AWSAthenaHook.FAILURE_STATES:
            raise Exception(
                'Final state of Athena job is {}, query_execution_id is {}.'
                .format(query_status, self.query_execution_id))
        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:
            raise Exception(
                'Final state of Athena job is {}. '
                'Max tries of poll status exceeded, query_execution_id is {}.'
                .format(query_status, self.query_execution_id))",python,"def execute(self, context):
        """"""
        Run Presto Query on Athena
        """"""
        self.hook = self.get_hook()
        self.hook.get_conn()

        self.query_execution_context['Database'] = self.database
        self.result_configuration['OutputLocation'] = self.output_location
        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,
                                                      self.result_configuration, self.client_request_token)
        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)

        if query_status in AWSAthenaHook.FAILURE_STATES:
            raise Exception(
                'Final state of Athena job is {}, query_execution_id is {}.'
                .format(query_status, self.query_execution_id))
        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:
            raise Exception(
                'Final state of Athena job is {}. '
                'Max tries of poll status exceeded, query_execution_id is {}.'
                .format(query_status, self.query_execution_id))",def,execute,(,self,",",context,),:,self,.,hook,=,self,.,get_hook,(,),self,.,hook,.,get_conn,(,),self,.,query_execution_context,[,'Database',],=,self,.,database,self,.,result_configuration,[,'OutputLocation',],=,self,.,output_location,self,.,query_execution_id,=,self,.,hook,.,Run Presto Query on Athena,Run,Presto,Query,on,Athena,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/aws_athena_operator.py#L70-L91,test,run_query,(,self,.,query,",",self,.,query_execution_context,",",self,.,result_configuration,",",self,.,client_request_token,),query_status,=,self,.,hook,.,poll_query_status,(,self,.,query_execution_id,",",self,.,max_tries,),if,query_status,in,AWSAthenaHook,.,FAILURE_STATES,:,raise,Exception,(,"'Final state of Athena job is {}, query_execution_id is {}.'",.,format,(,query_status,",",self,.,query_execution_id,),),elif,not,query_status,or,query_status,in,AWSAthenaHook,.,INTERMEDIATE_STATES,:,raise,Exception,(,'Final state of Athena job is {}. ',"'Max tries of poll status exceeded, query_execution_id is {}.'",.,format,(,query_status,",",self,.,query_execution_id,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/compression.py,uncompress_file,"def uncompress_file(input_file_name, file_extension, dest_dir):
    """"""
    Uncompress gz and bz2 files
    """"""
    if file_extension.lower() not in ('.gz', '.bz2'):
        raise NotImplementedError(""Received {} format. Only gz and bz2 ""
                                  ""files can currently be uncompressed.""
                                  .format(file_extension))
    if file_extension.lower() == '.gz':
        fmodule = gzip.GzipFile
    elif file_extension.lower() == '.bz2':
        fmodule = bz2.BZ2File
    with fmodule(input_file_name, mode='rb') as f_compressed,\
        NamedTemporaryFile(dir=dest_dir,
                           mode='wb',
                           delete=False) as f_uncompressed:
        shutil.copyfileobj(f_compressed, f_uncompressed)
    return f_uncompressed.name",python,"def uncompress_file(input_file_name, file_extension, dest_dir):
    """"""
    Uncompress gz and bz2 files
    """"""
    if file_extension.lower() not in ('.gz', '.bz2'):
        raise NotImplementedError(""Received {} format. Only gz and bz2 ""
                                  ""files can currently be uncompressed.""
                                  .format(file_extension))
    if file_extension.lower() == '.gz':
        fmodule = gzip.GzipFile
    elif file_extension.lower() == '.bz2':
        fmodule = bz2.BZ2File
    with fmodule(input_file_name, mode='rb') as f_compressed,\
        NamedTemporaryFile(dir=dest_dir,
                           mode='wb',
                           delete=False) as f_uncompressed:
        shutil.copyfileobj(f_compressed, f_uncompressed)
    return f_uncompressed.name",def,uncompress_file,(,input_file_name,",",file_extension,",",dest_dir,),:,if,file_extension,.,lower,(,),not,in,(,'.gz',",",'.bz2',),:,raise,NotImplementedError,(,"""Received {} format. Only gz and bz2 ""","""files can currently be uncompressed.""",.,format,(,file_extension,),),if,file_extension,.,lower,(,),==,'.gz',:,fmodule,=,gzip,.,GzipFile,elif,file_extension,.,Uncompress gz and bz2 files,Uncompress,gz,and,bz2,files,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/compression.py#L26-L43,test,lower,(,),==,'.bz2',:,fmodule,=,bz2,.,BZ2File,with,fmodule,(,input_file_name,",",mode,=,'rb',),as,f_compressed,",",NamedTemporaryFile,(,dir,=,dest_dir,",",mode,=,'wb',",",delete,=,False,),as,f_uncompressed,:,shutil,.,copyfileobj,(,f_compressed,",",f_uncompressed,),return,f_uncompressed,.,name,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mssql_to_gcs.py,MsSqlToGoogleCloudStorageOperator._query_mssql,"def _query_mssql(self):
        """"""
        Queries MSSQL and returns a cursor of results.

        :return: mssql cursor
        """"""
        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)
        conn = mssql.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql)
        return cursor",python,"def _query_mssql(self):
        """"""
        Queries MSSQL and returns a cursor of results.

        :return: mssql cursor
        """"""
        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)
        conn = mssql.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql)
        return cursor",def,_query_mssql,(,self,),:,mssql,=,MsSqlHook,(,mssql_conn_id,=,self,.,mssql_conn_id,),conn,=,mssql,.,get_conn,(,),cursor,=,conn,.,cursor,(,),cursor,.,execute,(,self,.,sql,),return,cursor,,,,,,,,,,,,,"Queries MSSQL and returns a cursor of results.

        :return: mssql cursor",Queries,MSSQL,and,returns,a,cursor,of,results,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mssql_to_gcs.py#L127-L137,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/cli.py,action_logging,"def action_logging(f):
    """"""
    Decorates function to execute function at the same time submitting action_logging
    but in CLI context. It will call action logger callbacks twice,
    one for pre-execution and the other one for post-execution.

    Action logger will be called with below keyword parameters:
        sub_command : name of sub-command
        start_datetime : start datetime instance by utc
        end_datetime : end datetime instance by utc
        full_command : full command line arguments
        user : current user
        log : airflow.models.log.Log ORM instance
        dag_id : dag id (optional)
        task_id : task_id (optional)
        execution_date : execution date (optional)
        error : exception instance if there's an exception

    :param f: function instance
    :return: wrapped function
    """"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        """"""
        An wrapper for cli functions. It assumes to have Namespace instance
        at 1st positional argument
        :param args: Positional argument. It assumes to have Namespace instance
        at 1st positional argument
        :param kwargs: A passthrough keyword argument
        """"""
        assert args
        assert isinstance(args[0], Namespace), \
            ""1st positional argument should be argparse.Namespace instance, "" \
            ""but {}"".format(args[0])
        metrics = _build_metrics(f.__name__, args[0])
        cli_action_loggers.on_pre_execution(**metrics)
        try:
            return f(*args, **kwargs)
        except Exception as e:
            metrics['error'] = e
            raise
        finally:
            metrics['end_datetime'] = datetime.utcnow()
            cli_action_loggers.on_post_execution(**metrics)

    return wrapper",python,"def action_logging(f):
    """"""
    Decorates function to execute function at the same time submitting action_logging
    but in CLI context. It will call action logger callbacks twice,
    one for pre-execution and the other one for post-execution.

    Action logger will be called with below keyword parameters:
        sub_command : name of sub-command
        start_datetime : start datetime instance by utc
        end_datetime : end datetime instance by utc
        full_command : full command line arguments
        user : current user
        log : airflow.models.log.Log ORM instance
        dag_id : dag id (optional)
        task_id : task_id (optional)
        execution_date : execution date (optional)
        error : exception instance if there's an exception

    :param f: function instance
    :return: wrapped function
    """"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        """"""
        An wrapper for cli functions. It assumes to have Namespace instance
        at 1st positional argument
        :param args: Positional argument. It assumes to have Namespace instance
        at 1st positional argument
        :param kwargs: A passthrough keyword argument
        """"""
        assert args
        assert isinstance(args[0], Namespace), \
            ""1st positional argument should be argparse.Namespace instance, "" \
            ""but {}"".format(args[0])
        metrics = _build_metrics(f.__name__, args[0])
        cli_action_loggers.on_pre_execution(**metrics)
        try:
            return f(*args, **kwargs)
        except Exception as e:
            metrics['error'] = e
            raise
        finally:
            metrics['end_datetime'] = datetime.utcnow()
            cli_action_loggers.on_post_execution(**metrics)

    return wrapper",def,action_logging,(,f,),:,@,functools,.,wraps,(,f,),def,wrapper,(,*,args,",",*,*,kwargs,),:,"""""""
        An wrapper for cli functions. It assumes to have Namespace instance
        at 1st positional argument
        :param args: Positional argument. It assumes to have Namespace instance
        at 1st positional argument
        :param kwargs: A passthrough keyword argument
        """"""",assert,args,assert,isinstance,(,args,[,0,],",",Namespace,),",","""1st positional argument should be argparse.Namespace instance, ""","""but {}""",.,format,(,args,[,0,],),metrics,=,_build_metrics,(,"Decorates function to execute function at the same time submitting action_logging
    but in CLI context. It will call action logger callbacks twice,
    one for pre-execution and the other one for post-execution.

    Action logger will be called with below keyword parameters:
        sub_command : name of sub-command
        start_datetime : start datetime instance by utc
        end_datetime : end datetime instance by utc
        full_command : full command line arguments
        user : current user
        log : airflow.models.log.Log ORM instance
        dag_id : dag id (optional)
        task_id : task_id (optional)
        execution_date : execution date (optional)
        error : exception instance if there's an exception

    :param f: function instance
    :return: wrapped function",Decorates,function,to,execute,function,at,the,same,time,submitting,action_logging,but,in,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/cli.py#L36-L81,test,f,.,__name__,",",args,[,0,],),cli_action_loggers,.,on_pre_execution,(,*,*,metrics,),try,:,return,f,(,*,args,",",*,*,kwargs,),except,Exception,as,e,:,metrics,[,'error',],=,e,raise,finally,:,metrics,[,'end_datetime',],=,datetime,.,utcnow,(,),cli_action_loggers,.,on_post_execution,(,*,*,metrics,),return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CLI,context,.,It,will,call,action,logger,callbacks,twice,one,for,pre,-,execution,and,the,other,one,for,post,-,execution,.,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/cli.py,_build_metrics,"def _build_metrics(func_name, namespace):
    """"""
    Builds metrics dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has Namespace instance where it optionally contains ""dag_id"", ""task_id"",
    and ""execution_date"".

    :param func_name: name of function
    :param namespace: Namespace instance from argparse
    :return: dict with metrics
    """"""

    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),
               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}

    assert isinstance(namespace, Namespace)
    tmp_dic = vars(namespace)
    metrics['dag_id'] = tmp_dic.get('dag_id')
    metrics['task_id'] = tmp_dic.get('task_id')
    metrics['execution_date'] = tmp_dic.get('execution_date')
    metrics['host_name'] = socket.gethostname()

    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))
    log = Log(
        event='cli_{}'.format(func_name),
        task_instance=None,
        owner=metrics['user'],
        extra=extra,
        task_id=metrics.get('task_id'),
        dag_id=metrics.get('dag_id'),
        execution_date=metrics.get('execution_date'))
    metrics['log'] = log
    return metrics",python,"def _build_metrics(func_name, namespace):
    """"""
    Builds metrics dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has Namespace instance where it optionally contains ""dag_id"", ""task_id"",
    and ""execution_date"".

    :param func_name: name of function
    :param namespace: Namespace instance from argparse
    :return: dict with metrics
    """"""

    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),
               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}

    assert isinstance(namespace, Namespace)
    tmp_dic = vars(namespace)
    metrics['dag_id'] = tmp_dic.get('dag_id')
    metrics['task_id'] = tmp_dic.get('task_id')
    metrics['execution_date'] = tmp_dic.get('execution_date')
    metrics['host_name'] = socket.gethostname()

    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))
    log = Log(
        event='cli_{}'.format(func_name),
        task_instance=None,
        owner=metrics['user'],
        extra=extra,
        task_id=metrics.get('task_id'),
        dag_id=metrics.get('dag_id'),
        execution_date=metrics.get('execution_date'))
    metrics['log'] = log
    return metrics",def,_build_metrics,(,func_name,",",namespace,),:,metrics,=,{,'sub_command',:,func_name,",",'start_datetime',:,datetime,.,utcnow,(,),",",'full_command',:,'{}',.,format,(,list,(,sys,.,argv,),),",",'user',:,getpass,.,getuser,(,),},assert,isinstance,(,namespace,",",Namespace,),"Builds metrics dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has Namespace instance where it optionally contains ""dag_id"", ""task_id"",
    and ""execution_date"".

    :param func_name: name of function
    :param namespace: Namespace instance from argparse
    :return: dict with metrics",Builds,metrics,dict,from,function,args,It,assumes,that,function,arguments,is,from,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/cli.py#L84-L116,test,tmp_dic,=,vars,(,namespace,),metrics,[,'dag_id',],=,tmp_dic,.,get,(,'dag_id',),metrics,[,'task_id',],=,tmp_dic,.,get,(,'task_id',),metrics,[,'execution_date',],=,tmp_dic,.,get,(,'execution_date',),metrics,[,'host_name',],=,socket,.,gethostname,(,),extra,=,json,.,dumps,(,dict,(,(,k,",",metrics,[,k,],),for,k,in,(,'host_name',",",'full_command',),),),log,=,Log,(,event,=,'cli_{}',.,format,(,func_name,),",",task_instance,=,None,",",owner,=,metrics,[,'user',],",",extra,=,extra,",",task_id,=,metrics,.,get,(,'task_id',),",",dag_id,=,metrics,.,get,(,'dag_id',),",",execution_date,=,metrics,.,get,(,'execution_date',),),metrics,[,'log',],=,log,return,metrics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,airflow,.,bin,.,cli,module,s,function,and,has,Namespace,instance,where,it,optionally,contains,dag_id,task_id,and,execution_date,.,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/task_runner/cgroup_task_runner.py,CgroupTaskRunner._create_cgroup,"def _create_cgroup(self, path):
        """"""
        Create the specified cgroup.

        :param path: The path of the cgroup to create.
        E.g. cpu/mygroup/mysubgroup
        :return: the Node associated with the created cgroup.
        :rtype: cgroupspy.nodes.Node
        """"""
        node = trees.Tree().root
        path_split = path.split(os.sep)
        for path_element in path_split:
            name_to_node = {x.name: x for x in node.children}
            if path_element not in name_to_node:
                self.log.debug(""Creating cgroup %s in %s"", path_element, node.path)
                node = node.create_cgroup(path_element)
            else:
                self.log.debug(
                    ""Not creating cgroup %s in %s since it already exists"",
                    path_element, node.path
                )
                node = name_to_node[path_element]
        return node",python,"def _create_cgroup(self, path):
        """"""
        Create the specified cgroup.

        :param path: The path of the cgroup to create.
        E.g. cpu/mygroup/mysubgroup
        :return: the Node associated with the created cgroup.
        :rtype: cgroupspy.nodes.Node
        """"""
        node = trees.Tree().root
        path_split = path.split(os.sep)
        for path_element in path_split:
            name_to_node = {x.name: x for x in node.children}
            if path_element not in name_to_node:
                self.log.debug(""Creating cgroup %s in %s"", path_element, node.path)
                node = node.create_cgroup(path_element)
            else:
                self.log.debug(
                    ""Not creating cgroup %s in %s since it already exists"",
                    path_element, node.path
                )
                node = name_to_node[path_element]
        return node",def,_create_cgroup,(,self,",",path,),:,node,=,trees,.,Tree,(,),.,root,path_split,=,path,.,split,(,os,.,sep,),for,path_element,in,path_split,:,name_to_node,=,{,x,.,name,:,x,for,x,in,node,.,children,},if,path_element,not,in,name_to_node,"Create the specified cgroup.

        :param path: The path of the cgroup to create.
        E.g. cpu/mygroup/mysubgroup
        :return: the Node associated with the created cgroup.
        :rtype: cgroupspy.nodes.Node",Create,the,specified,cgroup,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/task_runner/cgroup_task_runner.py#L66-L88,test,:,self,.,log,.,debug,(,"""Creating cgroup %s in %s""",",",path_element,",",node,.,path,),node,=,node,.,create_cgroup,(,path_element,),else,:,self,.,log,.,debug,(,"""Not creating cgroup %s in %s since it already exists""",",",path_element,",",node,.,path,),node,=,name_to_node,[,path_element,],return,node,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/task_runner/cgroup_task_runner.py,CgroupTaskRunner._delete_cgroup,"def _delete_cgroup(self, path):
        """"""
        Delete the specified cgroup.

        :param path: The path of the cgroup to delete.
        E.g. cpu/mygroup/mysubgroup
        """"""
        node = trees.Tree().root
        path_split = path.split(""/"")
        for path_element in path_split:
            name_to_node = {x.name: x for x in node.children}
            if path_element not in name_to_node:
                self.log.warning(""Cgroup does not exist: %s"", path)
                return
            else:
                node = name_to_node[path_element]
        # node is now the leaf node
        parent = node.parent
        self.log.debug(""Deleting cgroup %s/%s"", parent, node.name)
        parent.delete_cgroup(node.name)",python,"def _delete_cgroup(self, path):
        """"""
        Delete the specified cgroup.

        :param path: The path of the cgroup to delete.
        E.g. cpu/mygroup/mysubgroup
        """"""
        node = trees.Tree().root
        path_split = path.split(""/"")
        for path_element in path_split:
            name_to_node = {x.name: x for x in node.children}
            if path_element not in name_to_node:
                self.log.warning(""Cgroup does not exist: %s"", path)
                return
            else:
                node = name_to_node[path_element]
        # node is now the leaf node
        parent = node.parent
        self.log.debug(""Deleting cgroup %s/%s"", parent, node.name)
        parent.delete_cgroup(node.name)",def,_delete_cgroup,(,self,",",path,),:,node,=,trees,.,Tree,(,),.,root,path_split,=,path,.,split,(,"""/""",),for,path_element,in,path_split,:,name_to_node,=,{,x,.,name,:,x,for,x,in,node,.,children,},if,path_element,not,in,name_to_node,:,self,"Delete the specified cgroup.

        :param path: The path of the cgroup to delete.
        E.g. cpu/mygroup/mysubgroup",Delete,the,specified,cgroup,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/task_runner/cgroup_task_runner.py#L90-L109,test,.,log,.,warning,(,"""Cgroup does not exist: %s""",",",path,),return,else,:,node,=,name_to_node,[,path_element,],# node is now the leaf node,parent,=,node,.,parent,self,.,log,.,debug,(,"""Deleting cgroup %s/%s""",",",parent,",",node,.,name,),parent,.,delete_cgroup,(,node,.,name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/databricks_hook.py,DatabricksHook._parse_host,"def _parse_host(host):
        """"""
        The purpose of this function is to be robust to improper connections
        settings provided by users, specifically in the host field.

        For example -- when users supply ``https://xx.cloud.databricks.com`` as the
        host, we must strip out the protocol to get the host.::

            h = DatabricksHook()
            assert h._parse_host('https://xx.cloud.databricks.com') == \
                'xx.cloud.databricks.com'

        In the case where users supply the correct ``xx.cloud.databricks.com`` as the
        host, this function is a no-op.::

            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'

        """"""
        urlparse_host = urlparse.urlparse(host).hostname
        if urlparse_host:
            # In this case, host = https://xx.cloud.databricks.com
            return urlparse_host
        else:
            # In this case, host = xx.cloud.databricks.com
            return host",python,"def _parse_host(host):
        """"""
        The purpose of this function is to be robust to improper connections
        settings provided by users, specifically in the host field.

        For example -- when users supply ``https://xx.cloud.databricks.com`` as the
        host, we must strip out the protocol to get the host.::

            h = DatabricksHook()
            assert h._parse_host('https://xx.cloud.databricks.com') == \
                'xx.cloud.databricks.com'

        In the case where users supply the correct ``xx.cloud.databricks.com`` as the
        host, this function is a no-op.::

            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'

        """"""
        urlparse_host = urlparse.urlparse(host).hostname
        if urlparse_host:
            # In this case, host = https://xx.cloud.databricks.com
            return urlparse_host
        else:
            # In this case, host = xx.cloud.databricks.com
            return host",def,_parse_host,(,host,),:,urlparse_host,=,urlparse,.,urlparse,(,host,),.,hostname,if,urlparse_host,:,"# In this case, host = https://xx.cloud.databricks.com",return,urlparse_host,else,:,"# In this case, host = xx.cloud.databricks.com",return,host,,,,,,,,,,,,,,,,,,,,,,,,,,"The purpose of this function is to be robust to improper connections
        settings provided by users, specifically in the host field.

        For example -- when users supply ``https://xx.cloud.databricks.com`` as the
        host, we must strip out the protocol to get the host.::

            h = DatabricksHook()
            assert h._parse_host('https://xx.cloud.databricks.com') == \
                'xx.cloud.databricks.com'

        In the case where users supply the correct ``xx.cloud.databricks.com`` as the
        host, this function is a no-op.::

            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'",The,purpose,of,this,function,is,to,be,robust,to,improper,connections,settings,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/databricks_hook.py#L73-L97,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,provided,by,users,specifically,in,the,host,field,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/databricks_hook.py,DatabricksHook._do_api_call,"def _do_api_call(self, endpoint_info, json):
        """"""
        Utility function to perform an API call with retries

        :param endpoint_info: Tuple of method and endpoint
        :type endpoint_info: tuple[string, string]
        :param json: Parameters for this API call.
        :type json: dict
        :return: If the api call returns a OK status code,
            this function returns the response in JSON. Otherwise,
            we throw an AirflowException.
        :rtype: dict
        """"""
        method, endpoint = endpoint_info
        url = 'https://{host}/{endpoint}'.format(
            host=self._parse_host(self.databricks_conn.host),
            endpoint=endpoint)
        if 'token' in self.databricks_conn.extra_dejson:
            self.log.info('Using token auth.')
            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])
        else:
            self.log.info('Using basic auth.')
            auth = (self.databricks_conn.login, self.databricks_conn.password)
        if method == 'GET':
            request_func = requests.get
        elif method == 'POST':
            request_func = requests.post
        else:
            raise AirflowException('Unexpected HTTP Method: ' + method)

        attempt_num = 1
        while True:
            try:
                response = request_func(
                    url,
                    json=json,
                    auth=auth,
                    headers=USER_AGENT_HEADER,
                    timeout=self.timeout_seconds)
                response.raise_for_status()
                return response.json()
            except requests_exceptions.RequestException as e:
                if not _retryable_error(e):
                    # In this case, the user probably made a mistake.
                    # Don't retry.
                    raise AirflowException('Response: {0}, Status Code: {1}'.format(
                        e.response.content, e.response.status_code))

                self._log_request_error(attempt_num, e)

            if attempt_num == self.retry_limit:
                raise AirflowException(('API requests to Databricks failed {} times. ' +
                                        'Giving up.').format(self.retry_limit))

            attempt_num += 1
            sleep(self.retry_delay)",python,"def _do_api_call(self, endpoint_info, json):
        """"""
        Utility function to perform an API call with retries

        :param endpoint_info: Tuple of method and endpoint
        :type endpoint_info: tuple[string, string]
        :param json: Parameters for this API call.
        :type json: dict
        :return: If the api call returns a OK status code,
            this function returns the response in JSON. Otherwise,
            we throw an AirflowException.
        :rtype: dict
        """"""
        method, endpoint = endpoint_info
        url = 'https://{host}/{endpoint}'.format(
            host=self._parse_host(self.databricks_conn.host),
            endpoint=endpoint)
        if 'token' in self.databricks_conn.extra_dejson:
            self.log.info('Using token auth.')
            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])
        else:
            self.log.info('Using basic auth.')
            auth = (self.databricks_conn.login, self.databricks_conn.password)
        if method == 'GET':
            request_func = requests.get
        elif method == 'POST':
            request_func = requests.post
        else:
            raise AirflowException('Unexpected HTTP Method: ' + method)

        attempt_num = 1
        while True:
            try:
                response = request_func(
                    url,
                    json=json,
                    auth=auth,
                    headers=USER_AGENT_HEADER,
                    timeout=self.timeout_seconds)
                response.raise_for_status()
                return response.json()
            except requests_exceptions.RequestException as e:
                if not _retryable_error(e):
                    # In this case, the user probably made a mistake.
                    # Don't retry.
                    raise AirflowException('Response: {0}, Status Code: {1}'.format(
                        e.response.content, e.response.status_code))

                self._log_request_error(attempt_num, e)

            if attempt_num == self.retry_limit:
                raise AirflowException(('API requests to Databricks failed {} times. ' +
                                        'Giving up.').format(self.retry_limit))

            attempt_num += 1
            sleep(self.retry_delay)",def,_do_api_call,(,self,",",endpoint_info,",",json,),:,method,",",endpoint,=,endpoint_info,url,=,'https://{host}/{endpoint}',.,format,(,host,=,self,.,_parse_host,(,self,.,databricks_conn,.,host,),",",endpoint,=,endpoint,),if,'token',in,self,.,databricks_conn,.,extra_dejson,:,self,.,log,.,info,"Utility function to perform an API call with retries

        :param endpoint_info: Tuple of method and endpoint
        :type endpoint_info: tuple[string, string]
        :param json: Parameters for this API call.
        :type json: dict
        :return: If the api call returns a OK status code,
            this function returns the response in JSON. Otherwise,
            we throw an AirflowException.
        :rtype: dict",Utility,function,to,perform,an,API,call,with,retries,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/databricks_hook.py#L99-L154,test,(,'Using token auth.',),auth,=,_TokenAuth,(,self,.,databricks_conn,.,extra_dejson,[,'token',],),else,:,self,.,log,.,info,(,'Using basic auth.',),auth,=,(,self,.,databricks_conn,.,login,",",self,.,databricks_conn,.,password,),if,method,==,'GET',:,request_func,=,requests,.,get,elif,method,==,'POST',:,request_func,=,requests,.,post,else,:,raise,AirflowException,(,'Unexpected HTTP Method: ',+,method,),attempt_num,=,1,while,True,:,try,:,response,=,request_func,(,url,",",json,=,json,",",auth,=,auth,",",headers,=,USER_AGENT_HEADER,",",timeout,=,self,.,timeout_seconds,),response,.,raise_for_status,(,),return,response,.,json,(,),except,requests_exceptions,.,RequestException,as,e,:,if,not,_retryable_error,(,e,),:,"# In this case, the user probably made a mistake.",# Don't retry.,raise,AirflowException,(,"'Response: {0}, Status Code: {1}'",.,format,(,e,.,response,.,content,",",e,.,response,.,status_code,),),self,.,_log_request_error,(,attempt_num,",",e,),if,attempt_num,==,self,.,retry_limit,:,raise,AirflowException,(,(,'API requests to Databricks failed {} times. ',+,'Giving up.',),.,format,(,self,.,retry_limit,),),attempt_num,+=,1,sleep,(,self,.,retry_delay,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook.get_conn,"def get_conn(self):
        """"""
        Sign into Salesforce, only if we are not already signed in.
        """"""
        if not self.conn:
            connection = self.get_connection(self.conn_id)
            extras = connection.extra_dejson
            self.conn = Salesforce(
                username=connection.login,
                password=connection.password,
                security_token=extras['security_token'],
                instance_url=connection.host,
                sandbox=extras.get('sandbox', False)
            )
        return self.conn",python,"def get_conn(self):
        """"""
        Sign into Salesforce, only if we are not already signed in.
        """"""
        if not self.conn:
            connection = self.get_connection(self.conn_id)
            extras = connection.extra_dejson
            self.conn = Salesforce(
                username=connection.login,
                password=connection.password,
                security_token=extras['security_token'],
                instance_url=connection.host,
                sandbox=extras.get('sandbox', False)
            )
        return self.conn",def,get_conn,(,self,),:,if,not,self,.,conn,:,connection,=,self,.,get_connection,(,self,.,conn_id,),extras,=,connection,.,extra_dejson,self,.,conn,=,Salesforce,(,username,=,connection,.,login,",",password,=,connection,.,password,",",security_token,=,extras,[,'security_token',],",","Sign into Salesforce, only if we are not already signed in.",Sign,into,Salesforce,only,if,we,are,not,already,signed,in,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L60-L74,test,instance_url,=,connection,.,host,",",sandbox,=,extras,.,get,(,'sandbox',",",False,),),return,self,.,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook.make_query,"def make_query(self, query):
        """"""
        Make a query to Salesforce.

        :param query: The query to make to Salesforce.
        :type query: str
        :return: The query result.
        :rtype: dict
        """"""
        conn = self.get_conn()

        self.log.info(""Querying for all objects"")
        query_results = conn.query_all(query)

        self.log.info(""Received results: Total size: %s; Done: %s"",
                      query_results['totalSize'], query_results['done'])

        return query_results",python,"def make_query(self, query):
        """"""
        Make a query to Salesforce.

        :param query: The query to make to Salesforce.
        :type query: str
        :return: The query result.
        :rtype: dict
        """"""
        conn = self.get_conn()

        self.log.info(""Querying for all objects"")
        query_results = conn.query_all(query)

        self.log.info(""Received results: Total size: %s; Done: %s"",
                      query_results['totalSize'], query_results['done'])

        return query_results",def,make_query,(,self,",",query,),:,conn,=,self,.,get_conn,(,),self,.,log,.,info,(,"""Querying for all objects""",),query_results,=,conn,.,query_all,(,query,),self,.,log,.,info,(,"""Received results: Total size: %s; Done: %s""",",",query_results,[,'totalSize',],",",query_results,[,'done',],),return,query_results,,"Make a query to Salesforce.

        :param query: The query to make to Salesforce.
        :type query: str
        :return: The query result.
        :rtype: dict",Make,a,query,to,Salesforce,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L76-L93,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook.describe_object,"def describe_object(self, obj):
        """"""
        Get the description of an object from Salesforce.
        This description is the object's schema and
        some extra metadata that Salesforce stores for each object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the description of the Salesforce object.
        :rtype: dict
        """"""
        conn = self.get_conn()

        return conn.__getattr__(obj).describe()",python,"def describe_object(self, obj):
        """"""
        Get the description of an object from Salesforce.
        This description is the object's schema and
        some extra metadata that Salesforce stores for each object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the description of the Salesforce object.
        :rtype: dict
        """"""
        conn = self.get_conn()

        return conn.__getattr__(obj).describe()",def,describe_object,(,self,",",obj,),:,conn,=,self,.,get_conn,(,),return,conn,.,__getattr__,(,obj,),.,describe,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,"Get the description of an object from Salesforce.
        This description is the object's schema and
        some extra metadata that Salesforce stores for each object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the description of the Salesforce object.
        :rtype: dict",Get,the,description,of,an,object,from,Salesforce,.,This,description,is,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L95-L108,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,object,s,schema,and,some,extra,metadata,that,Salesforce,stores,for,each,object,.,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook.get_available_fields,"def get_available_fields(self, obj):
        """"""
        Get a list of all available fields for an object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the names of the fields.
        :rtype: list of str
        """"""
        self.get_conn()

        obj_description = self.describe_object(obj)

        return [field['name'] for field in obj_description['fields']]",python,"def get_available_fields(self, obj):
        """"""
        Get a list of all available fields for an object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the names of the fields.
        :rtype: list of str
        """"""
        self.get_conn()

        obj_description = self.describe_object(obj)

        return [field['name'] for field in obj_description['fields']]",def,get_available_fields,(,self,",",obj,),:,self,.,get_conn,(,),obj_description,=,self,.,describe_object,(,obj,),return,[,field,[,'name',],for,field,in,obj_description,[,'fields',],],,,,,,,,,,,,,,,,,,"Get a list of all available fields for an object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the names of the fields.
        :rtype: list of str",Get,a,list,of,all,available,fields,for,an,object,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L110-L123,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook.get_object_from_salesforce,"def get_object_from_salesforce(self, obj, fields):
        """"""
        Get all instances of the `object` from Salesforce.
        For each model, only get the fields specified in fields.

        All we really do underneath the hood is run:
            SELECT <fields> FROM <obj>;

        :param obj: The object name to get from Salesforce.
        :type obj: str
        :param fields: The fields to get from the object.
        :type fields: iterable
        :return: all instances of the object from Salesforce.
        :rtype: dict
        """"""
        query = ""SELECT {} FROM {}"".format("","".join(fields), obj)

        self.log.info(""Making query to Salesforce: %s"",
                      query if len(query) < 30 else "" ... "".join([query[:15], query[-15:]]))

        return self.make_query(query)",python,"def get_object_from_salesforce(self, obj, fields):
        """"""
        Get all instances of the `object` from Salesforce.
        For each model, only get the fields specified in fields.

        All we really do underneath the hood is run:
            SELECT <fields> FROM <obj>;

        :param obj: The object name to get from Salesforce.
        :type obj: str
        :param fields: The fields to get from the object.
        :type fields: iterable
        :return: all instances of the object from Salesforce.
        :rtype: dict
        """"""
        query = ""SELECT {} FROM {}"".format("","".join(fields), obj)

        self.log.info(""Making query to Salesforce: %s"",
                      query if len(query) < 30 else "" ... "".join([query[:15], query[-15:]]))

        return self.make_query(query)",def,get_object_from_salesforce,(,self,",",obj,",",fields,),:,query,=,"""SELECT {} FROM {}""",.,format,(,""",""",.,join,(,fields,),",",obj,),self,.,log,.,info,(,"""Making query to Salesforce: %s""",",",query,if,len,(,query,),<,30,else,""" ... """,.,join,(,[,query,[,:,15,],"Get all instances of the `object` from Salesforce.
        For each model, only get the fields specified in fields.

        All we really do underneath the hood is run:
            SELECT <fields> FROM <obj>;

        :param obj: The object name to get from Salesforce.
        :type obj: str
        :param fields: The fields to get from the object.
        :type fields: iterable
        :return: all instances of the object from Salesforce.
        :rtype: dict",Get,all,instances,of,the,object,from,Salesforce,.,For,each,model,only,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L125-L145,test,",",query,[,-,15,:,],],),),return,self,.,make_query,(,query,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get,the,fields,specified,in,fields,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook._to_timestamp,"def _to_timestamp(cls, column):
        """"""
        Convert a column of a dataframe to UNIX timestamps if applicable

        :param column: A Series object representing a column of a dataframe.
        :type column: pd.Series
        :return: a new series that maintains the same index as the original
        :rtype: pd.Series
        """"""
        # try and convert the column to datetimes
        # the column MUST have a four digit year somewhere in the string
        # there should be a better way to do this,
        # but just letting pandas try and convert every column without a format
        # caused it to convert floats as well
        # For example, a column of integers
        # between 0 and 10 are turned into timestamps
        # if the column cannot be converted,
        # just return the original column untouched
        try:
            column = pd.to_datetime(column)
        except ValueError:
            log = LoggingMixin().log
            log.warning(""Could not convert field to timestamps: %s"", column.name)
            return column

        # now convert the newly created datetimes into timestamps
        # we have to be careful here
        # because NaT cannot be converted to a timestamp
        # so we have to return NaN
        converted = []
        for value in column:
            try:
                converted.append(value.timestamp())
            except (ValueError, AttributeError):
                converted.append(pd.np.NaN)

        return pd.Series(converted, index=column.index)",python,"def _to_timestamp(cls, column):
        """"""
        Convert a column of a dataframe to UNIX timestamps if applicable

        :param column: A Series object representing a column of a dataframe.
        :type column: pd.Series
        :return: a new series that maintains the same index as the original
        :rtype: pd.Series
        """"""
        # try and convert the column to datetimes
        # the column MUST have a four digit year somewhere in the string
        # there should be a better way to do this,
        # but just letting pandas try and convert every column without a format
        # caused it to convert floats as well
        # For example, a column of integers
        # between 0 and 10 are turned into timestamps
        # if the column cannot be converted,
        # just return the original column untouched
        try:
            column = pd.to_datetime(column)
        except ValueError:
            log = LoggingMixin().log
            log.warning(""Could not convert field to timestamps: %s"", column.name)
            return column

        # now convert the newly created datetimes into timestamps
        # we have to be careful here
        # because NaT cannot be converted to a timestamp
        # so we have to return NaN
        converted = []
        for value in column:
            try:
                converted.append(value.timestamp())
            except (ValueError, AttributeError):
                converted.append(pd.np.NaN)

        return pd.Series(converted, index=column.index)",def,_to_timestamp,(,cls,",",column,),:,# try and convert the column to datetimes,# the column MUST have a four digit year somewhere in the string,"# there should be a better way to do this,",# but just letting pandas try and convert every column without a format,# caused it to convert floats as well,"# For example, a column of integers",# between 0 and 10 are turned into timestamps,"# if the column cannot be converted,",# just return the original column untouched,try,:,column,=,pd,.,to_datetime,(,column,),except,ValueError,:,log,=,LoggingMixin,(,),.,log,log,.,warning,(,"""Could not convert field to timestamps: %s""",",",column,.,name,),return,column,# now convert the newly created datetimes into timestamps,# we have to be careful here,# because NaT cannot be converted to a timestamp,"Convert a column of a dataframe to UNIX timestamps if applicable

        :param column: A Series object representing a column of a dataframe.
        :type column: pd.Series
        :return: a new series that maintains the same index as the original
        :rtype: pd.Series",Convert,a,column,of,a,dataframe,to,UNIX,timestamps,if,applicable,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L148-L184,test,# so we have to return NaN,converted,=,[,],for,value,in,column,:,try,:,converted,.,append,(,value,.,timestamp,(,),),except,(,ValueError,",",AttributeError,),:,converted,.,append,(,pd,.,np,.,NaN,),return,pd,.,Series,(,converted,",",index,=,column,.,index,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/salesforce_hook.py,SalesforceHook.write_object_to_file,"def write_object_to_file(self,
                             query_results,
                             filename,
                             fmt=""csv"",
                             coerce_to_timestamp=False,
                             record_time_added=False):
        """"""
        Write query results to file.

        Acceptable formats are:
            - csv:
                comma-separated-values file. This is the default format.
            - json:
                JSON array. Each element in the array is a different row.
            - ndjson:
                JSON array but each element is new-line delimited instead of comma delimited like in `json`

        This requires a significant amount of cleanup.
        Pandas doesn't handle output to CSV and json in a uniform way.
        This is especially painful for datetime types.
        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.

        By default, this function will try and leave all values as they are represented in Salesforce.
        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).
        This is can be greatly beneficial as it will make all of your datetime fields look the same,
        and makes it easier to work with in other database environments

        :param query_results: the results from a SQL query
        :type query_results: list of dict
        :param filename: the name of the file where the data should be dumped to
        :type filename: str
        :param fmt: the format you want the output in. Default:  'csv'
        :type fmt: str
        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.
            False if you want them to be left in the same format as they were in Salesforce.
            Leaving the value as False will result in datetimes being strings. Default: False
        :type coerce_to_timestamp: bool
        :param record_time_added: True if you want to add a Unix timestamp field
            to the resulting data that marks when the data was fetched from Salesforce. Default: False
        :type record_time_added: bool
        :return: the dataframe that gets written to the file.
        :rtype: pd.Dataframe
        """"""
        fmt = fmt.lower()
        if fmt not in ['csv', 'json', 'ndjson']:
            raise ValueError(""Format value is not recognized: {}"".format(fmt))

        # this line right here will convert all integers to floats
        # if there are any None/np.nan values in the column
        # that's because None/np.nan cannot exist in an integer column
        # we should write all of our timestamps as FLOATS in our final schema
        df = pd.DataFrame.from_records(query_results, exclude=[""attributes""])

        df.columns = [column.lower() for column in df.columns]

        # convert columns with datetime strings to datetimes
        # not all strings will be datetimes, so we ignore any errors that occur
        # we get the object's definition at this point and only consider
        # features that are DATE or DATETIME
        if coerce_to_timestamp and df.shape[0] > 0:
            # get the object name out of the query results
            # it's stored in the ""attributes"" dictionary
            # for each returned record
            object_name = query_results[0]['attributes']['type']

            self.log.info(""Coercing timestamps for: %s"", object_name)

            schema = self.describe_object(object_name)

            # possible columns that can be converted to timestamps
            # are the ones that are either date or datetime types
            # strings are too general and we risk unintentional conversion
            possible_timestamp_cols = [
                field['name'].lower()
                for field in schema['fields']
                if field['type'] in [""date"", ""datetime""] and field['name'].lower() in df.columns
            ]
            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)

        if record_time_added:
            fetched_time = time.time()
            df[""time_fetched_from_salesforce""] = fetched_time

        # write the CSV or JSON file depending on the option
        # NOTE:
        #   datetimes here are an issue.
        #   There is no good way to manage the difference
        #   for to_json, the options are an epoch or a ISO string
        #   but for to_csv, it will be a string output by datetime
        #   For JSON we decided to output the epoch timestamp in seconds
        #   (as is fairly standard for JavaScript)
        #   And for csv, we do a string
        if fmt == ""csv"":
            # there are also a ton of newline objects that mess up our ability to write to csv
            # we remove these newlines so that the output is a valid CSV format
            self.log.info(""Cleaning data and writing to CSV"")
            possible_strings = df.columns[df.dtypes == ""object""]
            df[possible_strings] = df[possible_strings].apply(
                lambda x: x.str.replace(""\r\n"", """").str.replace(""\n"", """")
            )
            # write the dataframe
            df.to_csv(filename, index=False)
        elif fmt == ""json"":
            df.to_json(filename, ""records"", date_unit=""s"")
        elif fmt == ""ndjson"":
            df.to_json(filename, ""records"", lines=True, date_unit=""s"")

        return df",python,"def write_object_to_file(self,
                             query_results,
                             filename,
                             fmt=""csv"",
                             coerce_to_timestamp=False,
                             record_time_added=False):
        """"""
        Write query results to file.

        Acceptable formats are:
            - csv:
                comma-separated-values file. This is the default format.
            - json:
                JSON array. Each element in the array is a different row.
            - ndjson:
                JSON array but each element is new-line delimited instead of comma delimited like in `json`

        This requires a significant amount of cleanup.
        Pandas doesn't handle output to CSV and json in a uniform way.
        This is especially painful for datetime types.
        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.

        By default, this function will try and leave all values as they are represented in Salesforce.
        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).
        This is can be greatly beneficial as it will make all of your datetime fields look the same,
        and makes it easier to work with in other database environments

        :param query_results: the results from a SQL query
        :type query_results: list of dict
        :param filename: the name of the file where the data should be dumped to
        :type filename: str
        :param fmt: the format you want the output in. Default:  'csv'
        :type fmt: str
        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.
            False if you want them to be left in the same format as they were in Salesforce.
            Leaving the value as False will result in datetimes being strings. Default: False
        :type coerce_to_timestamp: bool
        :param record_time_added: True if you want to add a Unix timestamp field
            to the resulting data that marks when the data was fetched from Salesforce. Default: False
        :type record_time_added: bool
        :return: the dataframe that gets written to the file.
        :rtype: pd.Dataframe
        """"""
        fmt = fmt.lower()
        if fmt not in ['csv', 'json', 'ndjson']:
            raise ValueError(""Format value is not recognized: {}"".format(fmt))

        # this line right here will convert all integers to floats
        # if there are any None/np.nan values in the column
        # that's because None/np.nan cannot exist in an integer column
        # we should write all of our timestamps as FLOATS in our final schema
        df = pd.DataFrame.from_records(query_results, exclude=[""attributes""])

        df.columns = [column.lower() for column in df.columns]

        # convert columns with datetime strings to datetimes
        # not all strings will be datetimes, so we ignore any errors that occur
        # we get the object's definition at this point and only consider
        # features that are DATE or DATETIME
        if coerce_to_timestamp and df.shape[0] > 0:
            # get the object name out of the query results
            # it's stored in the ""attributes"" dictionary
            # for each returned record
            object_name = query_results[0]['attributes']['type']

            self.log.info(""Coercing timestamps for: %s"", object_name)

            schema = self.describe_object(object_name)

            # possible columns that can be converted to timestamps
            # are the ones that are either date or datetime types
            # strings are too general and we risk unintentional conversion
            possible_timestamp_cols = [
                field['name'].lower()
                for field in schema['fields']
                if field['type'] in [""date"", ""datetime""] and field['name'].lower() in df.columns
            ]
            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)

        if record_time_added:
            fetched_time = time.time()
            df[""time_fetched_from_salesforce""] = fetched_time

        # write the CSV or JSON file depending on the option
        # NOTE:
        #   datetimes here are an issue.
        #   There is no good way to manage the difference
        #   for to_json, the options are an epoch or a ISO string
        #   but for to_csv, it will be a string output by datetime
        #   For JSON we decided to output the epoch timestamp in seconds
        #   (as is fairly standard for JavaScript)
        #   And for csv, we do a string
        if fmt == ""csv"":
            # there are also a ton of newline objects that mess up our ability to write to csv
            # we remove these newlines so that the output is a valid CSV format
            self.log.info(""Cleaning data and writing to CSV"")
            possible_strings = df.columns[df.dtypes == ""object""]
            df[possible_strings] = df[possible_strings].apply(
                lambda x: x.str.replace(""\r\n"", """").str.replace(""\n"", """")
            )
            # write the dataframe
            df.to_csv(filename, index=False)
        elif fmt == ""json"":
            df.to_json(filename, ""records"", date_unit=""s"")
        elif fmt == ""ndjson"":
            df.to_json(filename, ""records"", lines=True, date_unit=""s"")

        return df",def,write_object_to_file,(,self,",",query_results,",",filename,",",fmt,=,"""csv""",",",coerce_to_timestamp,=,False,",",record_time_added,=,False,),:,fmt,=,fmt,.,lower,(,),if,fmt,not,in,[,'csv',",",'json',",",'ndjson',],:,raise,ValueError,(,"""Format value is not recognized: {}""",.,format,(,fmt,),),# this line right here will convert all integers to floats,"Write query results to file.

        Acceptable formats are:
            - csv:
                comma-separated-values file. This is the default format.
            - json:
                JSON array. Each element in the array is a different row.
            - ndjson:
                JSON array but each element is new-line delimited instead of comma delimited like in `json`

        This requires a significant amount of cleanup.
        Pandas doesn't handle output to CSV and json in a uniform way.
        This is especially painful for datetime types.
        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.

        By default, this function will try and leave all values as they are represented in Salesforce.
        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).
        This is can be greatly beneficial as it will make all of your datetime fields look the same,
        and makes it easier to work with in other database environments

        :param query_results: the results from a SQL query
        :type query_results: list of dict
        :param filename: the name of the file where the data should be dumped to
        :type filename: str
        :param fmt: the format you want the output in. Default:  'csv'
        :type fmt: str
        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.
            False if you want them to be left in the same format as they were in Salesforce.
            Leaving the value as False will result in datetimes being strings. Default: False
        :type coerce_to_timestamp: bool
        :param record_time_added: True if you want to add a Unix timestamp field
            to the resulting data that marks when the data was fetched from Salesforce. Default: False
        :type record_time_added: bool
        :return: the dataframe that gets written to the file.
        :rtype: pd.Dataframe",Write,query,results,to,file,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L186-L293,test,# if there are any None/np.nan values in the column,# that's because None/np.nan cannot exist in an integer column,# we should write all of our timestamps as FLOATS in our final schema,df,=,pd,.,DataFrame,.,from_records,(,query_results,",",exclude,=,[,"""attributes""",],),df,.,columns,=,[,column,.,lower,(,),for,column,in,df,.,columns,],# convert columns with datetime strings to datetimes,"# not all strings will be datetimes, so we ignore any errors that occur",# we get the object's definition at this point and only consider,# features that are DATE or DATETIME,if,coerce_to_timestamp,and,df,.,shape,[,0,],>,0,:,# get the object name out of the query results,"# it's stored in the ""attributes"" dictionary",# for each returned record,object_name,=,query_results,[,0,],[,'attributes',],[,'type',],self,.,log,.,info,(,"""Coercing timestamps for: %s""",",",object_name,),schema,=,self,.,describe_object,(,object_name,),# possible columns that can be converted to timestamps,# are the ones that are either date or datetime types,# strings are too general and we risk unintentional conversion,possible_timestamp_cols,=,[,field,[,'name',],.,lower,(,),for,field,in,schema,[,'fields',],if,field,[,'type',],in,[,"""date""",",","""datetime""",],and,field,[,'name',],.,lower,(,),in,df,.,columns,],df,[,possible_timestamp_cols,],=,df,[,possible_timestamp_cols,],.,apply,(,self,.,_to_timestamp,),if,record_time_added,:,fetched_time,=,time,.,time,(,),df,[,"""time_fetched_from_salesforce""",],=,fetched_time,# write the CSV or JSON file depending on the option,# NOTE:,#   datetimes here are an issue.,#   There is no good way to manage the difference,"#   for to_json, the options are an epoch or a ISO string","#   but for to_csv, it will be a string output by datetime",#   For JSON we decided to output the epoch timestamp in seconds,#   (as is fairly standard for JavaScript),"#   And for csv, we do a string",if,fmt,==,"""csv""",:,# there are also a ton of newline objects that mess up our ability to write to csv,# we remove these newlines so that the output is a valid CSV format,self,.,log,.,info,(,"""Cleaning data and writing to CSV""",),possible_strings,=,df,.,columns,[,df,.,dtypes,==,"""object""",],df,[,possible_strings,],=,df,[,possible_strings,],.,apply,(,lambda,x,:,x,.,str,.,replace,(,"""\r\n""",",","""""",),.,str,.,replace,(,"""\n""",",","""""",),),# write the dataframe,df,.,to_csv,(,filename,",",index,=,False,),elif,fmt,==,"""json""",:,df,.,to_json,(,filename,",","""records""",",",date_unit,=,"""s""",),elif,fmt,==,"""ndjson""",:,df,.,to_json,(,filename,",","""records""",",",lines,=,True,",",date_unit,=,"""s""",),return,df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/mongo_hook.py,MongoHook.get_conn,"def get_conn(self):
        """"""
        Fetches PyMongo Client
        """"""
        if self.client is not None:
            return self.client

        # Mongo Connection Options dict that is unpacked when passed to MongoClient
        options = self.extras

        # If we are using SSL disable requiring certs from specific hostname
        if options.get('ssl', False):
            options.update({'ssl_cert_reqs': CERT_NONE})

        self.client = MongoClient(self.uri, **options)

        return self.client",python,"def get_conn(self):
        """"""
        Fetches PyMongo Client
        """"""
        if self.client is not None:
            return self.client

        # Mongo Connection Options dict that is unpacked when passed to MongoClient
        options = self.extras

        # If we are using SSL disable requiring certs from specific hostname
        if options.get('ssl', False):
            options.update({'ssl_cert_reqs': CERT_NONE})

        self.client = MongoClient(self.uri, **options)

        return self.client",def,get_conn,(,self,),:,if,self,.,client,is,not,None,:,return,self,.,client,# Mongo Connection Options dict that is unpacked when passed to MongoClient,options,=,self,.,extras,# If we are using SSL disable requiring certs from specific hostname,if,options,.,get,(,'ssl',",",False,),:,options,.,update,(,{,'ssl_cert_reqs',:,CERT_NONE,},),self,.,client,=,MongoClient,(,self,Fetches PyMongo Client,Fetches,PyMongo,Client,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L69-L85,test,.,uri,",",*,*,options,),return,self,.,client,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/mongo_hook.py,MongoHook.get_collection,"def get_collection(self, mongo_collection, mongo_db=None):
        """"""
        Fetches a mongo collection object for querying.

        Uses connection schema as DB unless specified.
        """"""
        mongo_db = mongo_db if mongo_db is not None else self.connection.schema
        mongo_conn = self.get_conn()

        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)",python,"def get_collection(self, mongo_collection, mongo_db=None):
        """"""
        Fetches a mongo collection object for querying.

        Uses connection schema as DB unless specified.
        """"""
        mongo_db = mongo_db if mongo_db is not None else self.connection.schema
        mongo_conn = self.get_conn()

        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)",def,get_collection,(,self,",",mongo_collection,",",mongo_db,=,None,),:,mongo_db,=,mongo_db,if,mongo_db,is,not,None,else,self,.,connection,.,schema,mongo_conn,=,self,.,get_conn,(,),return,mongo_conn,.,get_database,(,mongo_db,),.,get_collection,(,mongo_collection,),,,,,,,,"Fetches a mongo collection object for querying.

        Uses connection schema as DB unless specified.",Fetches,a,mongo,collection,object,for,querying,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L93-L102,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/mongo_hook.py,MongoHook.replace_many,"def replace_many(self, mongo_collection, docs,
                     filter_docs=None, mongo_db=None, upsert=False, collation=None,
                     **kwargs):
        """"""
        Replaces many documents in a mongo collection.

        Uses bulk_write with multiple ReplaceOne operations
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write

        .. note::
            If no ``filter_docs``are given, it is assumed that all
            replacement documents contain the ``_id`` field which are then
            used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param docs: The new documents.
        :type docs: list[dict]
        :param filter_docs: A list of queries that match the documents to replace.
            Can be omitted; then the _id fields from docs will be used.
        :type filter_docs: list[dict]
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str
        :param upsert: If ``True``, perform an insert if no documents
            match the filters for the replace operation.
        :type upsert: bool
        :param collation: An instance of
            :class:`~pymongo.collation.Collation`. This option is only
            supported on MongoDB 3.4 and above.
        :type collation: pymongo.collation.Collation

        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        if not filter_docs:
            filter_docs = [{'_id': doc['_id']} for doc in docs]

        requests = [
            ReplaceOne(
                filter_docs[i],
                docs[i],
                upsert=upsert,
                collation=collation)
            for i in range(len(docs))
        ]

        return collection.bulk_write(requests, **kwargs)",python,"def replace_many(self, mongo_collection, docs,
                     filter_docs=None, mongo_db=None, upsert=False, collation=None,
                     **kwargs):
        """"""
        Replaces many documents in a mongo collection.

        Uses bulk_write with multiple ReplaceOne operations
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write

        .. note::
            If no ``filter_docs``are given, it is assumed that all
            replacement documents contain the ``_id`` field which are then
            used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param docs: The new documents.
        :type docs: list[dict]
        :param filter_docs: A list of queries that match the documents to replace.
            Can be omitted; then the _id fields from docs will be used.
        :type filter_docs: list[dict]
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str
        :param upsert: If ``True``, perform an insert if no documents
            match the filters for the replace operation.
        :type upsert: bool
        :param collation: An instance of
            :class:`~pymongo.collation.Collation`. This option is only
            supported on MongoDB 3.4 and above.
        :type collation: pymongo.collation.Collation

        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        if not filter_docs:
            filter_docs = [{'_id': doc['_id']} for doc in docs]

        requests = [
            ReplaceOne(
                filter_docs[i],
                docs[i],
                upsert=upsert,
                collation=collation)
            for i in range(len(docs))
        ]

        return collection.bulk_write(requests, **kwargs)",def,replace_many,(,self,",",mongo_collection,",",docs,",",filter_docs,=,None,",",mongo_db,=,None,",",upsert,=,False,",",collation,=,None,",",*,*,kwargs,),:,collection,=,self,.,get_collection,(,mongo_collection,",",mongo_db,=,mongo_db,),if,not,filter_docs,:,filter_docs,=,[,{,'_id',:,"Replaces many documents in a mongo collection.

        Uses bulk_write with multiple ReplaceOne operations
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write

        .. note::
            If no ``filter_docs``are given, it is assumed that all
            replacement documents contain the ``_id`` field which are then
            used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param docs: The new documents.
        :type docs: list[dict]
        :param filter_docs: A list of queries that match the documents to replace.
            Can be omitted; then the _id fields from docs will be used.
        :type filter_docs: list[dict]
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str
        :param upsert: If ``True``, perform an insert if no documents
            match the filters for the replace operation.
        :type upsert: bool
        :param collation: An instance of
            :class:`~pymongo.collation.Collation`. This option is only
            supported on MongoDB 3.4 and above.
        :type collation: pymongo.collation.Collation",Replaces,many,documents,in,a,mongo,collection,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L214-L261,test,doc,[,'_id',],},for,doc,in,docs,],requests,=,[,ReplaceOne,(,filter_docs,[,i,],",",docs,[,i,],",",upsert,=,upsert,",",collation,=,collation,),for,i,in,range,(,len,(,docs,),),],return,collection,.,bulk_write,(,requests,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/imap_hook.py,ImapHook.has_mail_attachment,"def has_mail_attachment(self, name, mail_folder='INBOX', check_regex=False):
        """"""
        Checks the mail folder for mails containing attachments with the given name.

        :param name: The name of the attachment that will be searched for.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :returns: True if there is an attachment with the given name and False if not.
        :rtype: bool
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only=True)
        return len(mail_attachments) > 0",python,"def has_mail_attachment(self, name, mail_folder='INBOX', check_regex=False):
        """"""
        Checks the mail folder for mails containing attachments with the given name.

        :param name: The name of the attachment that will be searched for.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :returns: True if there is an attachment with the given name and False if not.
        :rtype: bool
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only=True)
        return len(mail_attachments) > 0",def,has_mail_attachment,(,self,",",name,",",mail_folder,=,'INBOX',",",check_regex,=,False,),:,mail_attachments,=,self,.,_retrieve_mails_attachments_by_name,(,name,",",mail_folder,",",check_regex,",",latest_only,=,True,),return,len,(,mail_attachments,),>,0,,,,,,,,,,,,,,"Checks the mail folder for mails containing attachments with the given name.

        :param name: The name of the attachment that will be searched for.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :returns: True if there is an attachment with the given name and False if not.
        :rtype: bool",Checks,the,mail,folder,for,mails,containing,attachments,with,the,given,name,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L49-L66,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/imap_hook.py,ImapHook.retrieve_mail_attachments,"def retrieve_mail_attachments(self,
                                  name,
                                  mail_folder='INBOX',
                                  check_regex=False,
                                  latest_only=False,
                                  not_found_mode='raise'):
        """"""
        Retrieves mail's attachments in the mail folder by its name.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only retrieve
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        :returns: a list of tuple each containing the attachment filename and its payload.
        :rtype: a list of tuple
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only)
        if not mail_attachments:
            self._handle_not_found_mode(not_found_mode)

        return mail_attachments",python,"def retrieve_mail_attachments(self,
                                  name,
                                  mail_folder='INBOX',
                                  check_regex=False,
                                  latest_only=False,
                                  not_found_mode='raise'):
        """"""
        Retrieves mail's attachments in the mail folder by its name.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only retrieve
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        :returns: a list of tuple each containing the attachment filename and its payload.
        :rtype: a list of tuple
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only)
        if not mail_attachments:
            self._handle_not_found_mode(not_found_mode)

        return mail_attachments",def,retrieve_mail_attachments,(,self,",",name,",",mail_folder,=,'INBOX',",",check_regex,=,False,",",latest_only,=,False,",",not_found_mode,=,'raise',),:,mail_attachments,=,self,.,_retrieve_mails_attachments_by_name,(,name,",",mail_folder,",",check_regex,",",latest_only,),if,not,mail_attachments,:,self,.,_handle_not_found_mode,(,not_found_mode,),return,mail_attachments,,,"Retrieves mail's attachments in the mail folder by its name.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only retrieve
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        :returns: a list of tuple each containing the attachment filename and its payload.
        :rtype: a list of tuple",Retrieves,mail,s,attachments,in,the,mail,folder,by,its,name,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L68-L102,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/imap_hook.py,ImapHook.download_mail_attachments,"def download_mail_attachments(self,
                                  name,
                                  local_output_directory,
                                  mail_folder='INBOX',
                                  check_regex=False,
                                  latest_only=False,
                                  not_found_mode='raise'):
        """"""
        Downloads mail's attachments in the mail folder by its name to the local directory.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param local_output_directory: The output directory on the local machine
                                       where the files will be downloaded to.
        :type local_output_directory: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only download
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only)

        if not mail_attachments:
            self._handle_not_found_mode(not_found_mode)

        self._create_files(mail_attachments, local_output_directory)",python,"def download_mail_attachments(self,
                                  name,
                                  local_output_directory,
                                  mail_folder='INBOX',
                                  check_regex=False,
                                  latest_only=False,
                                  not_found_mode='raise'):
        """"""
        Downloads mail's attachments in the mail folder by its name to the local directory.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param local_output_directory: The output directory on the local machine
                                       where the files will be downloaded to.
        :type local_output_directory: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only download
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only)

        if not mail_attachments:
            self._handle_not_found_mode(not_found_mode)

        self._create_files(mail_attachments, local_output_directory)",def,download_mail_attachments,(,self,",",name,",",local_output_directory,",",mail_folder,=,'INBOX',",",check_regex,=,False,",",latest_only,=,False,",",not_found_mode,=,'raise',),:,mail_attachments,=,self,.,_retrieve_mails_attachments_by_name,(,name,",",mail_folder,",",check_regex,",",latest_only,),if,not,mail_attachments,:,self,.,_handle_not_found_mode,(,not_found_mode,),self,.,"Downloads mail's attachments in the mail folder by its name to the local directory.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param local_output_directory: The output directory on the local machine
                                       where the files will be downloaded to.
        :type local_output_directory: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only download
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str",Downloads,mail,s,attachments,in,the,mail,folder,by,its,name,to,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L104-L141,test,_create_files,(,mail_attachments,",",local_output_directory,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,local,directory,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/imap_hook.py,Mail.get_attachments_by_name,"def get_attachments_by_name(self, name, check_regex, find_first=False):
        """"""
        Gets all attachments by name for the mail.

        :param name: The name of the attachment to look for.
        :type name: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param find_first: If set to True it will only find the first match and then quit.
        :type find_first: bool
        :returns: a list of tuples each containing name and payload
                  where the attachments name matches the given name.
        :rtype: list of tuple
        """"""
        attachments = []

        for part in self.mail.walk():
            mail_part = MailPart(part)
            if mail_part.is_attachment():
                found_attachment = mail_part.has_matching_name(name) if check_regex \
                    else mail_part.has_equal_name(name)
                if found_attachment:
                    file_name, file_payload = mail_part.get_file()
                    self.log.info('Found attachment: {}'.format(file_name))
                    attachments.append((file_name, file_payload))
                    if find_first:
                        break

        return attachments",python,"def get_attachments_by_name(self, name, check_regex, find_first=False):
        """"""
        Gets all attachments by name for the mail.

        :param name: The name of the attachment to look for.
        :type name: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param find_first: If set to True it will only find the first match and then quit.
        :type find_first: bool
        :returns: a list of tuples each containing name and payload
                  where the attachments name matches the given name.
        :rtype: list of tuple
        """"""
        attachments = []

        for part in self.mail.walk():
            mail_part = MailPart(part)
            if mail_part.is_attachment():
                found_attachment = mail_part.has_matching_name(name) if check_regex \
                    else mail_part.has_equal_name(name)
                if found_attachment:
                    file_name, file_payload = mail_part.get_file()
                    self.log.info('Found attachment: {}'.format(file_name))
                    attachments.append((file_name, file_payload))
                    if find_first:
                        break

        return attachments",def,get_attachments_by_name,(,self,",",name,",",check_regex,",",find_first,=,False,),:,attachments,=,[,],for,part,in,self,.,mail,.,walk,(,),:,mail_part,=,MailPart,(,part,),if,mail_part,.,is_attachment,(,),:,found_attachment,=,mail_part,.,has_matching_name,(,name,),if,check_regex,"Gets all attachments by name for the mail.

        :param name: The name of the attachment to look for.
        :type name: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param find_first: If set to True it will only find the first match and then quit.
        :type find_first: bool
        :returns: a list of tuples each containing name and payload
                  where the attachments name matches the given name.
        :rtype: list of tuple",Gets,all,attachments,by,name,for,the,mail,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L236-L264,test,else,mail_part,.,has_equal_name,(,name,),if,found_attachment,:,file_name,",",file_payload,=,mail_part,.,get_file,(,),self,.,log,.,info,(,'Found attachment: {}',.,format,(,file_name,),),attachments,.,append,(,(,file_name,",",file_payload,),),if,find_first,:,break,return,attachments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/imap_hook.py,MailPart.get_file,"def get_file(self):
        """"""
        Gets the file including name and payload.

        :returns: the part's name and payload.
        :rtype: tuple
        """"""
        return self.part.get_filename(), self.part.get_payload(decode=True)",python,"def get_file(self):
        """"""
        Gets the file including name and payload.

        :returns: the part's name and payload.
        :rtype: tuple
        """"""
        return self.part.get_filename(), self.part.get_payload(decode=True)",def,get_file,(,self,),:,return,self,.,part,.,get_filename,(,),",",self,.,part,.,get_payload,(,decode,=,True,),,,,,,,,,,,,,,,,,,,,,,,,,,,,"Gets the file including name and payload.

        :returns: the part's name and payload.
        :rtype: tuple",Gets,the,file,including,name,and,payload,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L309-L316,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_firehose_hook.py,AwsFirehoseHook.put_records,"def put_records(self, records):
        """"""
        Write batch records to Kinesis Firehose
        """"""

        firehose_conn = self.get_conn()

        response = firehose_conn.put_record_batch(
            DeliveryStreamName=self.delivery_stream,
            Records=records
        )

        return response",python,"def put_records(self, records):
        """"""
        Write batch records to Kinesis Firehose
        """"""

        firehose_conn = self.get_conn()

        response = firehose_conn.put_record_batch(
            DeliveryStreamName=self.delivery_stream,
            Records=records
        )

        return response",def,put_records,(,self,",",records,),:,firehose_conn,=,self,.,get_conn,(,),response,=,firehose_conn,.,put_record_batch,(,DeliveryStreamName,=,self,.,delivery_stream,",",Records,=,records,),return,response,,,,,,,,,,,,,,,,,,,,Write batch records to Kinesis Firehose,Write,batch,records,to,Kinesis,Firehose,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_firehose_hook.py#L44-L56,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/ti_deps/deps/ready_to_reschedule.py,ReadyToRescheduleDep._get_dep_statuses,"def _get_dep_statuses(self, ti, session, dep_context):
        """"""
        Determines whether a task is ready to be rescheduled. Only tasks in
        NONE state with at least one row in task_reschedule table are
        handled by this dependency class, otherwise this dependency is
        considered as passed. This dependency fails if the latest reschedule
        request's reschedule date is still in future.
        """"""
        if dep_context.ignore_in_reschedule_period:
            yield self._passing_status(
                reason=""The context specified that being in a reschedule period was ""
                       ""permitted."")
            return

        if ti.state not in self.RESCHEDULEABLE_STATES:
            yield self._passing_status(
                reason=""The task instance is not in State_UP_FOR_RESCHEDULE or NONE state."")
            return

        task_reschedules = TaskReschedule.find_for_task_instance(task_instance=ti)
        if not task_reschedules:
            yield self._passing_status(
                reason=""There is no reschedule request for this task instance."")
            return

        now = timezone.utcnow()
        next_reschedule_date = task_reschedules[-1].reschedule_date
        if now >= next_reschedule_date:
            yield self._passing_status(
                reason=""Task instance id ready for reschedule."")
            return

        yield self._failing_status(
            reason=""Task is not ready for reschedule yet but will be rescheduled ""
                   ""automatically. Current date is {0} and task will be rescheduled ""
                   ""at {1}."".format(now.isoformat(), next_reschedule_date.isoformat()))",python,"def _get_dep_statuses(self, ti, session, dep_context):
        """"""
        Determines whether a task is ready to be rescheduled. Only tasks in
        NONE state with at least one row in task_reschedule table are
        handled by this dependency class, otherwise this dependency is
        considered as passed. This dependency fails if the latest reschedule
        request's reschedule date is still in future.
        """"""
        if dep_context.ignore_in_reschedule_period:
            yield self._passing_status(
                reason=""The context specified that being in a reschedule period was ""
                       ""permitted."")
            return

        if ti.state not in self.RESCHEDULEABLE_STATES:
            yield self._passing_status(
                reason=""The task instance is not in State_UP_FOR_RESCHEDULE or NONE state."")
            return

        task_reschedules = TaskReschedule.find_for_task_instance(task_instance=ti)
        if not task_reschedules:
            yield self._passing_status(
                reason=""There is no reschedule request for this task instance."")
            return

        now = timezone.utcnow()
        next_reschedule_date = task_reschedules[-1].reschedule_date
        if now >= next_reschedule_date:
            yield self._passing_status(
                reason=""Task instance id ready for reschedule."")
            return

        yield self._failing_status(
            reason=""Task is not ready for reschedule yet but will be rescheduled ""
                   ""automatically. Current date is {0} and task will be rescheduled ""
                   ""at {1}."".format(now.isoformat(), next_reschedule_date.isoformat()))",def,_get_dep_statuses,(,self,",",ti,",",session,",",dep_context,),:,if,dep_context,.,ignore_in_reschedule_period,:,yield,self,.,_passing_status,(,reason,=,"""The context specified that being in a reschedule period was ""","""permitted.""",),return,if,ti,.,state,not,in,self,.,RESCHEDULEABLE_STATES,:,yield,self,.,_passing_status,(,reason,=,"""The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.""",),return,task_reschedules,=,TaskReschedule,.,"Determines whether a task is ready to be rescheduled. Only tasks in
        NONE state with at least one row in task_reschedule table are
        handled by this dependency class, otherwise this dependency is
        considered as passed. This dependency fails if the latest reschedule
        request's reschedule date is still in future.",Determines,whether,a,task,is,ready,to,be,rescheduled,.,Only,tasks,in,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/ready_to_reschedule.py#L34-L69,test,find_for_task_instance,(,task_instance,=,ti,),if,not,task_reschedules,:,yield,self,.,_passing_status,(,reason,=,"""There is no reschedule request for this task instance.""",),return,now,=,timezone,.,utcnow,(,),next_reschedule_date,=,task_reschedules,[,-,1,],.,reschedule_date,if,now,>=,next_reschedule_date,:,yield,self,.,_passing_status,(,reason,=,"""Task instance id ready for reschedule.""",),return,yield,self,.,_failing_status,(,reason,=,"""Task is not ready for reschedule yet but will be rescheduled ""","""automatically. Current date is {0} and task will be rescheduled ""","""at {1}.""",.,format,(,now,.,isoformat,(,),",",next_reschedule_date,.,isoformat,(,),),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,state,with,at,least,one,row,in,task_reschedule,table,are,handled,by,this,dependency,class,otherwise,this,dependency,is,considered,as,passed,.,This,dependency,fails,if,the,latest,reschedule,request,s,reschedule,date,is,still,in,future,.
apache/airflow,airflow/utils/email.py,send_email,"def send_email(to, subject, html_content,
               files=None, dryrun=False, cc=None, bcc=None,
               mime_subtype='mixed', mime_charset='utf-8', **kwargs):
    """"""
    Send email using backend specified in EMAIL_BACKEND.
    """"""
    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)
    module = importlib.import_module(path)
    backend = getattr(module, attr)
    to = get_email_address_list(to)
    to = "", "".join(to)

    return backend(to, subject, html_content, files=files,
                   dryrun=dryrun, cc=cc, bcc=bcc,
                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)",python,"def send_email(to, subject, html_content,
               files=None, dryrun=False, cc=None, bcc=None,
               mime_subtype='mixed', mime_charset='utf-8', **kwargs):
    """"""
    Send email using backend specified in EMAIL_BACKEND.
    """"""
    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)
    module = importlib.import_module(path)
    backend = getattr(module, attr)
    to = get_email_address_list(to)
    to = "", "".join(to)

    return backend(to, subject, html_content, files=files,
                   dryrun=dryrun, cc=cc, bcc=bcc,
                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)",def,send_email,(,to,",",subject,",",html_content,",",files,=,None,",",dryrun,=,False,",",cc,=,None,",",bcc,=,None,",",mime_subtype,=,'mixed',",",mime_charset,=,'utf-8',",",*,*,kwargs,),:,path,",",attr,=,configuration,.,conf,.,get,(,'email',",",'EMAIL_BACKEND',),Send email using backend specified in EMAIL_BACKEND.,Send,email,using,backend,specified,in,EMAIL_BACKEND,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/email.py#L36-L50,test,.,rsplit,(,'.',",",1,),module,=,importlib,.,import_module,(,path,),backend,=,getattr,(,module,",",attr,),to,=,get_email_address_list,(,to,),to,=,""", """,.,join,(,to,),return,backend,(,to,",",subject,",",html_content,",",files,=,files,",",dryrun,=,dryrun,",",cc,=,cc,",",bcc,=,bcc,",",mime_subtype,=,mime_subtype,",",mime_charset,=,mime_charset,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/email.py,send_email_smtp,"def send_email_smtp(to, subject, html_content, files=None,
                    dryrun=False, cc=None, bcc=None,
                    mime_subtype='mixed', mime_charset='utf-8',
                    **kwargs):
    """"""
    Send an email with html content

    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)
    """"""
    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')

    to = get_email_address_list(to)

    msg = MIMEMultipart(mime_subtype)
    msg['Subject'] = subject
    msg['From'] = smtp_mail_from
    msg['To'] = "", "".join(to)
    recipients = to
    if cc:
        cc = get_email_address_list(cc)
        msg['CC'] = "", "".join(cc)
        recipients = recipients + cc

    if bcc:
        # don't add bcc in header
        bcc = get_email_address_list(bcc)
        recipients = recipients + bcc

    msg['Date'] = formatdate(localtime=True)
    mime_text = MIMEText(html_content, 'html', mime_charset)
    msg.attach(mime_text)

    for fname in files or []:
        basename = os.path.basename(fname)
        with open(fname, ""rb"") as f:
            part = MIMEApplication(
                f.read(),
                Name=basename
            )
            part['Content-Disposition'] = 'attachment; filename=""%s""' % basename
            part['Content-ID'] = '<%s>' % basename
            msg.attach(part)

    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)",python,"def send_email_smtp(to, subject, html_content, files=None,
                    dryrun=False, cc=None, bcc=None,
                    mime_subtype='mixed', mime_charset='utf-8',
                    **kwargs):
    """"""
    Send an email with html content

    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)
    """"""
    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')

    to = get_email_address_list(to)

    msg = MIMEMultipart(mime_subtype)
    msg['Subject'] = subject
    msg['From'] = smtp_mail_from
    msg['To'] = "", "".join(to)
    recipients = to
    if cc:
        cc = get_email_address_list(cc)
        msg['CC'] = "", "".join(cc)
        recipients = recipients + cc

    if bcc:
        # don't add bcc in header
        bcc = get_email_address_list(bcc)
        recipients = recipients + bcc

    msg['Date'] = formatdate(localtime=True)
    mime_text = MIMEText(html_content, 'html', mime_charset)
    msg.attach(mime_text)

    for fname in files or []:
        basename = os.path.basename(fname)
        with open(fname, ""rb"") as f:
            part = MIMEApplication(
                f.read(),
                Name=basename
            )
            part['Content-Disposition'] = 'attachment; filename=""%s""' % basename
            part['Content-ID'] = '<%s>' % basename
            msg.attach(part)

    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)",def,send_email_smtp,(,to,",",subject,",",html_content,",",files,=,None,",",dryrun,=,False,",",cc,=,None,",",bcc,=,None,",",mime_subtype,=,'mixed',",",mime_charset,=,'utf-8',",",*,*,kwargs,),:,smtp_mail_from,=,configuration,.,conf,.,get,(,'smtp',",",'SMTP_MAIL_FROM',),to,=,"Send an email with html content

    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)",Send,an,email,with,html,content,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/email.py#L53-L96,test,get_email_address_list,(,to,),msg,=,MIMEMultipart,(,mime_subtype,),msg,[,'Subject',],=,subject,msg,[,'From',],=,smtp_mail_from,msg,[,'To',],=,""", """,.,join,(,to,),recipients,=,to,if,cc,:,cc,=,get_email_address_list,(,cc,),msg,[,'CC',],=,""", """,.,join,(,cc,),recipients,=,recipients,+,cc,if,bcc,:,# don't add bcc in header,bcc,=,get_email_address_list,(,bcc,),recipients,=,recipients,+,bcc,msg,[,'Date',],=,formatdate,(,localtime,=,True,),mime_text,=,MIMEText,(,html_content,",",'html',",",mime_charset,),msg,.,attach,(,mime_text,),for,fname,in,files,or,[,],:,basename,=,os,.,path,.,basename,(,fname,),with,open,(,fname,",","""rb""",),as,f,:,part,=,MIMEApplication,(,f,.,read,(,),",",Name,=,basename,),part,[,'Content-Disposition',],=,"'attachment; filename=""%s""'",%,basename,part,[,'Content-ID',],=,'<%s>',%,basename,msg,.,attach,(,part,),send_MIME_email,(,smtp_mail_from,",",recipients,",",msg,",",dryrun,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/sqlalchemy.py,UtcDateTime.process_result_value,"def process_result_value(self, value, dialect):
        """"""
        Processes DateTimes from the DB making sure it is always
        returning UTC. Not using timezone.convert_to_utc as that
        converts to configured TIMEZONE while the DB might be
        running with some other setting. We assume UTC datetimes
        in the database.
        """"""
        if value is not None:
            if value.tzinfo is None:
                value = value.replace(tzinfo=utc)
            else:
                value = value.astimezone(utc)

        return value",python,"def process_result_value(self, value, dialect):
        """"""
        Processes DateTimes from the DB making sure it is always
        returning UTC. Not using timezone.convert_to_utc as that
        converts to configured TIMEZONE while the DB might be
        running with some other setting. We assume UTC datetimes
        in the database.
        """"""
        if value is not None:
            if value.tzinfo is None:
                value = value.replace(tzinfo=utc)
            else:
                value = value.astimezone(utc)

        return value",def,process_result_value,(,self,",",value,",",dialect,),:,if,value,is,not,None,:,if,value,.,tzinfo,is,None,:,value,=,value,.,replace,(,tzinfo,=,utc,),else,:,value,=,value,.,astimezone,(,utc,),return,value,,,,,,,,"Processes DateTimes from the DB making sure it is always
        returning UTC. Not using timezone.convert_to_utc as that
        converts to configured TIMEZONE while the DB might be
        running with some other setting. We assume UTC datetimes
        in the database.",Processes,DateTimes,from,the,DB,making,sure,it,is,always,returning,UTC,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/sqlalchemy.py#L156-L170,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not,using,timezone,.,convert_to_utc,as,that,converts,to,configured,TIMEZONE,while,the,DB,might,be,running,with,some,other,setting,.,We,assume,UTC,datetimes,in,the,database,.,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/wasb_hook.py,WasbHook.check_for_blob,"def check_for_blob(self, container_name, blob_name, **kwargs):
        """"""
        Check if a blob exists on Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.exists()` takes.
        :type kwargs: object
        :return: True if the blob exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(container_name, blob_name, **kwargs)",python,"def check_for_blob(self, container_name, blob_name, **kwargs):
        """"""
        Check if a blob exists on Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.exists()` takes.
        :type kwargs: object
        :return: True if the blob exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(container_name, blob_name, **kwargs)",def,check_for_blob,(,self,",",container_name,",",blob_name,",",*,*,kwargs,),:,return,self,.,connection,.,exists,(,container_name,",",blob_name,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,"Check if a blob exists on Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.exists()` takes.
        :type kwargs: object
        :return: True if the blob exists, False otherwise.
        :rtype: bool",Check,if,a,blob,exists,on,Azure,Blob,Storage,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L50-L64,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/wasb_hook.py,WasbHook.check_for_prefix,"def check_for_prefix(self, container_name, prefix, **kwargs):
        """"""
        Check if a prefix exists on Azure Blob storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param prefix: Prefix of the blob.
        :type prefix: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.list_blobs()` takes.
        :type kwargs: object
        :return: True if blobs matching the prefix exist, False otherwise.
        :rtype: bool
        """"""
        matches = self.connection.list_blobs(container_name, prefix,
                                             num_results=1, **kwargs)
        return len(list(matches)) > 0",python,"def check_for_prefix(self, container_name, prefix, **kwargs):
        """"""
        Check if a prefix exists on Azure Blob storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param prefix: Prefix of the blob.
        :type prefix: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.list_blobs()` takes.
        :type kwargs: object
        :return: True if blobs matching the prefix exist, False otherwise.
        :rtype: bool
        """"""
        matches = self.connection.list_blobs(container_name, prefix,
                                             num_results=1, **kwargs)
        return len(list(matches)) > 0",def,check_for_prefix,(,self,",",container_name,",",prefix,",",*,*,kwargs,),:,matches,=,self,.,connection,.,list_blobs,(,container_name,",",prefix,",",num_results,=,1,",",*,*,kwargs,),return,len,(,list,(,matches,),),>,0,,,,,,,,,"Check if a prefix exists on Azure Blob storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param prefix: Prefix of the blob.
        :type prefix: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.list_blobs()` takes.
        :type kwargs: object
        :return: True if blobs matching the prefix exist, False otherwise.
        :rtype: bool",Check,if,a,prefix,exists,on,Azure,Blob,storage,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L66-L82,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/wasb_hook.py,WasbHook.load_string,"def load_string(self, string_data, container_name, blob_name, **kwargs):
        """"""
        Upload a string to Azure Blob Storage.

        :param string_data: String to load.
        :type string_data: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_text()` takes.
        :type kwargs: object
        """"""
        # Reorder the argument order from airflow.hooks.S3_hook.load_string.
        self.connection.create_blob_from_text(container_name, blob_name,
                                              string_data, **kwargs)",python,"def load_string(self, string_data, container_name, blob_name, **kwargs):
        """"""
        Upload a string to Azure Blob Storage.

        :param string_data: String to load.
        :type string_data: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_text()` takes.
        :type kwargs: object
        """"""
        # Reorder the argument order from airflow.hooks.S3_hook.load_string.
        self.connection.create_blob_from_text(container_name, blob_name,
                                              string_data, **kwargs)",def,load_string,(,self,",",string_data,",",container_name,",",blob_name,",",*,*,kwargs,),:,# Reorder the argument order from airflow.hooks.S3_hook.load_string.,self,.,connection,.,create_blob_from_text,(,container_name,",",blob_name,",",string_data,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,"Upload a string to Azure Blob Storage.

        :param string_data: String to load.
        :type string_data: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_text()` takes.
        :type kwargs: object",Upload,a,string,to,Azure,Blob,Storage,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L102-L118,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/wasb_hook.py,WasbHook.read_file,"def read_file(self, container_name, blob_name, **kwargs):
        """"""
        Read a file from Azure Blob Storage and return as a string.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""
        return self.connection.get_blob_to_text(container_name,
                                                blob_name,
                                                **kwargs).content",python,"def read_file(self, container_name, blob_name, **kwargs):
        """"""
        Read a file from Azure Blob Storage and return as a string.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""
        return self.connection.get_blob_to_text(container_name,
                                                blob_name,
                                                **kwargs).content",def,read_file,(,self,",",container_name,",",blob_name,",",*,*,kwargs,),:,return,self,.,connection,.,get_blob_to_text,(,container_name,",",blob_name,",",*,*,kwargs,),.,content,,,,,,,,,,,,,,,,,,,,,,"Read a file from Azure Blob Storage and return as a string.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object",Read,a,file,from,Azure,Blob,Storage,and,return,as,a,string,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L137-L151,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/wasb_hook.py,WasbHook.delete_file,"def delete_file(self, container_name, blob_name, is_prefix=False,
                    ignore_if_missing=False, **kwargs):
        """"""
        Delete a file from Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param is_prefix: If blob_name is a prefix, delete all matching files
        :type is_prefix: bool
        :param ignore_if_missing: if True, then return success even if the
            blob does not exist.
        :type ignore_if_missing: bool
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""

        if is_prefix:
            blobs_to_delete = [
                blob.name for blob in self.connection.list_blobs(
                    container_name, prefix=blob_name, **kwargs
                )
            ]
        elif self.check_for_blob(container_name, blob_name):
            blobs_to_delete = [blob_name]
        else:
            blobs_to_delete = []

        if not ignore_if_missing and len(blobs_to_delete) == 0:
            raise AirflowException('Blob(s) not found: {}'.format(blob_name))

        for blob_uri in blobs_to_delete:
            self.log.info(""Deleting blob: "" + blob_uri)
            self.connection.delete_blob(container_name,
                                        blob_uri,
                                        delete_snapshots='include',
                                        **kwargs)",python,"def delete_file(self, container_name, blob_name, is_prefix=False,
                    ignore_if_missing=False, **kwargs):
        """"""
        Delete a file from Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param is_prefix: If blob_name is a prefix, delete all matching files
        :type is_prefix: bool
        :param ignore_if_missing: if True, then return success even if the
            blob does not exist.
        :type ignore_if_missing: bool
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""

        if is_prefix:
            blobs_to_delete = [
                blob.name for blob in self.connection.list_blobs(
                    container_name, prefix=blob_name, **kwargs
                )
            ]
        elif self.check_for_blob(container_name, blob_name):
            blobs_to_delete = [blob_name]
        else:
            blobs_to_delete = []

        if not ignore_if_missing and len(blobs_to_delete) == 0:
            raise AirflowException('Blob(s) not found: {}'.format(blob_name))

        for blob_uri in blobs_to_delete:
            self.log.info(""Deleting blob: "" + blob_uri)
            self.connection.delete_blob(container_name,
                                        blob_uri,
                                        delete_snapshots='include',
                                        **kwargs)",def,delete_file,(,self,",",container_name,",",blob_name,",",is_prefix,=,False,",",ignore_if_missing,=,False,",",*,*,kwargs,),:,if,is_prefix,:,blobs_to_delete,=,[,blob,.,name,for,blob,in,self,.,connection,.,list_blobs,(,container_name,",",prefix,=,blob_name,",",*,*,kwargs,),],elif,"Delete a file from Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param is_prefix: If blob_name is a prefix, delete all matching files
        :type is_prefix: bool
        :param ignore_if_missing: if True, then return success even if the
            blob does not exist.
        :type ignore_if_missing: bool
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object",Delete,a,file,from,Azure,Blob,Storage,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L153-L191,test,self,.,check_for_blob,(,container_name,",",blob_name,),:,blobs_to_delete,=,[,blob_name,],else,:,blobs_to_delete,=,[,],if,not,ignore_if_missing,and,len,(,blobs_to_delete,),==,0,:,raise,AirflowException,(,'Blob(s) not found: {}',.,format,(,blob_name,),),for,blob_uri,in,blobs_to_delete,:,self,.,log,.,info,(,"""Deleting blob: """,+,blob_uri,),self,.,connection,.,delete_blob,(,container_name,",",blob_uri,",",delete_snapshots,=,'include',",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ftp_hook.py,mlsd,"def mlsd(conn, path="""", facts=None):
    """"""
    BACKPORT FROM PYTHON3 FTPLIB.

    List a directory in a standardized format by using MLSD
    command (RFC-3659). If path is omitted the current directory
    is assumed. ""facts"" is a list of strings representing the type
    of information desired (e.g. [""type"", ""size"", ""perm""]).

    Return a generator object yielding a tuple of two elements
    for every file found in path.
    First element is the file name, the second one is a dictionary
    including a variable number of ""facts"" depending on the server
    and whether ""facts"" argument has been provided.
    """"""
    facts = facts or []
    if facts:
        conn.sendcmd(""OPTS MLST "" + "";"".join(facts) + "";"")
    if path:
        cmd = ""MLSD %s"" % path
    else:
        cmd = ""MLSD""
    lines = []
    conn.retrlines(cmd, lines.append)
    for line in lines:
        facts_found, _, name = line.rstrip(ftplib.CRLF).partition(' ')
        entry = {}
        for fact in facts_found[:-1].split("";""):
            key, _, value = fact.partition(""="")
            entry[key.lower()] = value
        yield (name, entry)",python,"def mlsd(conn, path="""", facts=None):
    """"""
    BACKPORT FROM PYTHON3 FTPLIB.

    List a directory in a standardized format by using MLSD
    command (RFC-3659). If path is omitted the current directory
    is assumed. ""facts"" is a list of strings representing the type
    of information desired (e.g. [""type"", ""size"", ""perm""]).

    Return a generator object yielding a tuple of two elements
    for every file found in path.
    First element is the file name, the second one is a dictionary
    including a variable number of ""facts"" depending on the server
    and whether ""facts"" argument has been provided.
    """"""
    facts = facts or []
    if facts:
        conn.sendcmd(""OPTS MLST "" + "";"".join(facts) + "";"")
    if path:
        cmd = ""MLSD %s"" % path
    else:
        cmd = ""MLSD""
    lines = []
    conn.retrlines(cmd, lines.append)
    for line in lines:
        facts_found, _, name = line.rstrip(ftplib.CRLF).partition(' ')
        entry = {}
        for fact in facts_found[:-1].split("";""):
            key, _, value = fact.partition(""="")
            entry[key.lower()] = value
        yield (name, entry)",def,mlsd,(,conn,",",path,=,"""""",",",facts,=,None,),:,facts,=,facts,or,[,],if,facts,:,conn,.,sendcmd,(,"""OPTS MLST """,+,""";""",.,join,(,facts,),+,""";""",),if,path,:,cmd,=,"""MLSD %s""",%,path,else,:,cmd,=,"""MLSD""",lines,"BACKPORT FROM PYTHON3 FTPLIB.

    List a directory in a standardized format by using MLSD
    command (RFC-3659). If path is omitted the current directory
    is assumed. ""facts"" is a list of strings representing the type
    of information desired (e.g. [""type"", ""size"", ""perm""]).

    Return a generator object yielding a tuple of two elements
    for every file found in path.
    First element is the file name, the second one is a dictionary
    including a variable number of ""facts"" depending on the server
    and whether ""facts"" argument has been provided.",BACKPORT,FROM,PYTHON3,FTPLIB,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L28-L58,test,=,[,],conn,.,retrlines,(,cmd,",",lines,.,append,),for,line,in,lines,:,facts_found,",",_,",",name,=,line,.,rstrip,(,ftplib,.,CRLF,),.,partition,(,' ',),entry,=,{,},for,fact,in,facts_found,[,:,-,1,],.,split,(,""";""",),:,key,",",_,",",value,=,fact,.,partition,(,"""=""",),entry,[,key,.,lower,(,),],=,value,yield,(,name,",",entry,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ftp_hook.py,FTPHook.get_conn,"def get_conn(self):
        """"""
        Returns a FTP connection object
        """"""
        if self.conn is None:
            params = self.get_connection(self.ftp_conn_id)
            pasv = params.extra_dejson.get(""passive"", True)
            self.conn = ftplib.FTP(params.host, params.login, params.password)
            self.conn.set_pasv(pasv)

        return self.conn",python,"def get_conn(self):
        """"""
        Returns a FTP connection object
        """"""
        if self.conn is None:
            params = self.get_connection(self.ftp_conn_id)
            pasv = params.extra_dejson.get(""passive"", True)
            self.conn = ftplib.FTP(params.host, params.login, params.password)
            self.conn.set_pasv(pasv)

        return self.conn",def,get_conn,(,self,),:,if,self,.,conn,is,None,:,params,=,self,.,get_connection,(,self,.,ftp_conn_id,),pasv,=,params,.,extra_dejson,.,get,(,"""passive""",",",True,),self,.,conn,=,ftplib,.,FTP,(,params,.,host,",",params,.,login,",",params,Returns a FTP connection object,Returns,a,FTP,connection,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L81-L91,test,.,password,),self,.,conn,.,set_pasv,(,pasv,),return,self,.,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ftp_hook.py,FTPHook.list_directory,"def list_directory(self, path, nlst=False):
        """"""
        Returns a list of files on the remote system.

        :param path: full path to the remote directory to list
        :type path: str
        """"""
        conn = self.get_conn()
        conn.cwd(path)

        files = conn.nlst()
        return files",python,"def list_directory(self, path, nlst=False):
        """"""
        Returns a list of files on the remote system.

        :param path: full path to the remote directory to list
        :type path: str
        """"""
        conn = self.get_conn()
        conn.cwd(path)

        files = conn.nlst()
        return files",def,list_directory,(,self,",",path,",",nlst,=,False,),:,conn,=,self,.,get_conn,(,),conn,.,cwd,(,path,),files,=,conn,.,nlst,(,),return,files,,,,,,,,,,,,,,,,,,,"Returns a list of files on the remote system.

        :param path: full path to the remote directory to list
        :type path: str",Returns,a,list,of,files,on,the,remote,system,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L119-L130,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ftp_hook.py,FTPHook.retrieve_file,"def retrieve_file(
            self,
            remote_full_path,
            local_full_path_or_buffer,
            callback=None):
        """"""
        Transfers the remote file to a local location.

        If local_full_path_or_buffer is a string path, the file will be put
        at that location; if it is a file-like buffer, the file will
        be written to the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        :param callback: callback which is called each time a block of data
            is read. if you do not use a callback, these blocks will be written
            to the file or buffer passed in. if you do pass in a callback, note
            that writing to a file or buffer will need to be handled inside the
            callback.
            [default: output_handle.write()]
        :type callback: callable

        :Example::

            hook = FTPHook(ftp_conn_id='my_conn')

            remote_path = '/path/to/remote/file'
            local_path = '/path/to/local/file'

            # with a custom callback (in this case displaying progress on each read)
            def print_progress(percent_progress):
                self.log.info('Percent Downloaded: %s%%' % percent_progress)

            total_downloaded = 0
            total_file_size = hook.get_size(remote_path)
            output_handle = open(local_path, 'wb')
            def write_to_file_with_progress(data):
                total_downloaded += len(data)
                output_handle.write(data)
                percent_progress = (total_downloaded / total_file_size) * 100
                print_progress(percent_progress)
            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)

            # without a custom callback data is written to the local_path
            hook.retrieve_file(remote_path, local_path)
        """"""
        conn = self.get_conn()

        is_path = isinstance(local_full_path_or_buffer, basestring)

        # without a callback, default to writing to a user-provided file or
        # file-like buffer
        if not callback:
            if is_path:
                output_handle = open(local_full_path_or_buffer, 'wb')
            else:
                output_handle = local_full_path_or_buffer
            callback = output_handle.write
        else:
            output_handle = None

        remote_path, remote_file_name = os.path.split(remote_full_path)
        conn.cwd(remote_path)
        self.log.info('Retrieving file from FTP: %s', remote_full_path)
        conn.retrbinary('RETR %s' % remote_file_name, callback)
        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)

        if is_path and output_handle:
            output_handle.close()",python,"def retrieve_file(
            self,
            remote_full_path,
            local_full_path_or_buffer,
            callback=None):
        """"""
        Transfers the remote file to a local location.

        If local_full_path_or_buffer is a string path, the file will be put
        at that location; if it is a file-like buffer, the file will
        be written to the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        :param callback: callback which is called each time a block of data
            is read. if you do not use a callback, these blocks will be written
            to the file or buffer passed in. if you do pass in a callback, note
            that writing to a file or buffer will need to be handled inside the
            callback.
            [default: output_handle.write()]
        :type callback: callable

        :Example::

            hook = FTPHook(ftp_conn_id='my_conn')

            remote_path = '/path/to/remote/file'
            local_path = '/path/to/local/file'

            # with a custom callback (in this case displaying progress on each read)
            def print_progress(percent_progress):
                self.log.info('Percent Downloaded: %s%%' % percent_progress)

            total_downloaded = 0
            total_file_size = hook.get_size(remote_path)
            output_handle = open(local_path, 'wb')
            def write_to_file_with_progress(data):
                total_downloaded += len(data)
                output_handle.write(data)
                percent_progress = (total_downloaded / total_file_size) * 100
                print_progress(percent_progress)
            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)

            # without a custom callback data is written to the local_path
            hook.retrieve_file(remote_path, local_path)
        """"""
        conn = self.get_conn()

        is_path = isinstance(local_full_path_or_buffer, basestring)

        # without a callback, default to writing to a user-provided file or
        # file-like buffer
        if not callback:
            if is_path:
                output_handle = open(local_full_path_or_buffer, 'wb')
            else:
                output_handle = local_full_path_or_buffer
            callback = output_handle.write
        else:
            output_handle = None

        remote_path, remote_file_name = os.path.split(remote_full_path)
        conn.cwd(remote_path)
        self.log.info('Retrieving file from FTP: %s', remote_full_path)
        conn.retrbinary('RETR %s' % remote_file_name, callback)
        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)

        if is_path and output_handle:
            output_handle.close()",def,retrieve_file,(,self,",",remote_full_path,",",local_full_path_or_buffer,",",callback,=,None,),:,conn,=,self,.,get_conn,(,),is_path,=,isinstance,(,local_full_path_or_buffer,",",basestring,),"# without a callback, default to writing to a user-provided file or",# file-like buffer,if,not,callback,:,if,is_path,:,output_handle,=,open,(,local_full_path_or_buffer,",",'wb',),else,:,output_handle,=,local_full_path_or_buffer,callback,"Transfers the remote file to a local location.

        If local_full_path_or_buffer is a string path, the file will be put
        at that location; if it is a file-like buffer, the file will
        be written to the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        :param callback: callback which is called each time a block of data
            is read. if you do not use a callback, these blocks will be written
            to the file or buffer passed in. if you do pass in a callback, note
            that writing to a file or buffer will need to be handled inside the
            callback.
            [default: output_handle.write()]
        :type callback: callable

        :Example::

            hook = FTPHook(ftp_conn_id='my_conn')

            remote_path = '/path/to/remote/file'
            local_path = '/path/to/local/file'

            # with a custom callback (in this case displaying progress on each read)
            def print_progress(percent_progress):
                self.log.info('Percent Downloaded: %s%%' % percent_progress)

            total_downloaded = 0
            total_file_size = hook.get_size(remote_path)
            output_handle = open(local_path, 'wb')
            def write_to_file_with_progress(data):
                total_downloaded += len(data)
                output_handle.write(data)
                percent_progress = (total_downloaded / total_file_size) * 100
                print_progress(percent_progress)
            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)

            # without a custom callback data is written to the local_path
            hook.retrieve_file(remote_path, local_path)",Transfers,the,remote,file,to,a,local,location,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L152-L223,test,=,output_handle,.,write,else,:,output_handle,=,None,remote_path,",",remote_file_name,=,os,.,path,.,split,(,remote_full_path,),conn,.,cwd,(,remote_path,),self,.,log,.,info,(,'Retrieving file from FTP: %s',",",remote_full_path,),conn,.,retrbinary,(,'RETR %s',%,remote_file_name,",",callback,),self,.,log,.,info,(,'Finished retrieving file from FTP: %s',",",remote_full_path,),if,is_path,and,output_handle,:,output_handle,.,close,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ftp_hook.py,FTPHook.store_file,"def store_file(self, remote_full_path, local_full_path_or_buffer):
        """"""
        Transfers a local file to the remote location.

        If local_full_path_or_buffer is a string path, the file will be read
        from that location; if it is a file-like buffer, the file will
        be read from the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        """"""
        conn = self.get_conn()

        is_path = isinstance(local_full_path_or_buffer, basestring)

        if is_path:
            input_handle = open(local_full_path_or_buffer, 'rb')
        else:
            input_handle = local_full_path_or_buffer
        remote_path, remote_file_name = os.path.split(remote_full_path)
        conn.cwd(remote_path)
        conn.storbinary('STOR %s' % remote_file_name, input_handle)

        if is_path:
            input_handle.close()",python,"def store_file(self, remote_full_path, local_full_path_or_buffer):
        """"""
        Transfers a local file to the remote location.

        If local_full_path_or_buffer is a string path, the file will be read
        from that location; if it is a file-like buffer, the file will
        be read from the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        """"""
        conn = self.get_conn()

        is_path = isinstance(local_full_path_or_buffer, basestring)

        if is_path:
            input_handle = open(local_full_path_or_buffer, 'rb')
        else:
            input_handle = local_full_path_or_buffer
        remote_path, remote_file_name = os.path.split(remote_full_path)
        conn.cwd(remote_path)
        conn.storbinary('STOR %s' % remote_file_name, input_handle)

        if is_path:
            input_handle.close()",def,store_file,(,self,",",remote_full_path,",",local_full_path_or_buffer,),:,conn,=,self,.,get_conn,(,),is_path,=,isinstance,(,local_full_path_or_buffer,",",basestring,),if,is_path,:,input_handle,=,open,(,local_full_path_or_buffer,",",'rb',),else,:,input_handle,=,local_full_path_or_buffer,remote_path,",",remote_file_name,=,os,.,path,.,split,(,remote_full_path,"Transfers a local file to the remote location.

        If local_full_path_or_buffer is a string path, the file will be read
        from that location; if it is a file-like buffer, the file will
        be read from the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer",Transfers,a,local,file,to,the,remote,location,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L225-L252,test,),conn,.,cwd,(,remote_path,),conn,.,storbinary,(,'STOR %s',%,remote_file_name,",",input_handle,),if,is_path,:,input_handle,.,close,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/ftp_hook.py,FTPHook.get_mod_time,"def get_mod_time(self, path):
        """"""
        Returns a datetime object representing the last time the file was modified

        :param path: remote file path
        :type path: string
        """"""
        conn = self.get_conn()
        ftp_mdtm = conn.sendcmd('MDTM ' + path)
        time_val = ftp_mdtm[4:]
        # time_val optionally has microseconds
        try:
            return datetime.datetime.strptime(time_val, ""%Y%m%d%H%M%S.%f"")
        except ValueError:
            return datetime.datetime.strptime(time_val, '%Y%m%d%H%M%S')",python,"def get_mod_time(self, path):
        """"""
        Returns a datetime object representing the last time the file was modified

        :param path: remote file path
        :type path: string
        """"""
        conn = self.get_conn()
        ftp_mdtm = conn.sendcmd('MDTM ' + path)
        time_val = ftp_mdtm[4:]
        # time_val optionally has microseconds
        try:
            return datetime.datetime.strptime(time_val, ""%Y%m%d%H%M%S.%f"")
        except ValueError:
            return datetime.datetime.strptime(time_val, '%Y%m%d%H%M%S')",def,get_mod_time,(,self,",",path,),:,conn,=,self,.,get_conn,(,),ftp_mdtm,=,conn,.,sendcmd,(,'MDTM ',+,path,),time_val,=,ftp_mdtm,[,4,:,],# time_val optionally has microseconds,try,:,return,datetime,.,datetime,.,strptime,(,time_val,",","""%Y%m%d%H%M%S.%f""",),except,ValueError,:,return,datetime,.,"Returns a datetime object representing the last time the file was modified

        :param path: remote file path
        :type path: string",Returns,a,datetime,object,representing,the,last,time,the,file,was,modified,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L274-L288,test,datetime,.,strptime,(,time_val,",",'%Y%m%d%H%M%S',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/discord_webhook_operator.py,DiscordWebhookOperator.execute,"def execute(self, context):
        """"""
        Call the DiscordWebhookHook to post message
        """"""
        self.hook = DiscordWebhookHook(
            self.http_conn_id,
            self.webhook_endpoint,
            self.message,
            self.username,
            self.avatar_url,
            self.tts,
            self.proxy
        )
        self.hook.execute()",python,"def execute(self, context):
        """"""
        Call the DiscordWebhookHook to post message
        """"""
        self.hook = DiscordWebhookHook(
            self.http_conn_id,
            self.webhook_endpoint,
            self.message,
            self.username,
            self.avatar_url,
            self.tts,
            self.proxy
        )
        self.hook.execute()",def,execute,(,self,",",context,),:,self,.,hook,=,DiscordWebhookHook,(,self,.,http_conn_id,",",self,.,webhook_endpoint,",",self,.,message,",",self,.,username,",",self,.,avatar_url,",",self,.,tts,",",self,.,proxy,),self,.,hook,.,execute,(,),,,,Call the DiscordWebhookHook to post message,Call,the,DiscordWebhookHook,to,post,message,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/discord_webhook_operator.py#L85-L98,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.get_conn,"def get_conn(self):
        """"""Return the FileService object.""""""
        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        return FileService(account_name=conn.login,
                           account_key=conn.password, **service_options)",python,"def get_conn(self):
        """"""Return the FileService object.""""""
        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        return FileService(account_name=conn.login,
                           account_key=conn.password, **service_options)",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,conn_id,),service_options,=,conn,.,extra_dejson,return,FileService,(,account_name,=,conn,.,login,",",account_key,=,conn,.,password,",",*,*,service_options,),,,,,,,,,,,,,Return the FileService object.,Return,the,FileService,object,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L40-L45,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.check_for_directory,"def check_for_directory(self, share_name, directory_name, **kwargs):
        """"""
        Check if a directory exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(share_name, directory_name,
                                      **kwargs)",python,"def check_for_directory(self, share_name, directory_name, **kwargs):
        """"""
        Check if a directory exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(share_name, directory_name,
                                      **kwargs)",def,check_for_directory,(,self,",",share_name,",",directory_name,",",*,*,kwargs,),:,return,self,.,connection,.,exists,(,share_name,",",directory_name,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,"Check if a directory exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool",Check,if,a,directory,exists,on,Azure,File,Share,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L47-L62,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.check_for_file,"def check_for_file(self, share_name, directory_name, file_name, **kwargs):
        """"""
        Check if a file exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(share_name, directory_name,
                                      file_name, **kwargs)",python,"def check_for_file(self, share_name, directory_name, file_name, **kwargs):
        """"""
        Check if a file exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(share_name, directory_name,
                                      file_name, **kwargs)",def,check_for_file,(,self,",",share_name,",",directory_name,",",file_name,",",*,*,kwargs,),:,return,self,.,connection,.,exists,(,share_name,",",directory_name,",",file_name,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,"Check if a file exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool",Check,if,a,file,exists,on,Azure,File,Share,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L64-L81,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.list_directories_and_files,"def list_directories_and_files(self, share_name, directory_name=None, **kwargs):
        """"""
        Return the list of directories and files stored on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.list_directories_and_files()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list
        """"""
        return self.connection.list_directories_and_files(share_name,
                                                          directory_name,
                                                          **kwargs)",python,"def list_directories_and_files(self, share_name, directory_name=None, **kwargs):
        """"""
        Return the list of directories and files stored on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.list_directories_and_files()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list
        """"""
        return self.connection.list_directories_and_files(share_name,
                                                          directory_name,
                                                          **kwargs)",def,list_directories_and_files,(,self,",",share_name,",",directory_name,=,None,",",*,*,kwargs,),:,return,self,.,connection,.,list_directories_and_files,(,share_name,",",directory_name,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,"Return the list of directories and files stored on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.list_directories_and_files()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list",Return,the,list,of,directories,and,files,stored,on,a,Azure,File,Share,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L83-L99,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.create_directory,"def create_directory(self, share_name, directory_name, **kwargs):
        """"""
        Create a new directory on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_directory()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list
        """"""
        return self.connection.create_directory(share_name, directory_name, **kwargs)",python,"def create_directory(self, share_name, directory_name, **kwargs):
        """"""
        Create a new directory on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_directory()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list
        """"""
        return self.connection.create_directory(share_name, directory_name, **kwargs)",def,create_directory,(,self,",",share_name,",",directory_name,",",*,*,kwargs,),:,return,self,.,connection,.,create_directory,(,share_name,",",directory_name,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,"Create a new directory on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_directory()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list",Create,a,new,directory,on,a,Azure,File,Share,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L101-L115,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.load_file,"def load_file(self, file_path, share_name, directory_name, file_name, **kwargs):
        """"""
        Upload a file to Azure File Share.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_path()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_path(share_name, directory_name,
                                              file_name, file_path, **kwargs)",python,"def load_file(self, file_path, share_name, directory_name, file_name, **kwargs):
        """"""
        Upload a file to Azure File Share.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_path()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_path(share_name, directory_name,
                                              file_name, file_path, **kwargs)",def,load_file,(,self,",",file_path,",",share_name,",",directory_name,",",file_name,",",*,*,kwargs,),:,self,.,connection,.,create_file_from_path,(,share_name,",",directory_name,",",file_name,",",file_path,",",*,*,kwargs,),,,,,,,,,,,,,,,,,"Upload a file to Azure File Share.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_path()` takes.
        :type kwargs: object",Upload,a,file,to,Azure,File,Share,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L155-L172,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.load_string,"def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):
        """"""
        Upload a string to Azure File Share.

        :param string_data: String to load.
        :type string_data: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_text()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_text(share_name, directory_name,
                                              file_name, string_data, **kwargs)",python,"def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):
        """"""
        Upload a string to Azure File Share.

        :param string_data: String to load.
        :type string_data: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_text()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_text(share_name, directory_name,
                                              file_name, string_data, **kwargs)",def,load_string,(,self,",",string_data,",",share_name,",",directory_name,",",file_name,",",*,*,kwargs,),:,self,.,connection,.,create_file_from_text,(,share_name,",",directory_name,",",file_name,",",string_data,",",*,*,kwargs,),,,,,,,,,,,,,,,,,"Upload a string to Azure File Share.

        :param string_data: String to load.
        :type string_data: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_text()` takes.
        :type kwargs: object",Upload,a,string,to,Azure,File,Share,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L174-L191,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,AzureFileShareHook.load_stream,"def load_stream(self, stream, share_name, directory_name, file_name, count, **kwargs):
        """"""
        Upload a stream to Azure File Share.

        :param stream: Opened file/stream to upload as the file content.
        :type stream: file-like
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param count: Size of the stream in bytes
        :type count: int
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_stream()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_stream(share_name, directory_name,
                                                file_name, stream, count, **kwargs)",python,"def load_stream(self, stream, share_name, directory_name, file_name, count, **kwargs):
        """"""
        Upload a stream to Azure File Share.

        :param stream: Opened file/stream to upload as the file content.
        :type stream: file-like
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param count: Size of the stream in bytes
        :type count: int
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_stream()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_stream(share_name, directory_name,
                                                file_name, stream, count, **kwargs)",def,load_stream,(,self,",",stream,",",share_name,",",directory_name,",",file_name,",",count,",",*,*,kwargs,),:,self,.,connection,.,create_file_from_stream,(,share_name,",",directory_name,",",file_name,",",stream,",",count,",",*,*,kwargs,),,,,,,,,,,,,,"Upload a stream to Azure File Share.

        :param stream: Opened file/stream to upload as the file content.
        :type stream: file-like
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param count: Size of the stream in bytes
        :type count: int
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_stream()` takes.
        :type kwargs: object",Upload,a,stream,to,Azure,File,Share,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L193-L212,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.get_conn,"def get_conn(self):
        """"""
        Returns a Google Cloud Storage service object.
        """"""
        if not self._conn:
            self._conn = storage.Client(credentials=self._get_credentials())

        return self._conn",python,"def get_conn(self):
        """"""
        Returns a Google Cloud Storage service object.
        """"""
        if not self._conn:
            self._conn = storage.Client(credentials=self._get_credentials())

        return self._conn",def,get_conn,(,self,),:,if,not,self,.,_conn,:,self,.,_conn,=,storage,.,Client,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_conn,,,,,,,,,,,,,,,,,,,,,Returns a Google Cloud Storage service object.,Returns,a,Google,Cloud,Storage,service,object,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L45-L52,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.copy,"def copy(self, source_bucket, source_object, destination_bucket=None,
             destination_object=None):
        """"""
        Copies an object from a bucket to another, with renaming if requested.

        destination_bucket or destination_object can be omitted, in which case
        source bucket/object is used, but not both.

        :param source_bucket: The bucket of the object to copy from.
        :type source_bucket: str
        :param source_object: The object to copy.
        :type source_object: str
        :param destination_bucket: The destination of the object to copied to.
            Can be omitted; then the same bucket is used.
        :type destination_bucket: str
        :param destination_object: The (renamed) path of the object if given.
            Can be omitted; then the same name is used.
        :type destination_object: str
        """"""
        destination_bucket = destination_bucket or source_bucket
        destination_object = destination_object or source_object
        if source_bucket == destination_bucket and \
                source_object == destination_object:

            raise ValueError(
                'Either source/destination bucket or source/destination object '
                'must be different, not both the same: bucket=%s, object=%s' %
                (source_bucket, source_object))
        if not source_bucket or not source_object:
            raise ValueError('source_bucket and source_object cannot be empty.')

        client = self.get_conn()
        source_bucket = client.get_bucket(source_bucket)
        source_object = source_bucket.blob(source_object)
        destination_bucket = client.get_bucket(destination_bucket)
        destination_object = source_bucket.copy_blob(
            blob=source_object,
            destination_bucket=destination_bucket,
            new_name=destination_object)

        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',
                      source_object.name, source_bucket.name,
                      destination_object.name, destination_bucket.name)",python,"def copy(self, source_bucket, source_object, destination_bucket=None,
             destination_object=None):
        """"""
        Copies an object from a bucket to another, with renaming if requested.

        destination_bucket or destination_object can be omitted, in which case
        source bucket/object is used, but not both.

        :param source_bucket: The bucket of the object to copy from.
        :type source_bucket: str
        :param source_object: The object to copy.
        :type source_object: str
        :param destination_bucket: The destination of the object to copied to.
            Can be omitted; then the same bucket is used.
        :type destination_bucket: str
        :param destination_object: The (renamed) path of the object if given.
            Can be omitted; then the same name is used.
        :type destination_object: str
        """"""
        destination_bucket = destination_bucket or source_bucket
        destination_object = destination_object or source_object
        if source_bucket == destination_bucket and \
                source_object == destination_object:

            raise ValueError(
                'Either source/destination bucket or source/destination object '
                'must be different, not both the same: bucket=%s, object=%s' %
                (source_bucket, source_object))
        if not source_bucket or not source_object:
            raise ValueError('source_bucket and source_object cannot be empty.')

        client = self.get_conn()
        source_bucket = client.get_bucket(source_bucket)
        source_object = source_bucket.blob(source_object)
        destination_bucket = client.get_bucket(destination_bucket)
        destination_object = source_bucket.copy_blob(
            blob=source_object,
            destination_bucket=destination_bucket,
            new_name=destination_object)

        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',
                      source_object.name, source_bucket.name,
                      destination_object.name, destination_bucket.name)",def,copy,(,self,",",source_bucket,",",source_object,",",destination_bucket,=,None,",",destination_object,=,None,),:,destination_bucket,=,destination_bucket,or,source_bucket,destination_object,=,destination_object,or,source_object,if,source_bucket,==,destination_bucket,and,source_object,==,destination_object,:,raise,ValueError,(,'Either source/destination bucket or source/destination object ',"'must be different, not both the same: bucket=%s, object=%s'",%,(,source_bucket,",",source_object,),),if,not,source_bucket,"Copies an object from a bucket to another, with renaming if requested.

        destination_bucket or destination_object can be omitted, in which case
        source bucket/object is used, but not both.

        :param source_bucket: The bucket of the object to copy from.
        :type source_bucket: str
        :param source_object: The object to copy.
        :type source_object: str
        :param destination_bucket: The destination of the object to copied to.
            Can be omitted; then the same bucket is used.
        :type destination_bucket: str
        :param destination_object: The (renamed) path of the object if given.
            Can be omitted; then the same name is used.
        :type destination_object: str",Copies,an,object,from,a,bucket,to,another,with,renaming,if,requested,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L54-L96,test,or,not,source_object,:,raise,ValueError,(,'source_bucket and source_object cannot be empty.',),client,=,self,.,get_conn,(,),source_bucket,=,client,.,get_bucket,(,source_bucket,),source_object,=,source_bucket,.,blob,(,source_object,),destination_bucket,=,client,.,get_bucket,(,destination_bucket,),destination_object,=,source_bucket,.,copy_blob,(,blob,=,source_object,",",destination_bucket,=,destination_bucket,",",new_name,=,destination_object,),self,.,log,.,info,(,'Object %s in bucket %s copied to object %s in bucket %s',",",source_object,.,name,",",source_bucket,.,name,",",destination_object,.,name,",",destination_bucket,.,name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.download,"def download(self, bucket_name, object_name, filename=None):
        """"""
        Get a file from Google Cloud Storage.

        :param bucket_name: The bucket to fetch from.
        :type bucket_name: str
        :param object_name: The object to fetch.
        :type object_name: str
        :param filename: If set, a local file path where the file should be written to.
        :type filename: str
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name)
        blob = bucket.blob(blob_name=object_name)

        if filename:
            blob.download_to_filename(filename)
            self.log.info('File downloaded to %s', filename)

        return blob.download_as_string()",python,"def download(self, bucket_name, object_name, filename=None):
        """"""
        Get a file from Google Cloud Storage.

        :param bucket_name: The bucket to fetch from.
        :type bucket_name: str
        :param object_name: The object to fetch.
        :type object_name: str
        :param filename: If set, a local file path where the file should be written to.
        :type filename: str
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name)
        blob = bucket.blob(blob_name=object_name)

        if filename:
            blob.download_to_filename(filename)
            self.log.info('File downloaded to %s', filename)

        return blob.download_as_string()",def,download,(,self,",",bucket_name,",",object_name,",",filename,=,None,),:,client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,),blob,=,bucket,.,blob,(,blob_name,=,object_name,),if,filename,:,blob,.,download_to_filename,(,filename,),self,.,log,.,"Get a file from Google Cloud Storage.

        :param bucket_name: The bucket to fetch from.
        :type bucket_name: str
        :param object_name: The object to fetch.
        :type object_name: str
        :param filename: If set, a local file path where the file should be written to.
        :type filename: str",Get,a,file,from,Google,Cloud,Storage,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L152-L171,test,info,(,'File downloaded to %s',",",filename,),return,blob,.,download_as_string,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.upload,"def upload(self, bucket_name, object_name, filename,
               mime_type='application/octet-stream', gzip=False):
        """"""
        Uploads a local file to Google Cloud Storage.

        :param bucket_name: The bucket to upload to.
        :type bucket_name: str
        :param object_name: The object name to set when uploading the local file.
        :type object_name: str
        :param filename: The local file path to the file to be uploaded.
        :type filename: str
        :param mime_type: The MIME type to set when uploading the file.
        :type mime_type: str
        :param gzip: Option to compress file for upload
        :type gzip: bool
        """"""

        if gzip:
            filename_gz = filename + '.gz'

            with open(filename, 'rb') as f_in:
                with gz.open(filename_gz, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
                    filename = filename_gz

        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.blob(blob_name=object_name)
        blob.upload_from_filename(filename=filename,
                                  content_type=mime_type)

        if gzip:
            os.remove(filename)
        self.log.info('File %s uploaded to %s in %s bucket', filename, object_name, bucket_name)",python,"def upload(self, bucket_name, object_name, filename,
               mime_type='application/octet-stream', gzip=False):
        """"""
        Uploads a local file to Google Cloud Storage.

        :param bucket_name: The bucket to upload to.
        :type bucket_name: str
        :param object_name: The object name to set when uploading the local file.
        :type object_name: str
        :param filename: The local file path to the file to be uploaded.
        :type filename: str
        :param mime_type: The MIME type to set when uploading the file.
        :type mime_type: str
        :param gzip: Option to compress file for upload
        :type gzip: bool
        """"""

        if gzip:
            filename_gz = filename + '.gz'

            with open(filename, 'rb') as f_in:
                with gz.open(filename_gz, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
                    filename = filename_gz

        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.blob(blob_name=object_name)
        blob.upload_from_filename(filename=filename,
                                  content_type=mime_type)

        if gzip:
            os.remove(filename)
        self.log.info('File %s uploaded to %s in %s bucket', filename, object_name, bucket_name)",def,upload,(,self,",",bucket_name,",",object_name,",",filename,",",mime_type,=,'application/octet-stream',",",gzip,=,False,),:,if,gzip,:,filename_gz,=,filename,+,'.gz',with,open,(,filename,",",'rb',),as,f_in,:,with,gz,.,open,(,filename_gz,",",'wb',),as,f_out,:,shutil,.,"Uploads a local file to Google Cloud Storage.

        :param bucket_name: The bucket to upload to.
        :type bucket_name: str
        :param object_name: The object name to set when uploading the local file.
        :type object_name: str
        :param filename: The local file path to the file to be uploaded.
        :type filename: str
        :param mime_type: The MIME type to set when uploading the file.
        :type mime_type: str
        :param gzip: Option to compress file for upload
        :type gzip: bool",Uploads,a,local,file,to,Google,Cloud,Storage,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L173-L206,test,copyfileobj,(,f_in,",",f_out,),filename,=,filename_gz,client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),blob,=,bucket,.,blob,(,blob_name,=,object_name,),blob,.,upload_from_filename,(,filename,=,filename,",",content_type,=,mime_type,),if,gzip,:,os,.,remove,(,filename,),self,.,log,.,info,(,'File %s uploaded to %s in %s bucket',",",filename,",",object_name,",",bucket_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.exists,"def exists(self, bucket_name, object_name):
        """"""
        Checks for the existence of a file in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the object is.
        :type bucket_name: str
        :param object_name: The name of the blob_name to check in the Google cloud
            storage bucket.
        :type object_name: str
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.blob(blob_name=object_name)
        return blob.exists()",python,"def exists(self, bucket_name, object_name):
        """"""
        Checks for the existence of a file in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the object is.
        :type bucket_name: str
        :param object_name: The name of the blob_name to check in the Google cloud
            storage bucket.
        :type object_name: str
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.blob(blob_name=object_name)
        return blob.exists()",def,exists,(,self,",",bucket_name,",",object_name,),:,client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),blob,=,bucket,.,blob,(,blob_name,=,object_name,),return,blob,.,exists,(,),,,,,,,,,,"Checks for the existence of a file in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the object is.
        :type bucket_name: str
        :param object_name: The name of the blob_name to check in the Google cloud
            storage bucket.
        :type object_name: str",Checks,for,the,existence,of,a,file,in,Google,Cloud,Storage,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L208-L221,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.is_updated_after,"def is_updated_after(self, bucket_name, object_name, ts):
        """"""
        Checks if an blob_name is updated in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the object is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket.
        :type object_name: str
        :param ts: The timestamp to check against.
        :type ts: datetime.datetime
        """"""
        client = self.get_conn()
        bucket = storage.Bucket(client=client, name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()

        blob_update_time = blob.updated

        if blob_update_time is not None:
            import dateutil.tz

            if not ts.tzinfo:
                ts = ts.replace(tzinfo=dateutil.tz.tzutc())

            self.log.info(""Verify object date: %s > %s"", blob_update_time, ts)

            if blob_update_time > ts:
                return True

        return False",python,"def is_updated_after(self, bucket_name, object_name, ts):
        """"""
        Checks if an blob_name is updated in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the object is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket.
        :type object_name: str
        :param ts: The timestamp to check against.
        :type ts: datetime.datetime
        """"""
        client = self.get_conn()
        bucket = storage.Bucket(client=client, name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()

        blob_update_time = blob.updated

        if blob_update_time is not None:
            import dateutil.tz

            if not ts.tzinfo:
                ts = ts.replace(tzinfo=dateutil.tz.tzutc())

            self.log.info(""Verify object date: %s > %s"", blob_update_time, ts)

            if blob_update_time > ts:
                return True

        return False",def,is_updated_after,(,self,",",bucket_name,",",object_name,",",ts,),:,client,=,self,.,get_conn,(,),bucket,=,storage,.,Bucket,(,client,=,client,",",name,=,bucket_name,),blob,=,bucket,.,get_blob,(,blob_name,=,object_name,),blob,.,reload,(,),blob_update_time,=,blob,.,"Checks if an blob_name is updated in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the object is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket.
        :type object_name: str
        :param ts: The timestamp to check against.
        :type ts: datetime.datetime",Checks,if,an,blob_name,is,updated,in,Google,Cloud,Storage,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L223-L253,test,updated,if,blob_update_time,is,not,None,:,import,dateutil,.,tz,if,not,ts,.,tzinfo,:,ts,=,ts,.,replace,(,tzinfo,=,dateutil,.,tz,.,tzutc,(,),),self,.,log,.,info,(,"""Verify object date: %s > %s""",",",blob_update_time,",",ts,),if,blob_update_time,>,ts,:,return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.delete,"def delete(self, bucket_name, object_name):
        """"""
        Deletes an object from the bucket.

        :param bucket_name: name of the bucket, where the object resides
        :type bucket_name: str
        :param object_name: name of the object to delete
        :type object_name: str
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.blob(blob_name=object_name)
        blob.delete()

        self.log.info('Blob %s deleted.', object_name)",python,"def delete(self, bucket_name, object_name):
        """"""
        Deletes an object from the bucket.

        :param bucket_name: name of the bucket, where the object resides
        :type bucket_name: str
        :param object_name: name of the object to delete
        :type object_name: str
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.blob(blob_name=object_name)
        blob.delete()

        self.log.info('Blob %s deleted.', object_name)",def,delete,(,self,",",bucket_name,",",object_name,),:,client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),blob,=,bucket,.,blob,(,blob_name,=,object_name,),blob,.,delete,(,),self,.,log,.,info,(,'Blob %s deleted.',",",object_name,),"Deletes an object from the bucket.

        :param bucket_name: name of the bucket, where the object resides
        :type bucket_name: str
        :param object_name: name of the object to delete
        :type object_name: str",Deletes,an,object,from,the,bucket,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L255-L269,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.list,"def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None):
        """"""
        List all objects from the bucket with the give string prefix in name

        :param bucket_name: bucket name
        :type bucket_name: str
        :param versions: if true, list all versions of the objects
        :type versions: bool
        :param max_results: max count of items to return in a single page of responses
        :type max_results: int
        :param prefix: prefix string which filters objects whose name begin with
            this prefix
        :type prefix: str
        :param delimiter: filters objects based on the delimiter (for e.g '.csv')
        :type delimiter: str
        :return: a stream of object names matching the filtering criteria
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)

        ids = []
        pageToken = None
        while True:
            blobs = bucket.list_blobs(
                max_results=max_results,
                page_token=pageToken,
                prefix=prefix,
                delimiter=delimiter,
                versions=versions
            )

            blob_names = []
            for blob in blobs:
                blob_names.append(blob.name)

            prefixes = blobs.prefixes
            if prefixes:
                ids += list(prefixes)
            else:
                ids += blob_names

            pageToken = blobs.next_page_token
            if pageToken is None:
                # empty next page token
                break
        return ids",python,"def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None):
        """"""
        List all objects from the bucket with the give string prefix in name

        :param bucket_name: bucket name
        :type bucket_name: str
        :param versions: if true, list all versions of the objects
        :type versions: bool
        :param max_results: max count of items to return in a single page of responses
        :type max_results: int
        :param prefix: prefix string which filters objects whose name begin with
            this prefix
        :type prefix: str
        :param delimiter: filters objects based on the delimiter (for e.g '.csv')
        :type delimiter: str
        :return: a stream of object names matching the filtering criteria
        """"""
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)

        ids = []
        pageToken = None
        while True:
            blobs = bucket.list_blobs(
                max_results=max_results,
                page_token=pageToken,
                prefix=prefix,
                delimiter=delimiter,
                versions=versions
            )

            blob_names = []
            for blob in blobs:
                blob_names.append(blob.name)

            prefixes = blobs.prefixes
            if prefixes:
                ids += list(prefixes)
            else:
                ids += blob_names

            pageToken = blobs.next_page_token
            if pageToken is None:
                # empty next page token
                break
        return ids",def,list,(,self,",",bucket_name,",",versions,=,None,",",max_results,=,None,",",prefix,=,None,",",delimiter,=,None,),:,client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),ids,=,[,],pageToken,=,None,while,True,:,blobs,"List all objects from the bucket with the give string prefix in name

        :param bucket_name: bucket name
        :type bucket_name: str
        :param versions: if true, list all versions of the objects
        :type versions: bool
        :param max_results: max count of items to return in a single page of responses
        :type max_results: int
        :param prefix: prefix string which filters objects whose name begin with
            this prefix
        :type prefix: str
        :param delimiter: filters objects based on the delimiter (for e.g '.csv')
        :type delimiter: str
        :return: a stream of object names matching the filtering criteria",List,all,objects,from,the,bucket,with,the,give,string,prefix,in,name,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L271-L316,test,=,bucket,.,list_blobs,(,max_results,=,max_results,",",page_token,=,pageToken,",",prefix,=,prefix,",",delimiter,=,delimiter,",",versions,=,versions,),blob_names,=,[,],for,blob,in,blobs,:,blob_names,.,append,(,blob,.,name,),prefixes,=,blobs,.,prefixes,if,prefixes,:,ids,+=,list,(,prefixes,),else,:,ids,+=,blob_names,pageToken,=,blobs,.,next_page_token,if,pageToken,is,None,:,# empty next page token,break,return,ids,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.get_size,"def get_size(self, bucket_name, object_name):
        """"""
        Gets the size of a file in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google
            cloud storage bucket_name.
        :type object_name: str

        """"""
        self.log.info('Checking the file size of object: %s in bucket_name: %s',
                      object_name,
                      bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()
        blob_size = blob.size
        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)
        return blob_size",python,"def get_size(self, bucket_name, object_name):
        """"""
        Gets the size of a file in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google
            cloud storage bucket_name.
        :type object_name: str

        """"""
        self.log.info('Checking the file size of object: %s in bucket_name: %s',
                      object_name,
                      bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()
        blob_size = blob.size
        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)
        return blob_size",def,get_size,(,self,",",bucket_name,",",object_name,),:,self,.,log,.,info,(,'Checking the file size of object: %s in bucket_name: %s',",",object_name,",",bucket_name,),client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),blob,=,bucket,.,get_blob,(,blob_name,=,object_name,),blob,.,reload,"Gets the size of a file in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google
            cloud storage bucket_name.
        :type object_name: str",Gets,the,size,of,a,file,in,Google,Cloud,Storage,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L318-L338,test,(,),blob_size,=,blob,.,size,self,.,log,.,info,(,'The file size of %s is %s bytes.',",",object_name,",",blob_size,),return,blob_size,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.get_crc32c,"def get_crc32c(self, bucket_name, object_name):
        """"""
        Gets the CRC32c checksum of an object in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket_name.
        :type object_name: str
        """"""
        self.log.info('Retrieving the crc32c checksum of '
                      'object_name: %s in bucket_name: %s', object_name, bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()
        blob_crc32c = blob.crc32c
        self.log.info('The crc32c checksum of %s is %s', object_name, blob_crc32c)
        return blob_crc32c",python,"def get_crc32c(self, bucket_name, object_name):
        """"""
        Gets the CRC32c checksum of an object in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket_name.
        :type object_name: str
        """"""
        self.log.info('Retrieving the crc32c checksum of '
                      'object_name: %s in bucket_name: %s', object_name, bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()
        blob_crc32c = blob.crc32c
        self.log.info('The crc32c checksum of %s is %s', object_name, blob_crc32c)
        return blob_crc32c",def,get_crc32c,(,self,",",bucket_name,",",object_name,),:,self,.,log,.,info,(,'Retrieving the crc32c checksum of ','object_name: %s in bucket_name: %s',",",object_name,",",bucket_name,),client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),blob,=,bucket,.,get_blob,(,blob_name,=,object_name,),blob,.,"Gets the CRC32c checksum of an object in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket_name.
        :type object_name: str",Gets,the,CRC32c,checksum,of,an,object,in,Google,Cloud,Storage,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L340-L358,test,reload,(,),blob_crc32c,=,blob,.,crc32c,self,.,log,.,info,(,'The crc32c checksum of %s is %s',",",object_name,",",blob_crc32c,),return,blob_crc32c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.get_md5hash,"def get_md5hash(self, bucket_name, object_name):
        """"""
        Gets the MD5 hash of an object in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket_name.
        :type object_name: str
        """"""
        self.log.info('Retrieving the MD5 hash of '
                      'object: %s in bucket: %s', object_name, bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()
        blob_md5hash = blob.md5_hash
        self.log.info('The md5Hash of %s is %s', object_name, blob_md5hash)
        return blob_md5hash",python,"def get_md5hash(self, bucket_name, object_name):
        """"""
        Gets the MD5 hash of an object in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket_name.
        :type object_name: str
        """"""
        self.log.info('Retrieving the MD5 hash of '
                      'object: %s in bucket: %s', object_name, bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name=bucket_name)
        blob = bucket.get_blob(blob_name=object_name)
        blob.reload()
        blob_md5hash = blob.md5_hash
        self.log.info('The md5Hash of %s is %s', object_name, blob_md5hash)
        return blob_md5hash",def,get_md5hash,(,self,",",bucket_name,",",object_name,),:,self,.,log,.,info,(,'Retrieving the MD5 hash of ','object: %s in bucket: %s',",",object_name,",",bucket_name,),client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,=,bucket_name,),blob,=,bucket,.,get_blob,(,blob_name,=,object_name,),blob,.,"Gets the MD5 hash of an object in Google Cloud Storage.

        :param bucket_name: The Google cloud storage bucket where the blob_name is.
        :type bucket_name: str
        :param object_name: The name of the object to check in the Google cloud
            storage bucket_name.
        :type object_name: str",Gets,the,MD5,hash,of,an,object,in,Google,Cloud,Storage,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L360-L378,test,reload,(,),blob_md5hash,=,blob,.,md5_hash,self,.,log,.,info,(,'The md5Hash of %s is %s',",",object_name,",",blob_md5hash,),return,blob_md5hash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.create_bucket,"def create_bucket(self,
                      bucket_name,
                      resource=None,
                      storage_class='MULTI_REGIONAL',
                      location='US',
                      project_id=None,
                      labels=None
                      ):
        """"""
        Creates a new bucket. Google Cloud Storage uses a flat namespace, so
        you can't create a bucket with a name that is already in use.

        .. seealso::
            For more information, see Bucket Naming Guidelines:
            https://cloud.google.com/storage/docs/bucketnaming.html#requirements

        :param bucket_name: The name of the bucket.
        :type bucket_name: str
        :param resource: An optional dict with parameters for creating the bucket.
            For information on available parameters, see Cloud Storage API doc:
            https://cloud.google.com/storage/docs/json_api/v1/buckets/insert
        :type resource: dict
        :param storage_class: This defines how objects in the bucket are stored
            and determines the SLA and the cost of storage. Values include

            - ``MULTI_REGIONAL``
            - ``REGIONAL``
            - ``STANDARD``
            - ``NEARLINE``
            - ``COLDLINE``.

            If this value is not specified when the bucket is
            created, it will default to STANDARD.
        :type storage_class: str
        :param location: The location of the bucket.
            Object data for objects in the bucket resides in physical storage
            within this region. Defaults to US.

            .. seealso::
                https://developers.google.com/storage/docs/bucket-locations

        :type location: str
        :param project_id: The ID of the GCP Project.
        :type project_id: str
        :param labels: User-provided labels, in key/value pairs.
        :type labels: dict
        :return: If successful, it returns the ``id`` of the bucket.
        """"""

        self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',
                      bucket_name, location, storage_class)

        client = self.get_conn()
        bucket = client.bucket(bucket_name=bucket_name)
        bucket_resource = resource or {}

        for item in bucket_resource:
            if item != ""name"":
                bucket._patch_property(name=item, value=resource[item])

        bucket.storage_class = storage_class
        bucket.labels = labels or {}
        bucket.create(project=project_id, location=location)
        return bucket.id",python,"def create_bucket(self,
                      bucket_name,
                      resource=None,
                      storage_class='MULTI_REGIONAL',
                      location='US',
                      project_id=None,
                      labels=None
                      ):
        """"""
        Creates a new bucket. Google Cloud Storage uses a flat namespace, so
        you can't create a bucket with a name that is already in use.

        .. seealso::
            For more information, see Bucket Naming Guidelines:
            https://cloud.google.com/storage/docs/bucketnaming.html#requirements

        :param bucket_name: The name of the bucket.
        :type bucket_name: str
        :param resource: An optional dict with parameters for creating the bucket.
            For information on available parameters, see Cloud Storage API doc:
            https://cloud.google.com/storage/docs/json_api/v1/buckets/insert
        :type resource: dict
        :param storage_class: This defines how objects in the bucket are stored
            and determines the SLA and the cost of storage. Values include

            - ``MULTI_REGIONAL``
            - ``REGIONAL``
            - ``STANDARD``
            - ``NEARLINE``
            - ``COLDLINE``.

            If this value is not specified when the bucket is
            created, it will default to STANDARD.
        :type storage_class: str
        :param location: The location of the bucket.
            Object data for objects in the bucket resides in physical storage
            within this region. Defaults to US.

            .. seealso::
                https://developers.google.com/storage/docs/bucket-locations

        :type location: str
        :param project_id: The ID of the GCP Project.
        :type project_id: str
        :param labels: User-provided labels, in key/value pairs.
        :type labels: dict
        :return: If successful, it returns the ``id`` of the bucket.
        """"""

        self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',
                      bucket_name, location, storage_class)

        client = self.get_conn()
        bucket = client.bucket(bucket_name=bucket_name)
        bucket_resource = resource or {}

        for item in bucket_resource:
            if item != ""name"":
                bucket._patch_property(name=item, value=resource[item])

        bucket.storage_class = storage_class
        bucket.labels = labels or {}
        bucket.create(project=project_id, location=location)
        return bucket.id",def,create_bucket,(,self,",",bucket_name,",",resource,=,None,",",storage_class,=,'MULTI_REGIONAL',",",location,=,'US',",",project_id,=,None,",",labels,=,None,),:,self,.,log,.,info,(,'Creating Bucket: %s; Location: %s; Storage Class: %s',",",bucket_name,",",location,",",storage_class,),client,=,self,.,get_conn,(,),bucket,=,client,"Creates a new bucket. Google Cloud Storage uses a flat namespace, so
        you can't create a bucket with a name that is already in use.

        .. seealso::
            For more information, see Bucket Naming Guidelines:
            https://cloud.google.com/storage/docs/bucketnaming.html#requirements

        :param bucket_name: The name of the bucket.
        :type bucket_name: str
        :param resource: An optional dict with parameters for creating the bucket.
            For information on available parameters, see Cloud Storage API doc:
            https://cloud.google.com/storage/docs/json_api/v1/buckets/insert
        :type resource: dict
        :param storage_class: This defines how objects in the bucket are stored
            and determines the SLA and the cost of storage. Values include

            - ``MULTI_REGIONAL``
            - ``REGIONAL``
            - ``STANDARD``
            - ``NEARLINE``
            - ``COLDLINE``.

            If this value is not specified when the bucket is
            created, it will default to STANDARD.
        :type storage_class: str
        :param location: The location of the bucket.
            Object data for objects in the bucket resides in physical storage
            within this region. Defaults to US.

            .. seealso::
                https://developers.google.com/storage/docs/bucket-locations

        :type location: str
        :param project_id: The ID of the GCP Project.
        :type project_id: str
        :param labels: User-provided labels, in key/value pairs.
        :type labels: dict
        :return: If successful, it returns the ``id`` of the bucket.",Creates,a,new,bucket,.,Google,Cloud,Storage,uses,a,flat,namespace,so,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L382-L445,test,.,bucket,(,bucket_name,=,bucket_name,),bucket_resource,=,resource,or,{,},for,item,in,bucket_resource,:,if,item,!=,"""name""",:,bucket,.,_patch_property,(,name,=,item,",",value,=,resource,[,item,],),bucket,.,storage_class,=,storage_class,bucket,.,labels,=,labels,or,{,},bucket,.,create,(,project,=,project_id,",",location,=,location,),return,bucket,.,id,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,you,can,t,create,a,bucket,with,a,name,that,is,already,in,use,.,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcs_hook.py,GoogleCloudStorageHook.compose,"def compose(self, bucket_name, source_objects, destination_object):
        """"""
        Composes a list of existing object into a new object in the same storage bucket_name

        Currently it only supports up to 32 objects that can be concatenated
        in a single operation

        https://cloud.google.com/storage/docs/json_api/v1/objects/compose

        :param bucket_name: The name of the bucket containing the source objects.
            This is also the same bucket to store the composed destination object.
        :type bucket_name: str
        :param source_objects: The list of source objects that will be composed
            into a single object.
        :type source_objects: list
        :param destination_object: The path of the object if given.
        :type destination_object: str
        """"""

        if not source_objects or not len(source_objects):
            raise ValueError('source_objects cannot be empty.')

        if not bucket_name or not destination_object:
            raise ValueError('bucket_name and destination_object cannot be empty.')

        self.log.info(""Composing %s to %s in the bucket %s"",
                      source_objects, destination_object, bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name)
        destination_blob = bucket.blob(destination_object)
        destination_blob.compose(
            sources=[
                bucket.blob(blob_name=source_object) for source_object in source_objects
            ])

        self.log.info(""Completed successfully."")",python,"def compose(self, bucket_name, source_objects, destination_object):
        """"""
        Composes a list of existing object into a new object in the same storage bucket_name

        Currently it only supports up to 32 objects that can be concatenated
        in a single operation

        https://cloud.google.com/storage/docs/json_api/v1/objects/compose

        :param bucket_name: The name of the bucket containing the source objects.
            This is also the same bucket to store the composed destination object.
        :type bucket_name: str
        :param source_objects: The list of source objects that will be composed
            into a single object.
        :type source_objects: list
        :param destination_object: The path of the object if given.
        :type destination_object: str
        """"""

        if not source_objects or not len(source_objects):
            raise ValueError('source_objects cannot be empty.')

        if not bucket_name or not destination_object:
            raise ValueError('bucket_name and destination_object cannot be empty.')

        self.log.info(""Composing %s to %s in the bucket %s"",
                      source_objects, destination_object, bucket_name)
        client = self.get_conn()
        bucket = client.get_bucket(bucket_name)
        destination_blob = bucket.blob(destination_object)
        destination_blob.compose(
            sources=[
                bucket.blob(blob_name=source_object) for source_object in source_objects
            ])

        self.log.info(""Completed successfully."")",def,compose,(,self,",",bucket_name,",",source_objects,",",destination_object,),:,if,not,source_objects,or,not,len,(,source_objects,),:,raise,ValueError,(,'source_objects cannot be empty.',),if,not,bucket_name,or,not,destination_object,:,raise,ValueError,(,'bucket_name and destination_object cannot be empty.',),self,.,log,.,info,(,"""Composing %s to %s in the bucket %s""",",",source_objects,",",destination_object,",",bucket_name,"Composes a list of existing object into a new object in the same storage bucket_name

        Currently it only supports up to 32 objects that can be concatenated
        in a single operation

        https://cloud.google.com/storage/docs/json_api/v1/objects/compose

        :param bucket_name: The name of the bucket containing the source objects.
            This is also the same bucket to store the composed destination object.
        :type bucket_name: str
        :param source_objects: The list of source objects that will be composed
            into a single object.
        :type source_objects: list
        :param destination_object: The path of the object if given.
        :type destination_object: str",Composes,a,list,of,existing,object,into,a,new,object,in,the,same,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L515-L550,test,),client,=,self,.,get_conn,(,),bucket,=,client,.,get_bucket,(,bucket_name,),destination_blob,=,bucket,.,blob,(,destination_object,),destination_blob,.,compose,(,sources,=,[,bucket,.,blob,(,blob_name,=,source_object,),for,source_object,in,source_objects,],),self,.,log,.,info,(,"""Completed successfully.""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,storage,bucket_name,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,secondary_training_status_changed,"def secondary_training_status_changed(current_job_description, prev_job_description):
    """"""
    Returns true if training job's secondary status message has changed.

    :param current_job_description: Current job description, returned from DescribeTrainingJob call.
    :type current_job_description: dict
    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.
    :type prev_job_description: dict

    :return: Whether the secondary status message of a training job changed or not.
    """"""
    current_secondary_status_transitions = current_job_description.get('SecondaryStatusTransitions')
    if current_secondary_status_transitions is None or len(current_secondary_status_transitions) == 0:
        return False

    prev_job_secondary_status_transitions = prev_job_description.get('SecondaryStatusTransitions') \
        if prev_job_description is not None else None

    last_message = prev_job_secondary_status_transitions[-1]['StatusMessage'] \
        if prev_job_secondary_status_transitions is not None \
        and len(prev_job_secondary_status_transitions) > 0 else ''

    message = current_job_description['SecondaryStatusTransitions'][-1]['StatusMessage']

    return message != last_message",python,"def secondary_training_status_changed(current_job_description, prev_job_description):
    """"""
    Returns true if training job's secondary status message has changed.

    :param current_job_description: Current job description, returned from DescribeTrainingJob call.
    :type current_job_description: dict
    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.
    :type prev_job_description: dict

    :return: Whether the secondary status message of a training job changed or not.
    """"""
    current_secondary_status_transitions = current_job_description.get('SecondaryStatusTransitions')
    if current_secondary_status_transitions is None or len(current_secondary_status_transitions) == 0:
        return False

    prev_job_secondary_status_transitions = prev_job_description.get('SecondaryStatusTransitions') \
        if prev_job_description is not None else None

    last_message = prev_job_secondary_status_transitions[-1]['StatusMessage'] \
        if prev_job_secondary_status_transitions is not None \
        and len(prev_job_secondary_status_transitions) > 0 else ''

    message = current_job_description['SecondaryStatusTransitions'][-1]['StatusMessage']

    return message != last_message",def,secondary_training_status_changed,(,current_job_description,",",prev_job_description,),:,current_secondary_status_transitions,=,current_job_description,.,get,(,'SecondaryStatusTransitions',),if,current_secondary_status_transitions,is,None,or,len,(,current_secondary_status_transitions,),==,0,:,return,False,prev_job_secondary_status_transitions,=,prev_job_description,.,get,(,'SecondaryStatusTransitions',),if,prev_job_description,is,not,None,else,None,last_message,=,prev_job_secondary_status_transitions,[,-,1,],"Returns true if training job's secondary status message has changed.

    :param current_job_description: Current job description, returned from DescribeTrainingJob call.
    :type current_job_description: dict
    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.
    :type prev_job_description: dict

    :return: Whether the secondary status message of a training job changed or not.",Returns,true,if,training,job,s,secondary,status,message,has,changed,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L59-L83,test,[,'StatusMessage',],if,prev_job_secondary_status_transitions,is,not,None,and,len,(,prev_job_secondary_status_transitions,),>,0,else,'',message,=,current_job_description,[,'SecondaryStatusTransitions',],[,-,1,],[,'StatusMessage',],return,message,!=,last_message,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,secondary_training_status_message,"def secondary_training_status_message(job_description, prev_description):
    """"""
    Returns a string contains start time and the secondary training job status message.

    :param job_description: Returned response from DescribeTrainingJob call
    :type job_description: dict
    :param prev_description: Previous job description from DescribeTrainingJob call
    :type prev_description: dict

    :return: Job status string to be printed.
    """"""

    if job_description is None or job_description.get('SecondaryStatusTransitions') is None\
            or len(job_description.get('SecondaryStatusTransitions')) == 0:
        return ''

    prev_description_secondary_transitions = prev_description.get('SecondaryStatusTransitions')\
        if prev_description is not None else None
    prev_transitions_num = len(prev_description['SecondaryStatusTransitions'])\
        if prev_description_secondary_transitions is not None else 0
    current_transitions = job_description['SecondaryStatusTransitions']

    transitions_to_print = current_transitions[-1:] if len(current_transitions) == prev_transitions_num else \
        current_transitions[prev_transitions_num - len(current_transitions):]

    status_strs = []
    for transition in transitions_to_print:
        message = transition['StatusMessage']
        time_str = timezone.convert_to_utc(job_description['LastModifiedTime']).strftime('%Y-%m-%d %H:%M:%S')
        status_strs.append('{} {} - {}'.format(time_str, transition['Status'], message))

    return '\n'.join(status_strs)",python,"def secondary_training_status_message(job_description, prev_description):
    """"""
    Returns a string contains start time and the secondary training job status message.

    :param job_description: Returned response from DescribeTrainingJob call
    :type job_description: dict
    :param prev_description: Previous job description from DescribeTrainingJob call
    :type prev_description: dict

    :return: Job status string to be printed.
    """"""

    if job_description is None or job_description.get('SecondaryStatusTransitions') is None\
            or len(job_description.get('SecondaryStatusTransitions')) == 0:
        return ''

    prev_description_secondary_transitions = prev_description.get('SecondaryStatusTransitions')\
        if prev_description is not None else None
    prev_transitions_num = len(prev_description['SecondaryStatusTransitions'])\
        if prev_description_secondary_transitions is not None else 0
    current_transitions = job_description['SecondaryStatusTransitions']

    transitions_to_print = current_transitions[-1:] if len(current_transitions) == prev_transitions_num else \
        current_transitions[prev_transitions_num - len(current_transitions):]

    status_strs = []
    for transition in transitions_to_print:
        message = transition['StatusMessage']
        time_str = timezone.convert_to_utc(job_description['LastModifiedTime']).strftime('%Y-%m-%d %H:%M:%S')
        status_strs.append('{} {} - {}'.format(time_str, transition['Status'], message))

    return '\n'.join(status_strs)",def,secondary_training_status_message,(,job_description,",",prev_description,),:,if,job_description,is,None,or,job_description,.,get,(,'SecondaryStatusTransitions',),is,None,or,len,(,job_description,.,get,(,'SecondaryStatusTransitions',),),==,0,:,return,'',prev_description_secondary_transitions,=,prev_description,.,get,(,'SecondaryStatusTransitions',),if,prev_description,is,not,None,else,None,prev_transitions_num,"Returns a string contains start time and the secondary training job status message.

    :param job_description: Returned response from DescribeTrainingJob call
    :type job_description: dict
    :param prev_description: Previous job description from DescribeTrainingJob call
    :type prev_description: dict

    :return: Job status string to be printed.",Returns,a,string,contains,start,time,and,the,secondary,training,job,status,message,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L86-L117,test,=,len,(,prev_description,[,'SecondaryStatusTransitions',],),if,prev_description_secondary_transitions,is,not,None,else,0,current_transitions,=,job_description,[,'SecondaryStatusTransitions',],transitions_to_print,=,current_transitions,[,-,1,:,],if,len,(,current_transitions,),==,prev_transitions_num,else,current_transitions,[,prev_transitions_num,-,len,(,current_transitions,),:,],status_strs,=,[,],for,transition,in,transitions_to_print,:,message,=,transition,[,'StatusMessage',],time_str,=,timezone,.,convert_to_utc,(,job_description,[,'LastModifiedTime',],),.,strftime,(,'%Y-%m-%d %H:%M:%S',),status_strs,.,append,(,'{} {} - {}',.,format,(,time_str,",",transition,[,'Status',],",",message,),),return,'\n',.,join,(,status_strs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.tar_and_s3_upload,"def tar_and_s3_upload(self, path, key, bucket):
        """"""
        Tar the local file or directory and upload to s3

        :param path: local file or directory
        :type path: str
        :param key: s3 key
        :type key: str
        :param bucket: s3 bucket
        :type bucket: str
        :return: None
        """"""
        with tempfile.TemporaryFile() as temp_file:
            if os.path.isdir(path):
                files = [os.path.join(path, name) for name in os.listdir(path)]
            else:
                files = [path]
            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:
                for f in files:
                    tar_file.add(f, arcname=os.path.basename(f))
            temp_file.seek(0)
            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)",python,"def tar_and_s3_upload(self, path, key, bucket):
        """"""
        Tar the local file or directory and upload to s3

        :param path: local file or directory
        :type path: str
        :param key: s3 key
        :type key: str
        :param bucket: s3 bucket
        :type bucket: str
        :return: None
        """"""
        with tempfile.TemporaryFile() as temp_file:
            if os.path.isdir(path):
                files = [os.path.join(path, name) for name in os.listdir(path)]
            else:
                files = [path]
            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:
                for f in files:
                    tar_file.add(f, arcname=os.path.basename(f))
            temp_file.seek(0)
            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)",def,tar_and_s3_upload,(,self,",",path,",",key,",",bucket,),:,with,tempfile,.,TemporaryFile,(,),as,temp_file,:,if,os,.,path,.,isdir,(,path,),:,files,=,[,os,.,path,.,join,(,path,",",name,),for,name,in,os,.,listdir,(,path,"Tar the local file or directory and upload to s3

        :param path: local file or directory
        :type path: str
        :param key: s3 key
        :type key: str
        :param bucket: s3 bucket
        :type bucket: str
        :return: None",Tar,the,local,file,or,directory,and,upload,to,s3,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L134-L155,test,),],else,:,files,=,[,path,],with,tarfile,.,open,(,mode,=,'w:gz',",",fileobj,=,temp_file,),as,tar_file,:,for,f,in,files,:,tar_file,.,add,(,f,",",arcname,=,os,.,path,.,basename,(,f,),),temp_file,.,seek,(,0,),self,.,s3_hook,.,load_file_obj,(,temp_file,",",key,",",bucket,",",replace,=,True,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.configure_s3_resources,"def configure_s3_resources(self, config):
        """"""
        Extract the S3 operations from the configuration and execute them.

        :param config: config of SageMaker operation
        :type config: dict
        :rtype: dict
        """"""
        s3_operations = config.pop('S3Operations', None)

        if s3_operations is not None:
            create_bucket_ops = s3_operations.get('S3CreateBucket', [])
            upload_ops = s3_operations.get('S3Upload', [])
            for op in create_bucket_ops:
                self.s3_hook.create_bucket(bucket_name=op['Bucket'])
            for op in upload_ops:
                if op['Tar']:
                    self.tar_and_s3_upload(op['Path'], op['Key'],
                                           op['Bucket'])
                else:
                    self.s3_hook.load_file(op['Path'], op['Key'],
                                           op['Bucket'])",python,"def configure_s3_resources(self, config):
        """"""
        Extract the S3 operations from the configuration and execute them.

        :param config: config of SageMaker operation
        :type config: dict
        :rtype: dict
        """"""
        s3_operations = config.pop('S3Operations', None)

        if s3_operations is not None:
            create_bucket_ops = s3_operations.get('S3CreateBucket', [])
            upload_ops = s3_operations.get('S3Upload', [])
            for op in create_bucket_ops:
                self.s3_hook.create_bucket(bucket_name=op['Bucket'])
            for op in upload_ops:
                if op['Tar']:
                    self.tar_and_s3_upload(op['Path'], op['Key'],
                                           op['Bucket'])
                else:
                    self.s3_hook.load_file(op['Path'], op['Key'],
                                           op['Bucket'])",def,configure_s3_resources,(,self,",",config,),:,s3_operations,=,config,.,pop,(,'S3Operations',",",None,),if,s3_operations,is,not,None,:,create_bucket_ops,=,s3_operations,.,get,(,'S3CreateBucket',",",[,],),upload_ops,=,s3_operations,.,get,(,'S3Upload',",",[,],),for,op,in,create_bucket_ops,:,self,"Extract the S3 operations from the configuration and execute them.

        :param config: config of SageMaker operation
        :type config: dict
        :rtype: dict",Extract,the,S3,operations,from,the,configuration,and,execute,them,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L157-L178,test,.,s3_hook,.,create_bucket,(,bucket_name,=,op,[,'Bucket',],),for,op,in,upload_ops,:,if,op,[,'Tar',],:,self,.,tar_and_s3_upload,(,op,[,'Path',],",",op,[,'Key',],",",op,[,'Bucket',],),else,:,self,.,s3_hook,.,load_file,(,op,[,'Path',],",",op,[,'Key',],",",op,[,'Bucket',],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.check_s3_url,"def check_s3_url(self, s3url):
        """"""
        Check if an S3 URL exists

        :param s3url: S3 url
        :type s3url: str
        :rtype: bool
        """"""
        bucket, key = S3Hook.parse_s3_url(s3url)
        if not self.s3_hook.check_for_bucket(bucket_name=bucket):
            raise AirflowException(
                ""The input S3 Bucket {} does not exist "".format(bucket))
        if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket)\
           and not self.s3_hook.check_for_prefix(
                prefix=key, bucket_name=bucket, delimiter='/'):
            # check if s3 key exists in the case user provides a single file
            # or if s3 prefix exists in the case user provides multiple files in
            # a prefix
            raise AirflowException(""The input S3 Key ""
                                   ""or Prefix {} does not exist in the Bucket {}""
                                   .format(s3url, bucket))
        return True",python,"def check_s3_url(self, s3url):
        """"""
        Check if an S3 URL exists

        :param s3url: S3 url
        :type s3url: str
        :rtype: bool
        """"""
        bucket, key = S3Hook.parse_s3_url(s3url)
        if not self.s3_hook.check_for_bucket(bucket_name=bucket):
            raise AirflowException(
                ""The input S3 Bucket {} does not exist "".format(bucket))
        if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket)\
           and not self.s3_hook.check_for_prefix(
                prefix=key, bucket_name=bucket, delimiter='/'):
            # check if s3 key exists in the case user provides a single file
            # or if s3 prefix exists in the case user provides multiple files in
            # a prefix
            raise AirflowException(""The input S3 Key ""
                                   ""or Prefix {} does not exist in the Bucket {}""
                                   .format(s3url, bucket))
        return True",def,check_s3_url,(,self,",",s3url,),:,bucket,",",key,=,S3Hook,.,parse_s3_url,(,s3url,),if,not,self,.,s3_hook,.,check_for_bucket,(,bucket_name,=,bucket,),:,raise,AirflowException,(,"""The input S3 Bucket {} does not exist """,.,format,(,bucket,),),if,key,and,not,self,.,s3_hook,.,check_for_key,(,key,"Check if an S3 URL exists

        :param s3url: S3 url
        :type s3url: str
        :rtype: bool",Check,if,an,S3,URL,exists,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L180-L201,test,=,key,",",bucket_name,=,bucket,),and,not,self,.,s3_hook,.,check_for_prefix,(,prefix,=,key,",",bucket_name,=,bucket,",",delimiter,=,'/',),:,# check if s3 key exists in the case user provides a single file,# or if s3 prefix exists in the case user provides multiple files in,# a prefix,raise,AirflowException,(,"""The input S3 Key ""","""or Prefix {} does not exist in the Bucket {}""",.,format,(,s3url,",",bucket,),),return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.get_log_conn,"def get_log_conn(self):
        """"""
        Establish an AWS connection for retrieving logs during training

        :rtype: CloudWatchLogs.Client
        """"""
        config = botocore.config.Config(retries={'max_attempts': 15})
        return self.get_client_type('logs', config=config)",python,"def get_log_conn(self):
        """"""
        Establish an AWS connection for retrieving logs during training

        :rtype: CloudWatchLogs.Client
        """"""
        config = botocore.config.Config(retries={'max_attempts': 15})
        return self.get_client_type('logs', config=config)",def,get_log_conn,(,self,),:,config,=,botocore,.,config,.,Config,(,retries,=,{,'max_attempts',:,15,},),return,self,.,get_client_type,(,'logs',",",config,=,config,),,,,,,,,,,,,,,,,,,,,"Establish an AWS connection for retrieving logs during training

        :rtype: CloudWatchLogs.Client",Establish,an,AWS,connection,for,retrieving,logs,during,training,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L233-L240,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.create_training_job,"def create_training_job(self, config, wait_for_completion=True, print_log=True,
                            check_interval=30, max_ingestion_time=None):
        """"""
        Create a training job

        :param config: the config for training
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to training job creation
        """"""

        self.check_training_config(config)

        response = self.get_conn().create_training_job(**config)
        if print_log:
            self.check_training_status_with_log(config['TrainingJobName'],
                                                self.non_terminal_states,
                                                self.failed_states,
                                                wait_for_completion,
                                                check_interval, max_ingestion_time
                                                )
        elif wait_for_completion:
            describe_response = self.check_status(config['TrainingJobName'],
                                                  'TrainingJobStatus',
                                                  self.describe_training_job,
                                                  check_interval, max_ingestion_time
                                                  )

            billable_time = \
                (describe_response['TrainingEndTime'] - describe_response['TrainingStartTime']) * \
                describe_response['ResourceConfig']['InstanceCount']
            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))

        return response",python,"def create_training_job(self, config, wait_for_completion=True, print_log=True,
                            check_interval=30, max_ingestion_time=None):
        """"""
        Create a training job

        :param config: the config for training
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to training job creation
        """"""

        self.check_training_config(config)

        response = self.get_conn().create_training_job(**config)
        if print_log:
            self.check_training_status_with_log(config['TrainingJobName'],
                                                self.non_terminal_states,
                                                self.failed_states,
                                                wait_for_completion,
                                                check_interval, max_ingestion_time
                                                )
        elif wait_for_completion:
            describe_response = self.check_status(config['TrainingJobName'],
                                                  'TrainingJobStatus',
                                                  self.describe_training_job,
                                                  check_interval, max_ingestion_time
                                                  )

            billable_time = \
                (describe_response['TrainingEndTime'] - describe_response['TrainingStartTime']) * \
                describe_response['ResourceConfig']['InstanceCount']
            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))

        return response",def,create_training_job,(,self,",",config,",",wait_for_completion,=,True,",",print_log,=,True,",",check_interval,=,30,",",max_ingestion_time,=,None,),:,self,.,check_training_config,(,config,),response,=,self,.,get_conn,(,),.,create_training_job,(,*,*,config,),if,print_log,:,self,.,check_training_status_with_log,(,config,"Create a training job

        :param config: the config for training
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to training job creation",Create,a,training,job,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L325-L366,test,[,'TrainingJobName',],",",self,.,non_terminal_states,",",self,.,failed_states,",",wait_for_completion,",",check_interval,",",max_ingestion_time,),elif,wait_for_completion,:,describe_response,=,self,.,check_status,(,config,[,'TrainingJobName',],",",'TrainingJobStatus',",",self,.,describe_training_job,",",check_interval,",",max_ingestion_time,),billable_time,=,(,describe_response,[,'TrainingEndTime',],-,describe_response,[,'TrainingStartTime',],),*,describe_response,[,'ResourceConfig',],[,'InstanceCount',],self,.,log,.,info,(,'Billable seconds:{}',.,format,(,int,(,billable_time,.,total_seconds,(,),),+,1,),),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.create_tuning_job,"def create_tuning_job(self, config, wait_for_completion=True,
                          check_interval=30, max_ingestion_time=None):
        """"""
        Create a tuning job

        :param config: the config for tuning
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to tuning job creation
        """"""

        self.check_tuning_config(config)

        response = self.get_conn().create_hyper_parameter_tuning_job(**config)
        if wait_for_completion:
            self.check_status(config['HyperParameterTuningJobName'],
                              'HyperParameterTuningJobStatus',
                              self.describe_tuning_job,
                              check_interval, max_ingestion_time
                              )
        return response",python,"def create_tuning_job(self, config, wait_for_completion=True,
                          check_interval=30, max_ingestion_time=None):
        """"""
        Create a tuning job

        :param config: the config for tuning
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to tuning job creation
        """"""

        self.check_tuning_config(config)

        response = self.get_conn().create_hyper_parameter_tuning_job(**config)
        if wait_for_completion:
            self.check_status(config['HyperParameterTuningJobName'],
                              'HyperParameterTuningJobStatus',
                              self.describe_tuning_job,
                              check_interval, max_ingestion_time
                              )
        return response",def,create_tuning_job,(,self,",",config,",",wait_for_completion,=,True,",",check_interval,=,30,",",max_ingestion_time,=,None,),:,self,.,check_tuning_config,(,config,),response,=,self,.,get_conn,(,),.,create_hyper_parameter_tuning_job,(,*,*,config,),if,wait_for_completion,:,self,.,check_status,(,config,[,'HyperParameterTuningJobName',],",","Create a tuning job

        :param config: the config for tuning
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to tuning job creation",Create,a,tuning,job,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L368-L396,test,'HyperParameterTuningJobStatus',",",self,.,describe_tuning_job,",",check_interval,",",max_ingestion_time,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.create_transform_job,"def create_transform_job(self, config, wait_for_completion=True,
                             check_interval=30, max_ingestion_time=None):
        """"""
        Create a transform job

        :param config: the config for transform job
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to transform job creation
        """"""

        self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri'])

        response = self.get_conn().create_transform_job(**config)
        if wait_for_completion:
            self.check_status(config['TransformJobName'],
                              'TransformJobStatus',
                              self.describe_transform_job,
                              check_interval, max_ingestion_time
                              )
        return response",python,"def create_transform_job(self, config, wait_for_completion=True,
                             check_interval=30, max_ingestion_time=None):
        """"""
        Create a transform job

        :param config: the config for transform job
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to transform job creation
        """"""

        self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri'])

        response = self.get_conn().create_transform_job(**config)
        if wait_for_completion:
            self.check_status(config['TransformJobName'],
                              'TransformJobStatus',
                              self.describe_transform_job,
                              check_interval, max_ingestion_time
                              )
        return response",def,create_transform_job,(,self,",",config,",",wait_for_completion,=,True,",",check_interval,=,30,",",max_ingestion_time,=,None,),:,self,.,check_s3_url,(,config,[,'TransformInput',],[,'DataSource',],[,'S3DataSource',],[,'S3Uri',],),response,=,self,.,get_conn,(,),.,create_transform_job,(,*,*,config,),"Create a transform job

        :param config: the config for transform job
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to transform job creation",Create,a,transform,job,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L398-L426,test,if,wait_for_completion,:,self,.,check_status,(,config,[,'TransformJobName',],",",'TransformJobStatus',",",self,.,describe_transform_job,",",check_interval,",",max_ingestion_time,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.create_endpoint,"def create_endpoint(self, config, wait_for_completion=True,
                        check_interval=30, max_ingestion_time=None):
        """"""
        Create an endpoint

        :param config: the config for endpoint
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to endpoint creation
        """"""

        response = self.get_conn().create_endpoint(**config)
        if wait_for_completion:
            self.check_status(config['EndpointName'],
                              'EndpointStatus',
                              self.describe_endpoint,
                              check_interval, max_ingestion_time,
                              non_terminal_states=self.endpoint_non_terminal_states
                              )
        return response",python,"def create_endpoint(self, config, wait_for_completion=True,
                        check_interval=30, max_ingestion_time=None):
        """"""
        Create an endpoint

        :param config: the config for endpoint
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to endpoint creation
        """"""

        response = self.get_conn().create_endpoint(**config)
        if wait_for_completion:
            self.check_status(config['EndpointName'],
                              'EndpointStatus',
                              self.describe_endpoint,
                              check_interval, max_ingestion_time,
                              non_terminal_states=self.endpoint_non_terminal_states
                              )
        return response",def,create_endpoint,(,self,",",config,",",wait_for_completion,=,True,",",check_interval,=,30,",",max_ingestion_time,=,None,),:,response,=,self,.,get_conn,(,),.,create_endpoint,(,*,*,config,),if,wait_for_completion,:,self,.,check_status,(,config,[,'EndpointName',],",",'EndpointStatus',",",self,.,describe_endpoint,",","Create an endpoint

        :param config: the config for endpoint
        :type config: dict
        :param wait_for_completion: if the program should keep running until job finishes
        :type wait_for_completion: bool
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: A response to endpoint creation",Create,an,endpoint,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L450-L477,test,check_interval,",",max_ingestion_time,",",non_terminal_states,=,self,.,endpoint_non_terminal_states,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.describe_training_job_with_log,"def describe_training_job_with_log(self, job_name, positions, stream_names,
                                       instance_count, state, last_description,
                                       last_describe_job_call):
        """"""
        Return the training job info associated with job_name and print CloudWatch logs
        """"""
        log_group = '/aws/sagemaker/TrainingJobs'

        if len(stream_names) < instance_count:
            # Log streams are created whenever a container starts writing to stdout/err, so this list
            # may be dynamic until we have a stream for every instance.
            logs_conn = self.get_log_conn()
            try:
                streams = logs_conn.describe_log_streams(
                    logGroupName=log_group,
                    logStreamNamePrefix=job_name + '/',
                    orderBy='LogStreamName',
                    limit=instance_count
                )
                stream_names = [s['logStreamName'] for s in streams['logStreams']]
                positions.update([(s, Position(timestamp=0, skip=0))
                                  for s in stream_names if s not in positions])
            except logs_conn.exceptions.ResourceNotFoundException:
                # On the very first training job run on an account, there's no log group until
                # the container starts logging, so ignore any errors thrown about that
                pass

        if len(stream_names) > 0:
            for idx, event in self.multi_stream_iter(log_group, stream_names, positions):
                self.log.info(event['message'])
                ts, count = positions[stream_names[idx]]
                if event['timestamp'] == ts:
                    positions[stream_names[idx]] = Position(timestamp=ts, skip=count + 1)
                else:
                    positions[stream_names[idx]] = Position(timestamp=event['timestamp'], skip=1)

        if state == LogState.COMPLETE:
            return state, last_description, last_describe_job_call

        if state == LogState.JOB_COMPLETE:
            state = LogState.COMPLETE
        elif time.time() - last_describe_job_call >= 30:
            description = self.describe_training_job(job_name)
            last_describe_job_call = time.time()

            if secondary_training_status_changed(description, last_description):
                self.log.info(secondary_training_status_message(description, last_description))
                last_description = description

            status = description['TrainingJobStatus']

            if status not in self.non_terminal_states:
                state = LogState.JOB_COMPLETE
        return state, last_description, last_describe_job_call",python,"def describe_training_job_with_log(self, job_name, positions, stream_names,
                                       instance_count, state, last_description,
                                       last_describe_job_call):
        """"""
        Return the training job info associated with job_name and print CloudWatch logs
        """"""
        log_group = '/aws/sagemaker/TrainingJobs'

        if len(stream_names) < instance_count:
            # Log streams are created whenever a container starts writing to stdout/err, so this list
            # may be dynamic until we have a stream for every instance.
            logs_conn = self.get_log_conn()
            try:
                streams = logs_conn.describe_log_streams(
                    logGroupName=log_group,
                    logStreamNamePrefix=job_name + '/',
                    orderBy='LogStreamName',
                    limit=instance_count
                )
                stream_names = [s['logStreamName'] for s in streams['logStreams']]
                positions.update([(s, Position(timestamp=0, skip=0))
                                  for s in stream_names if s not in positions])
            except logs_conn.exceptions.ResourceNotFoundException:
                # On the very first training job run on an account, there's no log group until
                # the container starts logging, so ignore any errors thrown about that
                pass

        if len(stream_names) > 0:
            for idx, event in self.multi_stream_iter(log_group, stream_names, positions):
                self.log.info(event['message'])
                ts, count = positions[stream_names[idx]]
                if event['timestamp'] == ts:
                    positions[stream_names[idx]] = Position(timestamp=ts, skip=count + 1)
                else:
                    positions[stream_names[idx]] = Position(timestamp=event['timestamp'], skip=1)

        if state == LogState.COMPLETE:
            return state, last_description, last_describe_job_call

        if state == LogState.JOB_COMPLETE:
            state = LogState.COMPLETE
        elif time.time() - last_describe_job_call >= 30:
            description = self.describe_training_job(job_name)
            last_describe_job_call = time.time()

            if secondary_training_status_changed(description, last_description):
                self.log.info(secondary_training_status_message(description, last_description))
                last_description = description

            status = description['TrainingJobStatus']

            if status not in self.non_terminal_states:
                state = LogState.JOB_COMPLETE
        return state, last_description, last_describe_job_call",def,describe_training_job_with_log,(,self,",",job_name,",",positions,",",stream_names,",",instance_count,",",state,",",last_description,",",last_describe_job_call,),:,log_group,=,'/aws/sagemaker/TrainingJobs',if,len,(,stream_names,),<,instance_count,:,"# Log streams are created whenever a container starts writing to stdout/err, so this list",# may be dynamic until we have a stream for every instance.,logs_conn,=,self,.,get_log_conn,(,),try,:,streams,=,logs_conn,.,describe_log_streams,(,logGroupName,=,log_group,",",Return the training job info associated with job_name and print CloudWatch logs,Return,the,training,job,info,associated,with,job_name,and,print,CloudWatch,logs,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L519-L572,test,logStreamNamePrefix,=,job_name,+,'/',",",orderBy,=,'LogStreamName',",",limit,=,instance_count,),stream_names,=,[,s,[,'logStreamName',],for,s,in,streams,[,'logStreams',],],positions,.,update,(,[,(,s,",",Position,(,timestamp,=,0,",",skip,=,0,),),for,s,in,stream_names,if,s,not,in,positions,],),except,logs_conn,.,exceptions,.,ResourceNotFoundException,:,"# On the very first training job run on an account, there's no log group until","# the container starts logging, so ignore any errors thrown about that",pass,if,len,(,stream_names,),>,0,:,for,idx,",",event,in,self,.,multi_stream_iter,(,log_group,",",stream_names,",",positions,),:,self,.,log,.,info,(,event,[,'message',],),ts,",",count,=,positions,[,stream_names,[,idx,],],if,event,[,'timestamp',],==,ts,:,positions,[,stream_names,[,idx,],],=,Position,(,timestamp,=,ts,",",skip,=,count,+,1,),else,:,positions,[,stream_names,[,idx,],],=,Position,(,timestamp,=,event,[,'timestamp',],",",skip,=,1,),if,state,==,LogState,.,COMPLETE,:,return,state,",",last_description,",",last_describe_job_call,if,state,==,LogState,.,JOB_COMPLETE,:,state,=,LogState,.,COMPLETE,elif,time,.,time,(,),-,last_describe_job_call,>=,30,:,description,=,self,.,describe_training_job,(,job_name,),last_describe_job_call,=,time,.,time,(,),if,secondary_training_status_changed,(,description,",",last_description,),:,self,.,log,.,info,(,secondary_training_status_message,(,description,",",last_description,),),last_description,=,description,status,=,description,[,'TrainingJobStatus',],if,status,not,in,self,.,non_terminal_states,:,state,=,LogState,.,JOB_COMPLETE,return,state,",",last_description,",",last_describe_job_call,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.check_status,"def check_status(self, job_name, key,
                     describe_function, check_interval,
                     max_ingestion_time,
                     non_terminal_states=None):
        """"""
        Check status of a SageMaker job

        :param job_name: name of the job to check status
        :type job_name: str
        :param key: the key of the response dict
            that points to the state
        :type key: str
        :param describe_function: the function used to retrieve the status
        :type describe_function: python callable
        :param args: the arguments for the function
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :param non_terminal_states: the set of nonterminal states
        :type non_terminal_states: set
        :return: response of describe call after job is done
        """"""
        if not non_terminal_states:
            non_terminal_states = self.non_terminal_states

        sec = 0
        running = True

        while running:
            time.sleep(check_interval)
            sec = sec + check_interval

            try:
                response = describe_function(job_name)
                status = response[key]
                self.log.info('Job still running for %s seconds... '
                              'current status is %s' % (sec, status))
            except KeyError:
                raise AirflowException('Could not get status of the SageMaker job')
            except ClientError:
                raise AirflowException('AWS request failed, check logs for more info')

            if status in non_terminal_states:
                running = True
            elif status in self.failed_states:
                raise AirflowException('SageMaker job failed because %s' % response['FailureReason'])
            else:
                running = False

            if max_ingestion_time and sec > max_ingestion_time:
                # ensure that the job gets killed if the max ingestion time is exceeded
                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)

        self.log.info('SageMaker Job Compeleted')
        response = describe_function(job_name)
        return response",python,"def check_status(self, job_name, key,
                     describe_function, check_interval,
                     max_ingestion_time,
                     non_terminal_states=None):
        """"""
        Check status of a SageMaker job

        :param job_name: name of the job to check status
        :type job_name: str
        :param key: the key of the response dict
            that points to the state
        :type key: str
        :param describe_function: the function used to retrieve the status
        :type describe_function: python callable
        :param args: the arguments for the function
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :param non_terminal_states: the set of nonterminal states
        :type non_terminal_states: set
        :return: response of describe call after job is done
        """"""
        if not non_terminal_states:
            non_terminal_states = self.non_terminal_states

        sec = 0
        running = True

        while running:
            time.sleep(check_interval)
            sec = sec + check_interval

            try:
                response = describe_function(job_name)
                status = response[key]
                self.log.info('Job still running for %s seconds... '
                              'current status is %s' % (sec, status))
            except KeyError:
                raise AirflowException('Could not get status of the SageMaker job')
            except ClientError:
                raise AirflowException('AWS request failed, check logs for more info')

            if status in non_terminal_states:
                running = True
            elif status in self.failed_states:
                raise AirflowException('SageMaker job failed because %s' % response['FailureReason'])
            else:
                running = False

            if max_ingestion_time and sec > max_ingestion_time:
                # ensure that the job gets killed if the max ingestion time is exceeded
                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)

        self.log.info('SageMaker Job Compeleted')
        response = describe_function(job_name)
        return response",def,check_status,(,self,",",job_name,",",key,",",describe_function,",",check_interval,",",max_ingestion_time,",",non_terminal_states,=,None,),:,if,not,non_terminal_states,:,non_terminal_states,=,self,.,non_terminal_states,sec,=,0,running,=,True,while,running,:,time,.,sleep,(,check_interval,),sec,=,sec,+,check_interval,try,:,response,"Check status of a SageMaker job

        :param job_name: name of the job to check status
        :type job_name: str
        :param key: the key of the response dict
            that points to the state
        :type key: str
        :param describe_function: the function used to retrieve the status
        :type describe_function: python callable
        :param args: the arguments for the function
        :param check_interval: the time interval in seconds which the operator
            will check the status of any SageMaker job
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :param non_terminal_states: the set of nonterminal states
        :type non_terminal_states: set
        :return: response of describe call after job is done",Check,status,of,a,SageMaker,job,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L627-L686,test,=,describe_function,(,job_name,),status,=,response,[,key,],self,.,log,.,info,(,'Job still running for %s seconds... ','current status is %s',%,(,sec,",",status,),),except,KeyError,:,raise,AirflowException,(,'Could not get status of the SageMaker job',),except,ClientError,:,raise,AirflowException,(,"'AWS request failed, check logs for more info'",),if,status,in,non_terminal_states,:,running,=,True,elif,status,in,self,.,failed_states,:,raise,AirflowException,(,'SageMaker job failed because %s',%,response,[,'FailureReason',],),else,:,running,=,False,if,max_ingestion_time,and,sec,>,max_ingestion_time,:,# ensure that the job gets killed if the max ingestion time is exceeded,raise,AirflowException,(,'SageMaker job took more than %s seconds',",",max_ingestion_time,),self,.,log,.,info,(,'SageMaker Job Compeleted',),response,=,describe_function,(,job_name,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sagemaker_hook.py,SageMakerHook.check_training_status_with_log,"def check_training_status_with_log(self, job_name, non_terminal_states, failed_states,
                                       wait_for_completion, check_interval, max_ingestion_time):
        """"""
        Display the logs for a given training job, optionally tailing them until the
        job is complete.

        :param job_name: name of the training job to check status and display logs for
        :type job_name: str
        :param non_terminal_states: the set of non_terminal states
        :type non_terminal_states: set
        :param failed_states: the set of failed states
        :type failed_states: set
        :param wait_for_completion: Whether to keep looking for new log entries
            until the job completes
        :type wait_for_completion: bool
        :param check_interval: The interval in seconds between polling for new log entries and job completion
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: None
        """"""

        sec = 0
        description = self.describe_training_job(job_name)
        self.log.info(secondary_training_status_message(description, None))
        instance_count = description['ResourceConfig']['InstanceCount']
        status = description['TrainingJobStatus']

        stream_names = []  # The list of log streams
        positions = {}     # The current position in each stream, map of stream name -> position

        job_already_completed = status not in non_terminal_states

        state = LogState.TAILING if wait_for_completion and not job_already_completed else LogState.COMPLETE

        # The loop below implements a state machine that alternates between checking the job status and
        # reading whatever is available in the logs at this point. Note, that if we were called with
        # wait_for_completion == False, we never check the job status.
        #
        # If wait_for_completion == TRUE and job is not completed, the initial state is TAILING
        # If wait_for_completion == FALSE, the initial state is COMPLETE
        # (doesn't matter if the job really is complete).
        #
        # The state table:
        #
        # STATE               ACTIONS                        CONDITION             NEW STATE
        # ----------------    ----------------               -----------------     ----------------
        # TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE
        #                                                    Else                  TAILING
        # JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE
        # COMPLETE            Read logs, Exit                                      N/A
        #
        # Notes:
        # - The JOB_COMPLETE state forces us to do an extra pause and read any items that
        # got to Cloudwatch after the job was marked complete.
        last_describe_job_call = time.time()
        last_description = description

        while True:
            time.sleep(check_interval)
            sec = sec + check_interval

            state, last_description, last_describe_job_call = \
                self.describe_training_job_with_log(job_name, positions, stream_names,
                                                    instance_count, state, last_description,
                                                    last_describe_job_call)
            if state == LogState.COMPLETE:
                break

            if max_ingestion_time and sec > max_ingestion_time:
                # ensure that the job gets killed if the max ingestion time is exceeded
                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)

        if wait_for_completion:
            status = last_description['TrainingJobStatus']
            if status in failed_states:
                reason = last_description.get('FailureReason', '(No reason provided)')
                raise AirflowException('Error training {}: {} Reason: {}'.format(job_name, status, reason))
            billable_time = (last_description['TrainingEndTime'] - last_description['TrainingStartTime']) \
                * instance_count
            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))",python,"def check_training_status_with_log(self, job_name, non_terminal_states, failed_states,
                                       wait_for_completion, check_interval, max_ingestion_time):
        """"""
        Display the logs for a given training job, optionally tailing them until the
        job is complete.

        :param job_name: name of the training job to check status and display logs for
        :type job_name: str
        :param non_terminal_states: the set of non_terminal states
        :type non_terminal_states: set
        :param failed_states: the set of failed states
        :type failed_states: set
        :param wait_for_completion: Whether to keep looking for new log entries
            until the job completes
        :type wait_for_completion: bool
        :param check_interval: The interval in seconds between polling for new log entries and job completion
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: None
        """"""

        sec = 0
        description = self.describe_training_job(job_name)
        self.log.info(secondary_training_status_message(description, None))
        instance_count = description['ResourceConfig']['InstanceCount']
        status = description['TrainingJobStatus']

        stream_names = []  # The list of log streams
        positions = {}     # The current position in each stream, map of stream name -> position

        job_already_completed = status not in non_terminal_states

        state = LogState.TAILING if wait_for_completion and not job_already_completed else LogState.COMPLETE

        # The loop below implements a state machine that alternates between checking the job status and
        # reading whatever is available in the logs at this point. Note, that if we were called with
        # wait_for_completion == False, we never check the job status.
        #
        # If wait_for_completion == TRUE and job is not completed, the initial state is TAILING
        # If wait_for_completion == FALSE, the initial state is COMPLETE
        # (doesn't matter if the job really is complete).
        #
        # The state table:
        #
        # STATE               ACTIONS                        CONDITION             NEW STATE
        # ----------------    ----------------               -----------------     ----------------
        # TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE
        #                                                    Else                  TAILING
        # JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE
        # COMPLETE            Read logs, Exit                                      N/A
        #
        # Notes:
        # - The JOB_COMPLETE state forces us to do an extra pause and read any items that
        # got to Cloudwatch after the job was marked complete.
        last_describe_job_call = time.time()
        last_description = description

        while True:
            time.sleep(check_interval)
            sec = sec + check_interval

            state, last_description, last_describe_job_call = \
                self.describe_training_job_with_log(job_name, positions, stream_names,
                                                    instance_count, state, last_description,
                                                    last_describe_job_call)
            if state == LogState.COMPLETE:
                break

            if max_ingestion_time and sec > max_ingestion_time:
                # ensure that the job gets killed if the max ingestion time is exceeded
                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)

        if wait_for_completion:
            status = last_description['TrainingJobStatus']
            if status in failed_states:
                reason = last_description.get('FailureReason', '(No reason provided)')
                raise AirflowException('Error training {}: {} Reason: {}'.format(job_name, status, reason))
            billable_time = (last_description['TrainingEndTime'] - last_description['TrainingStartTime']) \
                * instance_count
            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))",def,check_training_status_with_log,(,self,",",job_name,",",non_terminal_states,",",failed_states,",",wait_for_completion,",",check_interval,",",max_ingestion_time,),:,sec,=,0,description,=,self,.,describe_training_job,(,job_name,),self,.,log,.,info,(,secondary_training_status_message,(,description,",",None,),),instance_count,=,description,[,'ResourceConfig',],[,'InstanceCount',],status,"Display the logs for a given training job, optionally tailing them until the
        job is complete.

        :param job_name: name of the training job to check status and display logs for
        :type job_name: str
        :param non_terminal_states: the set of non_terminal states
        :type non_terminal_states: set
        :param failed_states: the set of failed states
        :type failed_states: set
        :param wait_for_completion: Whether to keep looking for new log entries
            until the job completes
        :type wait_for_completion: bool
        :param check_interval: The interval in seconds between polling for new log entries and job completion
        :type check_interval: int
        :param max_ingestion_time: the maximum ingestion time in seconds. Any
            SageMaker jobs that run longer than this will fail. Setting this to
            None implies no timeout for any SageMaker job.
        :type max_ingestion_time: int
        :return: None",Display,the,logs,for,a,given,training,job,optionally,tailing,them,until,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sagemaker_hook.py#L688-L770,test,=,description,[,'TrainingJobStatus',],stream_names,=,[,],# The list of log streams,positions,=,{,},"# The current position in each stream, map of stream name -> position",job_already_completed,=,status,not,in,non_terminal_states,state,=,LogState,.,TAILING,if,wait_for_completion,and,not,job_already_completed,else,LogState,.,COMPLETE,# The loop below implements a state machine that alternates between checking the job status and,"# reading whatever is available in the logs at this point. Note, that if we were called with","# wait_for_completion == False, we never check the job status.",#,"# If wait_for_completion == TRUE and job is not completed, the initial state is TAILING","# If wait_for_completion == FALSE, the initial state is COMPLETE",# (doesn't matter if the job really is complete).,#,# The state table:,#,# STATE               ACTIONS                        CONDITION             NEW STATE,# ----------------    ----------------               -----------------     ----------------,"# TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE",#                                                    Else                  TAILING,"# JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE","# COMPLETE            Read logs, Exit                                      N/A",#,# Notes:,# - The JOB_COMPLETE state forces us to do an extra pause and read any items that,# got to Cloudwatch after the job was marked complete.,last_describe_job_call,=,time,.,time,(,),last_description,=,description,while,True,:,time,.,sleep,(,check_interval,),sec,=,sec,+,check_interval,state,",",last_description,",",last_describe_job_call,=,self,.,describe_training_job_with_log,(,job_name,",",positions,",",stream_names,",",instance_count,",",state,",",last_description,",",last_describe_job_call,),if,state,==,LogState,.,COMPLETE,:,break,if,max_ingestion_time,and,sec,>,max_ingestion_time,:,# ensure that the job gets killed if the max ingestion time is exceeded,raise,AirflowException,(,'SageMaker job took more than %s seconds',",",max_ingestion_time,),if,wait_for_completion,:,status,=,last_description,[,'TrainingJobStatus',],if,status,in,failed_states,:,reason,=,last_description,.,get,(,'FailureReason',",",'(No reason provided)',),raise,AirflowException,(,'Error training {}: {} Reason: {}',.,format,(,job_name,",",status,",",reason,),),billable_time,=,(,last_description,[,'TrainingEndTime',],-,last_description,[,'TrainingStartTime',],),*,instance_count,self,.,log,.,info,(,'Billable seconds:{}',.,format,(,int,(,billable_time,.,total_seconds,(,),),+,1,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,job,is,complete,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/dataflow_operator.py,DataFlowPythonOperator.execute,"def execute(self, context):
        """"""Execute the python dataflow job.""""""
        bucket_helper = GoogleCloudBucketHelper(
            self.gcp_conn_id, self.delegate_to)
        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)
        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,
                            delegate_to=self.delegate_to,
                            poll_sleep=self.poll_sleep)
        dataflow_options = self.dataflow_default_options.copy()
        dataflow_options.update(self.options)
        # Convert argument names from lowerCamelCase to snake case.
        camel_to_snake = lambda name: re.sub(
            r'[A-Z]', lambda x: '_' + x.group(0).lower(), name)
        formatted_options = {camel_to_snake(key): dataflow_options[key]
                             for key in dataflow_options}
        hook.start_python_dataflow(
            self.job_name, formatted_options,
            self.py_file, self.py_options)",python,"def execute(self, context):
        """"""Execute the python dataflow job.""""""
        bucket_helper = GoogleCloudBucketHelper(
            self.gcp_conn_id, self.delegate_to)
        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)
        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,
                            delegate_to=self.delegate_to,
                            poll_sleep=self.poll_sleep)
        dataflow_options = self.dataflow_default_options.copy()
        dataflow_options.update(self.options)
        # Convert argument names from lowerCamelCase to snake case.
        camel_to_snake = lambda name: re.sub(
            r'[A-Z]', lambda x: '_' + x.group(0).lower(), name)
        formatted_options = {camel_to_snake(key): dataflow_options[key]
                             for key in dataflow_options}
        hook.start_python_dataflow(
            self.job_name, formatted_options,
            self.py_file, self.py_options)",def,execute,(,self,",",context,),:,bucket_helper,=,GoogleCloudBucketHelper,(,self,.,gcp_conn_id,",",self,.,delegate_to,),self,.,py_file,=,bucket_helper,.,google_cloud_to_local,(,self,.,py_file,),hook,=,DataFlowHook,(,gcp_conn_id,=,self,.,gcp_conn_id,",",delegate_to,=,self,.,delegate_to,",",poll_sleep,=,self,.,Execute the python dataflow job.,Execute,the,python,dataflow,job,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/dataflow_operator.py#L363-L380,test,poll_sleep,),dataflow_options,=,self,.,dataflow_default_options,.,copy,(,),dataflow_options,.,update,(,self,.,options,),# Convert argument names from lowerCamelCase to snake case.,camel_to_snake,=,lambda,name,:,re,.,sub,(,r'[A-Z]',",",lambda,x,:,'_',+,x,.,group,(,0,),.,lower,(,),",",name,),formatted_options,=,{,camel_to_snake,(,key,),:,dataflow_options,[,key,],for,key,in,dataflow_options,},hook,.,start_python_dataflow,(,self,.,job_name,",",formatted_options,",",self,.,py_file,",",self,.,py_options,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/migrations/env.py,run_migrations_offline,"def run_migrations_offline():
    """"""Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """"""
    context.configure(
        url=settings.SQL_ALCHEMY_CONN, target_metadata=target_metadata,
        literal_binds=True, compare_type=COMPARE_TYPE)

    with context.begin_transaction():
        context.run_migrations()",python,"def run_migrations_offline():
    """"""Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """"""
    context.configure(
        url=settings.SQL_ALCHEMY_CONN, target_metadata=target_metadata,
        literal_binds=True, compare_type=COMPARE_TYPE)

    with context.begin_transaction():
        context.run_migrations()",def,run_migrations_offline,(,),:,context,.,configure,(,url,=,settings,.,SQL_ALCHEMY_CONN,",",target_metadata,=,target_metadata,",",literal_binds,=,True,",",compare_type,=,COMPARE_TYPE,),with,context,.,begin_transaction,(,),:,context,.,run_migrations,(,),,,,,,,,,,,,,,"Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.",Run,migrations,in,offline,mode,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/migrations/env.py#L48-L65,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/migrations/env.py,run_migrations_online,"def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""
    connectable = settings.engine

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            transaction_per_migration=True,
            target_metadata=target_metadata,
            compare_type=COMPARE_TYPE,
        )

        with context.begin_transaction():
            context.run_migrations()",python,"def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""
    connectable = settings.engine

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            transaction_per_migration=True,
            target_metadata=target_metadata,
            compare_type=COMPARE_TYPE,
        )

        with context.begin_transaction():
            context.run_migrations()",def,run_migrations_online,(,),:,connectable,=,settings,.,engine,with,connectable,.,connect,(,),as,connection,:,context,.,configure,(,connection,=,connection,",",transaction_per_migration,=,True,",",target_metadata,=,target_metadata,",",compare_type,=,COMPARE_TYPE,",",),with,context,.,begin_transaction,(,),:,context,.,run_migrations,(,),"Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.",Run,migrations,in,online,mode,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/migrations/env.py#L68-L86,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_bigtable_hook.py,BigtableHook.delete_instance,"def delete_instance(self, instance_id, project_id=None):
        """"""
        Deletes the specified Cloud Bigtable instance.
        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does
        not exist.

        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance_id: The ID of the Cloud Bigtable instance.
        :type instance_id: str
        """"""
        instance = self.get_instance(instance_id=instance_id, project_id=project_id)
        if instance:
            instance.delete()
        else:
            self.log.info(""The instance '%s' does not exist in project '%s'. Exiting"", instance_id,
                          project_id)",python,"def delete_instance(self, instance_id, project_id=None):
        """"""
        Deletes the specified Cloud Bigtable instance.
        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does
        not exist.

        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance_id: The ID of the Cloud Bigtable instance.
        :type instance_id: str
        """"""
        instance = self.get_instance(instance_id=instance_id, project_id=project_id)
        if instance:
            instance.delete()
        else:
            self.log.info(""The instance '%s' does not exist in project '%s'. Exiting"", instance_id,
                          project_id)",def,delete_instance,(,self,",",instance_id,",",project_id,=,None,),:,instance,=,self,.,get_instance,(,instance_id,=,instance_id,",",project_id,=,project_id,),if,instance,:,instance,.,delete,(,),else,:,self,.,log,.,info,(,"""The instance '%s' does not exist in project '%s'. Exiting""",",",instance_id,",",project_id,),,,,,"Deletes the specified Cloud Bigtable instance.
        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does
        not exist.

        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance_id: The ID of the Cloud Bigtable instance.
        :type instance_id: str",Deletes,the,specified,Cloud,Bigtable,instance,.,Raises,google,.,api_core,.,exceptions,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_bigtable_hook.py#L69-L87,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,NotFound,if,the,Cloud,Bigtable,instance,does,not,exist,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_bigtable_hook.py,BigtableHook.create_instance,"def create_instance(self,
                        instance_id,
                        main_cluster_id,
                        main_cluster_zone,
                        project_id=None,
                        replica_cluster_id=None,
                        replica_cluster_zone=None,
                        instance_display_name=None,
                        instance_type=enums.Instance.Type.TYPE_UNSPECIFIED,
                        instance_labels=None,
                        cluster_nodes=None,
                        cluster_storage_type=enums.StorageType.STORAGE_TYPE_UNSPECIFIED,
                        timeout=None):
        """"""
        Creates new instance.

        :type instance_id: str
        :param instance_id: The ID for the new instance.
        :type main_cluster_id: str
        :param main_cluster_id: The ID for main cluster for the new instance.
        :type main_cluster_zone: str
        :param main_cluster_zone: The zone for main cluster.
            See https://cloud.google.com/bigtable/docs/locations for more details.
        :type project_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type replica_cluster_id: str
        :param replica_cluster_id: (optional) The ID for replica cluster for the new
            instance.
        :type replica_cluster_zone: str
        :param replica_cluster_zone: (optional)  The zone for replica cluster.
        :type instance_type: enums.Instance.Type
        :param instance_type: (optional) The type of the instance.
        :type instance_display_name: str
        :param instance_display_name: (optional) Human-readable name of the instance.
                Defaults to ``instance_id``.
        :type instance_labels: dict
        :param instance_labels: (optional) Dictionary of labels to associate with the
            instance.
        :type cluster_nodes: int
        :param cluster_nodes: (optional) Number of nodes for cluster.
        :type cluster_storage_type: enums.StorageType
        :param cluster_storage_type: (optional) The type of storage.
        :type timeout: int
        :param timeout: (optional) timeout (in seconds) for instance creation.
                        If None is not specified, Operator will wait indefinitely.
        """"""
        cluster_storage_type = enums.StorageType(cluster_storage_type)
        instance_type = enums.Instance.Type(instance_type)

        instance = Instance(
            instance_id,
            self._get_client(project_id=project_id),
            instance_display_name,
            instance_type,
            instance_labels,
        )

        clusters = [
            instance.cluster(
                main_cluster_id,
                main_cluster_zone,
                cluster_nodes,
                cluster_storage_type
            )
        ]
        if replica_cluster_id and replica_cluster_zone:
            clusters.append(instance.cluster(
                replica_cluster_id,
                replica_cluster_zone,
                cluster_nodes,
                cluster_storage_type
            ))
        operation = instance.create(
            clusters=clusters
        )
        operation.result(timeout)
        return instance",python,"def create_instance(self,
                        instance_id,
                        main_cluster_id,
                        main_cluster_zone,
                        project_id=None,
                        replica_cluster_id=None,
                        replica_cluster_zone=None,
                        instance_display_name=None,
                        instance_type=enums.Instance.Type.TYPE_UNSPECIFIED,
                        instance_labels=None,
                        cluster_nodes=None,
                        cluster_storage_type=enums.StorageType.STORAGE_TYPE_UNSPECIFIED,
                        timeout=None):
        """"""
        Creates new instance.

        :type instance_id: str
        :param instance_id: The ID for the new instance.
        :type main_cluster_id: str
        :param main_cluster_id: The ID for main cluster for the new instance.
        :type main_cluster_zone: str
        :param main_cluster_zone: The zone for main cluster.
            See https://cloud.google.com/bigtable/docs/locations for more details.
        :type project_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type replica_cluster_id: str
        :param replica_cluster_id: (optional) The ID for replica cluster for the new
            instance.
        :type replica_cluster_zone: str
        :param replica_cluster_zone: (optional)  The zone for replica cluster.
        :type instance_type: enums.Instance.Type
        :param instance_type: (optional) The type of the instance.
        :type instance_display_name: str
        :param instance_display_name: (optional) Human-readable name of the instance.
                Defaults to ``instance_id``.
        :type instance_labels: dict
        :param instance_labels: (optional) Dictionary of labels to associate with the
            instance.
        :type cluster_nodes: int
        :param cluster_nodes: (optional) Number of nodes for cluster.
        :type cluster_storage_type: enums.StorageType
        :param cluster_storage_type: (optional) The type of storage.
        :type timeout: int
        :param timeout: (optional) timeout (in seconds) for instance creation.
                        If None is not specified, Operator will wait indefinitely.
        """"""
        cluster_storage_type = enums.StorageType(cluster_storage_type)
        instance_type = enums.Instance.Type(instance_type)

        instance = Instance(
            instance_id,
            self._get_client(project_id=project_id),
            instance_display_name,
            instance_type,
            instance_labels,
        )

        clusters = [
            instance.cluster(
                main_cluster_id,
                main_cluster_zone,
                cluster_nodes,
                cluster_storage_type
            )
        ]
        if replica_cluster_id and replica_cluster_zone:
            clusters.append(instance.cluster(
                replica_cluster_id,
                replica_cluster_zone,
                cluster_nodes,
                cluster_storage_type
            ))
        operation = instance.create(
            clusters=clusters
        )
        operation.result(timeout)
        return instance",def,create_instance,(,self,",",instance_id,",",main_cluster_id,",",main_cluster_zone,",",project_id,=,None,",",replica_cluster_id,=,None,",",replica_cluster_zone,=,None,",",instance_display_name,=,None,",",instance_type,=,enums,.,Instance,.,Type,.,TYPE_UNSPECIFIED,",",instance_labels,=,None,",",cluster_nodes,=,None,",",cluster_storage_type,=,enums,.,StorageType,.,STORAGE_TYPE_UNSPECIFIED,"Creates new instance.

        :type instance_id: str
        :param instance_id: The ID for the new instance.
        :type main_cluster_id: str
        :param main_cluster_id: The ID for main cluster for the new instance.
        :type main_cluster_zone: str
        :param main_cluster_zone: The zone for main cluster.
            See https://cloud.google.com/bigtable/docs/locations for more details.
        :type project_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type replica_cluster_id: str
        :param replica_cluster_id: (optional) The ID for replica cluster for the new
            instance.
        :type replica_cluster_zone: str
        :param replica_cluster_zone: (optional)  The zone for replica cluster.
        :type instance_type: enums.Instance.Type
        :param instance_type: (optional) The type of the instance.
        :type instance_display_name: str
        :param instance_display_name: (optional) Human-readable name of the instance.
                Defaults to ``instance_id``.
        :type instance_labels: dict
        :param instance_labels: (optional) Dictionary of labels to associate with the
            instance.
        :type cluster_nodes: int
        :param cluster_nodes: (optional) Number of nodes for cluster.
        :type cluster_storage_type: enums.StorageType
        :param cluster_storage_type: (optional) The type of storage.
        :type timeout: int
        :param timeout: (optional) timeout (in seconds) for instance creation.
                        If None is not specified, Operator will wait indefinitely.",Creates,new,instance,.,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_bigtable_hook.py#L90-L168,test,",",timeout,=,None,),:,cluster_storage_type,=,enums,.,StorageType,(,cluster_storage_type,),instance_type,=,enums,.,Instance,.,Type,(,instance_type,),instance,=,Instance,(,instance_id,",",self,.,_get_client,(,project_id,=,project_id,),",",instance_display_name,",",instance_type,",",instance_labels,",",),clusters,=,[,instance,.,cluster,(,main_cluster_id,",",main_cluster_zone,",",cluster_nodes,",",cluster_storage_type,),],if,replica_cluster_id,and,replica_cluster_zone,:,clusters,.,append,(,instance,.,cluster,(,replica_cluster_id,",",replica_cluster_zone,",",cluster_nodes,",",cluster_storage_type,),),operation,=,instance,.,create,(,clusters,=,clusters,),operation,.,result,(,timeout,),return,instance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_bigtable_hook.py,BigtableHook.create_table,"def create_table(instance,
                     table_id,
                     initial_split_keys=None,
                     column_families=None):
        """"""
        Creates the specified Cloud Bigtable table.
        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.

        :type instance: Instance
        :param instance: The Cloud Bigtable instance that owns the table.
        :type table_id: str
        :param table_id: The ID of the table to create in Cloud Bigtable.
        :type initial_split_keys: list
        :param initial_split_keys: (Optional) A list of row keys in bytes to use to
            initially split the table.
        :type column_families: dict
        :param column_families: (Optional) A map of columns to create. The key is the
            column_id str, and the value is a
            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.
        """"""
        if column_families is None:
            column_families = {}
        if initial_split_keys is None:
            initial_split_keys = []
        table = Table(table_id, instance)
        table.create(initial_split_keys, column_families)",python,"def create_table(instance,
                     table_id,
                     initial_split_keys=None,
                     column_families=None):
        """"""
        Creates the specified Cloud Bigtable table.
        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.

        :type instance: Instance
        :param instance: The Cloud Bigtable instance that owns the table.
        :type table_id: str
        :param table_id: The ID of the table to create in Cloud Bigtable.
        :type initial_split_keys: list
        :param initial_split_keys: (Optional) A list of row keys in bytes to use to
            initially split the table.
        :type column_families: dict
        :param column_families: (Optional) A map of columns to create. The key is the
            column_id str, and the value is a
            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.
        """"""
        if column_families is None:
            column_families = {}
        if initial_split_keys is None:
            initial_split_keys = []
        table = Table(table_id, instance)
        table.create(initial_split_keys, column_families)",def,create_table,(,instance,",",table_id,",",initial_split_keys,=,None,",",column_families,=,None,),:,if,column_families,is,None,:,column_families,=,{,},if,initial_split_keys,is,None,:,initial_split_keys,=,[,],table,=,Table,(,table_id,",",instance,),table,.,create,(,initial_split_keys,",",column_families,),,,"Creates the specified Cloud Bigtable table.
        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.

        :type instance: Instance
        :param instance: The Cloud Bigtable instance that owns the table.
        :type table_id: str
        :param table_id: The ID of the table to create in Cloud Bigtable.
        :type initial_split_keys: list
        :param initial_split_keys: (Optional) A list of row keys in bytes to use to
            initially split the table.
        :type column_families: dict
        :param column_families: (Optional) A map of columns to create. The key is the
            column_id str, and the value is a
            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.",Creates,the,specified,Cloud,Bigtable,table,.,Raises,google,.,api_core,.,exceptions,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_bigtable_hook.py#L171-L196,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,AlreadyExists,if,the,table,exists,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_bigtable_hook.py,BigtableHook.delete_table,"def delete_table(self, instance_id, table_id, project_id=None):
        """"""
        Deletes the specified table in Cloud Bigtable.
        Raises google.api_core.exceptions.NotFound if the table does not exist.

        :type instance_id: str
        :param instance_id: The ID of the Cloud Bigtable instance.
        :type table_id: str
        :param table_id: The ID of the table in Cloud Bigtable.
        :type project_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        """"""
        table = self.get_instance(instance_id=instance_id, project_id=project_id).table(table_id=table_id)
        table.delete()",python,"def delete_table(self, instance_id, table_id, project_id=None):
        """"""
        Deletes the specified table in Cloud Bigtable.
        Raises google.api_core.exceptions.NotFound if the table does not exist.

        :type instance_id: str
        :param instance_id: The ID of the Cloud Bigtable instance.
        :type table_id: str
        :param table_id: The ID of the table in Cloud Bigtable.
        :type project_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        """"""
        table = self.get_instance(instance_id=instance_id, project_id=project_id).table(table_id=table_id)
        table.delete()",def,delete_table,(,self,",",instance_id,",",table_id,",",project_id,=,None,),:,table,=,self,.,get_instance,(,instance_id,=,instance_id,",",project_id,=,project_id,),.,table,(,table_id,=,table_id,),table,.,delete,(,),,,,,,,,,,,,,"Deletes the specified table in Cloud Bigtable.
        Raises google.api_core.exceptions.NotFound if the table does not exist.

        :type instance_id: str
        :param instance_id: The ID of the Cloud Bigtable instance.
        :type table_id: str
        :param table_id: The ID of the table in Cloud Bigtable.
        :type project_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            BigTable exists. If set to None or missing,
            the default project_id from the GCP connection is used.",Deletes,the,specified,table,in,Cloud,Bigtable,.,Raises,google,.,api_core,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_bigtable_hook.py#L199-L214,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,exceptions,.,NotFound,if,the,table,does,not,exist,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_bigtable_hook.py,BigtableHook.update_cluster,"def update_cluster(instance, cluster_id, nodes):
        """"""
        Updates number of nodes in the specified Cloud Bigtable cluster.
        Raises google.api_core.exceptions.NotFound if the cluster does not exist.

        :type instance: Instance
        :param instance: The Cloud Bigtable instance that owns the cluster.
        :type cluster_id: str
        :param cluster_id: The ID of the cluster.
        :type nodes: int
        :param nodes: The desired number of nodes.
        """"""
        cluster = Cluster(cluster_id, instance)
        cluster.serve_nodes = nodes
        cluster.update()",python,"def update_cluster(instance, cluster_id, nodes):
        """"""
        Updates number of nodes in the specified Cloud Bigtable cluster.
        Raises google.api_core.exceptions.NotFound if the cluster does not exist.

        :type instance: Instance
        :param instance: The Cloud Bigtable instance that owns the cluster.
        :type cluster_id: str
        :param cluster_id: The ID of the cluster.
        :type nodes: int
        :param nodes: The desired number of nodes.
        """"""
        cluster = Cluster(cluster_id, instance)
        cluster.serve_nodes = nodes
        cluster.update()",def,update_cluster,(,instance,",",cluster_id,",",nodes,),:,cluster,=,Cluster,(,cluster_id,",",instance,),cluster,.,serve_nodes,=,nodes,cluster,.,update,(,),,,,,,,,,,,,,,,,,,,,,,,,,"Updates number of nodes in the specified Cloud Bigtable cluster.
        Raises google.api_core.exceptions.NotFound if the cluster does not exist.

        :type instance: Instance
        :param instance: The Cloud Bigtable instance that owns the cluster.
        :type cluster_id: str
        :param cluster_id: The ID of the cluster.
        :type nodes: int
        :param nodes: The desired number of nodes.",Updates,number,of,nodes,in,the,specified,Cloud,Bigtable,cluster,.,Raises,google,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_bigtable_hook.py#L217-L231,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,api_core,.,exceptions,.,NotFound,if,the,cluster,does,not,exist,.,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveCliHook._prepare_cli_cmd,"def _prepare_cli_cmd(self):
        """"""
        This function creates the command list from available information
        """"""
        conn = self.conn
        hive_bin = 'hive'
        cmd_extra = []

        if self.use_beeline:
            hive_bin = 'beeline'
            jdbc_url = ""jdbc:hive2://{host}:{port}/{schema}"".format(
                host=conn.host, port=conn.port, schema=conn.schema)
            if configuration.conf.get('core', 'security') == 'kerberos':
                template = conn.extra_dejson.get(
                    'principal', ""hive/_HOST@EXAMPLE.COM"")
                if ""_HOST"" in template:
                    template = utils.replace_hostname_pattern(
                        utils.get_components(template))

                proxy_user = """"  # noqa
                if conn.extra_dejson.get('proxy_user') == ""login"" and conn.login:
                    proxy_user = ""hive.server2.proxy.user={0}"".format(conn.login)
                elif conn.extra_dejson.get('proxy_user') == ""owner"" and self.run_as:
                    proxy_user = ""hive.server2.proxy.user={0}"".format(self.run_as)

                jdbc_url += "";principal={template};{proxy_user}"".format(
                    template=template, proxy_user=proxy_user)
            elif self.auth:
                jdbc_url += "";auth="" + self.auth

            jdbc_url = '""{}""'.format(jdbc_url)

            cmd_extra += ['-u', jdbc_url]
            if conn.login:
                cmd_extra += ['-n', conn.login]
            if conn.password:
                cmd_extra += ['-p', conn.password]

        hive_params_list = self.hive_cli_params.split()

        return [hive_bin] + cmd_extra + hive_params_list",python,"def _prepare_cli_cmd(self):
        """"""
        This function creates the command list from available information
        """"""
        conn = self.conn
        hive_bin = 'hive'
        cmd_extra = []

        if self.use_beeline:
            hive_bin = 'beeline'
            jdbc_url = ""jdbc:hive2://{host}:{port}/{schema}"".format(
                host=conn.host, port=conn.port, schema=conn.schema)
            if configuration.conf.get('core', 'security') == 'kerberos':
                template = conn.extra_dejson.get(
                    'principal', ""hive/_HOST@EXAMPLE.COM"")
                if ""_HOST"" in template:
                    template = utils.replace_hostname_pattern(
                        utils.get_components(template))

                proxy_user = """"  # noqa
                if conn.extra_dejson.get('proxy_user') == ""login"" and conn.login:
                    proxy_user = ""hive.server2.proxy.user={0}"".format(conn.login)
                elif conn.extra_dejson.get('proxy_user') == ""owner"" and self.run_as:
                    proxy_user = ""hive.server2.proxy.user={0}"".format(self.run_as)

                jdbc_url += "";principal={template};{proxy_user}"".format(
                    template=template, proxy_user=proxy_user)
            elif self.auth:
                jdbc_url += "";auth="" + self.auth

            jdbc_url = '""{}""'.format(jdbc_url)

            cmd_extra += ['-u', jdbc_url]
            if conn.login:
                cmd_extra += ['-n', conn.login]
            if conn.password:
                cmd_extra += ['-p', conn.password]

        hive_params_list = self.hive_cli_params.split()

        return [hive_bin] + cmd_extra + hive_params_list",def,_prepare_cli_cmd,(,self,),:,conn,=,self,.,conn,hive_bin,=,'hive',cmd_extra,=,[,],if,self,.,use_beeline,:,hive_bin,=,'beeline',jdbc_url,=,"""jdbc:hive2://{host}:{port}/{schema}""",.,format,(,host,=,conn,.,host,",",port,=,conn,.,port,",",schema,=,conn,.,schema,),if,configuration,This function creates the command list from available information,This,function,creates,the,command,list,from,available,information,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L106-L146,test,.,conf,.,get,(,'core',",",'security',),==,'kerberos',:,template,=,conn,.,extra_dejson,.,get,(,'principal',",","""hive/_HOST@EXAMPLE.COM""",),if,"""_HOST""",in,template,:,template,=,utils,.,replace_hostname_pattern,(,utils,.,get_components,(,template,),),proxy_user,=,"""""",# noqa,if,conn,.,extra_dejson,.,get,(,'proxy_user',),==,"""login""",and,conn,.,login,:,proxy_user,=,"""hive.server2.proxy.user={0}""",.,format,(,conn,.,login,),elif,conn,.,extra_dejson,.,get,(,'proxy_user',),==,"""owner""",and,self,.,run_as,:,proxy_user,=,"""hive.server2.proxy.user={0}""",.,format,(,self,.,run_as,),jdbc_url,+=,""";principal={template};{proxy_user}""",.,format,(,template,=,template,",",proxy_user,=,proxy_user,),elif,self,.,auth,:,jdbc_url,+=,""";auth=""",+,self,.,auth,jdbc_url,=,"'""{}""'",.,format,(,jdbc_url,),cmd_extra,+=,[,'-u',",",jdbc_url,],if,conn,.,login,:,cmd_extra,+=,[,'-n',",",conn,.,login,],if,conn,.,password,:,cmd_extra,+=,[,'-p',",",conn,.,password,],hive_params_list,=,self,.,hive_cli_params,.,split,(,),return,[,hive_bin,],+,cmd_extra,+,hive_params_list,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveCliHook._prepare_hiveconf,"def _prepare_hiveconf(d):
        """"""
        This function prepares a list of hiveconf params
        from a dictionary of key value pairs.

        :param d:
        :type d: dict

        >>> hh = HiveCliHook()
        >>> hive_conf = {""hive.exec.dynamic.partition"": ""true"",
        ... ""hive.exec.dynamic.partition.mode"": ""nonstrict""}
        >>> hh._prepare_hiveconf(hive_conf)
        [""-hiveconf"", ""hive.exec.dynamic.partition=true"",\
 ""-hiveconf"", ""hive.exec.dynamic.partition.mode=nonstrict""]
        """"""
        if not d:
            return []
        return as_flattened_list(
            zip([""-hiveconf""] * len(d),
                [""{}={}"".format(k, v) for k, v in d.items()])
        )",python,"def _prepare_hiveconf(d):
        """"""
        This function prepares a list of hiveconf params
        from a dictionary of key value pairs.

        :param d:
        :type d: dict

        >>> hh = HiveCliHook()
        >>> hive_conf = {""hive.exec.dynamic.partition"": ""true"",
        ... ""hive.exec.dynamic.partition.mode"": ""nonstrict""}
        >>> hh._prepare_hiveconf(hive_conf)
        [""-hiveconf"", ""hive.exec.dynamic.partition=true"",\
 ""-hiveconf"", ""hive.exec.dynamic.partition.mode=nonstrict""]
        """"""
        if not d:
            return []
        return as_flattened_list(
            zip([""-hiveconf""] * len(d),
                [""{}={}"".format(k, v) for k, v in d.items()])
        )",def,_prepare_hiveconf,(,d,),:,if,not,d,:,return,[,],return,as_flattened_list,(,zip,(,[,"""-hiveconf""",],*,len,(,d,),",",[,"""{}={}""",.,format,(,k,",",v,),for,k,",",v,in,d,.,items,(,),],),),,,,"This function prepares a list of hiveconf params
        from a dictionary of key value pairs.

        :param d:
        :type d: dict

        >>> hh = HiveCliHook()
        >>> hive_conf = {""hive.exec.dynamic.partition"": ""true"",
        ... ""hive.exec.dynamic.partition.mode"": ""nonstrict""}
        >>> hh._prepare_hiveconf(hive_conf)
        [""-hiveconf"", ""hive.exec.dynamic.partition=true"",\
 ""-hiveconf"", ""hive.exec.dynamic.partition.mode=nonstrict""]",This,function,prepares,a,list,of,hiveconf,params,from,a,dictionary,of,key,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L149-L169,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,value,pairs,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveCliHook.load_df,"def load_df(
            self,
            df,
            table,
            field_dict=None,
            delimiter=',',
            encoding='utf8',
            pandas_kwargs=None, **kwargs):
        """"""
        Loads a pandas DataFrame into hive.

        Hive data types will be inferred if not passed but column names will
        not be sanitized.

        :param df: DataFrame to load into a Hive table
        :type df: pandas.DataFrame
        :param table: target Hive table, use dot notation to target a
            specific database
        :type table: str
        :param field_dict: mapping from column name to hive data type.
            Note that it must be OrderedDict so as to keep columns' order.
        :type field_dict: collections.OrderedDict
        :param delimiter: field delimiter in the file
        :type delimiter: str
        :param encoding: str encoding to use when writing DataFrame to file
        :type encoding: str
        :param pandas_kwargs: passed to DataFrame.to_csv
        :type pandas_kwargs: dict
        :param kwargs: passed to self.load_file
        """"""

        def _infer_field_types_from_df(df):
            DTYPE_KIND_HIVE_TYPE = {
                'b': 'BOOLEAN',    # boolean
                'i': 'BIGINT',     # signed integer
                'u': 'BIGINT',     # unsigned integer
                'f': 'DOUBLE',     # floating-point
                'c': 'STRING',     # complex floating-point
                'M': 'TIMESTAMP',  # datetime
                'O': 'STRING',     # object
                'S': 'STRING',     # (byte-)string
                'U': 'STRING',     # Unicode
                'V': 'STRING'      # void
            }

            d = OrderedDict()
            for col, dtype in df.dtypes.iteritems():
                d[col] = DTYPE_KIND_HIVE_TYPE[dtype.kind]
            return d

        if pandas_kwargs is None:
            pandas_kwargs = {}

        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:
            with NamedTemporaryFile(dir=tmp_dir, mode=""w"") as f:

                if field_dict is None:
                    field_dict = _infer_field_types_from_df(df)

                df.to_csv(path_or_buf=f,
                          sep=delimiter,
                          header=False,
                          index=False,
                          encoding=encoding,
                          date_format=""%Y-%m-%d %H:%M:%S"",
                          **pandas_kwargs)
                f.flush()

                return self.load_file(filepath=f.name,
                                      table=table,
                                      delimiter=delimiter,
                                      field_dict=field_dict,
                                      **kwargs)",python,"def load_df(
            self,
            df,
            table,
            field_dict=None,
            delimiter=',',
            encoding='utf8',
            pandas_kwargs=None, **kwargs):
        """"""
        Loads a pandas DataFrame into hive.

        Hive data types will be inferred if not passed but column names will
        not be sanitized.

        :param df: DataFrame to load into a Hive table
        :type df: pandas.DataFrame
        :param table: target Hive table, use dot notation to target a
            specific database
        :type table: str
        :param field_dict: mapping from column name to hive data type.
            Note that it must be OrderedDict so as to keep columns' order.
        :type field_dict: collections.OrderedDict
        :param delimiter: field delimiter in the file
        :type delimiter: str
        :param encoding: str encoding to use when writing DataFrame to file
        :type encoding: str
        :param pandas_kwargs: passed to DataFrame.to_csv
        :type pandas_kwargs: dict
        :param kwargs: passed to self.load_file
        """"""

        def _infer_field_types_from_df(df):
            DTYPE_KIND_HIVE_TYPE = {
                'b': 'BOOLEAN',    # boolean
                'i': 'BIGINT',     # signed integer
                'u': 'BIGINT',     # unsigned integer
                'f': 'DOUBLE',     # floating-point
                'c': 'STRING',     # complex floating-point
                'M': 'TIMESTAMP',  # datetime
                'O': 'STRING',     # object
                'S': 'STRING',     # (byte-)string
                'U': 'STRING',     # Unicode
                'V': 'STRING'      # void
            }

            d = OrderedDict()
            for col, dtype in df.dtypes.iteritems():
                d[col] = DTYPE_KIND_HIVE_TYPE[dtype.kind]
            return d

        if pandas_kwargs is None:
            pandas_kwargs = {}

        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:
            with NamedTemporaryFile(dir=tmp_dir, mode=""w"") as f:

                if field_dict is None:
                    field_dict = _infer_field_types_from_df(df)

                df.to_csv(path_or_buf=f,
                          sep=delimiter,
                          header=False,
                          index=False,
                          encoding=encoding,
                          date_format=""%Y-%m-%d %H:%M:%S"",
                          **pandas_kwargs)
                f.flush()

                return self.load_file(filepath=f.name,
                                      table=table,
                                      delimiter=delimiter,
                                      field_dict=field_dict,
                                      **kwargs)",def,load_df,(,self,",",df,",",table,",",field_dict,=,None,",",delimiter,=,"','",",",encoding,=,'utf8',",",pandas_kwargs,=,None,",",*,*,kwargs,),:,def,_infer_field_types_from_df,(,df,),:,DTYPE_KIND_HIVE_TYPE,=,{,'b',:,'BOOLEAN',",",# boolean,'i',:,'BIGINT',",",# signed integer,'u',:,'BIGINT',"Loads a pandas DataFrame into hive.

        Hive data types will be inferred if not passed but column names will
        not be sanitized.

        :param df: DataFrame to load into a Hive table
        :type df: pandas.DataFrame
        :param table: target Hive table, use dot notation to target a
            specific database
        :type table: str
        :param field_dict: mapping from column name to hive data type.
            Note that it must be OrderedDict so as to keep columns' order.
        :type field_dict: collections.OrderedDict
        :param delimiter: field delimiter in the file
        :type delimiter: str
        :param encoding: str encoding to use when writing DataFrame to file
        :type encoding: str
        :param pandas_kwargs: passed to DataFrame.to_csv
        :type pandas_kwargs: dict
        :param kwargs: passed to self.load_file",Loads,a,pandas,DataFrame,into,hive,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L300-L372,test,",",# unsigned integer,'f',:,'DOUBLE',",",# floating-point,'c',:,'STRING',",",# complex floating-point,'M',:,'TIMESTAMP',",",# datetime,'O',:,'STRING',",",# object,'S',:,'STRING',",",# (byte-)string,'U',:,'STRING',",",# Unicode,'V',:,'STRING',# void,},d,=,OrderedDict,(,),for,col,",",dtype,in,df,.,dtypes,.,iteritems,(,),:,d,[,col,],=,DTYPE_KIND_HIVE_TYPE,[,dtype,.,kind,],return,d,if,pandas_kwargs,is,None,:,pandas_kwargs,=,{,},with,TemporaryDirectory,(,prefix,=,'airflow_hiveop_',),as,tmp_dir,:,with,NamedTemporaryFile,(,dir,=,tmp_dir,",",mode,=,"""w""",),as,f,:,if,field_dict,is,None,:,field_dict,=,_infer_field_types_from_df,(,df,),df,.,to_csv,(,path_or_buf,=,f,",",sep,=,delimiter,",",header,=,False,",",index,=,False,",",encoding,=,encoding,",",date_format,=,"""%Y-%m-%d %H:%M:%S""",",",*,*,pandas_kwargs,),f,.,flush,(,),return,self,.,load_file,(,filepath,=,f,.,name,",",table,=,table,",",delimiter,=,delimiter,",",field_dict,=,field_dict,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveCliHook.load_file,"def load_file(
            self,
            filepath,
            table,
            delimiter="","",
            field_dict=None,
            create=True,
            overwrite=True,
            partition=None,
            recreate=False,
            tblproperties=None):
        """"""
        Loads a local file into Hive

        Note that the table generated in Hive uses ``STORED AS textfile``
        which isn't the most efficient serialization format. If a
        large amount of data is loaded and/or if the tables gets
        queried considerably, you may want to use this operator only to
        stage the data into a temporary table before loading it into its
        final destination using a ``HiveOperator``.

        :param filepath: local filepath of the file to load
        :type filepath: str
        :param table: target Hive table, use dot notation to target a
            specific database
        :type table: str
        :param delimiter: field delimiter in the file
        :type delimiter: str
        :param field_dict: A dictionary of the fields name in the file
            as keys and their Hive types as values.
            Note that it must be OrderedDict so as to keep columns' order.
        :type field_dict: collections.OrderedDict
        :param create: whether to create the table if it doesn't exist
        :type create: bool
        :param overwrite: whether to overwrite the data in table or partition
        :type overwrite: bool
        :param partition: target partition as a dict of partition columns
            and values
        :type partition: dict
        :param recreate: whether to drop and recreate the table at every
            execution
        :type recreate: bool
        :param tblproperties: TBLPROPERTIES of the hive table being created
        :type tblproperties: dict
        """"""
        hql = ''
        if recreate:
            hql += ""DROP TABLE IF EXISTS {table};\n"".format(table=table)
        if create or recreate:
            if field_dict is None:
                raise ValueError(""Must provide a field dict when creating a table"")
            fields = "",\n    "".join(
                [k + ' ' + v for k, v in field_dict.items()])
            hql += ""CREATE TABLE IF NOT EXISTS {table} (\n{fields})\n"".format(
                table=table, fields=fields)
            if partition:
                pfields = "",\n    "".join(
                    [p + "" STRING"" for p in partition])
                hql += ""PARTITIONED BY ({pfields})\n"".format(pfields=pfields)
            hql += ""ROW FORMAT DELIMITED\n""
            hql += ""FIELDS TERMINATED BY '{delimiter}'\n"".format(delimiter=delimiter)
            hql += ""STORED AS textfile\n""
            if tblproperties is not None:
                tprops = "", "".join(
                    [""'{0}'='{1}'"".format(k, v) for k, v in tblproperties.items()])
                hql += ""TBLPROPERTIES({tprops})\n"".format(tprops=tprops)
        hql += "";""
        self.log.info(hql)
        self.run_cli(hql)
        hql = ""LOAD DATA LOCAL INPATH '{filepath}' "".format(filepath=filepath)
        if overwrite:
            hql += ""OVERWRITE ""
        hql += ""INTO TABLE {table} "".format(table=table)
        if partition:
            pvals = "", "".join(
                [""{0}='{1}'"".format(k, v) for k, v in partition.items()])
            hql += ""PARTITION ({pvals})"".format(pvals=pvals)

        # As a workaround for HIVE-10541, add a newline character
        # at the end of hql (AIRFLOW-2412).
        hql += ';\n'

        self.log.info(hql)
        self.run_cli(hql)",python,"def load_file(
            self,
            filepath,
            table,
            delimiter="","",
            field_dict=None,
            create=True,
            overwrite=True,
            partition=None,
            recreate=False,
            tblproperties=None):
        """"""
        Loads a local file into Hive

        Note that the table generated in Hive uses ``STORED AS textfile``
        which isn't the most efficient serialization format. If a
        large amount of data is loaded and/or if the tables gets
        queried considerably, you may want to use this operator only to
        stage the data into a temporary table before loading it into its
        final destination using a ``HiveOperator``.

        :param filepath: local filepath of the file to load
        :type filepath: str
        :param table: target Hive table, use dot notation to target a
            specific database
        :type table: str
        :param delimiter: field delimiter in the file
        :type delimiter: str
        :param field_dict: A dictionary of the fields name in the file
            as keys and their Hive types as values.
            Note that it must be OrderedDict so as to keep columns' order.
        :type field_dict: collections.OrderedDict
        :param create: whether to create the table if it doesn't exist
        :type create: bool
        :param overwrite: whether to overwrite the data in table or partition
        :type overwrite: bool
        :param partition: target partition as a dict of partition columns
            and values
        :type partition: dict
        :param recreate: whether to drop and recreate the table at every
            execution
        :type recreate: bool
        :param tblproperties: TBLPROPERTIES of the hive table being created
        :type tblproperties: dict
        """"""
        hql = ''
        if recreate:
            hql += ""DROP TABLE IF EXISTS {table};\n"".format(table=table)
        if create or recreate:
            if field_dict is None:
                raise ValueError(""Must provide a field dict when creating a table"")
            fields = "",\n    "".join(
                [k + ' ' + v for k, v in field_dict.items()])
            hql += ""CREATE TABLE IF NOT EXISTS {table} (\n{fields})\n"".format(
                table=table, fields=fields)
            if partition:
                pfields = "",\n    "".join(
                    [p + "" STRING"" for p in partition])
                hql += ""PARTITIONED BY ({pfields})\n"".format(pfields=pfields)
            hql += ""ROW FORMAT DELIMITED\n""
            hql += ""FIELDS TERMINATED BY '{delimiter}'\n"".format(delimiter=delimiter)
            hql += ""STORED AS textfile\n""
            if tblproperties is not None:
                tprops = "", "".join(
                    [""'{0}'='{1}'"".format(k, v) for k, v in tblproperties.items()])
                hql += ""TBLPROPERTIES({tprops})\n"".format(tprops=tprops)
        hql += "";""
        self.log.info(hql)
        self.run_cli(hql)
        hql = ""LOAD DATA LOCAL INPATH '{filepath}' "".format(filepath=filepath)
        if overwrite:
            hql += ""OVERWRITE ""
        hql += ""INTO TABLE {table} "".format(table=table)
        if partition:
            pvals = "", "".join(
                [""{0}='{1}'"".format(k, v) for k, v in partition.items()])
            hql += ""PARTITION ({pvals})"".format(pvals=pvals)

        # As a workaround for HIVE-10541, add a newline character
        # at the end of hql (AIRFLOW-2412).
        hql += ';\n'

        self.log.info(hql)
        self.run_cli(hql)",def,load_file,(,self,",",filepath,",",table,",",delimiter,=,""",""",",",field_dict,=,None,",",create,=,True,",",overwrite,=,True,",",partition,=,None,",",recreate,=,False,",",tblproperties,=,None,),:,hql,=,'',if,recreate,:,hql,+=,"""DROP TABLE IF EXISTS {table};\n""",.,format,(,table,=,"Loads a local file into Hive

        Note that the table generated in Hive uses ``STORED AS textfile``
        which isn't the most efficient serialization format. If a
        large amount of data is loaded and/or if the tables gets
        queried considerably, you may want to use this operator only to
        stage the data into a temporary table before loading it into its
        final destination using a ``HiveOperator``.

        :param filepath: local filepath of the file to load
        :type filepath: str
        :param table: target Hive table, use dot notation to target a
            specific database
        :type table: str
        :param delimiter: field delimiter in the file
        :type delimiter: str
        :param field_dict: A dictionary of the fields name in the file
            as keys and their Hive types as values.
            Note that it must be OrderedDict so as to keep columns' order.
        :type field_dict: collections.OrderedDict
        :param create: whether to create the table if it doesn't exist
        :type create: bool
        :param overwrite: whether to overwrite the data in table or partition
        :type overwrite: bool
        :param partition: target partition as a dict of partition columns
            and values
        :type partition: dict
        :param recreate: whether to drop and recreate the table at every
            execution
        :type recreate: bool
        :param tblproperties: TBLPROPERTIES of the hive table being created
        :type tblproperties: dict",Loads,a,local,file,into,Hive,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L374-L457,test,table,),if,create,or,recreate,:,if,field_dict,is,None,:,raise,ValueError,(,"""Must provide a field dict when creating a table""",),fields,=,""",\n    """,.,join,(,[,k,+,' ',+,v,for,k,",",v,in,field_dict,.,items,(,),],),hql,+=,"""CREATE TABLE IF NOT EXISTS {table} (\n{fields})\n""",.,format,(,table,=,table,",",fields,=,fields,),if,partition,:,pfields,=,""",\n    """,.,join,(,[,p,+,""" STRING""",for,p,in,partition,],),hql,+=,"""PARTITIONED BY ({pfields})\n""",.,format,(,pfields,=,pfields,),hql,+=,"""ROW FORMAT DELIMITED\n""",hql,+=,"""FIELDS TERMINATED BY '{delimiter}'\n""",.,format,(,delimiter,=,delimiter,),hql,+=,"""STORED AS textfile\n""",if,tblproperties,is,not,None,:,tprops,=,""", """,.,join,(,[,"""'{0}'='{1}'""",.,format,(,k,",",v,),for,k,",",v,in,tblproperties,.,items,(,),],),hql,+=,"""TBLPROPERTIES({tprops})\n""",.,format,(,tprops,=,tprops,),hql,+=,""";""",self,.,log,.,info,(,hql,),self,.,run_cli,(,hql,),hql,=,"""LOAD DATA LOCAL INPATH '{filepath}' """,.,format,(,filepath,=,filepath,),if,overwrite,:,hql,+=,"""OVERWRITE """,hql,+=,"""INTO TABLE {table} """,.,format,(,table,=,table,),if,partition,:,pvals,=,""", """,.,join,(,[,"""{0}='{1}'""",.,format,(,k,",",v,),for,k,",",v,in,partition,.,items,(,),],),hql,+=,"""PARTITION ({pvals})""",.,format,(,pvals,=,pvals,),"# As a workaround for HIVE-10541, add a newline character",# at the end of hql (AIRFLOW-2412).,hql,+=,';\n',self,.,log,.,info,(,hql,),self,.,run_cli,(,hql,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveMetastoreHook.get_metastore_client,"def get_metastore_client(self):
        """"""
        Returns a Hive thrift client.
        """"""
        import hmsclient
        from thrift.transport import TSocket, TTransport
        from thrift.protocol import TBinaryProtocol
        ms = self.metastore_conn
        auth_mechanism = ms.extra_dejson.get('authMechanism', 'NOSASL')
        if configuration.conf.get('core', 'security') == 'kerberos':
            auth_mechanism = ms.extra_dejson.get('authMechanism', 'GSSAPI')
            kerberos_service_name = ms.extra_dejson.get('kerberos_service_name', 'hive')

        socket = TSocket.TSocket(ms.host, ms.port)
        if configuration.conf.get('core', 'security') == 'kerberos' \
                and auth_mechanism == 'GSSAPI':
            try:
                import saslwrapper as sasl
            except ImportError:
                import sasl

            def sasl_factory():
                sasl_client = sasl.Client()
                sasl_client.setAttr(""host"", ms.host)
                sasl_client.setAttr(""service"", kerberos_service_name)
                sasl_client.init()
                return sasl_client

            from thrift_sasl import TSaslClientTransport
            transport = TSaslClientTransport(sasl_factory, ""GSSAPI"", socket)
        else:
            transport = TTransport.TBufferedTransport(socket)

        protocol = TBinaryProtocol.TBinaryProtocol(transport)

        return hmsclient.HMSClient(iprot=protocol)",python,"def get_metastore_client(self):
        """"""
        Returns a Hive thrift client.
        """"""
        import hmsclient
        from thrift.transport import TSocket, TTransport
        from thrift.protocol import TBinaryProtocol
        ms = self.metastore_conn
        auth_mechanism = ms.extra_dejson.get('authMechanism', 'NOSASL')
        if configuration.conf.get('core', 'security') == 'kerberos':
            auth_mechanism = ms.extra_dejson.get('authMechanism', 'GSSAPI')
            kerberos_service_name = ms.extra_dejson.get('kerberos_service_name', 'hive')

        socket = TSocket.TSocket(ms.host, ms.port)
        if configuration.conf.get('core', 'security') == 'kerberos' \
                and auth_mechanism == 'GSSAPI':
            try:
                import saslwrapper as sasl
            except ImportError:
                import sasl

            def sasl_factory():
                sasl_client = sasl.Client()
                sasl_client.setAttr(""host"", ms.host)
                sasl_client.setAttr(""service"", kerberos_service_name)
                sasl_client.init()
                return sasl_client

            from thrift_sasl import TSaslClientTransport
            transport = TSaslClientTransport(sasl_factory, ""GSSAPI"", socket)
        else:
            transport = TTransport.TBufferedTransport(socket)

        protocol = TBinaryProtocol.TBinaryProtocol(transport)

        return hmsclient.HMSClient(iprot=protocol)",def,get_metastore_client,(,self,),:,import,hmsclient,from,thrift,.,transport,import,TSocket,",",TTransport,from,thrift,.,protocol,import,TBinaryProtocol,ms,=,self,.,metastore_conn,auth_mechanism,=,ms,.,extra_dejson,.,get,(,'authMechanism',",",'NOSASL',),if,configuration,.,conf,.,get,(,'core',",",'security',),==,'kerberos',Returns a Hive thrift client.,Returns,a,Hive,thrift,client,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L489-L524,test,:,auth_mechanism,=,ms,.,extra_dejson,.,get,(,'authMechanism',",",'GSSAPI',),kerberos_service_name,=,ms,.,extra_dejson,.,get,(,'kerberos_service_name',",",'hive',),socket,=,TSocket,.,TSocket,(,ms,.,host,",",ms,.,port,),if,configuration,.,conf,.,get,(,'core',",",'security',),==,'kerberos',and,auth_mechanism,==,'GSSAPI',:,try,:,import,saslwrapper,as,sasl,except,ImportError,:,import,sasl,def,sasl_factory,(,),:,sasl_client,=,sasl,.,Client,(,),sasl_client,.,setAttr,(,"""host""",",",ms,.,host,),sasl_client,.,setAttr,(,"""service""",",",kerberos_service_name,),sasl_client,.,init,(,),return,sasl_client,from,thrift_sasl,import,TSaslClientTransport,transport,=,TSaslClientTransport,(,sasl_factory,",","""GSSAPI""",",",socket,),else,:,transport,=,TTransport,.,TBufferedTransport,(,socket,),protocol,=,TBinaryProtocol,.,TBinaryProtocol,(,transport,),return,hmsclient,.,HMSClient,(,iprot,=,protocol,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveMetastoreHook.check_for_named_partition,"def check_for_named_partition(self, schema, table, partition_name):
        """"""
        Checks whether a partition with a given name exists

        :param schema: Name of hive schema (database) @table belongs to
        :type schema: str
        :param table: Name of hive table @partition belongs to
        :type schema: str
        :partition: Name of the partitions to check for (eg `a=b/c=d`)
        :type schema: str
        :rtype: bool

        >>> hh = HiveMetastoreHook()
        >>> t = 'static_babynames_partitioned'
        >>> hh.check_for_named_partition('airflow', t, ""ds=2015-01-01"")
        True
        >>> hh.check_for_named_partition('airflow', t, ""ds=xxx"")
        False
        """"""
        with self.metastore as client:
            return client.check_for_named_partition(schema, table, partition_name)",python,"def check_for_named_partition(self, schema, table, partition_name):
        """"""
        Checks whether a partition with a given name exists

        :param schema: Name of hive schema (database) @table belongs to
        :type schema: str
        :param table: Name of hive table @partition belongs to
        :type schema: str
        :partition: Name of the partitions to check for (eg `a=b/c=d`)
        :type schema: str
        :rtype: bool

        >>> hh = HiveMetastoreHook()
        >>> t = 'static_babynames_partitioned'
        >>> hh.check_for_named_partition('airflow', t, ""ds=2015-01-01"")
        True
        >>> hh.check_for_named_partition('airflow', t, ""ds=xxx"")
        False
        """"""
        with self.metastore as client:
            return client.check_for_named_partition(schema, table, partition_name)",def,check_for_named_partition,(,self,",",schema,",",table,",",partition_name,),:,with,self,.,metastore,as,client,:,return,client,.,check_for_named_partition,(,schema,",",table,",",partition_name,),,,,,,,,,,,,,,,,,,,,,,,"Checks whether a partition with a given name exists

        :param schema: Name of hive schema (database) @table belongs to
        :type schema: str
        :param table: Name of hive table @partition belongs to
        :type schema: str
        :partition: Name of the partitions to check for (eg `a=b/c=d`)
        :type schema: str
        :rtype: bool

        >>> hh = HiveMetastoreHook()
        >>> t = 'static_babynames_partitioned'
        >>> hh.check_for_named_partition('airflow', t, ""ds=2015-01-01"")
        True
        >>> hh.check_for_named_partition('airflow', t, ""ds=xxx"")
        False",Checks,whether,a,partition,with,a,given,name,exists,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L556-L576,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveMetastoreHook.table_exists,"def table_exists(self, table_name, db='default'):
        """"""
        Check if table exists

        >>> hh = HiveMetastoreHook()
        >>> hh.table_exists(db='airflow', table_name='static_babynames')
        True
        >>> hh.table_exists(db='airflow', table_name='does_not_exist')
        False
        """"""
        try:
            self.get_table(table_name, db)
            return True
        except Exception:
            return False",python,"def table_exists(self, table_name, db='default'):
        """"""
        Check if table exists

        >>> hh = HiveMetastoreHook()
        >>> hh.table_exists(db='airflow', table_name='static_babynames')
        True
        >>> hh.table_exists(db='airflow', table_name='does_not_exist')
        False
        """"""
        try:
            self.get_table(table_name, db)
            return True
        except Exception:
            return False",def,table_exists,(,self,",",table_name,",",db,=,'default',),:,try,:,self,.,get_table,(,table_name,",",db,),return,True,except,Exception,:,return,False,,,,,,,,,,,,,,,,,,,,,,,,"Check if table exists

        >>> hh = HiveMetastoreHook()
        >>> hh.table_exists(db='airflow', table_name='static_babynames')
        True
        >>> hh.table_exists(db='airflow', table_name='does_not_exist')
        False",Check,if,table,exists,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L731-L745,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveServer2Hook.get_conn,"def get_conn(self, schema=None):
        """"""
        Returns a Hive connection object.
        """"""
        db = self.get_connection(self.hiveserver2_conn_id)
        auth_mechanism = db.extra_dejson.get('authMechanism', 'NONE')
        if auth_mechanism == 'NONE' and db.login is None:
            # we need to give a username
            username = 'airflow'
        kerberos_service_name = None
        if configuration.conf.get('core', 'security') == 'kerberos':
            auth_mechanism = db.extra_dejson.get('authMechanism', 'KERBEROS')
            kerberos_service_name = db.extra_dejson.get('kerberos_service_name', 'hive')

        # pyhive uses GSSAPI instead of KERBEROS as a auth_mechanism identifier
        if auth_mechanism == 'GSSAPI':
            self.log.warning(
                ""Detected deprecated 'GSSAPI' for authMechanism ""
                ""for %s. Please use 'KERBEROS' instead"",
                self.hiveserver2_conn_id
            )
            auth_mechanism = 'KERBEROS'

        from pyhive.hive import connect
        return connect(
            host=db.host,
            port=db.port,
            auth=auth_mechanism,
            kerberos_service_name=kerberos_service_name,
            username=db.login or username,
            password=db.password,
            database=schema or db.schema or 'default')",python,"def get_conn(self, schema=None):
        """"""
        Returns a Hive connection object.
        """"""
        db = self.get_connection(self.hiveserver2_conn_id)
        auth_mechanism = db.extra_dejson.get('authMechanism', 'NONE')
        if auth_mechanism == 'NONE' and db.login is None:
            # we need to give a username
            username = 'airflow'
        kerberos_service_name = None
        if configuration.conf.get('core', 'security') == 'kerberos':
            auth_mechanism = db.extra_dejson.get('authMechanism', 'KERBEROS')
            kerberos_service_name = db.extra_dejson.get('kerberos_service_name', 'hive')

        # pyhive uses GSSAPI instead of KERBEROS as a auth_mechanism identifier
        if auth_mechanism == 'GSSAPI':
            self.log.warning(
                ""Detected deprecated 'GSSAPI' for authMechanism ""
                ""for %s. Please use 'KERBEROS' instead"",
                self.hiveserver2_conn_id
            )
            auth_mechanism = 'KERBEROS'

        from pyhive.hive import connect
        return connect(
            host=db.host,
            port=db.port,
            auth=auth_mechanism,
            kerberos_service_name=kerberos_service_name,
            username=db.login or username,
            password=db.password,
            database=schema or db.schema or 'default')",def,get_conn,(,self,",",schema,=,None,),:,db,=,self,.,get_connection,(,self,.,hiveserver2_conn_id,),auth_mechanism,=,db,.,extra_dejson,.,get,(,'authMechanism',",",'NONE',),if,auth_mechanism,==,'NONE',and,db,.,login,is,None,:,# we need to give a username,username,=,'airflow',kerberos_service_name,=,None,if,configuration,Returns a Hive connection object.,Returns,a,Hive,connection,object,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L758-L789,test,.,conf,.,get,(,'core',",",'security',),==,'kerberos',:,auth_mechanism,=,db,.,extra_dejson,.,get,(,'authMechanism',",",'KERBEROS',),kerberos_service_name,=,db,.,extra_dejson,.,get,(,'kerberos_service_name',",",'hive',),# pyhive uses GSSAPI instead of KERBEROS as a auth_mechanism identifier,if,auth_mechanism,==,'GSSAPI',:,self,.,log,.,warning,(,"""Detected deprecated 'GSSAPI' for authMechanism ""","""for %s. Please use 'KERBEROS' instead""",",",self,.,hiveserver2_conn_id,),auth_mechanism,=,'KERBEROS',from,pyhive,.,hive,import,connect,return,connect,(,host,=,db,.,host,",",port,=,db,.,port,",",auth,=,auth_mechanism,",",kerberos_service_name,=,kerberos_service_name,",",username,=,db,.,login,or,username,",",password,=,db,.,password,",",database,=,schema,or,db,.,schema,or,'default',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveServer2Hook.get_results,"def get_results(self, hql, schema='default', fetch_size=None, hive_conf=None):
        """"""
        Get results of the provided hql in target schema.

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param fetch_size: max size of result to fetch.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict
        :return: results of hql execution, dict with data (list of results) and header
        :rtype: dict
        """"""
        results_iter = self._get_results(hql, schema,
                                         fetch_size=fetch_size, hive_conf=hive_conf)
        header = next(results_iter)
        results = {
            'data': list(results_iter),
            'header': header
        }
        return results",python,"def get_results(self, hql, schema='default', fetch_size=None, hive_conf=None):
        """"""
        Get results of the provided hql in target schema.

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param fetch_size: max size of result to fetch.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict
        :return: results of hql execution, dict with data (list of results) and header
        :rtype: dict
        """"""
        results_iter = self._get_results(hql, schema,
                                         fetch_size=fetch_size, hive_conf=hive_conf)
        header = next(results_iter)
        results = {
            'data': list(results_iter),
            'header': header
        }
        return results",def,get_results,(,self,",",hql,",",schema,=,'default',",",fetch_size,=,None,",",hive_conf,=,None,),:,results_iter,=,self,.,_get_results,(,hql,",",schema,",",fetch_size,=,fetch_size,",",hive_conf,=,hive_conf,),header,=,next,(,results_iter,),results,=,{,'data',:,list,(,results_iter,"Get results of the provided hql in target schema.

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param fetch_size: max size of result to fetch.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict
        :return: results of hql execution, dict with data (list of results) and header
        :rtype: dict",Get,results,of,the,provided,hql,in,target,schema,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L834-L856,test,),",",'header',:,header,},return,results,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveServer2Hook.to_csv,"def to_csv(
            self,
            hql,
            csv_filepath,
            schema='default',
            delimiter=',',
            lineterminator='\r\n',
            output_header=True,
            fetch_size=1000,
            hive_conf=None):
        """"""
        Execute hql in target schema and write results to a csv file.

        :param hql: hql to be executed.
        :type hql: str or list
        :param csv_filepath: filepath of csv to write results into.
        :type csv_filepath: str
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param delimiter: delimiter of the csv file, default to ','.
        :type delimiter: str
        :param lineterminator: lineterminator of the csv file.
        :type lineterminator: str
        :param output_header: header of the csv file, default to True.
        :type output_header: bool
        :param fetch_size: number of result rows to write into the csv file, default to 1000.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict

        """"""

        results_iter = self._get_results(hql, schema,
                                         fetch_size=fetch_size, hive_conf=hive_conf)
        header = next(results_iter)
        message = None

        i = 0
        with open(csv_filepath, 'wb') as f:
            writer = csv.writer(f,
                                delimiter=delimiter,
                                lineterminator=lineterminator,
                                encoding='utf-8')
            try:
                if output_header:
                    self.log.debug('Cursor description is %s', header)
                    writer.writerow([c[0] for c in header])

                for i, row in enumerate(results_iter, 1):
                    writer.writerow(row)
                    if i % fetch_size == 0:
                        self.log.info(""Written %s rows so far."", i)
            except ValueError as exception:
                message = str(exception)

        if message:
            # need to clean up the file first
            os.remove(csv_filepath)
            raise ValueError(message)

        self.log.info(""Done. Loaded a total of %s rows."", i)",python,"def to_csv(
            self,
            hql,
            csv_filepath,
            schema='default',
            delimiter=',',
            lineterminator='\r\n',
            output_header=True,
            fetch_size=1000,
            hive_conf=None):
        """"""
        Execute hql in target schema and write results to a csv file.

        :param hql: hql to be executed.
        :type hql: str or list
        :param csv_filepath: filepath of csv to write results into.
        :type csv_filepath: str
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param delimiter: delimiter of the csv file, default to ','.
        :type delimiter: str
        :param lineterminator: lineterminator of the csv file.
        :type lineterminator: str
        :param output_header: header of the csv file, default to True.
        :type output_header: bool
        :param fetch_size: number of result rows to write into the csv file, default to 1000.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict

        """"""

        results_iter = self._get_results(hql, schema,
                                         fetch_size=fetch_size, hive_conf=hive_conf)
        header = next(results_iter)
        message = None

        i = 0
        with open(csv_filepath, 'wb') as f:
            writer = csv.writer(f,
                                delimiter=delimiter,
                                lineterminator=lineterminator,
                                encoding='utf-8')
            try:
                if output_header:
                    self.log.debug('Cursor description is %s', header)
                    writer.writerow([c[0] for c in header])

                for i, row in enumerate(results_iter, 1):
                    writer.writerow(row)
                    if i % fetch_size == 0:
                        self.log.info(""Written %s rows so far."", i)
            except ValueError as exception:
                message = str(exception)

        if message:
            # need to clean up the file first
            os.remove(csv_filepath)
            raise ValueError(message)

        self.log.info(""Done. Loaded a total of %s rows."", i)",def,to_csv,(,self,",",hql,",",csv_filepath,",",schema,=,'default',",",delimiter,=,"','",",",lineterminator,=,'\r\n',",",output_header,=,True,",",fetch_size,=,1000,",",hive_conf,=,None,),:,results_iter,=,self,.,_get_results,(,hql,",",schema,",",fetch_size,=,fetch_size,",",hive_conf,=,hive_conf,),"Execute hql in target schema and write results to a csv file.

        :param hql: hql to be executed.
        :type hql: str or list
        :param csv_filepath: filepath of csv to write results into.
        :type csv_filepath: str
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param delimiter: delimiter of the csv file, default to ','.
        :type delimiter: str
        :param lineterminator: lineterminator of the csv file.
        :type lineterminator: str
        :param output_header: header of the csv file, default to True.
        :type output_header: bool
        :param fetch_size: number of result rows to write into the csv file, default to 1000.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict",Execute,hql,in,target,schema,and,write,results,to,a,csv,file,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L858-L918,test,header,=,next,(,results_iter,),message,=,None,i,=,0,with,open,(,csv_filepath,",",'wb',),as,f,:,writer,=,csv,.,writer,(,f,",",delimiter,=,delimiter,",",lineterminator,=,lineterminator,",",encoding,=,'utf-8',),try,:,if,output_header,:,self,.,log,.,debug,(,'Cursor description is %s',",",header,),writer,.,writerow,(,[,c,[,0,],for,c,in,header,],),for,i,",",row,in,enumerate,(,results_iter,",",1,),:,writer,.,writerow,(,row,),if,i,%,fetch_size,==,0,:,self,.,log,.,info,(,"""Written %s rows so far.""",",",i,),except,ValueError,as,exception,:,message,=,str,(,exception,),if,message,:,# need to clean up the file first,os,.,remove,(,csv_filepath,),raise,ValueError,(,message,),self,.,log,.,info,(,"""Done. Loaded a total of %s rows.""",",",i,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveServer2Hook.get_records,"def get_records(self, hql, schema='default', hive_conf=None):
        """"""
        Get a set of records from a Hive query.

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict
        :return: result of hive execution
        :rtype: list

        >>> hh = HiveServer2Hook()
        >>> sql = ""SELECT * FROM airflow.static_babynames LIMIT 100""
        >>> len(hh.get_records(sql))
        100
        """"""
        return self.get_results(hql, schema=schema, hive_conf=hive_conf)['data']",python,"def get_records(self, hql, schema='default', hive_conf=None):
        """"""
        Get a set of records from a Hive query.

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict
        :return: result of hive execution
        :rtype: list

        >>> hh = HiveServer2Hook()
        >>> sql = ""SELECT * FROM airflow.static_babynames LIMIT 100""
        >>> len(hh.get_records(sql))
        100
        """"""
        return self.get_results(hql, schema=schema, hive_conf=hive_conf)['data']",def,get_records,(,self,",",hql,",",schema,=,'default',",",hive_conf,=,None,),:,return,self,.,get_results,(,hql,",",schema,=,schema,",",hive_conf,=,hive_conf,),[,'data',],,,,,,,,,,,,,,,,,,,"Get a set of records from a Hive query.

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict
        :return: result of hive execution
        :rtype: list

        >>> hh = HiveServer2Hook()
        >>> sql = ""SELECT * FROM airflow.static_babynames LIMIT 100""
        >>> len(hh.get_records(sql))
        100",Get,a,set,of,records,from,a,Hive,query,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L920-L938,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hive_hooks.py,HiveServer2Hook.get_pandas_df,"def get_pandas_df(self, hql, schema='default'):
        """"""
        Get a pandas dataframe from a Hive query

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :return: result of hql execution
        :rtype: DataFrame

        >>> hh = HiveServer2Hook()
        >>> sql = ""SELECT * FROM airflow.static_babynames LIMIT 100""
        >>> df = hh.get_pandas_df(sql)
        >>> len(df.index)
        100

        :return: pandas.DateFrame
        """"""
        import pandas as pd
        res = self.get_results(hql, schema=schema)
        df = pd.DataFrame(res['data'])
        df.columns = [c[0] for c in res['header']]
        return df",python,"def get_pandas_df(self, hql, schema='default'):
        """"""
        Get a pandas dataframe from a Hive query

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :return: result of hql execution
        :rtype: DataFrame

        >>> hh = HiveServer2Hook()
        >>> sql = ""SELECT * FROM airflow.static_babynames LIMIT 100""
        >>> df = hh.get_pandas_df(sql)
        >>> len(df.index)
        100

        :return: pandas.DateFrame
        """"""
        import pandas as pd
        res = self.get_results(hql, schema=schema)
        df = pd.DataFrame(res['data'])
        df.columns = [c[0] for c in res['header']]
        return df",def,get_pandas_df,(,self,",",hql,",",schema,=,'default',),:,import,pandas,as,pd,res,=,self,.,get_results,(,hql,",",schema,=,schema,),df,=,pd,.,DataFrame,(,res,[,'data',],),df,.,columns,=,[,c,[,0,],for,c,in,res,"Get a pandas dataframe from a Hive query

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :return: result of hql execution
        :rtype: DataFrame

        >>> hh = HiveServer2Hook()
        >>> sql = ""SELECT * FROM airflow.static_babynames LIMIT 100""
        >>> df = hh.get_pandas_df(sql)
        >>> len(df.index)
        100

        :return: pandas.DateFrame",Get,a,pandas,dataframe,from,a,Hive,query,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L940-L963,test,[,'header',],],return,df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_vision_hook.py,CloudVisionHook.get_conn,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Vision.

        :return: Google Cloud Vision client object.
        :rtype: google.cloud.vision_v1.ProductSearchClient
        """"""
        if not self._client:
            self._client = ProductSearchClient(credentials=self._get_credentials())
        return self._client",python,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Vision.

        :return: Google Cloud Vision client object.
        :rtype: google.cloud.vision_v1.ProductSearchClient
        """"""
        if not self._client:
            self._client = ProductSearchClient(credentials=self._get_credentials())
        return self._client",def,get_conn,(,self,),:,if,not,self,.,_client,:,self,.,_client,=,ProductSearchClient,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_client,,,,,,,,,,,,,,,,,,,,,,,"Retrieves connection to Cloud Vision.

        :return: Google Cloud Vision client object.
        :rtype: google.cloud.vision_v1.ProductSearchClient",Retrieves,connection,to,Cloud,Vision,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_vision_hook.py#L106-L115,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/dingding_hook.py,DingdingHook._get_endpoint,"def _get_endpoint(self):
        """"""
        Get Dingding endpoint for sending message.
        """"""
        conn = self.get_connection(self.http_conn_id)
        token = conn.password
        if not token:
            raise AirflowException('Dingding token is requests but get nothing, '
                                   'check you conn_id configuration.')
        return 'robot/send?access_token={}'.format(token)",python,"def _get_endpoint(self):
        """"""
        Get Dingding endpoint for sending message.
        """"""
        conn = self.get_connection(self.http_conn_id)
        token = conn.password
        if not token:
            raise AirflowException('Dingding token is requests but get nothing, '
                                   'check you conn_id configuration.')
        return 'robot/send?access_token={}'.format(token)",def,_get_endpoint,(,self,),:,conn,=,self,.,get_connection,(,self,.,http_conn_id,),token,=,conn,.,password,if,not,token,:,raise,AirflowException,(,"'Dingding token is requests but get nothing, '",'check you conn_id configuration.',),return,'robot/send?access_token={}',.,format,(,token,),,,,,,,,,,,,,,,Get Dingding endpoint for sending message.,Get,Dingding,endpoint,for,sending,message,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/dingding_hook.py#L65-L74,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/dingding_hook.py,DingdingHook.send,"def send(self):
        """"""
        Send Dingding message
        """"""
        support_type = ['text', 'link', 'markdown', 'actionCard', 'feedCard']
        if self.message_type not in support_type:
            raise ValueError('DingdingWebhookHook only support {} '
                             'so far, but receive {}'.format(support_type, self.message_type))

        data = self._build_message()
        self.log.info('Sending Dingding type %s message %s', self.message_type, data)
        resp = self.run(endpoint=self._get_endpoint(),
                        data=data,
                        headers={'Content-Type': 'application/json'})

        # Dingding success send message will with errcode equal to 0
        if int(resp.json().get('errcode')) != 0:
            raise AirflowException('Send Dingding message failed, receive error '
                                   'message %s', resp.text)
        self.log.info('Success Send Dingding message')",python,"def send(self):
        """"""
        Send Dingding message
        """"""
        support_type = ['text', 'link', 'markdown', 'actionCard', 'feedCard']
        if self.message_type not in support_type:
            raise ValueError('DingdingWebhookHook only support {} '
                             'so far, but receive {}'.format(support_type, self.message_type))

        data = self._build_message()
        self.log.info('Sending Dingding type %s message %s', self.message_type, data)
        resp = self.run(endpoint=self._get_endpoint(),
                        data=data,
                        headers={'Content-Type': 'application/json'})

        # Dingding success send message will with errcode equal to 0
        if int(resp.json().get('errcode')) != 0:
            raise AirflowException('Send Dingding message failed, receive error '
                                   'message %s', resp.text)
        self.log.info('Success Send Dingding message')",def,send,(,self,),:,support_type,=,[,'text',",",'link',",",'markdown',",",'actionCard',",",'feedCard',],if,self,.,message_type,not,in,support_type,:,raise,ValueError,(,'DingdingWebhookHook only support {} ',"'so far, but receive {}'",.,format,(,support_type,",",self,.,message_type,),),data,=,self,.,_build_message,(,),self,.,log,Send Dingding message,Send,Dingding,message,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/dingding_hook.py#L115-L134,test,.,info,(,'Sending Dingding type %s message %s',",",self,.,message_type,",",data,),resp,=,self,.,run,(,endpoint,=,self,.,_get_endpoint,(,),",",data,=,data,",",headers,=,{,'Content-Type',:,'application/json',},),# Dingding success send message will with errcode equal to 0,if,int,(,resp,.,json,(,),.,get,(,'errcode',),),!=,0,:,raise,AirflowException,(,"'Send Dingding message failed, receive error '",'message %s',",",resp,.,text,),self,.,log,.,info,(,'Success Send Dingding message',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,_bind_parameters,"def _bind_parameters(operation, parameters):
    """""" Helper method that binds parameters to a SQL query. """"""
    # inspired by MySQL Python Connector (conversion.py)
    string_parameters = {}
    for (name, value) in iteritems(parameters):
        if value is None:
            string_parameters[name] = 'NULL'
        elif isinstance(value, basestring):
            string_parameters[name] = ""'"" + _escape(value) + ""'""
        else:
            string_parameters[name] = str(value)
    return operation % string_parameters",python,"def _bind_parameters(operation, parameters):
    """""" Helper method that binds parameters to a SQL query. """"""
    # inspired by MySQL Python Connector (conversion.py)
    string_parameters = {}
    for (name, value) in iteritems(parameters):
        if value is None:
            string_parameters[name] = 'NULL'
        elif isinstance(value, basestring):
            string_parameters[name] = ""'"" + _escape(value) + ""'""
        else:
            string_parameters[name] = str(value)
    return operation % string_parameters",def,_bind_parameters,(,operation,",",parameters,),:,# inspired by MySQL Python Connector (conversion.py),string_parameters,=,{,},for,(,name,",",value,),in,iteritems,(,parameters,),:,if,value,is,None,:,string_parameters,[,name,],=,'NULL',elif,isinstance,(,value,",",basestring,),:,string_parameters,[,name,],=,"""'""",+,_escape,Helper method that binds parameters to a SQL query.,Helper,method,that,binds,parameters,to,a,SQL,query,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1948-L1959,test,(,value,),+,"""'""",else,:,string_parameters,[,name,],=,str,(,value,),return,operation,%,string_parameters,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,_escape,"def _escape(s):
    """""" Helper method that escapes parameters to a SQL query. """"""
    e = s
    e = e.replace('\\', '\\\\')
    e = e.replace('\n', '\\n')
    e = e.replace('\r', '\\r')
    e = e.replace(""'"", ""\\'"")
    e = e.replace('""', '\\""')
    return e",python,"def _escape(s):
    """""" Helper method that escapes parameters to a SQL query. """"""
    e = s
    e = e.replace('\\', '\\\\')
    e = e.replace('\n', '\\n')
    e = e.replace('\r', '\\r')
    e = e.replace(""'"", ""\\'"")
    e = e.replace('""', '\\""')
    return e",def,_escape,(,s,),:,e,=,s,e,=,e,.,replace,(,'\\',",",'\\\\',),e,=,e,.,replace,(,'\n',",",'\\n',),e,=,e,.,replace,(,'\r',",",'\\r',),e,=,e,.,replace,(,"""'""",",","""\\'""",),e,=,e,Helper method that escapes parameters to a SQL query.,Helper,method,that,escapes,parameters,to,a,SQL,query,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1962-L1970,test,.,replace,(,"'""'",",","'\\""'",),return,e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,_bq_cast,"def _bq_cast(string_field, bq_type):
    """"""
    Helper method that casts a BigQuery row to the appropriate data types.
    This is useful because BigQuery returns all fields as strings.
    """"""
    if string_field is None:
        return None
    elif bq_type == 'INTEGER':
        return int(string_field)
    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':
        return float(string_field)
    elif bq_type == 'BOOLEAN':
        if string_field not in ['true', 'false']:
            raise ValueError(""{} must have value 'true' or 'false'"".format(
                string_field))
        return string_field == 'true'
    else:
        return string_field",python,"def _bq_cast(string_field, bq_type):
    """"""
    Helper method that casts a BigQuery row to the appropriate data types.
    This is useful because BigQuery returns all fields as strings.
    """"""
    if string_field is None:
        return None
    elif bq_type == 'INTEGER':
        return int(string_field)
    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':
        return float(string_field)
    elif bq_type == 'BOOLEAN':
        if string_field not in ['true', 'false']:
            raise ValueError(""{} must have value 'true' or 'false'"".format(
                string_field))
        return string_field == 'true'
    else:
        return string_field",def,_bq_cast,(,string_field,",",bq_type,),:,if,string_field,is,None,:,return,None,elif,bq_type,==,'INTEGER',:,return,int,(,string_field,),elif,bq_type,==,'FLOAT',or,bq_type,==,'TIMESTAMP',:,return,float,(,string_field,),elif,bq_type,==,'BOOLEAN',:,if,string_field,not,in,[,'true',",",'false',"Helper method that casts a BigQuery row to the appropriate data types.
    This is useful because BigQuery returns all fields as strings.",Helper,method,that,casts,a,BigQuery,row,to,the,appropriate,data,types,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1973-L1990,test,],:,raise,ValueError,(,"""{} must have value 'true' or 'false'""",.,format,(,string_field,),),return,string_field,==,'true',else,:,return,string_field,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This,is,useful,because,BigQuery,returns,all,fields,as,strings,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,_validate_value,"def _validate_value(key, value, expected_type):
    """""" function to check expected type and raise
    error if type is not correct """"""
    if not isinstance(value, expected_type):
        raise TypeError(""{} argument must have a type {} not {}"".format(
            key, expected_type, type(value)))",python,"def _validate_value(key, value, expected_type):
    """""" function to check expected type and raise
    error if type is not correct """"""
    if not isinstance(value, expected_type):
        raise TypeError(""{} argument must have a type {} not {}"".format(
            key, expected_type, type(value)))",def,_validate_value,(,key,",",value,",",expected_type,),:,if,not,isinstance,(,value,",",expected_type,),:,raise,TypeError,(,"""{} argument must have a type {} not {}""",.,format,(,key,",",expected_type,",",type,(,value,),),),,,,,,,,,,,,,,,,,"function to check expected type and raise
    error if type is not correct",function,to,check,expected,type,and,raise,error,if,type,is,not,correct,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L2071-L2076,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryHook.get_conn,"def get_conn(self):
        """"""
        Returns a BigQuery PEP 249 connection object.
        """"""
        service = self.get_service()
        project = self._get_field('project')
        return BigQueryConnection(
            service=service,
            project_id=project,
            use_legacy_sql=self.use_legacy_sql,
            location=self.location,
            num_retries=self.num_retries
        )",python,"def get_conn(self):
        """"""
        Returns a BigQuery PEP 249 connection object.
        """"""
        service = self.get_service()
        project = self._get_field('project')
        return BigQueryConnection(
            service=service,
            project_id=project,
            use_legacy_sql=self.use_legacy_sql,
            location=self.location,
            num_retries=self.num_retries
        )",def,get_conn,(,self,),:,service,=,self,.,get_service,(,),project,=,self,.,_get_field,(,'project',),return,BigQueryConnection,(,service,=,service,",",project_id,=,project,",",use_legacy_sql,=,self,.,use_legacy_sql,",",location,=,self,.,location,",",num_retries,=,self,.,num_retries,),,,Returns a BigQuery PEP 249 connection object.,Returns,a,BigQuery,PEP,249,connection,object,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L65-L77,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryHook.get_service,"def get_service(self):
        """"""
        Returns a BigQuery service object.
        """"""
        http_authorized = self._authorize()
        return build(
            'bigquery', 'v2', http=http_authorized, cache_discovery=False)",python,"def get_service(self):
        """"""
        Returns a BigQuery service object.
        """"""
        http_authorized = self._authorize()
        return build(
            'bigquery', 'v2', http=http_authorized, cache_discovery=False)",def,get_service,(,self,),:,http_authorized,=,self,.,_authorize,(,),return,build,(,'bigquery',",",'v2',",",http,=,http_authorized,",",cache_discovery,=,False,),,,,,,,,,,,,,,,,,,,,,,,,,Returns a BigQuery service object.,Returns,a,BigQuery,service,object,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L79-L85,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryHook.table_exists,"def table_exists(self, project_id, dataset_id, table_id):
        """"""
        Checks for the existence of a table in Google BigQuery.

        :param project_id: The Google cloud project in which to look for the
            table. The connection supplied to the hook must provide access to
            the specified project.
        :type project_id: str
        :param dataset_id: The name of the dataset in which to look for the
            table.
        :type dataset_id: str
        :param table_id: The name of the table to check the existence of.
        :type table_id: str
        """"""
        service = self.get_service()
        try:
            service.tables().get(
                projectId=project_id, datasetId=dataset_id,
                tableId=table_id).execute(num_retries=self.num_retries)
            return True
        except HttpError as e:
            if e.resp['status'] == '404':
                return False
            raise",python,"def table_exists(self, project_id, dataset_id, table_id):
        """"""
        Checks for the existence of a table in Google BigQuery.

        :param project_id: The Google cloud project in which to look for the
            table. The connection supplied to the hook must provide access to
            the specified project.
        :type project_id: str
        :param dataset_id: The name of the dataset in which to look for the
            table.
        :type dataset_id: str
        :param table_id: The name of the table to check the existence of.
        :type table_id: str
        """"""
        service = self.get_service()
        try:
            service.tables().get(
                projectId=project_id, datasetId=dataset_id,
                tableId=table_id).execute(num_retries=self.num_retries)
            return True
        except HttpError as e:
            if e.resp['status'] == '404':
                return False
            raise",def,table_exists,(,self,",",project_id,",",dataset_id,",",table_id,),:,service,=,self,.,get_service,(,),try,:,service,.,tables,(,),.,get,(,projectId,=,project_id,",",datasetId,=,dataset_id,",",tableId,=,table_id,),.,execute,(,num_retries,=,self,.,num_retries,),return,True,"Checks for the existence of a table in Google BigQuery.

        :param project_id: The Google cloud project in which to look for the
            table. The connection supplied to the hook must provide access to
            the specified project.
        :type project_id: str
        :param dataset_id: The name of the dataset in which to look for the
            table.
        :type dataset_id: str
        :param table_id: The name of the table to check the existence of.
        :type table_id: str",Checks,for,the,existence,of,a,table,in,Google,BigQuery,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L124-L147,test,except,HttpError,as,e,:,if,e,.,resp,[,'status',],==,'404',:,return,False,raise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.create_empty_table,"def create_empty_table(self,
                           project_id,
                           dataset_id,
                           table_id,
                           schema_fields=None,
                           time_partitioning=None,
                           cluster_fields=None,
                           labels=None,
                           view=None,
                           num_retries=None):
        """"""
        Creates a new, empty table in the dataset.
        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg

        :param project_id: The project to create the table into.
        :type project_id: str
        :param dataset_id: The dataset to create the table into.
        :type dataset_id: str
        :param table_id: The Name of the table to be created.
        :type table_id: str
        :param schema_fields: If set, the schema field list as defined here:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema
        :type schema_fields: list
        :param labels: a dictionary containing labels for the table, passed to BigQuery
        :type labels: dict

        **Example**: ::

            schema_fields=[{""name"": ""emp_name"", ""type"": ""STRING"", ""mode"": ""REQUIRED""},
                           {""name"": ""salary"", ""type"": ""INTEGER"", ""mode"": ""NULLABLE""}]

        :param time_partitioning: configure optional time partitioning fields i.e.
            partition by field, type and expiration as per API specifications.

            .. seealso::
                https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning
        :type time_partitioning: dict
        :param cluster_fields: [Optional] The fields used for clustering.
            Must be specified with time_partitioning, data in the table will be first
            partitioned and subsequently clustered.
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields
        :type cluster_fields: list
        :param view: [Optional] A dictionary containing definition for the view.
            If set, it will create a view instead of a table:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view
        :type view: dict

        **Example**: ::

            view = {
                ""query"": ""SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000"",
                ""useLegacySql"": False
            }

        :return: None
        """"""

        project_id = project_id if project_id is not None else self.project_id

        table_resource = {
            'tableReference': {
                'tableId': table_id
            }
        }

        if schema_fields:
            table_resource['schema'] = {'fields': schema_fields}

        if time_partitioning:
            table_resource['timePartitioning'] = time_partitioning

        if cluster_fields:
            table_resource['clustering'] = {
                'fields': cluster_fields
            }

        if labels:
            table_resource['labels'] = labels

        if view:
            table_resource['view'] = view

        num_retries = num_retries if num_retries else self.num_retries

        self.log.info('Creating Table %s:%s.%s',
                      project_id, dataset_id, table_id)

        try:
            self.service.tables().insert(
                projectId=project_id,
                datasetId=dataset_id,
                body=table_resource).execute(num_retries=num_retries)

            self.log.info('Table created successfully: %s:%s.%s',
                          project_id, dataset_id, table_id)

        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content)
            )",python,"def create_empty_table(self,
                           project_id,
                           dataset_id,
                           table_id,
                           schema_fields=None,
                           time_partitioning=None,
                           cluster_fields=None,
                           labels=None,
                           view=None,
                           num_retries=None):
        """"""
        Creates a new, empty table in the dataset.
        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg

        :param project_id: The project to create the table into.
        :type project_id: str
        :param dataset_id: The dataset to create the table into.
        :type dataset_id: str
        :param table_id: The Name of the table to be created.
        :type table_id: str
        :param schema_fields: If set, the schema field list as defined here:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema
        :type schema_fields: list
        :param labels: a dictionary containing labels for the table, passed to BigQuery
        :type labels: dict

        **Example**: ::

            schema_fields=[{""name"": ""emp_name"", ""type"": ""STRING"", ""mode"": ""REQUIRED""},
                           {""name"": ""salary"", ""type"": ""INTEGER"", ""mode"": ""NULLABLE""}]

        :param time_partitioning: configure optional time partitioning fields i.e.
            partition by field, type and expiration as per API specifications.

            .. seealso::
                https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning
        :type time_partitioning: dict
        :param cluster_fields: [Optional] The fields used for clustering.
            Must be specified with time_partitioning, data in the table will be first
            partitioned and subsequently clustered.
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields
        :type cluster_fields: list
        :param view: [Optional] A dictionary containing definition for the view.
            If set, it will create a view instead of a table:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view
        :type view: dict

        **Example**: ::

            view = {
                ""query"": ""SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000"",
                ""useLegacySql"": False
            }

        :return: None
        """"""

        project_id = project_id if project_id is not None else self.project_id

        table_resource = {
            'tableReference': {
                'tableId': table_id
            }
        }

        if schema_fields:
            table_resource['schema'] = {'fields': schema_fields}

        if time_partitioning:
            table_resource['timePartitioning'] = time_partitioning

        if cluster_fields:
            table_resource['clustering'] = {
                'fields': cluster_fields
            }

        if labels:
            table_resource['labels'] = labels

        if view:
            table_resource['view'] = view

        num_retries = num_retries if num_retries else self.num_retries

        self.log.info('Creating Table %s:%s.%s',
                      project_id, dataset_id, table_id)

        try:
            self.service.tables().insert(
                projectId=project_id,
                datasetId=dataset_id,
                body=table_resource).execute(num_retries=num_retries)

            self.log.info('Table created successfully: %s:%s.%s',
                          project_id, dataset_id, table_id)

        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content)
            )",def,create_empty_table,(,self,",",project_id,",",dataset_id,",",table_id,",",schema_fields,=,None,",",time_partitioning,=,None,",",cluster_fields,=,None,",",labels,=,None,",",view,=,None,",",num_retries,=,None,),:,project_id,=,project_id,if,project_id,is,not,None,else,self,.,project_id,table_resource,=,{,'tableReference',"Creates a new, empty table in the dataset.
        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg

        :param project_id: The project to create the table into.
        :type project_id: str
        :param dataset_id: The dataset to create the table into.
        :type dataset_id: str
        :param table_id: The Name of the table to be created.
        :type table_id: str
        :param schema_fields: If set, the schema field list as defined here:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema
        :type schema_fields: list
        :param labels: a dictionary containing labels for the table, passed to BigQuery
        :type labels: dict

        **Example**: ::

            schema_fields=[{""name"": ""emp_name"", ""type"": ""STRING"", ""mode"": ""REQUIRED""},
                           {""name"": ""salary"", ""type"": ""INTEGER"", ""mode"": ""NULLABLE""}]

        :param time_partitioning: configure optional time partitioning fields i.e.
            partition by field, type and expiration as per API specifications.

            .. seealso::
                https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning
        :type time_partitioning: dict
        :param cluster_fields: [Optional] The fields used for clustering.
            Must be specified with time_partitioning, data in the table will be first
            partitioned and subsequently clustered.
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields
        :type cluster_fields: list
        :param view: [Optional] A dictionary containing definition for the view.
            If set, it will create a view instead of a table:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view
        :type view: dict

        **Example**: ::

            view = {
                ""query"": ""SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000"",
                ""useLegacySql"": False
            }

        :return: None",Creates,a,new,empty,table,in,the,dataset,.,To,create,a,view,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L229-L328,test,:,{,'tableId',:,table_id,},},if,schema_fields,:,table_resource,[,'schema',],=,{,'fields',:,schema_fields,},if,time_partitioning,:,table_resource,[,'timePartitioning',],=,time_partitioning,if,cluster_fields,:,table_resource,[,'clustering',],=,{,'fields',:,cluster_fields,},if,labels,:,table_resource,[,'labels',],=,labels,if,view,:,table_resource,[,'view',],=,view,num_retries,=,num_retries,if,num_retries,else,self,.,num_retries,self,.,log,.,info,(,'Creating Table %s:%s.%s',",",project_id,",",dataset_id,",",table_id,),try,:,self,.,service,.,tables,(,),.,insert,(,projectId,=,project_id,",",datasetId,=,dataset_id,",",body,=,table_resource,),.,execute,(,num_retries,=,num_retries,),self,.,log,.,info,(,'Table created successfully: %s:%s.%s',",",project_id,",",dataset_id,",",table_id,),except,HttpError,as,err,:,raise,AirflowException,(,'BigQuery job failed. Error was: {}',.,format,(,err,.,content,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,which,is,defined,by,a,SQL,query,parse,a,dictionary,to,view,kwarg,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.patch_table,"def patch_table(self,
                    dataset_id,
                    table_id,
                    project_id=None,
                    description=None,
                    expiration_time=None,
                    external_data_configuration=None,
                    friendly_name=None,
                    labels=None,
                    schema=None,
                    time_partitioning=None,
                    view=None,
                    require_partition_filter=None):
        """"""
        Patch information in an existing table.
        It only updates fileds that are provided in the request object.

        Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch

        :param dataset_id: The dataset containing the table to be patched.
        :type dataset_id: str
        :param table_id: The Name of the table to be patched.
        :type table_id: str
        :param project_id: The project containing the table to be patched.
        :type project_id: str
        :param description: [Optional] A user-friendly description of this table.
        :type description: str
        :param expiration_time: [Optional] The time when this table expires,
            in milliseconds since the epoch.
        :type expiration_time: int
        :param external_data_configuration: [Optional] A dictionary containing
            properties of a table stored outside of BigQuery.
        :type external_data_configuration: dict
        :param friendly_name: [Optional] A descriptive name for this table.
        :type friendly_name: str
        :param labels: [Optional] A dictionary containing labels associated with this table.
        :type labels: dict
        :param schema: [Optional] If set, the schema field list as defined here:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema
            The supported schema modifications and unsupported schema modification are listed here:
            https://cloud.google.com/bigquery/docs/managing-table-schemas
            **Example**: ::

                schema=[{""name"": ""emp_name"", ""type"": ""STRING"", ""mode"": ""REQUIRED""},
                               {""name"": ""salary"", ""type"": ""INTEGER"", ""mode"": ""NULLABLE""}]

        :type schema: list
        :param time_partitioning: [Optional] A dictionary containing time-based partitioning
             definition for the table.
        :type time_partitioning: dict
        :param view: [Optional] A dictionary containing definition for the view.
            If set, it will patch a view instead of a table:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view
            **Example**: ::

                view = {
                    ""query"": ""SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500"",
                    ""useLegacySql"": False
                }

        :type view: dict
        :param require_partition_filter: [Optional] If true, queries over the this table require a
            partition filter. If false, queries over the table
        :type require_partition_filter: bool

        """"""

        project_id = project_id if project_id is not None else self.project_id

        table_resource = {}

        if description is not None:
            table_resource['description'] = description
        if expiration_time is not None:
            table_resource['expirationTime'] = expiration_time
        if external_data_configuration:
            table_resource['externalDataConfiguration'] = external_data_configuration
        if friendly_name is not None:
            table_resource['friendlyName'] = friendly_name
        if labels:
            table_resource['labels'] = labels
        if schema:
            table_resource['schema'] = {'fields': schema}
        if time_partitioning:
            table_resource['timePartitioning'] = time_partitioning
        if view:
            table_resource['view'] = view
        if require_partition_filter is not None:
            table_resource['requirePartitionFilter'] = require_partition_filter

        self.log.info('Patching Table %s:%s.%s',
                      project_id, dataset_id, table_id)

        try:
            self.service.tables().patch(
                projectId=project_id,
                datasetId=dataset_id,
                tableId=table_id,
                body=table_resource).execute(num_retries=self.num_retries)

            self.log.info('Table patched successfully: %s:%s.%s',
                          project_id, dataset_id, table_id)

        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content)
            )",python,"def patch_table(self,
                    dataset_id,
                    table_id,
                    project_id=None,
                    description=None,
                    expiration_time=None,
                    external_data_configuration=None,
                    friendly_name=None,
                    labels=None,
                    schema=None,
                    time_partitioning=None,
                    view=None,
                    require_partition_filter=None):
        """"""
        Patch information in an existing table.
        It only updates fileds that are provided in the request object.

        Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch

        :param dataset_id: The dataset containing the table to be patched.
        :type dataset_id: str
        :param table_id: The Name of the table to be patched.
        :type table_id: str
        :param project_id: The project containing the table to be patched.
        :type project_id: str
        :param description: [Optional] A user-friendly description of this table.
        :type description: str
        :param expiration_time: [Optional] The time when this table expires,
            in milliseconds since the epoch.
        :type expiration_time: int
        :param external_data_configuration: [Optional] A dictionary containing
            properties of a table stored outside of BigQuery.
        :type external_data_configuration: dict
        :param friendly_name: [Optional] A descriptive name for this table.
        :type friendly_name: str
        :param labels: [Optional] A dictionary containing labels associated with this table.
        :type labels: dict
        :param schema: [Optional] If set, the schema field list as defined here:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema
            The supported schema modifications and unsupported schema modification are listed here:
            https://cloud.google.com/bigquery/docs/managing-table-schemas
            **Example**: ::

                schema=[{""name"": ""emp_name"", ""type"": ""STRING"", ""mode"": ""REQUIRED""},
                               {""name"": ""salary"", ""type"": ""INTEGER"", ""mode"": ""NULLABLE""}]

        :type schema: list
        :param time_partitioning: [Optional] A dictionary containing time-based partitioning
             definition for the table.
        :type time_partitioning: dict
        :param view: [Optional] A dictionary containing definition for the view.
            If set, it will patch a view instead of a table:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view
            **Example**: ::

                view = {
                    ""query"": ""SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500"",
                    ""useLegacySql"": False
                }

        :type view: dict
        :param require_partition_filter: [Optional] If true, queries over the this table require a
            partition filter. If false, queries over the table
        :type require_partition_filter: bool

        """"""

        project_id = project_id if project_id is not None else self.project_id

        table_resource = {}

        if description is not None:
            table_resource['description'] = description
        if expiration_time is not None:
            table_resource['expirationTime'] = expiration_time
        if external_data_configuration:
            table_resource['externalDataConfiguration'] = external_data_configuration
        if friendly_name is not None:
            table_resource['friendlyName'] = friendly_name
        if labels:
            table_resource['labels'] = labels
        if schema:
            table_resource['schema'] = {'fields': schema}
        if time_partitioning:
            table_resource['timePartitioning'] = time_partitioning
        if view:
            table_resource['view'] = view
        if require_partition_filter is not None:
            table_resource['requirePartitionFilter'] = require_partition_filter

        self.log.info('Patching Table %s:%s.%s',
                      project_id, dataset_id, table_id)

        try:
            self.service.tables().patch(
                projectId=project_id,
                datasetId=dataset_id,
                tableId=table_id,
                body=table_resource).execute(num_retries=self.num_retries)

            self.log.info('Table patched successfully: %s:%s.%s',
                          project_id, dataset_id, table_id)

        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content)
            )",def,patch_table,(,self,",",dataset_id,",",table_id,",",project_id,=,None,",",description,=,None,",",expiration_time,=,None,",",external_data_configuration,=,None,",",friendly_name,=,None,",",labels,=,None,",",schema,=,None,",",time_partitioning,=,None,",",view,=,None,",",require_partition_filter,=,None,),:,project_id,=,"Patch information in an existing table.
        It only updates fileds that are provided in the request object.

        Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch

        :param dataset_id: The dataset containing the table to be patched.
        :type dataset_id: str
        :param table_id: The Name of the table to be patched.
        :type table_id: str
        :param project_id: The project containing the table to be patched.
        :type project_id: str
        :param description: [Optional] A user-friendly description of this table.
        :type description: str
        :param expiration_time: [Optional] The time when this table expires,
            in milliseconds since the epoch.
        :type expiration_time: int
        :param external_data_configuration: [Optional] A dictionary containing
            properties of a table stored outside of BigQuery.
        :type external_data_configuration: dict
        :param friendly_name: [Optional] A descriptive name for this table.
        :type friendly_name: str
        :param labels: [Optional] A dictionary containing labels associated with this table.
        :type labels: dict
        :param schema: [Optional] If set, the schema field list as defined here:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema
            The supported schema modifications and unsupported schema modification are listed here:
            https://cloud.google.com/bigquery/docs/managing-table-schemas
            **Example**: ::

                schema=[{""name"": ""emp_name"", ""type"": ""STRING"", ""mode"": ""REQUIRED""},
                               {""name"": ""salary"", ""type"": ""INTEGER"", ""mode"": ""NULLABLE""}]

        :type schema: list
        :param time_partitioning: [Optional] A dictionary containing time-based partitioning
             definition for the table.
        :type time_partitioning: dict
        :param view: [Optional] A dictionary containing definition for the view.
            If set, it will patch a view instead of a table:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view
            **Example**: ::

                view = {
                    ""query"": ""SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500"",
                    ""useLegacySql"": False
                }

        :type view: dict
        :param require_partition_filter: [Optional] If true, queries over the this table require a
            partition filter. If false, queries over the table
        :type require_partition_filter: bool",Patch,information,in,an,existing,table,.,It,only,updates,fileds,that,are,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L526-L632,test,project_id,if,project_id,is,not,None,else,self,.,project_id,table_resource,=,{,},if,description,is,not,None,:,table_resource,[,'description',],=,description,if,expiration_time,is,not,None,:,table_resource,[,'expirationTime',],=,expiration_time,if,external_data_configuration,:,table_resource,[,'externalDataConfiguration',],=,external_data_configuration,if,friendly_name,is,not,None,:,table_resource,[,'friendlyName',],=,friendly_name,if,labels,:,table_resource,[,'labels',],=,labels,if,schema,:,table_resource,[,'schema',],=,{,'fields',:,schema,},if,time_partitioning,:,table_resource,[,'timePartitioning',],=,time_partitioning,if,view,:,table_resource,[,'view',],=,view,if,require_partition_filter,is,not,None,:,table_resource,[,'requirePartitionFilter',],=,require_partition_filter,self,.,log,.,info,(,'Patching Table %s:%s.%s',",",project_id,",",dataset_id,",",table_id,),try,:,self,.,service,.,tables,(,),.,patch,(,projectId,=,project_id,",",datasetId,=,dataset_id,",",tableId,=,table_id,",",body,=,table_resource,),.,execute,(,num_retries,=,self,.,num_retries,),self,.,log,.,info,(,'Table patched successfully: %s:%s.%s',",",project_id,",",dataset_id,",",table_id,),except,HttpError,as,err,:,raise,AirflowException,(,'BigQuery job failed. Error was: {}',.,format,(,err,.,content,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,provided,in,the,request,object,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.cancel_query,"def cancel_query(self):
        """"""
        Cancel all started queries that have not yet completed
        """"""
        jobs = self.service.jobs()
        if (self.running_job_id and
                not self.poll_job_complete(self.running_job_id)):
            self.log.info('Attempting to cancel job : %s, %s', self.project_id,
                          self.running_job_id)
            if self.location:
                jobs.cancel(
                    projectId=self.project_id,
                    jobId=self.running_job_id,
                    location=self.location).execute(num_retries=self.num_retries)
            else:
                jobs.cancel(
                    projectId=self.project_id,
                    jobId=self.running_job_id).execute(num_retries=self.num_retries)
        else:
            self.log.info('No running BigQuery jobs to cancel.')
            return

        # Wait for all the calls to cancel to finish
        max_polling_attempts = 12
        polling_attempts = 0

        job_complete = False
        while polling_attempts < max_polling_attempts and not job_complete:
            polling_attempts = polling_attempts + 1
            job_complete = self.poll_job_complete(self.running_job_id)
            if job_complete:
                self.log.info('Job successfully canceled: %s, %s',
                              self.project_id, self.running_job_id)
            elif polling_attempts == max_polling_attempts:
                self.log.info(
                    ""Stopping polling due to timeout. Job with id %s ""
                    ""has not completed cancel and may or may not finish."",
                    self.running_job_id)
            else:
                self.log.info('Waiting for canceled job with id %s to finish.',
                              self.running_job_id)
                time.sleep(5)",python,"def cancel_query(self):
        """"""
        Cancel all started queries that have not yet completed
        """"""
        jobs = self.service.jobs()
        if (self.running_job_id and
                not self.poll_job_complete(self.running_job_id)):
            self.log.info('Attempting to cancel job : %s, %s', self.project_id,
                          self.running_job_id)
            if self.location:
                jobs.cancel(
                    projectId=self.project_id,
                    jobId=self.running_job_id,
                    location=self.location).execute(num_retries=self.num_retries)
            else:
                jobs.cancel(
                    projectId=self.project_id,
                    jobId=self.running_job_id).execute(num_retries=self.num_retries)
        else:
            self.log.info('No running BigQuery jobs to cancel.')
            return

        # Wait for all the calls to cancel to finish
        max_polling_attempts = 12
        polling_attempts = 0

        job_complete = False
        while polling_attempts < max_polling_attempts and not job_complete:
            polling_attempts = polling_attempts + 1
            job_complete = self.poll_job_complete(self.running_job_id)
            if job_complete:
                self.log.info('Job successfully canceled: %s, %s',
                              self.project_id, self.running_job_id)
            elif polling_attempts == max_polling_attempts:
                self.log.info(
                    ""Stopping polling due to timeout. Job with id %s ""
                    ""has not completed cancel and may or may not finish."",
                    self.running_job_id)
            else:
                self.log.info('Waiting for canceled job with id %s to finish.',
                              self.running_job_id)
                time.sleep(5)",def,cancel_query,(,self,),:,jobs,=,self,.,service,.,jobs,(,),if,(,self,.,running_job_id,and,not,self,.,poll_job_complete,(,self,.,running_job_id,),),:,self,.,log,.,info,(,"'Attempting to cancel job : %s, %s'",",",self,.,project_id,",",self,.,running_job_id,),if,self,.,location,Cancel all started queries that have not yet completed,Cancel,all,started,queries,that,have,not,yet,completed,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1298-L1339,test,:,jobs,.,cancel,(,projectId,=,self,.,project_id,",",jobId,=,self,.,running_job_id,",",location,=,self,.,location,),.,execute,(,num_retries,=,self,.,num_retries,),else,:,jobs,.,cancel,(,projectId,=,self,.,project_id,",",jobId,=,self,.,running_job_id,),.,execute,(,num_retries,=,self,.,num_retries,),else,:,self,.,log,.,info,(,'No running BigQuery jobs to cancel.',),return,# Wait for all the calls to cancel to finish,max_polling_attempts,=,12,polling_attempts,=,0,job_complete,=,False,while,polling_attempts,<,max_polling_attempts,and,not,job_complete,:,polling_attempts,=,polling_attempts,+,1,job_complete,=,self,.,poll_job_complete,(,self,.,running_job_id,),if,job_complete,:,self,.,log,.,info,(,"'Job successfully canceled: %s, %s'",",",self,.,project_id,",",self,.,running_job_id,),elif,polling_attempts,==,max_polling_attempts,:,self,.,log,.,info,(,"""Stopping polling due to timeout. Job with id %s ""","""has not completed cancel and may or may not finish.""",",",self,.,running_job_id,),else,:,self,.,log,.,info,(,'Waiting for canceled job with id %s to finish.',",",self,.,running_job_id,),time,.,sleep,(,5,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.run_table_delete,"def run_table_delete(self, deletion_dataset_table,
                         ignore_if_missing=False):
        """"""
        Delete an existing table from the dataset;
        If the table does not exist, return an error unless ignore_if_missing
        is set to True.

        :param deletion_dataset_table: A dotted
            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table
            will be deleted.
        :type deletion_dataset_table: str
        :param ignore_if_missing: if True, then return success even if the
            requested table does not exist.
        :type ignore_if_missing: bool
        :return:
        """"""
        deletion_project, deletion_dataset, deletion_table = \
            _split_tablename(table_input=deletion_dataset_table,
                             default_project_id=self.project_id)

        try:
            self.service.tables() \
                .delete(projectId=deletion_project,
                        datasetId=deletion_dataset,
                        tableId=deletion_table) \
                .execute(num_retries=self.num_retries)
            self.log.info('Deleted table %s:%s.%s.', deletion_project,
                          deletion_dataset, deletion_table)
        except HttpError:
            if not ignore_if_missing:
                raise Exception('Table deletion failed. Table does not exist.')
            else:
                self.log.info('Table does not exist. Skipping.')",python,"def run_table_delete(self, deletion_dataset_table,
                         ignore_if_missing=False):
        """"""
        Delete an existing table from the dataset;
        If the table does not exist, return an error unless ignore_if_missing
        is set to True.

        :param deletion_dataset_table: A dotted
            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table
            will be deleted.
        :type deletion_dataset_table: str
        :param ignore_if_missing: if True, then return success even if the
            requested table does not exist.
        :type ignore_if_missing: bool
        :return:
        """"""
        deletion_project, deletion_dataset, deletion_table = \
            _split_tablename(table_input=deletion_dataset_table,
                             default_project_id=self.project_id)

        try:
            self.service.tables() \
                .delete(projectId=deletion_project,
                        datasetId=deletion_dataset,
                        tableId=deletion_table) \
                .execute(num_retries=self.num_retries)
            self.log.info('Deleted table %s:%s.%s.', deletion_project,
                          deletion_dataset, deletion_table)
        except HttpError:
            if not ignore_if_missing:
                raise Exception('Table deletion failed. Table does not exist.')
            else:
                self.log.info('Table does not exist. Skipping.')",def,run_table_delete,(,self,",",deletion_dataset_table,",",ignore_if_missing,=,False,),:,deletion_project,",",deletion_dataset,",",deletion_table,=,_split_tablename,(,table_input,=,deletion_dataset_table,",",default_project_id,=,self,.,project_id,),try,:,self,.,service,.,tables,(,),.,delete,(,projectId,=,deletion_project,",",datasetId,=,deletion_dataset,",",tableId,=,"Delete an existing table from the dataset;
        If the table does not exist, return an error unless ignore_if_missing
        is set to True.

        :param deletion_dataset_table: A dotted
            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table
            will be deleted.
        :type deletion_dataset_table: str
        :param ignore_if_missing: if True, then return success even if the
            requested table does not exist.
        :type ignore_if_missing: bool
        :return:",Delete,an,existing,table,from,the,dataset,;,If,the,table,does,not,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1387-L1419,test,deletion_table,),.,execute,(,num_retries,=,self,.,num_retries,),self,.,log,.,info,(,'Deleted table %s:%s.%s.',",",deletion_project,",",deletion_dataset,",",deletion_table,),except,HttpError,:,if,not,ignore_if_missing,:,raise,Exception,(,'Table deletion failed. Table does not exist.',),else,:,self,.,log,.,info,(,'Table does not exist. Skipping.',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,exist,return,an,error,unless,ignore_if_missing,is,set,to,True,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.run_table_upsert,"def run_table_upsert(self, dataset_id, table_resource, project_id=None):
        """"""
        creates a new, empty table in the dataset;
        If the table already exists, update the existing table.
        Since BigQuery does not natively allow table upserts, this is not an
        atomic operation.

        :param dataset_id: the dataset to upsert the table into.
        :type dataset_id: str
        :param table_resource: a table resource. see
            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource
        :type table_resource: dict
        :param project_id: the project to upsert the table into.  If None,
            project will be self.project_id.
        :return:
        """"""
        # check to see if the table exists
        table_id = table_resource['tableReference']['tableId']
        project_id = project_id if project_id is not None else self.project_id
        tables_list_resp = self.service.tables().list(
            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)
        while True:
            for table in tables_list_resp.get('tables', []):
                if table['tableReference']['tableId'] == table_id:
                    # found the table, do update
                    self.log.info('Table %s:%s.%s exists, updating.',
                                  project_id, dataset_id, table_id)
                    return self.service.tables().update(
                        projectId=project_id,
                        datasetId=dataset_id,
                        tableId=table_id,
                        body=table_resource).execute(num_retries=self.num_retries)
            # If there is a next page, we need to check the next page.
            if 'nextPageToken' in tables_list_resp:
                tables_list_resp = self.service.tables()\
                    .list(projectId=project_id,
                          datasetId=dataset_id,
                          pageToken=tables_list_resp['nextPageToken'])\
                    .execute(num_retries=self.num_retries)
            # If there is no next page, then the table doesn't exist.
            else:
                # do insert
                self.log.info('Table %s:%s.%s does not exist. creating.',
                              project_id, dataset_id, table_id)
                return self.service.tables().insert(
                    projectId=project_id,
                    datasetId=dataset_id,
                    body=table_resource).execute(num_retries=self.num_retries)",python,"def run_table_upsert(self, dataset_id, table_resource, project_id=None):
        """"""
        creates a new, empty table in the dataset;
        If the table already exists, update the existing table.
        Since BigQuery does not natively allow table upserts, this is not an
        atomic operation.

        :param dataset_id: the dataset to upsert the table into.
        :type dataset_id: str
        :param table_resource: a table resource. see
            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource
        :type table_resource: dict
        :param project_id: the project to upsert the table into.  If None,
            project will be self.project_id.
        :return:
        """"""
        # check to see if the table exists
        table_id = table_resource['tableReference']['tableId']
        project_id = project_id if project_id is not None else self.project_id
        tables_list_resp = self.service.tables().list(
            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)
        while True:
            for table in tables_list_resp.get('tables', []):
                if table['tableReference']['tableId'] == table_id:
                    # found the table, do update
                    self.log.info('Table %s:%s.%s exists, updating.',
                                  project_id, dataset_id, table_id)
                    return self.service.tables().update(
                        projectId=project_id,
                        datasetId=dataset_id,
                        tableId=table_id,
                        body=table_resource).execute(num_retries=self.num_retries)
            # If there is a next page, we need to check the next page.
            if 'nextPageToken' in tables_list_resp:
                tables_list_resp = self.service.tables()\
                    .list(projectId=project_id,
                          datasetId=dataset_id,
                          pageToken=tables_list_resp['nextPageToken'])\
                    .execute(num_retries=self.num_retries)
            # If there is no next page, then the table doesn't exist.
            else:
                # do insert
                self.log.info('Table %s:%s.%s does not exist. creating.',
                              project_id, dataset_id, table_id)
                return self.service.tables().insert(
                    projectId=project_id,
                    datasetId=dataset_id,
                    body=table_resource).execute(num_retries=self.num_retries)",def,run_table_upsert,(,self,",",dataset_id,",",table_resource,",",project_id,=,None,),:,# check to see if the table exists,table_id,=,table_resource,[,'tableReference',],[,'tableId',],project_id,=,project_id,if,project_id,is,not,None,else,self,.,project_id,tables_list_resp,=,self,.,service,.,tables,(,),.,list,(,projectId,=,project_id,",","creates a new, empty table in the dataset;
        If the table already exists, update the existing table.
        Since BigQuery does not natively allow table upserts, this is not an
        atomic operation.

        :param dataset_id: the dataset to upsert the table into.
        :type dataset_id: str
        :param table_resource: a table resource. see
            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource
        :type table_resource: dict
        :param project_id: the project to upsert the table into.  If None,
            project will be self.project_id.
        :return:",creates,a,new,empty,table,in,the,dataset,;,If,the,table,already,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1421-L1468,test,datasetId,=,dataset_id,),.,execute,(,num_retries,=,self,.,num_retries,),while,True,:,for,table,in,tables_list_resp,.,get,(,'tables',",",[,],),:,if,table,[,'tableReference',],[,'tableId',],==,table_id,:,"# found the table, do update",self,.,log,.,info,(,"'Table %s:%s.%s exists, updating.'",",",project_id,",",dataset_id,",",table_id,),return,self,.,service,.,tables,(,),.,update,(,projectId,=,project_id,",",datasetId,=,dataset_id,",",tableId,=,table_id,",",body,=,table_resource,),.,execute,(,num_retries,=,self,.,num_retries,),"# If there is a next page, we need to check the next page.",if,'nextPageToken',in,tables_list_resp,:,tables_list_resp,=,self,.,service,.,tables,(,),.,list,(,projectId,=,project_id,",",datasetId,=,dataset_id,",",pageToken,=,tables_list_resp,[,'nextPageToken',],),.,execute,(,num_retries,=,self,.,num_retries,),"# If there is no next page, then the table doesn't exist.",else,:,# do insert,self,.,log,.,info,(,'Table %s:%s.%s does not exist. creating.',",",project_id,",",dataset_id,",",table_id,),return,self,.,service,.,tables,(,),.,insert,(,projectId,=,project_id,",",datasetId,=,dataset_id,",",body,=,table_resource,),.,execute,(,num_retries,=,self,.,num_retries,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,exists,update,the,existing,table,.,Since,BigQuery,does,not,natively,allow,table,upserts,this,is,not,an,atomic,operation,.,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.run_grant_dataset_view_access,"def run_grant_dataset_view_access(self,
                                      source_dataset,
                                      view_dataset,
                                      view_table,
                                      source_project=None,
                                      view_project=None):
        """"""
        Grant authorized view access of a dataset to a view table.
        If this view has already been granted access to the dataset, do nothing.
        This method is not atomic.  Running it may clobber a simultaneous update.

        :param source_dataset: the source dataset
        :type source_dataset: str
        :param view_dataset: the dataset that the view is in
        :type view_dataset: str
        :param view_table: the table of the view
        :type view_table: str
        :param source_project: the project of the source dataset. If None,
            self.project_id will be used.
        :type source_project: str
        :param view_project: the project that the view is in. If None,
            self.project_id will be used.
        :type view_project: str
        :return: the datasets resource of the source dataset.
        """"""

        # Apply default values to projects
        source_project = source_project if source_project else self.project_id
        view_project = view_project if view_project else self.project_id

        # we don't want to clobber any existing accesses, so we have to get
        # info on the dataset before we can add view access
        source_dataset_resource = self.service.datasets().get(
            projectId=source_project, datasetId=source_dataset).execute(num_retries=self.num_retries)
        access = source_dataset_resource[
            'access'] if 'access' in source_dataset_resource else []
        view_access = {
            'view': {
                'projectId': view_project,
                'datasetId': view_dataset,
                'tableId': view_table
            }
        }
        # check to see if the view we want to add already exists.
        if view_access not in access:
            self.log.info(
                'Granting table %s:%s.%s authorized view access to %s:%s dataset.',
                view_project, view_dataset, view_table, source_project,
                source_dataset)
            access.append(view_access)
            return self.service.datasets().patch(
                projectId=source_project,
                datasetId=source_dataset,
                body={
                    'access': access
                }).execute(num_retries=self.num_retries)
        else:
            # if view is already in access, do nothing.
            self.log.info(
                'Table %s:%s.%s already has authorized view access to %s:%s dataset.',
                view_project, view_dataset, view_table, source_project, source_dataset)
            return source_dataset_resource",python,"def run_grant_dataset_view_access(self,
                                      source_dataset,
                                      view_dataset,
                                      view_table,
                                      source_project=None,
                                      view_project=None):
        """"""
        Grant authorized view access of a dataset to a view table.
        If this view has already been granted access to the dataset, do nothing.
        This method is not atomic.  Running it may clobber a simultaneous update.

        :param source_dataset: the source dataset
        :type source_dataset: str
        :param view_dataset: the dataset that the view is in
        :type view_dataset: str
        :param view_table: the table of the view
        :type view_table: str
        :param source_project: the project of the source dataset. If None,
            self.project_id will be used.
        :type source_project: str
        :param view_project: the project that the view is in. If None,
            self.project_id will be used.
        :type view_project: str
        :return: the datasets resource of the source dataset.
        """"""

        # Apply default values to projects
        source_project = source_project if source_project else self.project_id
        view_project = view_project if view_project else self.project_id

        # we don't want to clobber any existing accesses, so we have to get
        # info on the dataset before we can add view access
        source_dataset_resource = self.service.datasets().get(
            projectId=source_project, datasetId=source_dataset).execute(num_retries=self.num_retries)
        access = source_dataset_resource[
            'access'] if 'access' in source_dataset_resource else []
        view_access = {
            'view': {
                'projectId': view_project,
                'datasetId': view_dataset,
                'tableId': view_table
            }
        }
        # check to see if the view we want to add already exists.
        if view_access not in access:
            self.log.info(
                'Granting table %s:%s.%s authorized view access to %s:%s dataset.',
                view_project, view_dataset, view_table, source_project,
                source_dataset)
            access.append(view_access)
            return self.service.datasets().patch(
                projectId=source_project,
                datasetId=source_dataset,
                body={
                    'access': access
                }).execute(num_retries=self.num_retries)
        else:
            # if view is already in access, do nothing.
            self.log.info(
                'Table %s:%s.%s already has authorized view access to %s:%s dataset.',
                view_project, view_dataset, view_table, source_project, source_dataset)
            return source_dataset_resource",def,run_grant_dataset_view_access,(,self,",",source_dataset,",",view_dataset,",",view_table,",",source_project,=,None,",",view_project,=,None,),:,# Apply default values to projects,source_project,=,source_project,if,source_project,else,self,.,project_id,view_project,=,view_project,if,view_project,else,self,.,project_id,"# we don't want to clobber any existing accesses, so we have to get",# info on the dataset before we can add view access,source_dataset_resource,=,self,.,service,.,datasets,(,),.,get,"Grant authorized view access of a dataset to a view table.
        If this view has already been granted access to the dataset, do nothing.
        This method is not atomic.  Running it may clobber a simultaneous update.

        :param source_dataset: the source dataset
        :type source_dataset: str
        :param view_dataset: the dataset that the view is in
        :type view_dataset: str
        :param view_table: the table of the view
        :type view_table: str
        :param source_project: the project of the source dataset. If None,
            self.project_id will be used.
        :type source_project: str
        :param view_project: the project that the view is in. If None,
            self.project_id will be used.
        :type view_project: str
        :return: the datasets resource of the source dataset.",Grant,authorized,view,access,of,a,dataset,to,a,view,table,.,If,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1470-L1531,test,(,projectId,=,source_project,",",datasetId,=,source_dataset,),.,execute,(,num_retries,=,self,.,num_retries,),access,=,source_dataset_resource,[,'access',],if,'access',in,source_dataset_resource,else,[,],view_access,=,{,'view',:,{,'projectId',:,view_project,",",'datasetId',:,view_dataset,",",'tableId',:,view_table,},},# check to see if the view we want to add already exists.,if,view_access,not,in,access,:,self,.,log,.,info,(,'Granting table %s:%s.%s authorized view access to %s:%s dataset.',",",view_project,",",view_dataset,",",view_table,",",source_project,",",source_dataset,),access,.,append,(,view_access,),return,self,.,service,.,datasets,(,),.,patch,(,projectId,=,source_project,",",datasetId,=,source_dataset,",",body,=,{,'access',:,access,},),.,execute,(,num_retries,=,self,.,num_retries,),else,:,"# if view is already in access, do nothing.",self,.,log,.,info,(,'Table %s:%s.%s already has authorized view access to %s:%s dataset.',",",view_project,",",view_dataset,",",view_table,",",source_project,",",source_dataset,),return,source_dataset_resource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,this,view,has,already,been,granted,access,to,the,dataset,do,nothing,.,This,method,is,not,atomic,.,Running,it,may,clobber,a,simultaneous,update,.,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.get_dataset,"def get_dataset(self, dataset_id, project_id=None):
        """"""
        Method returns dataset_resource if dataset exist
        and raised 404 error if dataset does not exist

        :param dataset_id: The BigQuery Dataset ID
        :type dataset_id: str
        :param project_id: The GCP Project ID
        :type project_id: str
        :return: dataset_resource

            .. seealso::
                For more information, see Dataset Resource content:
                https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource
        """"""

        if not dataset_id or not isinstance(dataset_id, str):
            raise ValueError(""dataset_id argument must be provided and has ""
                             ""a type 'str'. You provided: {}"".format(dataset_id))

        dataset_project_id = project_id if project_id else self.project_id

        try:
            dataset_resource = self.service.datasets().get(
                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)
            self.log.info(""Dataset Resource: %s"", dataset_resource)
        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content))

        return dataset_resource",python,"def get_dataset(self, dataset_id, project_id=None):
        """"""
        Method returns dataset_resource if dataset exist
        and raised 404 error if dataset does not exist

        :param dataset_id: The BigQuery Dataset ID
        :type dataset_id: str
        :param project_id: The GCP Project ID
        :type project_id: str
        :return: dataset_resource

            .. seealso::
                For more information, see Dataset Resource content:
                https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource
        """"""

        if not dataset_id or not isinstance(dataset_id, str):
            raise ValueError(""dataset_id argument must be provided and has ""
                             ""a type 'str'. You provided: {}"".format(dataset_id))

        dataset_project_id = project_id if project_id else self.project_id

        try:
            dataset_resource = self.service.datasets().get(
                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)
            self.log.info(""Dataset Resource: %s"", dataset_resource)
        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content))

        return dataset_resource",def,get_dataset,(,self,",",dataset_id,",",project_id,=,None,),:,if,not,dataset_id,or,not,isinstance,(,dataset_id,",",str,),:,raise,ValueError,(,"""dataset_id argument must be provided and has ""","""a type 'str'. You provided: {}""",.,format,(,dataset_id,),),dataset_project_id,=,project_id,if,project_id,else,self,.,project_id,try,:,dataset_resource,=,self,.,service,.,"Method returns dataset_resource if dataset exist
        and raised 404 error if dataset does not exist

        :param dataset_id: The BigQuery Dataset ID
        :type dataset_id: str
        :param project_id: The GCP Project ID
        :type project_id: str
        :return: dataset_resource

            .. seealso::
                For more information, see Dataset Resource content:
                https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource",Method,returns,dataset_resource,if,dataset,exist,and,raised,404,error,if,dataset,does,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1625-L1655,test,datasets,(,),.,get,(,datasetId,=,dataset_id,",",projectId,=,dataset_project_id,),.,execute,(,num_retries,=,self,.,num_retries,),self,.,log,.,info,(,"""Dataset Resource: %s""",",",dataset_resource,),except,HttpError,as,err,:,raise,AirflowException,(,'BigQuery job failed. Error was: {}',.,format,(,err,.,content,),),return,dataset_resource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,not,exist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.get_datasets_list,"def get_datasets_list(self, project_id=None):
        """"""
        Method returns full list of BigQuery datasets in the current project

        .. seealso::
            For more information, see:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list

        :param project_id: Google Cloud Project for which you
            try to get all datasets
        :type project_id: str
        :return: datasets_list

            Example of returned datasets_list: ::

                   {
                      ""kind"":""bigquery#dataset"",
                      ""location"":""US"",
                      ""id"":""your-project:dataset_2_test"",
                      ""datasetReference"":{
                         ""projectId"":""your-project"",
                         ""datasetId"":""dataset_2_test""
                      }
                   },
                   {
                      ""kind"":""bigquery#dataset"",
                      ""location"":""US"",
                      ""id"":""your-project:dataset_1_test"",
                      ""datasetReference"":{
                         ""projectId"":""your-project"",
                         ""datasetId"":""dataset_1_test""
                      }
                   }
                ]
        """"""
        dataset_project_id = project_id if project_id else self.project_id

        try:
            datasets_list = self.service.datasets().list(
                projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets']
            self.log.info(""Datasets List: %s"", datasets_list)

        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content))

        return datasets_list",python,"def get_datasets_list(self, project_id=None):
        """"""
        Method returns full list of BigQuery datasets in the current project

        .. seealso::
            For more information, see:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list

        :param project_id: Google Cloud Project for which you
            try to get all datasets
        :type project_id: str
        :return: datasets_list

            Example of returned datasets_list: ::

                   {
                      ""kind"":""bigquery#dataset"",
                      ""location"":""US"",
                      ""id"":""your-project:dataset_2_test"",
                      ""datasetReference"":{
                         ""projectId"":""your-project"",
                         ""datasetId"":""dataset_2_test""
                      }
                   },
                   {
                      ""kind"":""bigquery#dataset"",
                      ""location"":""US"",
                      ""id"":""your-project:dataset_1_test"",
                      ""datasetReference"":{
                         ""projectId"":""your-project"",
                         ""datasetId"":""dataset_1_test""
                      }
                   }
                ]
        """"""
        dataset_project_id = project_id if project_id else self.project_id

        try:
            datasets_list = self.service.datasets().list(
                projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets']
            self.log.info(""Datasets List: %s"", datasets_list)

        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content))

        return datasets_list",def,get_datasets_list,(,self,",",project_id,=,None,),:,dataset_project_id,=,project_id,if,project_id,else,self,.,project_id,try,:,datasets_list,=,self,.,service,.,datasets,(,),.,list,(,projectId,=,dataset_project_id,),.,execute,(,num_retries,=,self,.,num_retries,),[,'datasets',],self,.,log,"Method returns full list of BigQuery datasets in the current project

        .. seealso::
            For more information, see:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list

        :param project_id: Google Cloud Project for which you
            try to get all datasets
        :type project_id: str
        :return: datasets_list

            Example of returned datasets_list: ::

                   {
                      ""kind"":""bigquery#dataset"",
                      ""location"":""US"",
                      ""id"":""your-project:dataset_2_test"",
                      ""datasetReference"":{
                         ""projectId"":""your-project"",
                         ""datasetId"":""dataset_2_test""
                      }
                   },
                   {
                      ""kind"":""bigquery#dataset"",
                      ""location"":""US"",
                      ""id"":""your-project:dataset_1_test"",
                      ""datasetReference"":{
                         ""projectId"":""your-project"",
                         ""datasetId"":""dataset_1_test""
                      }
                   }
                ]",Method,returns,full,list,of,BigQuery,datasets,in,the,current,project,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1657-L1703,test,.,info,(,"""Datasets List: %s""",",",datasets_list,),except,HttpError,as,err,:,raise,AirflowException,(,'BigQuery job failed. Error was: {}',.,format,(,err,.,content,),),return,datasets_list,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryBaseCursor.insert_all,"def insert_all(self, project_id, dataset_id, table_id,
                   rows, ignore_unknown_values=False,
                   skip_invalid_rows=False, fail_on_error=False):
        """"""
        Method to stream data into BigQuery one record at a time without needing
        to run a load job

        .. seealso::
            For more information, see:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll

        :param project_id: The name of the project where we have the table
        :type project_id: str
        :param dataset_id: The name of the dataset where we have the table
        :type dataset_id: str
        :param table_id: The name of the table
        :type table_id: str
        :param rows: the rows to insert
        :type rows: list

        **Example or rows**:
            rows=[{""json"": {""a_key"": ""a_value_0""}}, {""json"": {""a_key"": ""a_value_1""}}]

        :param ignore_unknown_values: [Optional] Accept rows that contain values
            that do not match the schema. The unknown values are ignored.
            The default value  is false, which treats unknown values as errors.
        :type ignore_unknown_values: bool
        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,
            even if invalid rows exist. The default value is false, which causes
            the entire request to fail if any invalid rows exist.
        :type skip_invalid_rows: bool
        :param fail_on_error: [Optional] Force the task to fail if any errors occur.
            The default value is false, which indicates the task should not fail
            even if any insertion errors occur.
        :type fail_on_error: bool
        """"""

        dataset_project_id = project_id if project_id else self.project_id

        body = {
            ""rows"": rows,
            ""ignoreUnknownValues"": ignore_unknown_values,
            ""kind"": ""bigquery#tableDataInsertAllRequest"",
            ""skipInvalidRows"": skip_invalid_rows,
        }

        try:
            self.log.info(
                'Inserting %s row(s) into Table %s:%s.%s',
                len(rows), dataset_project_id, dataset_id, table_id
            )

            resp = self.service.tabledata().insertAll(
                projectId=dataset_project_id, datasetId=dataset_id,
                tableId=table_id, body=body
            ).execute(num_retries=self.num_retries)

            if 'insertErrors' not in resp:
                self.log.info(
                    'All row(s) inserted successfully: %s:%s.%s',
                    dataset_project_id, dataset_id, table_id
                )
            else:
                error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}'.format(
                    len(resp['insertErrors']),
                    dataset_project_id, dataset_id, table_id, resp['insertErrors'])
                if fail_on_error:
                    raise AirflowException(
                        'BigQuery job failed. Error was: {}'.format(error_msg)
                    )
                self.log.info(error_msg)
        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content)
            )",python,"def insert_all(self, project_id, dataset_id, table_id,
                   rows, ignore_unknown_values=False,
                   skip_invalid_rows=False, fail_on_error=False):
        """"""
        Method to stream data into BigQuery one record at a time without needing
        to run a load job

        .. seealso::
            For more information, see:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll

        :param project_id: The name of the project where we have the table
        :type project_id: str
        :param dataset_id: The name of the dataset where we have the table
        :type dataset_id: str
        :param table_id: The name of the table
        :type table_id: str
        :param rows: the rows to insert
        :type rows: list

        **Example or rows**:
            rows=[{""json"": {""a_key"": ""a_value_0""}}, {""json"": {""a_key"": ""a_value_1""}}]

        :param ignore_unknown_values: [Optional] Accept rows that contain values
            that do not match the schema. The unknown values are ignored.
            The default value  is false, which treats unknown values as errors.
        :type ignore_unknown_values: bool
        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,
            even if invalid rows exist. The default value is false, which causes
            the entire request to fail if any invalid rows exist.
        :type skip_invalid_rows: bool
        :param fail_on_error: [Optional] Force the task to fail if any errors occur.
            The default value is false, which indicates the task should not fail
            even if any insertion errors occur.
        :type fail_on_error: bool
        """"""

        dataset_project_id = project_id if project_id else self.project_id

        body = {
            ""rows"": rows,
            ""ignoreUnknownValues"": ignore_unknown_values,
            ""kind"": ""bigquery#tableDataInsertAllRequest"",
            ""skipInvalidRows"": skip_invalid_rows,
        }

        try:
            self.log.info(
                'Inserting %s row(s) into Table %s:%s.%s',
                len(rows), dataset_project_id, dataset_id, table_id
            )

            resp = self.service.tabledata().insertAll(
                projectId=dataset_project_id, datasetId=dataset_id,
                tableId=table_id, body=body
            ).execute(num_retries=self.num_retries)

            if 'insertErrors' not in resp:
                self.log.info(
                    'All row(s) inserted successfully: %s:%s.%s',
                    dataset_project_id, dataset_id, table_id
                )
            else:
                error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}'.format(
                    len(resp['insertErrors']),
                    dataset_project_id, dataset_id, table_id, resp['insertErrors'])
                if fail_on_error:
                    raise AirflowException(
                        'BigQuery job failed. Error was: {}'.format(error_msg)
                    )
                self.log.info(error_msg)
        except HttpError as err:
            raise AirflowException(
                'BigQuery job failed. Error was: {}'.format(err.content)
            )",def,insert_all,(,self,",",project_id,",",dataset_id,",",table_id,",",rows,",",ignore_unknown_values,=,False,",",skip_invalid_rows,=,False,",",fail_on_error,=,False,),:,dataset_project_id,=,project_id,if,project_id,else,self,.,project_id,body,=,{,"""rows""",:,rows,",","""ignoreUnknownValues""",:,ignore_unknown_values,",","""kind""",:,"""bigquery#tableDataInsertAllRequest""",",","""skipInvalidRows""",:,"Method to stream data into BigQuery one record at a time without needing
        to run a load job

        .. seealso::
            For more information, see:
            https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll

        :param project_id: The name of the project where we have the table
        :type project_id: str
        :param dataset_id: The name of the dataset where we have the table
        :type dataset_id: str
        :param table_id: The name of the table
        :type table_id: str
        :param rows: the rows to insert
        :type rows: list

        **Example or rows**:
            rows=[{""json"": {""a_key"": ""a_value_0""}}, {""json"": {""a_key"": ""a_value_1""}}]

        :param ignore_unknown_values: [Optional] Accept rows that contain values
            that do not match the schema. The unknown values are ignored.
            The default value  is false, which treats unknown values as errors.
        :type ignore_unknown_values: bool
        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,
            even if invalid rows exist. The default value is false, which causes
            the entire request to fail if any invalid rows exist.
        :type skip_invalid_rows: bool
        :param fail_on_error: [Optional] Force the task to fail if any errors occur.
            The default value is false, which indicates the task should not fail
            even if any insertion errors occur.
        :type fail_on_error: bool",Method,to,stream,data,into,BigQuery,one,record,at,a,time,without,needing,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1705-L1779,test,skip_invalid_rows,",",},try,:,self,.,log,.,info,(,'Inserting %s row(s) into Table %s:%s.%s',",",len,(,rows,),",",dataset_project_id,",",dataset_id,",",table_id,),resp,=,self,.,service,.,tabledata,(,),.,insertAll,(,projectId,=,dataset_project_id,",",datasetId,=,dataset_id,",",tableId,=,table_id,",",body,=,body,),.,execute,(,num_retries,=,self,.,num_retries,),if,'insertErrors',not,in,resp,:,self,.,log,.,info,(,'All row(s) inserted successfully: %s:%s.%s',",",dataset_project_id,",",dataset_id,",",table_id,),else,:,error_msg,=,'{} insert error(s) occurred: {}:{}.{}. Details: {}',.,format,(,len,(,resp,[,'insertErrors',],),",",dataset_project_id,",",dataset_id,",",table_id,",",resp,[,'insertErrors',],),if,fail_on_error,:,raise,AirflowException,(,'BigQuery job failed. Error was: {}',.,format,(,error_msg,),),self,.,log,.,info,(,error_msg,),except,HttpError,as,err,:,raise,AirflowException,(,'BigQuery job failed. Error was: {}',.,format,(,err,.,content,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,run,a,load,job,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryCursor.execute,"def execute(self, operation, parameters=None):
        """"""
        Executes a BigQuery query, and returns the job ID.

        :param operation: The query to execute.
        :type operation: str
        :param parameters: Parameters to substitute into the query.
        :type parameters: dict
        """"""
        sql = _bind_parameters(operation,
                               parameters) if parameters else operation
        self.job_id = self.run_query(sql)",python,"def execute(self, operation, parameters=None):
        """"""
        Executes a BigQuery query, and returns the job ID.

        :param operation: The query to execute.
        :type operation: str
        :param parameters: Parameters to substitute into the query.
        :type parameters: dict
        """"""
        sql = _bind_parameters(operation,
                               parameters) if parameters else operation
        self.job_id = self.run_query(sql)",def,execute,(,self,",",operation,",",parameters,=,None,),:,sql,=,_bind_parameters,(,operation,",",parameters,),if,parameters,else,operation,self,.,job_id,=,self,.,run_query,(,sql,),,,,,,,,,,,,,,,,,,,"Executes a BigQuery query, and returns the job ID.

        :param operation: The query to execute.
        :type operation: str
        :param parameters: Parameters to substitute into the query.
        :type parameters: dict",Executes,a,BigQuery,query,and,returns,the,job,ID,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1819-L1830,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryCursor.executemany,"def executemany(self, operation, seq_of_parameters):
        """"""
        Execute a BigQuery query multiple times with different parameters.

        :param operation: The query to execute.
        :type operation: str
        :param seq_of_parameters: List of dictionary parameters to substitute into the
            query.
        :type seq_of_parameters: list
        """"""
        for parameters in seq_of_parameters:
            self.execute(operation, parameters)",python,"def executemany(self, operation, seq_of_parameters):
        """"""
        Execute a BigQuery query multiple times with different parameters.

        :param operation: The query to execute.
        :type operation: str
        :param seq_of_parameters: List of dictionary parameters to substitute into the
            query.
        :type seq_of_parameters: list
        """"""
        for parameters in seq_of_parameters:
            self.execute(operation, parameters)",def,executemany,(,self,",",operation,",",seq_of_parameters,),:,for,parameters,in,seq_of_parameters,:,self,.,execute,(,operation,",",parameters,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Execute a BigQuery query multiple times with different parameters.

        :param operation: The query to execute.
        :type operation: str
        :param seq_of_parameters: List of dictionary parameters to substitute into the
            query.
        :type seq_of_parameters: list",Execute,a,BigQuery,query,multiple,times,with,different,parameters,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1832-L1843,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/bigquery_hook.py,BigQueryCursor.next,"def next(self):
        """"""
        Helper method for fetchone, which returns the next row from a buffer.
        If the buffer is empty, attempts to paginate through the result set for
        the next page, and load it into the buffer.
        """"""
        if not self.job_id:
            return None

        if len(self.buffer) == 0:
            if self.all_pages_loaded:
                return None

            query_results = (self.service.jobs().getQueryResults(
                projectId=self.project_id,
                jobId=self.job_id,
                pageToken=self.page_token).execute(num_retries=self.num_retries))

            if 'rows' in query_results and query_results['rows']:
                self.page_token = query_results.get('pageToken')
                fields = query_results['schema']['fields']
                col_types = [field['type'] for field in fields]
                rows = query_results['rows']

                for dict_row in rows:
                    typed_row = ([
                        _bq_cast(vs['v'], col_types[idx])
                        for idx, vs in enumerate(dict_row['f'])
                    ])
                    self.buffer.append(typed_row)

                if not self.page_token:
                    self.all_pages_loaded = True

            else:
                # Reset all state since we've exhausted the results.
                self.page_token = None
                self.job_id = None
                self.page_token = None
                return None

        return self.buffer.pop(0)",python,"def next(self):
        """"""
        Helper method for fetchone, which returns the next row from a buffer.
        If the buffer is empty, attempts to paginate through the result set for
        the next page, and load it into the buffer.
        """"""
        if not self.job_id:
            return None

        if len(self.buffer) == 0:
            if self.all_pages_loaded:
                return None

            query_results = (self.service.jobs().getQueryResults(
                projectId=self.project_id,
                jobId=self.job_id,
                pageToken=self.page_token).execute(num_retries=self.num_retries))

            if 'rows' in query_results and query_results['rows']:
                self.page_token = query_results.get('pageToken')
                fields = query_results['schema']['fields']
                col_types = [field['type'] for field in fields]
                rows = query_results['rows']

                for dict_row in rows:
                    typed_row = ([
                        _bq_cast(vs['v'], col_types[idx])
                        for idx, vs in enumerate(dict_row['f'])
                    ])
                    self.buffer.append(typed_row)

                if not self.page_token:
                    self.all_pages_loaded = True

            else:
                # Reset all state since we've exhausted the results.
                self.page_token = None
                self.job_id = None
                self.page_token = None
                return None

        return self.buffer.pop(0)",def,next,(,self,),:,if,not,self,.,job_id,:,return,None,if,len,(,self,.,buffer,),==,0,:,if,self,.,all_pages_loaded,:,return,None,query_results,=,(,self,.,service,.,jobs,(,),.,getQueryResults,(,projectId,=,self,.,project_id,",",jobId,=,"Helper method for fetchone, which returns the next row from a buffer.
        If the buffer is empty, attempts to paginate through the result set for
        the next page, and load it into the buffer.",Helper,method,for,fetchone,which,returns,the,next,row,from,a,buffer,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1849-L1890,test,self,.,job_id,",",pageToken,=,self,.,page_token,),.,execute,(,num_retries,=,self,.,num_retries,),),if,'rows',in,query_results,and,query_results,[,'rows',],:,self,.,page_token,=,query_results,.,get,(,'pageToken',),fields,=,query_results,[,'schema',],[,'fields',],col_types,=,[,field,[,'type',],for,field,in,fields,],rows,=,query_results,[,'rows',],for,dict_row,in,rows,:,typed_row,=,(,[,_bq_cast,(,vs,[,'v',],",",col_types,[,idx,],),for,idx,",",vs,in,enumerate,(,dict_row,[,'f',],),],),self,.,buffer,.,append,(,typed_row,),if,not,self,.,page_token,:,self,.,all_pages_loaded,=,True,else,:,# Reset all state since we've exhausted the results.,self,.,page_token,=,None,self,.,job_id,=,None,self,.,page_token,=,None,return,None,return,self,.,buffer,.,pop,(,0,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,If,the,buffer,is,empty,attempts,to,paginate,through,the,result,set,for,the,next,page,and,load,it,into,the,buffer,.,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/postgres_to_gcs_operator.py,PostgresToGoogleCloudStorageOperator._query_postgres,"def _query_postgres(self):
        """"""
        Queries Postgres and returns a cursor to the results.
        """"""
        postgres = PostgresHook(postgres_conn_id=self.postgres_conn_id)
        conn = postgres.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql, self.parameters)
        return cursor",python,"def _query_postgres(self):
        """"""
        Queries Postgres and returns a cursor to the results.
        """"""
        postgres = PostgresHook(postgres_conn_id=self.postgres_conn_id)
        conn = postgres.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql, self.parameters)
        return cursor",def,_query_postgres,(,self,),:,postgres,=,PostgresHook,(,postgres_conn_id,=,self,.,postgres_conn_id,),conn,=,postgres,.,get_conn,(,),cursor,=,conn,.,cursor,(,),cursor,.,execute,(,self,.,sql,",",self,.,parameters,),return,cursor,,,,,,,,,Queries Postgres and returns a cursor to the results.,Queries,Postgres,and,returns,a,cursor,to,the,results,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/postgres_to_gcs_operator.py#L114-L122,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/sftp_operator.py,_make_intermediate_dirs,"def _make_intermediate_dirs(sftp_client, remote_directory):
    """"""
    Create all the intermediate directories in a remote host

    :param sftp_client: A Paramiko SFTP client.
    :param remote_directory: Absolute Path of the directory containing the file
    :return:
    """"""
    if remote_directory == '/':
        sftp_client.chdir('/')
        return
    if remote_directory == '':
        return
    try:
        sftp_client.chdir(remote_directory)
    except IOError:
        dirname, basename = os.path.split(remote_directory.rstrip('/'))
        _make_intermediate_dirs(sftp_client, dirname)
        sftp_client.mkdir(basename)
        sftp_client.chdir(basename)
        return",python,"def _make_intermediate_dirs(sftp_client, remote_directory):
    """"""
    Create all the intermediate directories in a remote host

    :param sftp_client: A Paramiko SFTP client.
    :param remote_directory: Absolute Path of the directory containing the file
    :return:
    """"""
    if remote_directory == '/':
        sftp_client.chdir('/')
        return
    if remote_directory == '':
        return
    try:
        sftp_client.chdir(remote_directory)
    except IOError:
        dirname, basename = os.path.split(remote_directory.rstrip('/'))
        _make_intermediate_dirs(sftp_client, dirname)
        sftp_client.mkdir(basename)
        sftp_client.chdir(basename)
        return",def,_make_intermediate_dirs,(,sftp_client,",",remote_directory,),:,if,remote_directory,==,'/',:,sftp_client,.,chdir,(,'/',),return,if,remote_directory,==,'',:,return,try,:,sftp_client,.,chdir,(,remote_directory,),except,IOError,:,dirname,",",basename,=,os,.,path,.,split,(,remote_directory,.,rstrip,(,'/',"Create all the intermediate directories in a remote host

    :param sftp_client: A Paramiko SFTP client.
    :param remote_directory: Absolute Path of the directory containing the file
    :return:",Create,all,the,intermediate,directories,in,a,remote,host,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/sftp_operator.py#L160-L180,test,),),_make_intermediate_dirs,(,sftp_client,",",dirname,),sftp_client,.,mkdir,(,basename,),sftp_client,.,chdir,(,basename,),return,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_sqs_hook.py,SQSHook.create_queue,"def create_queue(self, queue_name, attributes=None):
        """"""
        Create queue using connection object

        :param queue_name: name of the queue.
        :type queue_name: str
        :param attributes: additional attributes for the queue (default: None)
            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`
        :type attributes: dict

        :return: dict with the information about the queue
            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`
        :rtype: dict
        """"""
        return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})",python,"def create_queue(self, queue_name, attributes=None):
        """"""
        Create queue using connection object

        :param queue_name: name of the queue.
        :type queue_name: str
        :param attributes: additional attributes for the queue (default: None)
            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`
        :type attributes: dict

        :return: dict with the information about the queue
            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`
        :rtype: dict
        """"""
        return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})",def,create_queue,(,self,",",queue_name,",",attributes,=,None,),:,return,self,.,get_conn,(,),.,create_queue,(,QueueName,=,queue_name,",",Attributes,=,attributes,or,{,},),,,,,,,,,,,,,,,,,,,,,"Create queue using connection object

        :param queue_name: name of the queue.
        :type queue_name: str
        :param attributes: additional attributes for the queue (default: None)
            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`
        :type attributes: dict

        :return: dict with the information about the queue
            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`
        :rtype: dict",Create,queue,using,connection,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_sqs_hook.py#L34-L48,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_sqs_hook.py,SQSHook.send_message,"def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):
        """"""
        Send message to the queue

        :param queue_url: queue url
        :type queue_url: str
        :param message_body: the contents of the message
        :type message_body: str
        :param delay_seconds: seconds to delay the message
        :type delay_seconds: int
        :param message_attributes: additional attributes for the message (default: None)
            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`
        :type message_attributes: dict

        :return: dict with the information about the message sent
            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`
        :rtype: dict
        """"""
        return self.get_conn().send_message(QueueUrl=queue_url,
                                            MessageBody=message_body,
                                            DelaySeconds=delay_seconds,
                                            MessageAttributes=message_attributes or {})",python,"def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):
        """"""
        Send message to the queue

        :param queue_url: queue url
        :type queue_url: str
        :param message_body: the contents of the message
        :type message_body: str
        :param delay_seconds: seconds to delay the message
        :type delay_seconds: int
        :param message_attributes: additional attributes for the message (default: None)
            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`
        :type message_attributes: dict

        :return: dict with the information about the message sent
            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`
        :rtype: dict
        """"""
        return self.get_conn().send_message(QueueUrl=queue_url,
                                            MessageBody=message_body,
                                            DelaySeconds=delay_seconds,
                                            MessageAttributes=message_attributes or {})",def,send_message,(,self,",",queue_url,",",message_body,",",delay_seconds,=,0,",",message_attributes,=,None,),:,return,self,.,get_conn,(,),.,send_message,(,QueueUrl,=,queue_url,",",MessageBody,=,message_body,",",DelaySeconds,=,delay_seconds,",",MessageAttributes,=,message_attributes,or,{,},),,,,,,,"Send message to the queue

        :param queue_url: queue url
        :type queue_url: str
        :param message_body: the contents of the message
        :type message_body: str
        :param delay_seconds: seconds to delay the message
        :type delay_seconds: int
        :param message_attributes: additional attributes for the message (default: None)
            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`
        :type message_attributes: dict

        :return: dict with the information about the message sent
            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`
        :rtype: dict",Send,message,to,the,queue,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_sqs_hook.py#L50-L71,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/task/task_runner/base_task_runner.py,BaseTaskRunner.run_command,"def run_command(self, run_with=None, join_args=False):
        """"""
        Run the task command.

        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``
        :type run_with: list
        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs
            ``['airflow run']``
        :param join_args: bool
        :return: the process that was run
        :rtype: subprocess.Popen
        """"""
        run_with = run_with or []
        cmd = ["" "".join(self._command)] if join_args else self._command
        full_cmd = run_with + cmd

        self.log.info('Running: %s', full_cmd)
        proc = subprocess.Popen(
            full_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            close_fds=True,
            env=os.environ.copy(),
            preexec_fn=os.setsid
        )

        # Start daemon thread to read subprocess logging output
        log_reader = threading.Thread(
            target=self._read_task_logs,
            args=(proc.stdout,),
        )
        log_reader.daemon = True
        log_reader.start()
        return proc",python,"def run_command(self, run_with=None, join_args=False):
        """"""
        Run the task command.

        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``
        :type run_with: list
        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs
            ``['airflow run']``
        :param join_args: bool
        :return: the process that was run
        :rtype: subprocess.Popen
        """"""
        run_with = run_with or []
        cmd = ["" "".join(self._command)] if join_args else self._command
        full_cmd = run_with + cmd

        self.log.info('Running: %s', full_cmd)
        proc = subprocess.Popen(
            full_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            close_fds=True,
            env=os.environ.copy(),
            preexec_fn=os.setsid
        )

        # Start daemon thread to read subprocess logging output
        log_reader = threading.Thread(
            target=self._read_task_logs,
            args=(proc.stdout,),
        )
        log_reader.daemon = True
        log_reader.start()
        return proc",def,run_command,(,self,",",run_with,=,None,",",join_args,=,False,),:,run_with,=,run_with,or,[,],cmd,=,[,""" """,.,join,(,self,.,_command,),],if,join_args,else,self,.,_command,full_cmd,=,run_with,+,cmd,self,.,log,.,info,(,'Running: %s',",",full_cmd,"Run the task command.

        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``
        :type run_with: list
        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs
            ``['airflow run']``
        :param join_args: bool
        :return: the process that was run
        :rtype: subprocess.Popen",Run,the,task,command,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/task/task_runner/base_task_runner.py#L101-L135,test,),proc,=,subprocess,.,Popen,(,full_cmd,",",stdout,=,subprocess,.,PIPE,",",stderr,=,subprocess,.,STDOUT,",",universal_newlines,=,True,",",close_fds,=,True,",",env,=,os,.,environ,.,copy,(,),",",preexec_fn,=,os,.,setsid,),# Start daemon thread to read subprocess logging output,log_reader,=,threading,.,Thread,(,target,=,self,.,_read_task_logs,",",args,=,(,proc,.,stdout,",",),",",),log_reader,.,daemon,=,True,log_reader,.,start,(,),return,proc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/task/task_runner/base_task_runner.py,BaseTaskRunner.on_finish,"def on_finish(self):
        """"""
        A callback that should be called when this is done running.
        """"""
        if self._cfg_path and os.path.isfile(self._cfg_path):
            if self.run_as_user:
                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)
            else:
                os.remove(self._cfg_path)",python,"def on_finish(self):
        """"""
        A callback that should be called when this is done running.
        """"""
        if self._cfg_path and os.path.isfile(self._cfg_path):
            if self.run_as_user:
                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)
            else:
                os.remove(self._cfg_path)",def,on_finish,(,self,),:,if,self,.,_cfg_path,and,os,.,path,.,isfile,(,self,.,_cfg_path,),:,if,self,.,run_as_user,:,subprocess,.,call,(,[,'sudo',",",'rm',",",self,.,_cfg_path,],",",close_fds,=,True,),else,:,os,.,remove,(,self,A callback that should be called when this is done running.,A,callback,that,should,be,called,when,this,is,done,running,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/task/task_runner/base_task_runner.py#L157-L165,test,.,_cfg_path,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/_vendor/nvd3/NVD3Chart.py,_main,"def _main():
    """"""
    Parse options and process commands
    """"""
    # Parse arguments
    usage = ""usage: nvd3.py [options]""
    parser = OptionParser(usage=usage,
                          version=(""python-nvd3 - Charts generator with ""
                                   ""nvd3.js and d3.js""))
    parser.add_option(""-q"", ""--quiet"",
                      action=""store_false"", dest=""verbose"", default=True,
                      help=""don't print messages to stdout"")

    (options, args) = parser.parse_args()",python,"def _main():
    """"""
    Parse options and process commands
    """"""
    # Parse arguments
    usage = ""usage: nvd3.py [options]""
    parser = OptionParser(usage=usage,
                          version=(""python-nvd3 - Charts generator with ""
                                   ""nvd3.js and d3.js""))
    parser.add_option(""-q"", ""--quiet"",
                      action=""store_false"", dest=""verbose"", default=True,
                      help=""don't print messages to stdout"")

    (options, args) = parser.parse_args()",def,_main,(,),:,# Parse arguments,usage,=,"""usage: nvd3.py [options]""",parser,=,OptionParser,(,usage,=,usage,",",version,=,(,"""python-nvd3 - Charts generator with ""","""nvd3.js and d3.js""",),),parser,.,add_option,(,"""-q""",",","""--quiet""",",",action,=,"""store_false""",",",dest,=,"""verbose""",",",default,=,True,",",help,=,"""don't print messages to stdout""",),(,options,",",args,Parse options and process commands,Parse,options,and,process,commands,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L488-L501,test,),=,parser,.,parse_args,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/_vendor/nvd3/NVD3Chart.py,NVD3Chart.buildhtmlheader,"def buildhtmlheader(self):
        """"""generate HTML header content""""""
        self.htmlheader = ''
        # If the JavaScript assets have already been injected, don't bother re-sourcing them.
        global _js_initialized
        if '_js_initialized' not in globals() or not _js_initialized:
            for css in self.header_css:
                self.htmlheader += css
            for js in self.header_js:
                self.htmlheader += js",python,"def buildhtmlheader(self):
        """"""generate HTML header content""""""
        self.htmlheader = ''
        # If the JavaScript assets have already been injected, don't bother re-sourcing them.
        global _js_initialized
        if '_js_initialized' not in globals() or not _js_initialized:
            for css in self.header_css:
                self.htmlheader += css
            for js in self.header_js:
                self.htmlheader += js",def,buildhtmlheader,(,self,),:,self,.,htmlheader,=,'',"# If the JavaScript assets have already been injected, don't bother re-sourcing them.",global,_js_initialized,if,'_js_initialized',not,in,globals,(,),or,not,_js_initialized,:,for,css,in,self,.,header_css,:,self,.,htmlheader,+=,css,for,js,in,self,.,header_js,:,self,.,htmlheader,+=,js,,,,generate HTML header content,generate,HTML,header,content,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L374-L383,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/_vendor/nvd3/NVD3Chart.py,NVD3Chart.buildcontainer,"def buildcontainer(self):
        """"""generate HTML div""""""
        if self.container:
            return

        # Create SVG div with style
        if self.width:
            if self.width[-1] != '%':
                self.style += 'width:%spx;' % self.width
            else:
                self.style += 'width:%s;' % self.width
        if self.height:
            if self.height[-1] != '%':
                self.style += 'height:%spx;' % self.height
            else:
                self.style += 'height:%s;' % self.height
        if self.style:
            self.style = 'style=""%s""' % self.style

        self.container = self.containerheader + \
            '<div id=""%s""><svg %s></svg></div>\n' % (self.name, self.style)",python,"def buildcontainer(self):
        """"""generate HTML div""""""
        if self.container:
            return

        # Create SVG div with style
        if self.width:
            if self.width[-1] != '%':
                self.style += 'width:%spx;' % self.width
            else:
                self.style += 'width:%s;' % self.width
        if self.height:
            if self.height[-1] != '%':
                self.style += 'height:%spx;' % self.height
            else:
                self.style += 'height:%s;' % self.height
        if self.style:
            self.style = 'style=""%s""' % self.style

        self.container = self.containerheader + \
            '<div id=""%s""><svg %s></svg></div>\n' % (self.name, self.style)",def,buildcontainer,(,self,),:,if,self,.,container,:,return,# Create SVG div with style,if,self,.,width,:,if,self,.,width,[,-,1,],!=,'%',:,self,.,style,+=,'width:%spx;',%,self,.,width,else,:,self,.,style,+=,'width:%s;',%,self,.,width,if,self,.,generate HTML div,generate,HTML,div,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L385-L405,test,height,:,if,self,.,height,[,-,1,],!=,'%',:,self,.,style,+=,'height:%spx;',%,self,.,height,else,:,self,.,style,+=,'height:%s;',%,self,.,height,if,self,.,style,:,self,.,style,=,"'style=""%s""'",%,self,.,style,self,.,container,=,self,.,containerheader,+,"'<div id=""%s""><svg %s></svg></div>\n'",%,(,self,.,name,",",self,.,style,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/_vendor/nvd3/NVD3Chart.py,NVD3Chart.buildjschart,"def buildjschart(self):
        """"""generate javascript code for the chart""""""
        self.jschart = ''

        # add custom tooltip string in jschart
        # default condition (if build_custom_tooltip is not called explicitly with date_flag=True)
        if self.tooltip_condition_string == '':
            self.tooltip_condition_string = 'var y = String(graph.point.y);\n'

        # Include data
        self.series_js = json.dumps(self.series)",python,"def buildjschart(self):
        """"""generate javascript code for the chart""""""
        self.jschart = ''

        # add custom tooltip string in jschart
        # default condition (if build_custom_tooltip is not called explicitly with date_flag=True)
        if self.tooltip_condition_string == '':
            self.tooltip_condition_string = 'var y = String(graph.point.y);\n'

        # Include data
        self.series_js = json.dumps(self.series)",def,buildjschart,(,self,),:,self,.,jschart,=,'',# add custom tooltip string in jschart,# default condition (if build_custom_tooltip is not called explicitly with date_flag=True),if,self,.,tooltip_condition_string,==,'',:,self,.,tooltip_condition_string,=,'var y = String(graph.point.y);\n',# Include data,self,.,series_js,=,json,.,dumps,(,self,.,series,),,,,,,,,,,,,,,,generate javascript code for the chart,generate,javascript,code,for,the,chart,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L407-L417,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/_vendor/nvd3/NVD3Chart.py,NVD3Chart.create_x_axis,"def create_x_axis(self, name, label=None, format=None, date=False, custom_format=False):
        """"""Create X-axis""""""
        axis = {}
        if custom_format and format:
            axis['tickFormat'] = format
        elif format:
            if format == 'AM_PM':
                axis['tickFormat'] = ""function(d) { return get_am_pm(parseInt(d)); }""
            else:
                axis['tickFormat'] = ""d3.format(',%s')"" % format

        if label:
            axis['axisLabel'] = ""'"" + label + ""'""

        # date format : see https://github.com/mbostock/d3/wiki/Time-Formatting
        if date:
            self.dateformat = format
            axis['tickFormat'] = (""function(d) { return d3.time.format('%s')""
                                  ""(new Date(parseInt(d))) }\n""
                                  """" % self.dateformat)
            # flag is the x Axis is a date
            if name[0] == 'x':
                self.x_axis_date = True

        # Add new axis to list of axis
        self.axislist[name] = axis

        # Create x2Axis if focus_enable
        if name == ""xAxis"" and self.focus_enable:
            self.axislist['x2Axis'] = axis",python,"def create_x_axis(self, name, label=None, format=None, date=False, custom_format=False):
        """"""Create X-axis""""""
        axis = {}
        if custom_format and format:
            axis['tickFormat'] = format
        elif format:
            if format == 'AM_PM':
                axis['tickFormat'] = ""function(d) { return get_am_pm(parseInt(d)); }""
            else:
                axis['tickFormat'] = ""d3.format(',%s')"" % format

        if label:
            axis['axisLabel'] = ""'"" + label + ""'""

        # date format : see https://github.com/mbostock/d3/wiki/Time-Formatting
        if date:
            self.dateformat = format
            axis['tickFormat'] = (""function(d) { return d3.time.format('%s')""
                                  ""(new Date(parseInt(d))) }\n""
                                  """" % self.dateformat)
            # flag is the x Axis is a date
            if name[0] == 'x':
                self.x_axis_date = True

        # Add new axis to list of axis
        self.axislist[name] = axis

        # Create x2Axis if focus_enable
        if name == ""xAxis"" and self.focus_enable:
            self.axislist['x2Axis'] = axis",def,create_x_axis,(,self,",",name,",",label,=,None,",",format,=,None,",",date,=,False,",",custom_format,=,False,),:,axis,=,{,},if,custom_format,and,format,:,axis,[,'tickFormat',],=,format,elif,format,:,if,format,==,'AM_PM',:,axis,[,'tickFormat',],=,Create X-axis,Create,X,-,axis,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L419-L448,test,"""function(d) { return get_am_pm(parseInt(d)); }""",else,:,axis,[,'tickFormat',],=,"""d3.format(',%s')""",%,format,if,label,:,axis,[,'axisLabel',],=,"""'""",+,label,+,"""'""",# date format : see https://github.com/mbostock/d3/wiki/Time-Formatting,if,date,:,self,.,dateformat,=,format,axis,[,'tickFormat',],=,(,"""function(d) { return d3.time.format('%s')""","""(new Date(parseInt(d))) }\n""","""""",%,self,.,dateformat,),# flag is the x Axis is a date,if,name,[,0,],==,'x',:,self,.,x_axis_date,=,True,# Add new axis to list of axis,self,.,axislist,[,name,],=,axis,# Create x2Axis if focus_enable,if,name,==,"""xAxis""",and,self,.,focus_enable,:,self,.,axislist,[,'x2Axis',],=,axis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/_vendor/nvd3/NVD3Chart.py,NVD3Chart.create_y_axis,"def create_y_axis(self, name, label=None, format=None, custom_format=False):
        """"""
        Create Y-axis
        """"""
        axis = {}

        if custom_format and format:
            axis['tickFormat'] = format
        elif format:
            axis['tickFormat'] = ""d3.format(',%s')"" % format

        if label:
            axis['axisLabel'] = ""'"" + label + ""'""

        # Add new axis to list of axis
        self.axislist[name] = axis",python,"def create_y_axis(self, name, label=None, format=None, custom_format=False):
        """"""
        Create Y-axis
        """"""
        axis = {}

        if custom_format and format:
            axis['tickFormat'] = format
        elif format:
            axis['tickFormat'] = ""d3.format(',%s')"" % format

        if label:
            axis['axisLabel'] = ""'"" + label + ""'""

        # Add new axis to list of axis
        self.axislist[name] = axis",def,create_y_axis,(,self,",",name,",",label,=,None,",",format,=,None,",",custom_format,=,False,),:,axis,=,{,},if,custom_format,and,format,:,axis,[,'tickFormat',],=,format,elif,format,:,axis,[,'tickFormat',],=,"""d3.format(',%s')""",%,format,if,label,:,axis,[,'axisLabel',Create Y-axis,Create,Y,-,axis,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L450-L465,test,],=,"""'""",+,label,+,"""'""",# Add new axis to list of axis,self,.,axislist,[,name,],=,axis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/sqlite_hook.py,SqliteHook.get_conn,"def get_conn(self):
        """"""
        Returns a sqlite connection object
        """"""
        conn = self.get_connection(self.sqlite_conn_id)
        conn = sqlite3.connect(conn.host)
        return conn",python,"def get_conn(self):
        """"""
        Returns a sqlite connection object
        """"""
        conn = self.get_connection(self.sqlite_conn_id)
        conn = sqlite3.connect(conn.host)
        return conn",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,sqlite_conn_id,),conn,=,sqlite3,.,connect,(,conn,.,host,),return,conn,,,,,,,,,,,,,,,,,,,,,,,,,Returns a sqlite connection object,Returns,a,sqlite,connection,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/sqlite_hook.py#L35-L41,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/decorators.py,action_logging,"def action_logging(f):
    """"""
    Decorator to log user actions
    """"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):

        with create_session() as session:
            if g.user.is_anonymous:
                user = 'anonymous'
            else:
                user = g.user.username

            log = Log(
                event=f.__name__,
                task_instance=None,
                owner=user,
                extra=str(list(request.args.items())),
                task_id=request.args.get('task_id'),
                dag_id=request.args.get('dag_id'))

            if 'execution_date' in request.args:
                log.execution_date = pendulum.parse(
                    request.args.get('execution_date'))

            session.add(log)

        return f(*args, **kwargs)

    return wrapper",python,"def action_logging(f):
    """"""
    Decorator to log user actions
    """"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):

        with create_session() as session:
            if g.user.is_anonymous:
                user = 'anonymous'
            else:
                user = g.user.username

            log = Log(
                event=f.__name__,
                task_instance=None,
                owner=user,
                extra=str(list(request.args.items())),
                task_id=request.args.get('task_id'),
                dag_id=request.args.get('dag_id'))

            if 'execution_date' in request.args:
                log.execution_date = pendulum.parse(
                    request.args.get('execution_date'))

            session.add(log)

        return f(*args, **kwargs)

    return wrapper",def,action_logging,(,f,),:,@,functools,.,wraps,(,f,),def,wrapper,(,*,args,",",*,*,kwargs,),:,with,create_session,(,),as,session,:,if,g,.,user,.,is_anonymous,:,user,=,'anonymous',else,:,user,=,g,.,user,.,username,log,=,Decorator to log user actions,Decorator,to,log,user,actions,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/decorators.py#L29-L58,test,Log,(,event,=,f,.,__name__,",",task_instance,=,None,",",owner,=,user,",",extra,=,str,(,list,(,request,.,args,.,items,(,),),),",",task_id,=,request,.,args,.,get,(,'task_id',),",",dag_id,=,request,.,args,.,get,(,'dag_id',),),if,'execution_date',in,request,.,args,:,log,.,execution_date,=,pendulum,.,parse,(,request,.,args,.,get,(,'execution_date',),),session,.,add,(,log,),return,f,(,*,args,",",*,*,kwargs,),return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/decorators.py,gzipped,"def gzipped(f):
    """"""
    Decorator to make a view compressed
    """"""
    @functools.wraps(f)
    def view_func(*args, **kwargs):
        @after_this_request
        def zipper(response):
            accept_encoding = request.headers.get('Accept-Encoding', '')

            if 'gzip' not in accept_encoding.lower():
                return response

            response.direct_passthrough = False

            if (response.status_code < 200 or response.status_code >= 300 or
                    'Content-Encoding' in response.headers):
                return response
            gzip_buffer = IO()
            gzip_file = gzip.GzipFile(mode='wb',
                                      fileobj=gzip_buffer)
            gzip_file.write(response.data)
            gzip_file.close()

            response.data = gzip_buffer.getvalue()
            response.headers['Content-Encoding'] = 'gzip'
            response.headers['Vary'] = 'Accept-Encoding'
            response.headers['Content-Length'] = len(response.data)

            return response

        return f(*args, **kwargs)

    return view_func",python,"def gzipped(f):
    """"""
    Decorator to make a view compressed
    """"""
    @functools.wraps(f)
    def view_func(*args, **kwargs):
        @after_this_request
        def zipper(response):
            accept_encoding = request.headers.get('Accept-Encoding', '')

            if 'gzip' not in accept_encoding.lower():
                return response

            response.direct_passthrough = False

            if (response.status_code < 200 or response.status_code >= 300 or
                    'Content-Encoding' in response.headers):
                return response
            gzip_buffer = IO()
            gzip_file = gzip.GzipFile(mode='wb',
                                      fileobj=gzip_buffer)
            gzip_file.write(response.data)
            gzip_file.close()

            response.data = gzip_buffer.getvalue()
            response.headers['Content-Encoding'] = 'gzip'
            response.headers['Vary'] = 'Accept-Encoding'
            response.headers['Content-Length'] = len(response.data)

            return response

        return f(*args, **kwargs)

    return view_func",def,gzipped,(,f,),:,@,functools,.,wraps,(,f,),def,view_func,(,*,args,",",*,*,kwargs,),:,@,after_this_request,def,zipper,(,response,),:,accept_encoding,=,request,.,headers,.,get,(,'Accept-Encoding',",",'',),if,'gzip',not,in,accept_encoding,.,lower,(,Decorator to make a view compressed,Decorator,to,make,a,view,compressed,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/decorators.py#L61-L94,test,),:,return,response,response,.,direct_passthrough,=,False,if,(,response,.,status_code,<,200,or,response,.,status_code,>=,300,or,'Content-Encoding',in,response,.,headers,),:,return,response,gzip_buffer,=,IO,(,),gzip_file,=,gzip,.,GzipFile,(,mode,=,'wb',",",fileobj,=,gzip_buffer,),gzip_file,.,write,(,response,.,data,),gzip_file,.,close,(,),response,.,data,=,gzip_buffer,.,getvalue,(,),response,.,headers,[,'Content-Encoding',],=,'gzip',response,.,headers,[,'Vary',],=,'Accept-Encoding',response,.,headers,[,'Content-Length',],=,len,(,response,.,data,),return,response,return,f,(,*,args,",",*,*,kwargs,),return,view_func,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dag.py,get_last_dagrun,"def get_last_dagrun(dag_id, session, include_externally_triggered=False):
    """"""
    Returns the last dag run for a dag, None if there was none.
    Last dag run can be any type of run eg. scheduled or backfilled.
    Overridden DagRuns are ignored.
    """"""
    DR = DagRun
    query = session.query(DR).filter(DR.dag_id == dag_id)
    if not include_externally_triggered:
        query = query.filter(DR.external_trigger == False)  # noqa
    query = query.order_by(DR.execution_date.desc())
    return query.first()",python,"def get_last_dagrun(dag_id, session, include_externally_triggered=False):
    """"""
    Returns the last dag run for a dag, None if there was none.
    Last dag run can be any type of run eg. scheduled or backfilled.
    Overridden DagRuns are ignored.
    """"""
    DR = DagRun
    query = session.query(DR).filter(DR.dag_id == dag_id)
    if not include_externally_triggered:
        query = query.filter(DR.external_trigger == False)  # noqa
    query = query.order_by(DR.execution_date.desc())
    return query.first()",def,get_last_dagrun,(,dag_id,",",session,",",include_externally_triggered,=,False,),:,DR,=,DagRun,query,=,session,.,query,(,DR,),.,filter,(,DR,.,dag_id,==,dag_id,),if,not,include_externally_triggered,:,query,=,query,.,filter,(,DR,.,external_trigger,==,False,),# noqa,query,=,query,"Returns the last dag run for a dag, None if there was none.
    Last dag run can be any type of run eg. scheduled or backfilled.
    Overridden DagRuns are ignored.",Returns,the,last,dag,run,for,a,dag,None,if,there,was,none,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dag.py#L64-L75,test,.,order_by,(,DR,.,execution_date,.,desc,(,),),return,query,.,first,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,Last,dag,run,can,be,any,type,of,run,eg,.,scheduled,or,backfilled,.,Overridden,DagRuns,are,ignored,.,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/dag.py,DagModel.create_dagrun,"def create_dagrun(self,
                      run_id,
                      state,
                      execution_date,
                      start_date=None,
                      external_trigger=False,
                      conf=None,
                      session=None):
        """"""
        Creates a dag run from this dag including the tasks associated with this dag.
        Returns the dag run.

        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date of this dag run
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param start_date: the date this dag run should be evaluated
        :type start_date: datetime.datetime
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        """"""

        return self.get_dag().create_dagrun(run_id=run_id,
                                            state=state,
                                            execution_date=execution_date,
                                            start_date=start_date,
                                            external_trigger=external_trigger,
                                            conf=conf,
                                            session=session)",python,"def create_dagrun(self,
                      run_id,
                      state,
                      execution_date,
                      start_date=None,
                      external_trigger=False,
                      conf=None,
                      session=None):
        """"""
        Creates a dag run from this dag including the tasks associated with this dag.
        Returns the dag run.

        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date of this dag run
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param start_date: the date this dag run should be evaluated
        :type start_date: datetime.datetime
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        """"""

        return self.get_dag().create_dagrun(run_id=run_id,
                                            state=state,
                                            execution_date=execution_date,
                                            start_date=start_date,
                                            external_trigger=external_trigger,
                                            conf=conf,
                                            session=session)",def,create_dagrun,(,self,",",run_id,",",state,",",execution_date,",",start_date,=,None,",",external_trigger,=,False,",",conf,=,None,",",session,=,None,),:,return,self,.,get_dag,(,),.,create_dagrun,(,run_id,=,run_id,",",state,=,state,",",execution_date,=,execution_date,",",start_date,=,start_date,"Creates a dag run from this dag including the tasks associated with this dag.
        Returns the dag run.

        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date of this dag run
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param start_date: the date this dag run should be evaluated
        :type start_date: datetime.datetime
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session",Creates,a,dag,run,from,this,dag,including,the,tasks,associated,with,this,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dag.py#L1494-L1526,test,",",external_trigger,=,external_trigger,",",conf,=,conf,",",session,=,session,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dag,.,Returns,the,dag,run,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/aws_sqs_publish_operator.py,SQSPublishOperator.execute,"def execute(self, context):
        """"""
        Publish the message to SQS queue

        :param context: the context object
        :type context: dict
        :return: dict with information about the message sent
            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`
        :rtype: dict
        """"""

        hook = SQSHook(aws_conn_id=self.aws_conn_id)

        result = hook.send_message(queue_url=self.sqs_queue,
                                   message_body=self.message_content,
                                   delay_seconds=self.delay_seconds,
                                   message_attributes=self.message_attributes)

        self.log.info('result is send_message is %s', result)

        return result",python,"def execute(self, context):
        """"""
        Publish the message to SQS queue

        :param context: the context object
        :type context: dict
        :return: dict with information about the message sent
            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`
        :rtype: dict
        """"""

        hook = SQSHook(aws_conn_id=self.aws_conn_id)

        result = hook.send_message(queue_url=self.sqs_queue,
                                   message_body=self.message_content,
                                   delay_seconds=self.delay_seconds,
                                   message_attributes=self.message_attributes)

        self.log.info('result is send_message is %s', result)

        return result",def,execute,(,self,",",context,),:,hook,=,SQSHook,(,aws_conn_id,=,self,.,aws_conn_id,),result,=,hook,.,send_message,(,queue_url,=,self,.,sqs_queue,",",message_body,=,self,.,message_content,",",delay_seconds,=,self,.,delay_seconds,",",message_attributes,=,self,.,message_attributes,),self,.,log,.,"Publish the message to SQS queue

        :param context: the context object
        :type context: dict
        :return: dict with information about the message sent
            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`
        :rtype: dict",Publish,the,message,to,SQS,queue,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/aws_sqs_publish_operator.py#L61-L81,test,info,(,'result is send_message is %s',",",result,),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/utils.py,json_response,"def json_response(obj):
    """"""
    returns a json response from a json serializable python object
    """"""
    return Response(
        response=json.dumps(
            obj, indent=4, cls=AirflowJsonEncoder),
        status=200,
        mimetype=""application/json"")",python,"def json_response(obj):
    """"""
    returns a json response from a json serializable python object
    """"""
    return Response(
        response=json.dumps(
            obj, indent=4, cls=AirflowJsonEncoder),
        status=200,
        mimetype=""application/json"")",def,json_response,(,obj,),:,return,Response,(,response,=,json,.,dumps,(,obj,",",indent,=,4,",",cls,=,AirflowJsonEncoder,),",",status,=,200,",",mimetype,=,"""application/json""",),,,,,,,,,,,,,,,,,,,returns a json response from a json serializable python object,returns,a,json,response,from,a,json,serializable,python,object,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/utils.py#L193-L201,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/utils.py,open_maybe_zipped,"def open_maybe_zipped(f, mode='r'):
    """"""
    Opens the given file. If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive, opening the file inside the archive.

    :return: a file object, as in `open`, or as in `ZipFile.open`.
    """"""

    _, archive, filename = ZIP_REGEX.search(f).groups()
    if archive and zipfile.is_zipfile(archive):
        return zipfile.ZipFile(archive, mode=mode).open(filename)
    else:
        return io.open(f, mode=mode)",python,"def open_maybe_zipped(f, mode='r'):
    """"""
    Opens the given file. If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive, opening the file inside the archive.

    :return: a file object, as in `open`, or as in `ZipFile.open`.
    """"""

    _, archive, filename = ZIP_REGEX.search(f).groups()
    if archive and zipfile.is_zipfile(archive):
        return zipfile.ZipFile(archive, mode=mode).open(filename)
    else:
        return io.open(f, mode=mode)",def,open_maybe_zipped,(,f,",",mode,=,'r',),:,_,",",archive,",",filename,=,ZIP_REGEX,.,search,(,f,),.,groups,(,),if,archive,and,zipfile,.,is_zipfile,(,archive,),:,return,zipfile,.,ZipFile,(,archive,",",mode,=,mode,),.,open,(,filename,),"Opens the given file. If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive, opening the file inside the archive.

    :return: a file object, as in `open`, or as in `ZipFile.open`.",Opens,the,given,file,.,If,the,path,contains,a,folder,with,a,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/utils.py#L207-L219,test,else,:,return,io,.,open,(,f,",",mode,=,mode,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,zip,suffix,then,the,folder,is,treated,as,a,zip,archive,opening,the,file,inside,the,archive,.,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/utils.py,make_cache_key,"def make_cache_key(*args, **kwargs):
    """"""
    Used by cache to get a unique key per URL
    """"""
    path = request.path
    args = str(hash(frozenset(request.args.items())))
    return (path + args).encode('ascii', 'ignore')",python,"def make_cache_key(*args, **kwargs):
    """"""
    Used by cache to get a unique key per URL
    """"""
    path = request.path
    args = str(hash(frozenset(request.args.items())))
    return (path + args).encode('ascii', 'ignore')",def,make_cache_key,(,*,args,",",*,*,kwargs,),:,path,=,request,.,path,args,=,str,(,hash,(,frozenset,(,request,.,args,.,items,(,),),),),return,(,path,+,args,),.,encode,(,'ascii',",",'ignore',),,,,,,Used by cache to get a unique key per URL,Used,by,cache,to,get,a,unique,key,per,URL,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/utils.py#L222-L228,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_video_intelligence_hook.py,CloudVideoIntelligenceHook.get_conn,"def get_conn(self):
        """"""
        Returns Gcp Video Intelligence Service client

        :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient
        """"""
        if not self._conn:
            self._conn = VideoIntelligenceServiceClient(credentials=self._get_credentials())
        return self._conn",python,"def get_conn(self):
        """"""
        Returns Gcp Video Intelligence Service client

        :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient
        """"""
        if not self._conn:
            self._conn = VideoIntelligenceServiceClient(credentials=self._get_credentials())
        return self._conn",def,get_conn,(,self,),:,if,not,self,.,_conn,:,self,.,_conn,=,VideoIntelligenceServiceClient,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_conn,,,,,,,,,,,,,,,,,,,,,,,"Returns Gcp Video Intelligence Service client

        :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient",Returns,Gcp,Video,Intelligence,Service,client,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_video_intelligence_hook.py#L41-L49,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_video_intelligence_hook.py,CloudVideoIntelligenceHook.annotate_video,"def annotate_video(
        self,
        input_uri=None,
        input_content=None,
        features=None,
        video_context=None,
        output_uri=None,
        location=None,
        retry=None,
        timeout=None,
        metadata=None,
    ):
        """"""
        Performs video annotation.

        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,
            which must be specified in the following format: ``gs://bucket-id/object-id``.
        :type input_uri: str
        :param input_content: The video data bytes.
            If unset, the input video(s) should be specified via ``input_uri``.
            If set, ``input_uri`` should be unset.
        :type input_content: bytes
        :param features: Requested video annotation features.
        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]
        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,
            only Google Cloud Storage URIs are supported, which must be specified in the following format:
            ``gs://bucket-id/object-id``.
        :type output_uri: str
        :param video_context: Optional, Additional video context and/or feature-specific parameters.
        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext
        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:
            us-east1, us-west1, europe-west1, asia-east1.
            If no region is specified, a region will be determined based on video file location.
        :type location: str
        :param retry: Retry object used to determine when/if to retry requests.
            If None is specified, requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Optional, Additional metadata that is provided to the method.
        :type metadata: seq[tuple[str, str]]
        """"""
        client = self.get_conn()
        return client.annotate_video(
            input_uri=input_uri,
            input_content=input_content,
            features=features,
            video_context=video_context,
            output_uri=output_uri,
            location_id=location,
            retry=retry,
            timeout=timeout,
            metadata=metadata,
        )",python,"def annotate_video(
        self,
        input_uri=None,
        input_content=None,
        features=None,
        video_context=None,
        output_uri=None,
        location=None,
        retry=None,
        timeout=None,
        metadata=None,
    ):
        """"""
        Performs video annotation.

        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,
            which must be specified in the following format: ``gs://bucket-id/object-id``.
        :type input_uri: str
        :param input_content: The video data bytes.
            If unset, the input video(s) should be specified via ``input_uri``.
            If set, ``input_uri`` should be unset.
        :type input_content: bytes
        :param features: Requested video annotation features.
        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]
        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,
            only Google Cloud Storage URIs are supported, which must be specified in the following format:
            ``gs://bucket-id/object-id``.
        :type output_uri: str
        :param video_context: Optional, Additional video context and/or feature-specific parameters.
        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext
        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:
            us-east1, us-west1, europe-west1, asia-east1.
            If no region is specified, a region will be determined based on video file location.
        :type location: str
        :param retry: Retry object used to determine when/if to retry requests.
            If None is specified, requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Optional, Additional metadata that is provided to the method.
        :type metadata: seq[tuple[str, str]]
        """"""
        client = self.get_conn()
        return client.annotate_video(
            input_uri=input_uri,
            input_content=input_content,
            features=features,
            video_context=video_context,
            output_uri=output_uri,
            location_id=location,
            retry=retry,
            timeout=timeout,
            metadata=metadata,
        )",def,annotate_video,(,self,",",input_uri,=,None,",",input_content,=,None,",",features,=,None,",",video_context,=,None,",",output_uri,=,None,",",location,=,None,",",retry,=,None,",",timeout,=,None,",",metadata,=,None,",",),:,client,=,self,.,get_conn,(,),return,client,"Performs video annotation.

        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,
            which must be specified in the following format: ``gs://bucket-id/object-id``.
        :type input_uri: str
        :param input_content: The video data bytes.
            If unset, the input video(s) should be specified via ``input_uri``.
            If set, ``input_uri`` should be unset.
        :type input_content: bytes
        :param features: Requested video annotation features.
        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]
        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,
            only Google Cloud Storage URIs are supported, which must be specified in the following format:
            ``gs://bucket-id/object-id``.
        :type output_uri: str
        :param video_context: Optional, Additional video context and/or feature-specific parameters.
        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext
        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:
            us-east1, us-west1, europe-west1, asia-east1.
            If no region is specified, a region will be determined based on video file location.
        :type location: str
        :param retry: Retry object used to determine when/if to retry requests.
            If None is specified, requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Optional, Additional metadata that is provided to the method.
        :type metadata: seq[tuple[str, str]]",Performs,video,annotation,.,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_video_intelligence_hook.py#L51-L105,test,.,annotate_video,(,input_uri,=,input_uri,",",input_content,=,input_content,",",features,=,features,",",video_context,=,video_context,",",output_uri,=,output_uri,",",location_id,=,location,",",retry,=,retry,",",timeout,=,timeout,",",metadata,=,metadata,",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/opsgenie_alert_hook.py,OpsgenieAlertHook._get_api_key,"def _get_api_key(self):
        """"""
        Get Opsgenie api_key for creating alert
        """"""
        conn = self.get_connection(self.http_conn_id)
        api_key = conn.password
        if not api_key:
            raise AirflowException('Opsgenie API Key is required for this hook, '
                                   'please check your conn_id configuration.')
        return api_key",python,"def _get_api_key(self):
        """"""
        Get Opsgenie api_key for creating alert
        """"""
        conn = self.get_connection(self.http_conn_id)
        api_key = conn.password
        if not api_key:
            raise AirflowException('Opsgenie API Key is required for this hook, '
                                   'please check your conn_id configuration.')
        return api_key",def,_get_api_key,(,self,),:,conn,=,self,.,get_connection,(,self,.,http_conn_id,),api_key,=,conn,.,password,if,not,api_key,:,raise,AirflowException,(,"'Opsgenie API Key is required for this hook, '",'please check your conn_id configuration.',),return,api_key,,,,,,,,,,,,,,,,,,,,Get Opsgenie api_key for creating alert,Get,Opsgenie,api_key,for,creating,alert,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/opsgenie_alert_hook.py#L50-L59,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/opsgenie_alert_hook.py,OpsgenieAlertHook.get_conn,"def get_conn(self, headers=None):
        """"""
        Overwrite HttpHook get_conn because this hook just needs base_url
        and headers, and does not need generic params

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        """"""
        conn = self.get_connection(self.http_conn_id)
        self.base_url = conn.host if conn.host else 'https://api.opsgenie.com'
        session = requests.Session()
        if headers:
            session.headers.update(headers)
        return session",python,"def get_conn(self, headers=None):
        """"""
        Overwrite HttpHook get_conn because this hook just needs base_url
        and headers, and does not need generic params

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        """"""
        conn = self.get_connection(self.http_conn_id)
        self.base_url = conn.host if conn.host else 'https://api.opsgenie.com'
        session = requests.Session()
        if headers:
            session.headers.update(headers)
        return session",def,get_conn,(,self,",",headers,=,None,),:,conn,=,self,.,get_connection,(,self,.,http_conn_id,),self,.,base_url,=,conn,.,host,if,conn,.,host,else,'https://api.opsgenie.com',session,=,requests,.,Session,(,),if,headers,:,session,.,headers,.,update,(,headers,),return,"Overwrite HttpHook get_conn because this hook just needs base_url
        and headers, and does not need generic params

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict",Overwrite,HttpHook,get_conn,because,this,hook,just,needs,base_url,and,headers,and,does,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/opsgenie_alert_hook.py#L61-L74,test,session,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,not,need,generic,params,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/opsgenie_alert_hook.py,OpsgenieAlertHook.execute,"def execute(self, payload={}):
        """"""
        Execute the Opsgenie Alert call

        :param payload: Opsgenie API Create Alert payload values
            See https://docs.opsgenie.com/docs/alert-api#section-create-alert
        :type payload: dict
        """"""
        api_key = self._get_api_key()
        return self.run(endpoint='v2/alerts',
                        data=json.dumps(payload),
                        headers={'Content-Type': 'application/json',
                                 'Authorization': 'GenieKey %s' % api_key})",python,"def execute(self, payload={}):
        """"""
        Execute the Opsgenie Alert call

        :param payload: Opsgenie API Create Alert payload values
            See https://docs.opsgenie.com/docs/alert-api#section-create-alert
        :type payload: dict
        """"""
        api_key = self._get_api_key()
        return self.run(endpoint='v2/alerts',
                        data=json.dumps(payload),
                        headers={'Content-Type': 'application/json',
                                 'Authorization': 'GenieKey %s' % api_key})",def,execute,(,self,",",payload,=,{,},),:,api_key,=,self,.,_get_api_key,(,),return,self,.,run,(,endpoint,=,'v2/alerts',",",data,=,json,.,dumps,(,payload,),",",headers,=,{,'Content-Type',:,'application/json',",",'Authorization',:,'GenieKey %s',%,api_key,},),,,"Execute the Opsgenie Alert call

        :param payload: Opsgenie API Create Alert payload values
            See https://docs.opsgenie.com/docs/alert-api#section-create-alert
        :type payload: dict",Execute,the,Opsgenie,Alert,call,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/opsgenie_alert_hook.py#L76-L88,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/opsgenie_alert_operator.py,OpsgenieAlertOperator._build_opsgenie_payload,"def _build_opsgenie_payload(self):
        """"""
        Construct the Opsgenie JSON payload. All relevant parameters are combined here
        to a valid Opsgenie JSON payload.

        :return: Opsgenie payload (dict) to send
        """"""
        payload = {}

        for key in [
            ""message"", ""alias"", ""description"", ""responders"",
            ""visibleTo"", ""actions"", ""tags"", ""details"", ""entity"",
            ""source"", ""priority"", ""user"", ""note""
        ]:
            val = getattr(self, key)
            if val:
                payload[key] = val
        return payload",python,"def _build_opsgenie_payload(self):
        """"""
        Construct the Opsgenie JSON payload. All relevant parameters are combined here
        to a valid Opsgenie JSON payload.

        :return: Opsgenie payload (dict) to send
        """"""
        payload = {}

        for key in [
            ""message"", ""alias"", ""description"", ""responders"",
            ""visibleTo"", ""actions"", ""tags"", ""details"", ""entity"",
            ""source"", ""priority"", ""user"", ""note""
        ]:
            val = getattr(self, key)
            if val:
                payload[key] = val
        return payload",def,_build_opsgenie_payload,(,self,),:,payload,=,{,},for,key,in,[,"""message""",",","""alias""",",","""description""",",","""responders""",",","""visibleTo""",",","""actions""",",","""tags""",",","""details""",",","""entity""",",","""source""",",","""priority""",",","""user""",",","""note""",],:,val,=,getattr,(,self,",",key,),if,val,:,"Construct the Opsgenie JSON payload. All relevant parameters are combined here
        to a valid Opsgenie JSON payload.

        :return: Opsgenie payload (dict) to send",Construct,the,Opsgenie,JSON,payload,.,All,relevant,parameters,are,combined,here,to,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/opsgenie_alert_operator.py#L107-L124,test,payload,[,key,],=,val,return,payload,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,a,valid,Opsgenie,JSON,payload,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/opsgenie_alert_operator.py,OpsgenieAlertOperator.execute,"def execute(self, context):
        """"""
        Call the OpsgenieAlertHook to post message
        """"""
        self.hook = OpsgenieAlertHook(self.opsgenie_conn_id)
        self.hook.execute(self._build_opsgenie_payload())",python,"def execute(self, context):
        """"""
        Call the OpsgenieAlertHook to post message
        """"""
        self.hook = OpsgenieAlertHook(self.opsgenie_conn_id)
        self.hook.execute(self._build_opsgenie_payload())",def,execute,(,self,",",context,),:,self,.,hook,=,OpsgenieAlertHook,(,self,.,opsgenie_conn_id,),self,.,hook,.,execute,(,self,.,_build_opsgenie_payload,(,),),,,,,,,,,,,,,,,,,,,,,,,Call the OpsgenieAlertHook to post message,Call,the,OpsgenieAlertHook,to,post,message,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/opsgenie_alert_operator.py#L126-L131,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_athena_hook.py,AWSAthenaHook.get_conn,"def get_conn(self):
        """"""
        check if aws conn exists already or create one and return it

        :return: boto3 session
        """"""
        if not self.conn:
            self.conn = self.get_client_type('athena')
        return self.conn",python,"def get_conn(self):
        """"""
        check if aws conn exists already or create one and return it

        :return: boto3 session
        """"""
        if not self.conn:
            self.conn = self.get_client_type('athena')
        return self.conn",def,get_conn,(,self,),:,if,not,self,.,conn,:,self,.,conn,=,self,.,get_client_type,(,'athena',),return,self,.,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,"check if aws conn exists already or create one and return it

        :return: boto3 session",check,if,aws,conn,exists,already,or,create,one,and,return,it,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_athena_hook.py#L43-L51,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_athena_hook.py,AWSAthenaHook.run_query,"def run_query(self, query, query_context, result_configuration, client_request_token=None):
        """"""
        Run Presto query on athena with provided config and return submitted query_execution_id

        :param query: Presto query to run
        :type query: str
        :param query_context: Context in which query need to be run
        :type query_context: dict
        :param result_configuration: Dict with path to store results in and config related to encryption
        :type result_configuration: dict
        :param client_request_token: Unique token created by user to avoid multiple executions of same query
        :type client_request_token: str
        :return: str
        """"""
        response = self.conn.start_query_execution(QueryString=query,
                                                   ClientRequestToken=client_request_token,
                                                   QueryExecutionContext=query_context,
                                                   ResultConfiguration=result_configuration)
        query_execution_id = response['QueryExecutionId']
        return query_execution_id",python,"def run_query(self, query, query_context, result_configuration, client_request_token=None):
        """"""
        Run Presto query on athena with provided config and return submitted query_execution_id

        :param query: Presto query to run
        :type query: str
        :param query_context: Context in which query need to be run
        :type query_context: dict
        :param result_configuration: Dict with path to store results in and config related to encryption
        :type result_configuration: dict
        :param client_request_token: Unique token created by user to avoid multiple executions of same query
        :type client_request_token: str
        :return: str
        """"""
        response = self.conn.start_query_execution(QueryString=query,
                                                   ClientRequestToken=client_request_token,
                                                   QueryExecutionContext=query_context,
                                                   ResultConfiguration=result_configuration)
        query_execution_id = response['QueryExecutionId']
        return query_execution_id",def,run_query,(,self,",",query,",",query_context,",",result_configuration,",",client_request_token,=,None,),:,response,=,self,.,conn,.,start_query_execution,(,QueryString,=,query,",",ClientRequestToken,=,client_request_token,",",QueryExecutionContext,=,query_context,",",ResultConfiguration,=,result_configuration,),query_execution_id,=,response,[,'QueryExecutionId',],return,query_execution_id,,,,,"Run Presto query on athena with provided config and return submitted query_execution_id

        :param query: Presto query to run
        :type query: str
        :param query_context: Context in which query need to be run
        :type query_context: dict
        :param result_configuration: Dict with path to store results in and config related to encryption
        :type result_configuration: dict
        :param client_request_token: Unique token created by user to avoid multiple executions of same query
        :type client_request_token: str
        :return: str",Run,Presto,query,on,athena,with,provided,config,and,return,submitted,query_execution_id,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_athena_hook.py#L53-L72,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_athena_hook.py,AWSAthenaHook.check_query_status,"def check_query_status(self, query_execution_id):
        """"""
        Fetch the status of submitted athena query. Returns None or one of valid query states.

        :param query_execution_id: Id of submitted athena query
        :type query_execution_id: str
        :return: str
        """"""
        response = self.conn.get_query_execution(QueryExecutionId=query_execution_id)
        state = None
        try:
            state = response['QueryExecution']['Status']['State']
        except Exception as ex:
            self.log.error('Exception while getting query state', ex)
        finally:
            return state",python,"def check_query_status(self, query_execution_id):
        """"""
        Fetch the status of submitted athena query. Returns None or one of valid query states.

        :param query_execution_id: Id of submitted athena query
        :type query_execution_id: str
        :return: str
        """"""
        response = self.conn.get_query_execution(QueryExecutionId=query_execution_id)
        state = None
        try:
            state = response['QueryExecution']['Status']['State']
        except Exception as ex:
            self.log.error('Exception while getting query state', ex)
        finally:
            return state",def,check_query_status,(,self,",",query_execution_id,),:,response,=,self,.,conn,.,get_query_execution,(,QueryExecutionId,=,query_execution_id,),state,=,None,try,:,state,=,response,[,'QueryExecution',],[,'Status',],[,'State',],except,Exception,as,ex,:,self,.,log,.,error,(,'Exception while getting query state',",",ex,),"Fetch the status of submitted athena query. Returns None or one of valid query states.

        :param query_execution_id: Id of submitted athena query
        :type query_execution_id: str
        :return: str",Fetch,the,status,of,submitted,athena,query,.,Returns,None,or,one,of,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_athena_hook.py#L74-L89,test,finally,:,return,state,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,valid,query,states,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_athena_hook.py,AWSAthenaHook.poll_query_status,"def poll_query_status(self, query_execution_id, max_tries=None):
        """"""
        Poll the status of submitted athena query until query state reaches final state.
        Returns one of the final states

        :param query_execution_id: Id of submitted athena query
        :type query_execution_id: str
        :param max_tries: Number of times to poll for query state before function exits
        :type max_tries: int
        :return: str
        """"""
        try_number = 1
        final_query_state = None  # Query state when query reaches final state or max_tries reached
        while True:
            query_state = self.check_query_status(query_execution_id)
            if query_state is None:
                self.log.info('Trial {try_number}: Invalid query state. Retrying again'.format(
                    try_number=try_number))
            elif query_state in self.INTERMEDIATE_STATES:
                self.log.info('Trial {try_number}: Query is still in an intermediate state - {state}'
                              .format(try_number=try_number, state=query_state))
            else:
                self.log.info('Trial {try_number}: Query execution completed. Final state is {state}'
                              .format(try_number=try_number, state=query_state))
                final_query_state = query_state
                break
            if max_tries and try_number >= max_tries:  # Break loop if max_tries reached
                final_query_state = query_state
                break
            try_number += 1
            sleep(self.sleep_time)
        return final_query_state",python,"def poll_query_status(self, query_execution_id, max_tries=None):
        """"""
        Poll the status of submitted athena query until query state reaches final state.
        Returns one of the final states

        :param query_execution_id: Id of submitted athena query
        :type query_execution_id: str
        :param max_tries: Number of times to poll for query state before function exits
        :type max_tries: int
        :return: str
        """"""
        try_number = 1
        final_query_state = None  # Query state when query reaches final state or max_tries reached
        while True:
            query_state = self.check_query_status(query_execution_id)
            if query_state is None:
                self.log.info('Trial {try_number}: Invalid query state. Retrying again'.format(
                    try_number=try_number))
            elif query_state in self.INTERMEDIATE_STATES:
                self.log.info('Trial {try_number}: Query is still in an intermediate state - {state}'
                              .format(try_number=try_number, state=query_state))
            else:
                self.log.info('Trial {try_number}: Query execution completed. Final state is {state}'
                              .format(try_number=try_number, state=query_state))
                final_query_state = query_state
                break
            if max_tries and try_number >= max_tries:  # Break loop if max_tries reached
                final_query_state = query_state
                break
            try_number += 1
            sleep(self.sleep_time)
        return final_query_state",def,poll_query_status,(,self,",",query_execution_id,",",max_tries,=,None,),:,try_number,=,1,final_query_state,=,None,# Query state when query reaches final state or max_tries reached,while,True,:,query_state,=,self,.,check_query_status,(,query_execution_id,),if,query_state,is,None,:,self,.,log,.,info,(,'Trial {try_number}: Invalid query state. Retrying again',.,format,(,try_number,=,try_number,),),elif,query_state,"Poll the status of submitted athena query until query state reaches final state.
        Returns one of the final states

        :param query_execution_id: Id of submitted athena query
        :type query_execution_id: str
        :param max_tries: Number of times to poll for query state before function exits
        :type max_tries: int
        :return: str",Poll,the,status,of,submitted,athena,query,until,query,state,reaches,final,state,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_athena_hook.py#L109-L140,test,in,self,.,INTERMEDIATE_STATES,:,self,.,log,.,info,(,'Trial {try_number}: Query is still in an intermediate state - {state}',.,format,(,try_number,=,try_number,",",state,=,query_state,),),else,:,self,.,log,.,info,(,'Trial {try_number}: Query execution completed. Final state is {state}',.,format,(,try_number,=,try_number,",",state,=,query_state,),),final_query_state,=,query_state,break,if,max_tries,and,try_number,>=,max_tries,:,# Break loop if max_tries reached,final_query_state,=,query_state,break,try_number,+=,1,sleep,(,self,.,sleep_time,),return,final_query_state,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,Returns,one,of,the,final,states,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sftp_hook.py,SFTPHook.get_conn,"def get_conn(self):
        """"""
        Returns an SFTP connection object
        """"""
        if self.conn is None:
            cnopts = pysftp.CnOpts()
            if self.no_host_key_check:
                cnopts.hostkeys = None
            cnopts.compression = self.compress
            conn_params = {
                'host': self.remote_host,
                'port': self.port,
                'username': self.username,
                'cnopts': cnopts
            }
            if self.password and self.password.strip():
                conn_params['password'] = self.password
            if self.key_file:
                conn_params['private_key'] = self.key_file
            if self.private_key_pass:
                conn_params['private_key_pass'] = self.private_key_pass

            self.conn = pysftp.Connection(**conn_params)
        return self.conn",python,"def get_conn(self):
        """"""
        Returns an SFTP connection object
        """"""
        if self.conn is None:
            cnopts = pysftp.CnOpts()
            if self.no_host_key_check:
                cnopts.hostkeys = None
            cnopts.compression = self.compress
            conn_params = {
                'host': self.remote_host,
                'port': self.port,
                'username': self.username,
                'cnopts': cnopts
            }
            if self.password and self.password.strip():
                conn_params['password'] = self.password
            if self.key_file:
                conn_params['private_key'] = self.key_file
            if self.private_key_pass:
                conn_params['private_key_pass'] = self.private_key_pass

            self.conn = pysftp.Connection(**conn_params)
        return self.conn",def,get_conn,(,self,),:,if,self,.,conn,is,None,:,cnopts,=,pysftp,.,CnOpts,(,),if,self,.,no_host_key_check,:,cnopts,.,hostkeys,=,None,cnopts,.,compression,=,self,.,compress,conn_params,=,{,'host',:,self,.,remote_host,",",'port',:,self,.,port,",",Returns an SFTP connection object,Returns,an,SFTP,connection,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sftp_hook.py#L92-L115,test,'username',:,self,.,username,",",'cnopts',:,cnopts,},if,self,.,password,and,self,.,password,.,strip,(,),:,conn_params,[,'password',],=,self,.,password,if,self,.,key_file,:,conn_params,[,'private_key',],=,self,.,key_file,if,self,.,private_key_pass,:,conn_params,[,'private_key_pass',],=,self,.,private_key_pass,self,.,conn,=,pysftp,.,Connection,(,*,*,conn_params,),return,self,.,conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/zendesk_hook.py,ZendeskHook.__handle_rate_limit_exception,"def __handle_rate_limit_exception(self, rate_limit_exception):
        """"""
        Sleep for the time specified in the exception. If not specified, wait
        for 60 seconds.
        """"""
        retry_after = int(
            rate_limit_exception.response.headers.get('Retry-After', 60))
        self.log.info(
            ""Hit Zendesk API rate limit. Pausing for %s seconds"",
            retry_after
        )
        time.sleep(retry_after)",python,"def __handle_rate_limit_exception(self, rate_limit_exception):
        """"""
        Sleep for the time specified in the exception. If not specified, wait
        for 60 seconds.
        """"""
        retry_after = int(
            rate_limit_exception.response.headers.get('Retry-After', 60))
        self.log.info(
            ""Hit Zendesk API rate limit. Pausing for %s seconds"",
            retry_after
        )
        time.sleep(retry_after)",def,__handle_rate_limit_exception,(,self,",",rate_limit_exception,),:,retry_after,=,int,(,rate_limit_exception,.,response,.,headers,.,get,(,'Retry-After',",",60,),),self,.,log,.,info,(,"""Hit Zendesk API rate limit. Pausing for %s seconds""",",",retry_after,),time,.,sleep,(,retry_after,),,,,,,,,,,,,"Sleep for the time specified in the exception. If not specified, wait
        for 60 seconds.",Sleep,for,the,time,specified,in,the,exception,.,If,not,specified,wait,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/zendesk_hook.py#L39-L50,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,for,60,seconds,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/zendesk_hook.py,ZendeskHook.call,"def call(self, path, query=None, get_all_pages=True, side_loading=False):
        """"""
        Call Zendesk API and return results

        :param path: The Zendesk API to call
        :param query: Query parameters
        :param get_all_pages: Accumulate results over all pages before
               returning. Due to strict rate limiting, this can often timeout.
               Waits for recommended period between tries after a timeout.
        :param side_loading: Retrieve related records as part of a single
               request. In order to enable side-loading, add an 'include'
               query parameter containing a comma-separated list of resources
               to load. For more information on side-loading see
               https://developer.zendesk.com/rest_api/docs/core/side_loading
        """"""
        zendesk = self.get_conn()
        first_request_successful = False

        while not first_request_successful:
            try:
                results = zendesk.call(path, query)
                first_request_successful = True
            except RateLimitError as rle:
                self.__handle_rate_limit_exception(rle)

        # Find the key with the results
        keys = [path.split(""/"")[-1].split("".json"")[0]]
        next_page = results['next_page']
        if side_loading:
            keys += query['include'].split(',')
        results = {key: results[key] for key in keys}

        if get_all_pages:
            while next_page is not None:
                try:
                    # Need to split because the next page URL has
                    # `github.zendesk...`
                    # in it, but the call function needs it removed.
                    next_url = next_page.split(self.__url)[1]
                    self.log.info(""Calling %s"", next_url)
                    more_res = zendesk.call(next_url)
                    for key in results:
                        results[key].extend(more_res[key])
                    if next_page == more_res['next_page']:
                        # Unfortunately zdesk doesn't always throw ZendeskError
                        # when we are done getting all the data. Sometimes the
                        # next just refers to the current set of results.
                        # Hence, need to deal with this special case
                        break
                    else:
                        next_page = more_res['next_page']
                except RateLimitError as rle:
                    self.__handle_rate_limit_exception(rle)
                except ZendeskError as ze:
                    if b""Use a start_time older than 5 minutes"" in ze.msg:
                        # We have pretty up to date data
                        break
                    else:
                        raise ze

        return results",python,"def call(self, path, query=None, get_all_pages=True, side_loading=False):
        """"""
        Call Zendesk API and return results

        :param path: The Zendesk API to call
        :param query: Query parameters
        :param get_all_pages: Accumulate results over all pages before
               returning. Due to strict rate limiting, this can often timeout.
               Waits for recommended period between tries after a timeout.
        :param side_loading: Retrieve related records as part of a single
               request. In order to enable side-loading, add an 'include'
               query parameter containing a comma-separated list of resources
               to load. For more information on side-loading see
               https://developer.zendesk.com/rest_api/docs/core/side_loading
        """"""
        zendesk = self.get_conn()
        first_request_successful = False

        while not first_request_successful:
            try:
                results = zendesk.call(path, query)
                first_request_successful = True
            except RateLimitError as rle:
                self.__handle_rate_limit_exception(rle)

        # Find the key with the results
        keys = [path.split(""/"")[-1].split("".json"")[0]]
        next_page = results['next_page']
        if side_loading:
            keys += query['include'].split(',')
        results = {key: results[key] for key in keys}

        if get_all_pages:
            while next_page is not None:
                try:
                    # Need to split because the next page URL has
                    # `github.zendesk...`
                    # in it, but the call function needs it removed.
                    next_url = next_page.split(self.__url)[1]
                    self.log.info(""Calling %s"", next_url)
                    more_res = zendesk.call(next_url)
                    for key in results:
                        results[key].extend(more_res[key])
                    if next_page == more_res['next_page']:
                        # Unfortunately zdesk doesn't always throw ZendeskError
                        # when we are done getting all the data. Sometimes the
                        # next just refers to the current set of results.
                        # Hence, need to deal with this special case
                        break
                    else:
                        next_page = more_res['next_page']
                except RateLimitError as rle:
                    self.__handle_rate_limit_exception(rle)
                except ZendeskError as ze:
                    if b""Use a start_time older than 5 minutes"" in ze.msg:
                        # We have pretty up to date data
                        break
                    else:
                        raise ze

        return results",def,call,(,self,",",path,",",query,=,None,",",get_all_pages,=,True,",",side_loading,=,False,),:,zendesk,=,self,.,get_conn,(,),first_request_successful,=,False,while,not,first_request_successful,:,try,:,results,=,zendesk,.,call,(,path,",",query,),first_request_successful,=,True,except,RateLimitError,as,"Call Zendesk API and return results

        :param path: The Zendesk API to call
        :param query: Query parameters
        :param get_all_pages: Accumulate results over all pages before
               returning. Due to strict rate limiting, this can often timeout.
               Waits for recommended period between tries after a timeout.
        :param side_loading: Retrieve related records as part of a single
               request. In order to enable side-loading, add an 'include'
               query parameter containing a comma-separated list of resources
               to load. For more information on side-loading see
               https://developer.zendesk.com/rest_api/docs/core/side_loading",Call,Zendesk,API,and,return,results,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/zendesk_hook.py#L52-L112,test,rle,:,self,.,__handle_rate_limit_exception,(,rle,),# Find the key with the results,keys,=,[,path,.,split,(,"""/""",),[,-,1,],.,split,(,""".json""",),[,0,],],next_page,=,results,[,'next_page',],if,side_loading,:,keys,+=,query,[,'include',],.,split,(,"','",),results,=,{,key,:,results,[,key,],for,key,in,keys,},if,get_all_pages,:,while,next_page,is,not,None,:,try,:,# Need to split because the next page URL has,# `github.zendesk...`,"# in it, but the call function needs it removed.",next_url,=,next_page,.,split,(,self,.,__url,),[,1,],self,.,log,.,info,(,"""Calling %s""",",",next_url,),more_res,=,zendesk,.,call,(,next_url,),for,key,in,results,:,results,[,key,],.,extend,(,more_res,[,key,],),if,next_page,==,more_res,[,'next_page',],:,# Unfortunately zdesk doesn't always throw ZendeskError,# when we are done getting all the data. Sometimes the,# next just refers to the current set of results.,"# Hence, need to deal with this special case",break,else,:,next_page,=,more_res,[,'next_page',],except,RateLimitError,as,rle,:,self,.,__handle_rate_limit_exception,(,rle,),except,ZendeskError,as,ze,:,if,"b""Use a start_time older than 5 minutes""",in,ze,.,msg,:,# We have pretty up to date data,break,else,:,raise,ze,return,results,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_glue_catalog_hook.py,AwsGlueCatalogHook.get_partitions,"def get_partitions(self,
                       database_name,
                       table_name,
                       expression='',
                       page_size=None,
                       max_items=None):
        """"""
        Retrieves the partition values for a table.

        :param database_name: The name of the catalog database where the partitions reside.
        :type database_name: str
        :param table_name: The name of the partitions' table.
        :type table_name: str
        :param expression: An expression filtering the partitions to be returned.
            Please see official AWS documentation for further information.
            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions
        :type expression: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        :return: set of partition values where each value is a tuple since
            a partition may be composed of multiple columns. For example:
            ``{('2018-01-01','1'), ('2018-01-01','2')}``
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('get_partitions')
        response = paginator.paginate(
            DatabaseName=database_name,
            TableName=table_name,
            Expression=expression,
            PaginationConfig=config
        )

        partitions = set()
        for page in response:
            for p in page['Partitions']:
                partitions.add(tuple(p['Values']))

        return partitions",python,"def get_partitions(self,
                       database_name,
                       table_name,
                       expression='',
                       page_size=None,
                       max_items=None):
        """"""
        Retrieves the partition values for a table.

        :param database_name: The name of the catalog database where the partitions reside.
        :type database_name: str
        :param table_name: The name of the partitions' table.
        :type table_name: str
        :param expression: An expression filtering the partitions to be returned.
            Please see official AWS documentation for further information.
            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions
        :type expression: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        :return: set of partition values where each value is a tuple since
            a partition may be composed of multiple columns. For example:
            ``{('2018-01-01','1'), ('2018-01-01','2')}``
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('get_partitions')
        response = paginator.paginate(
            DatabaseName=database_name,
            TableName=table_name,
            Expression=expression,
            PaginationConfig=config
        )

        partitions = set()
        for page in response:
            for p in page['Partitions']:
                partitions.add(tuple(p['Values']))

        return partitions",def,get_partitions,(,self,",",database_name,",",table_name,",",expression,=,'',",",page_size,=,None,",",max_items,=,None,),:,config,=,{,'PageSize',:,page_size,",",'MaxItems',:,max_items,",",},paginator,=,self,.,get_conn,(,),.,get_paginator,(,'get_partitions',),response,=,paginator,.,paginate,(,"Retrieves the partition values for a table.

        :param database_name: The name of the catalog database where the partitions reside.
        :type database_name: str
        :param table_name: The name of the partitions' table.
        :type table_name: str
        :param expression: An expression filtering the partitions to be returned.
            Please see official AWS documentation for further information.
            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions
        :type expression: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        :return: set of partition values where each value is a tuple since
            a partition may be composed of multiple columns. For example:
            ``{('2018-01-01','1'), ('2018-01-01','2')}``",Retrieves,the,partition,values,for,a,table,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L50-L93,test,DatabaseName,=,database_name,",",TableName,=,table_name,",",Expression,=,expression,",",PaginationConfig,=,config,),partitions,=,set,(,),for,page,in,response,:,for,p,in,page,[,'Partitions',],:,partitions,.,add,(,tuple,(,p,[,'Values',],),),return,partitions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_glue_catalog_hook.py,AwsGlueCatalogHook.get_table,"def get_table(self, database_name, table_name):
        """"""
        Get the information of the table

        :param database_name: Name of hive database (schema) @table belongs to
        :type database_name: str
        :param table_name: Name of hive table
        :type table_name: str
        :rtype: dict

        >>> hook = AwsGlueCatalogHook()
        >>> r = hook.get_table('db', 'table_foo')
        >>> r['Name'] = 'table_foo'
        """"""

        result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)

        return result['Table']",python,"def get_table(self, database_name, table_name):
        """"""
        Get the information of the table

        :param database_name: Name of hive database (schema) @table belongs to
        :type database_name: str
        :param table_name: Name of hive table
        :type table_name: str
        :rtype: dict

        >>> hook = AwsGlueCatalogHook()
        >>> r = hook.get_table('db', 'table_foo')
        >>> r['Name'] = 'table_foo'
        """"""

        result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)

        return result['Table']",def,get_table,(,self,",",database_name,",",table_name,),:,result,=,self,.,get_conn,(,),.,get_table,(,DatabaseName,=,database_name,",",Name,=,table_name,),return,result,[,'Table',],,,,,,,,,,,,,,,,,,,,"Get the information of the table

        :param database_name: Name of hive database (schema) @table belongs to
        :type database_name: str
        :param table_name: Name of hive table
        :type table_name: str
        :rtype: dict

        >>> hook = AwsGlueCatalogHook()
        >>> r = hook.get_table('db', 'table_foo')
        >>> r['Name'] = 'table_foo'",Get,the,information,of,the,table,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L120-L137,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/aws_glue_catalog_hook.py,AwsGlueCatalogHook.get_table_location,"def get_table_location(self, database_name, table_name):
        """"""
        Get the physical location of the table

        :param database_name: Name of hive database (schema) @table belongs to
        :type database_name: str
        :param table_name: Name of hive table
        :type table_name: str
        :return: str
        """"""

        table = self.get_table(database_name, table_name)

        return table['StorageDescriptor']['Location']",python,"def get_table_location(self, database_name, table_name):
        """"""
        Get the physical location of the table

        :param database_name: Name of hive database (schema) @table belongs to
        :type database_name: str
        :param table_name: Name of hive table
        :type table_name: str
        :return: str
        """"""

        table = self.get_table(database_name, table_name)

        return table['StorageDescriptor']['Location']",def,get_table_location,(,self,",",database_name,",",table_name,),:,table,=,self,.,get_table,(,database_name,",",table_name,),return,table,[,'StorageDescriptor',],[,'Location',],,,,,,,,,,,,,,,,,,,,,,,,,"Get the physical location of the table

        :param database_name: Name of hive database (schema) @table belongs to
        :type database_name: str
        :param table_name: Name of hive table
        :type table_name: str
        :return: str",Get,the,physical,location,of,the,table,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L139-L152,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/redshift_hook.py,RedshiftHook.cluster_status,"def cluster_status(self, cluster_identifier):
        """"""
        Return status of a cluster

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        """"""
        conn = self.get_conn()
        try:
            response = conn.describe_clusters(
                ClusterIdentifier=cluster_identifier)['Clusters']
            return response[0]['ClusterStatus'] if response else None
        except conn.exceptions.ClusterNotFoundFault:
            return 'cluster_not_found'",python,"def cluster_status(self, cluster_identifier):
        """"""
        Return status of a cluster

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        """"""
        conn = self.get_conn()
        try:
            response = conn.describe_clusters(
                ClusterIdentifier=cluster_identifier)['Clusters']
            return response[0]['ClusterStatus'] if response else None
        except conn.exceptions.ClusterNotFoundFault:
            return 'cluster_not_found'",def,cluster_status,(,self,",",cluster_identifier,),:,conn,=,self,.,get_conn,(,),try,:,response,=,conn,.,describe_clusters,(,ClusterIdentifier,=,cluster_identifier,),[,'Clusters',],return,response,[,0,],[,'ClusterStatus',],if,response,else,None,except,conn,.,exceptions,.,ClusterNotFoundFault,:,return,'cluster_not_found',,"Return status of a cluster

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str",Return,status,of,a,cluster,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/redshift_hook.py#L31-L44,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/redshift_hook.py,RedshiftHook.delete_cluster,"def delete_cluster(
            self,
            cluster_identifier,
            skip_final_cluster_snapshot=True,
            final_cluster_snapshot_identifier=''):
        """"""
        Delete a cluster and optionally create a snapshot

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        :param skip_final_cluster_snapshot: determines cluster snapshot creation
        :type skip_final_cluster_snapshot: bool
        :param final_cluster_snapshot_identifier: name of final cluster snapshot
        :type final_cluster_snapshot_identifier: str
        """"""
        response = self.get_conn().delete_cluster(
            ClusterIdentifier=cluster_identifier,
            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,
            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier
        )
        return response['Cluster'] if response['Cluster'] else None",python,"def delete_cluster(
            self,
            cluster_identifier,
            skip_final_cluster_snapshot=True,
            final_cluster_snapshot_identifier=''):
        """"""
        Delete a cluster and optionally create a snapshot

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        :param skip_final_cluster_snapshot: determines cluster snapshot creation
        :type skip_final_cluster_snapshot: bool
        :param final_cluster_snapshot_identifier: name of final cluster snapshot
        :type final_cluster_snapshot_identifier: str
        """"""
        response = self.get_conn().delete_cluster(
            ClusterIdentifier=cluster_identifier,
            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,
            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier
        )
        return response['Cluster'] if response['Cluster'] else None",def,delete_cluster,(,self,",",cluster_identifier,",",skip_final_cluster_snapshot,=,True,",",final_cluster_snapshot_identifier,=,'',),:,response,=,self,.,get_conn,(,),.,delete_cluster,(,ClusterIdentifier,=,cluster_identifier,",",SkipFinalClusterSnapshot,=,skip_final_cluster_snapshot,",",FinalClusterSnapshotIdentifier,=,final_cluster_snapshot_identifier,),return,response,[,'Cluster',],if,response,[,'Cluster',],else,None,,,"Delete a cluster and optionally create a snapshot

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        :param skip_final_cluster_snapshot: determines cluster snapshot creation
        :type skip_final_cluster_snapshot: bool
        :param final_cluster_snapshot_identifier: name of final cluster snapshot
        :type final_cluster_snapshot_identifier: str",Delete,a,cluster,and,optionally,create,a,snapshot,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/redshift_hook.py#L46-L66,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/redshift_hook.py,RedshiftHook.describe_cluster_snapshots,"def describe_cluster_snapshots(self, cluster_identifier):
        """"""
        Gets a list of snapshots for a cluster

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        """"""
        response = self.get_conn().describe_cluster_snapshots(
            ClusterIdentifier=cluster_identifier
        )
        if 'Snapshots' not in response:
            return None
        snapshots = response['Snapshots']
        snapshots = filter(lambda x: x['Status'], snapshots)
        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)
        return snapshots",python,"def describe_cluster_snapshots(self, cluster_identifier):
        """"""
        Gets a list of snapshots for a cluster

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        """"""
        response = self.get_conn().describe_cluster_snapshots(
            ClusterIdentifier=cluster_identifier
        )
        if 'Snapshots' not in response:
            return None
        snapshots = response['Snapshots']
        snapshots = filter(lambda x: x['Status'], snapshots)
        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)
        return snapshots",def,describe_cluster_snapshots,(,self,",",cluster_identifier,),:,response,=,self,.,get_conn,(,),.,describe_cluster_snapshots,(,ClusterIdentifier,=,cluster_identifier,),if,'Snapshots',not,in,response,:,return,None,snapshots,=,response,[,'Snapshots',],snapshots,=,filter,(,lambda,x,:,x,[,'Status',],",",snapshots,),snapshots,.,"Gets a list of snapshots for a cluster

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str",Gets,a,list,of,snapshots,for,a,cluster,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/redshift_hook.py#L68-L83,test,sort,(,key,=,lambda,x,:,x,[,'SnapshotCreateTime',],",",reverse,=,True,),return,snapshots,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/redshift_hook.py,RedshiftHook.restore_from_cluster_snapshot,"def restore_from_cluster_snapshot(self, cluster_identifier, snapshot_identifier):
        """"""
        Restores a cluster from its snapshot

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        :param snapshot_identifier: unique identifier for a snapshot of a cluster
        :type snapshot_identifier: str
        """"""
        response = self.get_conn().restore_from_cluster_snapshot(
            ClusterIdentifier=cluster_identifier,
            SnapshotIdentifier=snapshot_identifier
        )
        return response['Cluster'] if response['Cluster'] else None",python,"def restore_from_cluster_snapshot(self, cluster_identifier, snapshot_identifier):
        """"""
        Restores a cluster from its snapshot

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        :param snapshot_identifier: unique identifier for a snapshot of a cluster
        :type snapshot_identifier: str
        """"""
        response = self.get_conn().restore_from_cluster_snapshot(
            ClusterIdentifier=cluster_identifier,
            SnapshotIdentifier=snapshot_identifier
        )
        return response['Cluster'] if response['Cluster'] else None",def,restore_from_cluster_snapshot,(,self,",",cluster_identifier,",",snapshot_identifier,),:,response,=,self,.,get_conn,(,),.,restore_from_cluster_snapshot,(,ClusterIdentifier,=,cluster_identifier,",",SnapshotIdentifier,=,snapshot_identifier,),return,response,[,'Cluster',],if,response,[,'Cluster',],else,None,,,,,,,,,,,,,"Restores a cluster from its snapshot

        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        :param snapshot_identifier: unique identifier for a snapshot of a cluster
        :type snapshot_identifier: str",Restores,a,cluster,from,its,snapshot,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/redshift_hook.py#L85-L98,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/redshift_hook.py,RedshiftHook.create_cluster_snapshot,"def create_cluster_snapshot(self, snapshot_identifier, cluster_identifier):
        """"""
        Creates a snapshot of a cluster

        :param snapshot_identifier: unique identifier for a snapshot of a cluster
        :type snapshot_identifier: str
        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        """"""
        response = self.get_conn().create_cluster_snapshot(
            SnapshotIdentifier=snapshot_identifier,
            ClusterIdentifier=cluster_identifier,
        )
        return response['Snapshot'] if response['Snapshot'] else None",python,"def create_cluster_snapshot(self, snapshot_identifier, cluster_identifier):
        """"""
        Creates a snapshot of a cluster

        :param snapshot_identifier: unique identifier for a snapshot of a cluster
        :type snapshot_identifier: str
        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str
        """"""
        response = self.get_conn().create_cluster_snapshot(
            SnapshotIdentifier=snapshot_identifier,
            ClusterIdentifier=cluster_identifier,
        )
        return response['Snapshot'] if response['Snapshot'] else None",def,create_cluster_snapshot,(,self,",",snapshot_identifier,",",cluster_identifier,),:,response,=,self,.,get_conn,(,),.,create_cluster_snapshot,(,SnapshotIdentifier,=,snapshot_identifier,",",ClusterIdentifier,=,cluster_identifier,",",),return,response,[,'Snapshot',],if,response,[,'Snapshot',],else,None,,,,,,,,,,,,"Creates a snapshot of a cluster

        :param snapshot_identifier: unique identifier for a snapshot of a cluster
        :type snapshot_identifier: str
        :param cluster_identifier: unique identifier of a cluster
        :type cluster_identifier: str",Creates,a,snapshot,of,a,cluster,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/redshift_hook.py#L100-L113,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/operators/slack_operator.py,SlackAPIOperator.execute,"def execute(self, **kwargs):
        """"""
        SlackAPIOperator calls will not fail even if the call is not unsuccessful.
        It should not prevent a DAG from completing in success
        """"""
        if not self.api_params:
            self.construct_api_call_params()
        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)
        slack.call(self.method, self.api_params)",python,"def execute(self, **kwargs):
        """"""
        SlackAPIOperator calls will not fail even if the call is not unsuccessful.
        It should not prevent a DAG from completing in success
        """"""
        if not self.api_params:
            self.construct_api_call_params()
        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)
        slack.call(self.method, self.api_params)",def,execute,(,self,",",*,*,kwargs,),:,if,not,self,.,api_params,:,self,.,construct_api_call_params,(,),slack,=,SlackHook,(,token,=,self,.,token,",",slack_conn_id,=,self,.,slack_conn_id,),slack,.,call,(,self,.,method,",",self,.,api_params,),,,,"SlackAPIOperator calls will not fail even if the call is not unsuccessful.
        It should not prevent a DAG from completing in success",SlackAPIOperator,calls,will,not,fail,even,if,the,call,is,not,unsuccessful,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/slack_operator.py#L79-L87,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,It,should,not,prevent,a,DAG,from,completing,in,success,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/emr_hook.py,EmrHook.create_job_flow,"def create_job_flow(self, job_flow_overrides):
        """"""
        Creates a job flow using the config from the EMR connection.
        Keys of the json extra hash may have the arguments of the boto3
        run_job_flow method.
        Overrides for this config may be passed as the job_flow_overrides.
        """"""

        if not self.emr_conn_id:
            raise AirflowException('emr_conn_id must be present to use create_job_flow')

        emr_conn = self.get_connection(self.emr_conn_id)

        config = emr_conn.extra_dejson.copy()
        config.update(job_flow_overrides)

        response = self.get_conn().run_job_flow(**config)

        return response",python,"def create_job_flow(self, job_flow_overrides):
        """"""
        Creates a job flow using the config from the EMR connection.
        Keys of the json extra hash may have the arguments of the boto3
        run_job_flow method.
        Overrides for this config may be passed as the job_flow_overrides.
        """"""

        if not self.emr_conn_id:
            raise AirflowException('emr_conn_id must be present to use create_job_flow')

        emr_conn = self.get_connection(self.emr_conn_id)

        config = emr_conn.extra_dejson.copy()
        config.update(job_flow_overrides)

        response = self.get_conn().run_job_flow(**config)

        return response",def,create_job_flow,(,self,",",job_flow_overrides,),:,if,not,self,.,emr_conn_id,:,raise,AirflowException,(,'emr_conn_id must be present to use create_job_flow',),emr_conn,=,self,.,get_connection,(,self,.,emr_conn_id,),config,=,emr_conn,.,extra_dejson,.,copy,(,),config,.,update,(,job_flow_overrides,),response,=,self,.,get_conn,(,),.,"Creates a job flow using the config from the EMR connection.
        Keys of the json extra hash may have the arguments of the boto3
        run_job_flow method.
        Overrides for this config may be passed as the job_flow_overrides.",Creates,a,job,flow,using,the,config,from,the,EMR,connection,.,Keys,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/emr_hook.py#L39-L57,test,run_job_flow,(,*,*,config,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,of,the,json,extra,hash,may,have,the,arguments,of,the,boto3,run_job_flow,method,.,Overrides,for,this,config,may,be,passed,as,the,job_flow_overrides,.,,,,,,,,,,,,,,
apache/airflow,airflow/sensors/hdfs_sensor.py,HdfsSensor.filter_for_filesize,"def filter_for_filesize(result, size=None):
        """"""
        Will test the filepath result and test if its size is at least self.filesize

        :param result: a list of dicts returned by Snakebite ls
        :param size: the file size in MB a file should be at least to trigger True
        :return: (bool) depending on the matching criteria
        """"""
        if size:
            log = LoggingMixin().log
            log.debug(
                'Filtering for file size >= %s in files: %s',
                size, map(lambda x: x['path'], result)
            )
            size *= settings.MEGABYTE
            result = [x for x in result if x['length'] >= size]
            log.debug('HdfsSensor.poke: after size filter result is %s', result)
        return result",python,"def filter_for_filesize(result, size=None):
        """"""
        Will test the filepath result and test if its size is at least self.filesize

        :param result: a list of dicts returned by Snakebite ls
        :param size: the file size in MB a file should be at least to trigger True
        :return: (bool) depending on the matching criteria
        """"""
        if size:
            log = LoggingMixin().log
            log.debug(
                'Filtering for file size >= %s in files: %s',
                size, map(lambda x: x['path'], result)
            )
            size *= settings.MEGABYTE
            result = [x for x in result if x['length'] >= size]
            log.debug('HdfsSensor.poke: after size filter result is %s', result)
        return result",def,filter_for_filesize,(,result,",",size,=,None,),:,if,size,:,log,=,LoggingMixin,(,),.,log,log,.,debug,(,'Filtering for file size >= %s in files: %s',",",size,",",map,(,lambda,x,:,x,[,'path',],",",result,),),size,*=,settings,.,MEGABYTE,result,=,[,x,for,x,"Will test the filepath result and test if its size is at least self.filesize

        :param result: a list of dicts returned by Snakebite ls
        :param size: the file size in MB a file should be at least to trigger True
        :return: (bool) depending on the matching criteria",Will,test,the,filepath,result,and,test,if,its,size,is,at,least,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/sensors/hdfs_sensor.py#L59-L76,test,in,result,if,x,[,'length',],>=,size,],log,.,debug,(,'HdfsSensor.poke: after size filter result is %s',",",result,),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,self,.,filesize,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/sensors/hdfs_sensor.py,HdfsSensor.filter_for_ignored_ext,"def filter_for_ignored_ext(result, ignored_ext, ignore_copying):
        """"""
        Will filter if instructed to do so the result to remove matching criteria

        :param result: list of dicts returned by Snakebite ls
        :type result: list[dict]
        :param ignored_ext: list of ignored extensions
        :type ignored_ext: list
        :param ignore_copying: shall we ignore ?
        :type ignore_copying: bool
        :return: list of dicts which were not removed
        :rtype: list[dict]
        """"""
        if ignore_copying:
            log = LoggingMixin().log
            regex_builder = r""^.*\.(%s$)$"" % '$|'.join(ignored_ext)
            ignored_extensions_regex = re.compile(regex_builder)
            log.debug(
                'Filtering result for ignored extensions: %s in files %s',
                ignored_extensions_regex.pattern, map(lambda x: x['path'], result)
            )
            result = [x for x in result if not ignored_extensions_regex.match(x['path'])]
            log.debug('HdfsSensor.poke: after ext filter result is %s', result)
        return result",python,"def filter_for_ignored_ext(result, ignored_ext, ignore_copying):
        """"""
        Will filter if instructed to do so the result to remove matching criteria

        :param result: list of dicts returned by Snakebite ls
        :type result: list[dict]
        :param ignored_ext: list of ignored extensions
        :type ignored_ext: list
        :param ignore_copying: shall we ignore ?
        :type ignore_copying: bool
        :return: list of dicts which were not removed
        :rtype: list[dict]
        """"""
        if ignore_copying:
            log = LoggingMixin().log
            regex_builder = r""^.*\.(%s$)$"" % '$|'.join(ignored_ext)
            ignored_extensions_regex = re.compile(regex_builder)
            log.debug(
                'Filtering result for ignored extensions: %s in files %s',
                ignored_extensions_regex.pattern, map(lambda x: x['path'], result)
            )
            result = [x for x in result if not ignored_extensions_regex.match(x['path'])]
            log.debug('HdfsSensor.poke: after ext filter result is %s', result)
        return result",def,filter_for_ignored_ext,(,result,",",ignored_ext,",",ignore_copying,),:,if,ignore_copying,:,log,=,LoggingMixin,(,),.,log,regex_builder,=,"r""^.*\.(%s$)$""",%,'$|',.,join,(,ignored_ext,),ignored_extensions_regex,=,re,.,compile,(,regex_builder,),log,.,debug,(,'Filtering result for ignored extensions: %s in files %s',",",ignored_extensions_regex,.,pattern,",",map,(,lambda,x,"Will filter if instructed to do so the result to remove matching criteria

        :param result: list of dicts returned by Snakebite ls
        :type result: list[dict]
        :param ignored_ext: list of ignored extensions
        :type ignored_ext: list
        :param ignore_copying: shall we ignore ?
        :type ignore_copying: bool
        :return: list of dicts which were not removed
        :rtype: list[dict]",Will,filter,if,instructed,to,do,so,the,result,to,remove,matching,criteria,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/sensors/hdfs_sensor.py#L79-L102,test,:,x,[,'path',],",",result,),),result,=,[,x,for,x,in,result,if,not,ignored_extensions_regex,.,match,(,x,[,'path',],),],log,.,debug,(,'HdfsSensor.poke: after ext filter result is %s',",",result,),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mongo_to_s3.py,MongoToS3Operator.execute,"def execute(self, context):
        """"""
        Executed by task_instance at runtime
        """"""
        s3_conn = S3Hook(self.s3_conn_id)

        # Grab collection and execute query according to whether or not it is a pipeline
        if self.is_pipeline:
            results = MongoHook(self.mongo_conn_id).aggregate(
                mongo_collection=self.mongo_collection,
                aggregate_query=self.mongo_query,
                mongo_db=self.mongo_db
            )

        else:
            results = MongoHook(self.mongo_conn_id).find(
                mongo_collection=self.mongo_collection,
                query=self.mongo_query,
                mongo_db=self.mongo_db
            )

        # Performs transform then stringifies the docs results into json format
        docs_str = self._stringify(self.transform(results))

        # Load Into S3
        s3_conn.load_string(
            string_data=docs_str,
            key=self.s3_key,
            bucket_name=self.s3_bucket,
            replace=self.replace
        )

        return True",python,"def execute(self, context):
        """"""
        Executed by task_instance at runtime
        """"""
        s3_conn = S3Hook(self.s3_conn_id)

        # Grab collection and execute query according to whether or not it is a pipeline
        if self.is_pipeline:
            results = MongoHook(self.mongo_conn_id).aggregate(
                mongo_collection=self.mongo_collection,
                aggregate_query=self.mongo_query,
                mongo_db=self.mongo_db
            )

        else:
            results = MongoHook(self.mongo_conn_id).find(
                mongo_collection=self.mongo_collection,
                query=self.mongo_query,
                mongo_db=self.mongo_db
            )

        # Performs transform then stringifies the docs results into json format
        docs_str = self._stringify(self.transform(results))

        # Load Into S3
        s3_conn.load_string(
            string_data=docs_str,
            key=self.s3_key,
            bucket_name=self.s3_bucket,
            replace=self.replace
        )

        return True",def,execute,(,self,",",context,),:,s3_conn,=,S3Hook,(,self,.,s3_conn_id,),# Grab collection and execute query according to whether or not it is a pipeline,if,self,.,is_pipeline,:,results,=,MongoHook,(,self,.,mongo_conn_id,),.,aggregate,(,mongo_collection,=,self,.,mongo_collection,",",aggregate_query,=,self,.,mongo_query,",",mongo_db,=,self,.,mongo_db,),else,Executed by task_instance at runtime,Executed,by,task_instance,at,runtime,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mongo_to_s3.py#L71-L103,test,:,results,=,MongoHook,(,self,.,mongo_conn_id,),.,find,(,mongo_collection,=,self,.,mongo_collection,",",query,=,self,.,mongo_query,",",mongo_db,=,self,.,mongo_db,),# Performs transform then stringifies the docs results into json format,docs_str,=,self,.,_stringify,(,self,.,transform,(,results,),),# Load Into S3,s3_conn,.,load_string,(,string_data,=,docs_str,",",key,=,self,.,s3_key,",",bucket_name,=,self,.,s3_bucket,",",replace,=,self,.,replace,),return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/api/common/experimental/pool.py,get_pool,"def get_pool(name, session=None):
    """"""Get pool by a given name.""""""
    if not (name and name.strip()):
        raise AirflowBadRequest(""Pool name shouldn't be empty"")

    pool = session.query(Pool).filter_by(pool=name).first()
    if pool is None:
        raise PoolNotFound(""Pool '%s' doesn't exist"" % name)

    return pool",python,"def get_pool(name, session=None):
    """"""Get pool by a given name.""""""
    if not (name and name.strip()):
        raise AirflowBadRequest(""Pool name shouldn't be empty"")

    pool = session.query(Pool).filter_by(pool=name).first()
    if pool is None:
        raise PoolNotFound(""Pool '%s' doesn't exist"" % name)

    return pool",def,get_pool,(,name,",",session,=,None,),:,if,not,(,name,and,name,.,strip,(,),),:,raise,AirflowBadRequest,(,"""Pool name shouldn't be empty""",),pool,=,session,.,query,(,Pool,),.,filter_by,(,pool,=,name,),.,first,(,),if,pool,is,None,:,raise,Get pool by a given name.,Get,pool,by,a,given,name,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/pool.py#L26-L35,test,PoolNotFound,(,"""Pool '%s' doesn't exist""",%,name,),return,pool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/api/common/experimental/pool.py,create_pool,"def create_pool(name, slots, description, session=None):
    """"""Create a pool with a given parameters.""""""
    if not (name and name.strip()):
        raise AirflowBadRequest(""Pool name shouldn't be empty"")

    try:
        slots = int(slots)
    except ValueError:
        raise AirflowBadRequest(""Bad value for `slots`: %s"" % slots)

    session.expire_on_commit = False
    pool = session.query(Pool).filter_by(pool=name).first()
    if pool is None:
        pool = Pool(pool=name, slots=slots, description=description)
        session.add(pool)
    else:
        pool.slots = slots
        pool.description = description

    session.commit()

    return pool",python,"def create_pool(name, slots, description, session=None):
    """"""Create a pool with a given parameters.""""""
    if not (name and name.strip()):
        raise AirflowBadRequest(""Pool name shouldn't be empty"")

    try:
        slots = int(slots)
    except ValueError:
        raise AirflowBadRequest(""Bad value for `slots`: %s"" % slots)

    session.expire_on_commit = False
    pool = session.query(Pool).filter_by(pool=name).first()
    if pool is None:
        pool = Pool(pool=name, slots=slots, description=description)
        session.add(pool)
    else:
        pool.slots = slots
        pool.description = description

    session.commit()

    return pool",def,create_pool,(,name,",",slots,",",description,",",session,=,None,),:,if,not,(,name,and,name,.,strip,(,),),:,raise,AirflowBadRequest,(,"""Pool name shouldn't be empty""",),try,:,slots,=,int,(,slots,),except,ValueError,:,raise,AirflowBadRequest,(,"""Bad value for `slots`: %s""",%,slots,),session,.,expire_on_commit,Create a pool with a given parameters.,Create,a,pool,with,a,given,parameters,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/pool.py#L45-L66,test,=,False,pool,=,session,.,query,(,Pool,),.,filter_by,(,pool,=,name,),.,first,(,),if,pool,is,None,:,pool,=,Pool,(,pool,=,name,",",slots,=,slots,",",description,=,description,),session,.,add,(,pool,),else,:,pool,.,slots,=,slots,pool,.,description,=,description,session,.,commit,(,),return,pool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/api/common/experimental/pool.py,delete_pool,"def delete_pool(name, session=None):
    """"""Delete pool by a given name.""""""
    if not (name and name.strip()):
        raise AirflowBadRequest(""Pool name shouldn't be empty"")

    pool = session.query(Pool).filter_by(pool=name).first()
    if pool is None:
        raise PoolNotFound(""Pool '%s' doesn't exist"" % name)

    session.delete(pool)
    session.commit()

    return pool",python,"def delete_pool(name, session=None):
    """"""Delete pool by a given name.""""""
    if not (name and name.strip()):
        raise AirflowBadRequest(""Pool name shouldn't be empty"")

    pool = session.query(Pool).filter_by(pool=name).first()
    if pool is None:
        raise PoolNotFound(""Pool '%s' doesn't exist"" % name)

    session.delete(pool)
    session.commit()

    return pool",def,delete_pool,(,name,",",session,=,None,),:,if,not,(,name,and,name,.,strip,(,),),:,raise,AirflowBadRequest,(,"""Pool name shouldn't be empty""",),pool,=,session,.,query,(,Pool,),.,filter_by,(,pool,=,name,),.,first,(,),if,pool,is,None,:,raise,Delete pool by a given name.,Delete,pool,by,a,given,name,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/pool.py#L70-L82,test,PoolNotFound,(,"""Pool '%s' doesn't exist""",%,name,),session,.,delete,(,pool,),session,.,commit,(,),return,pool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_container_hook.py,GKEClusterHook._dict_to_proto,"def _dict_to_proto(py_dict, proto):
        """"""
        Converts a python dictionary to the proto supplied

        :param py_dict: The dictionary to convert
        :type py_dict: dict
        :param proto: The proto object to merge with dictionary
        :type proto: protobuf
        :return: A parsed python dictionary in provided proto format
        :raises:
            ParseError: On JSON parsing problems.
        """"""
        dict_json_str = json.dumps(py_dict)
        return json_format.Parse(dict_json_str, proto)",python,"def _dict_to_proto(py_dict, proto):
        """"""
        Converts a python dictionary to the proto supplied

        :param py_dict: The dictionary to convert
        :type py_dict: dict
        :param proto: The proto object to merge with dictionary
        :type proto: protobuf
        :return: A parsed python dictionary in provided proto format
        :raises:
            ParseError: On JSON parsing problems.
        """"""
        dict_json_str = json.dumps(py_dict)
        return json_format.Parse(dict_json_str, proto)",def,_dict_to_proto,(,py_dict,",",proto,),:,dict_json_str,=,json,.,dumps,(,py_dict,),return,json_format,.,Parse,(,dict_json_str,",",proto,),,,,,,,,,,,,,,,,,,,,,,,,,,,,"Converts a python dictionary to the proto supplied

        :param py_dict: The dictionary to convert
        :type py_dict: dict
        :param proto: The proto object to merge with dictionary
        :type proto: protobuf
        :return: A parsed python dictionary in provided proto format
        :raises:
            ParseError: On JSON parsing problems.",Converts,a,python,dictionary,to,the,proto,supplied,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L57-L70,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_container_hook.py,GKEClusterHook.wait_for_operation,"def wait_for_operation(self, operation, project_id=None):
        """"""
        Given an operation, continuously fetches the status from Google Cloud until either
        completion or an error occurring

        :param operation: The Operation to wait for
        :type operation: google.cloud.container_V1.gapic.enums.Operation
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :return: A new, updated operation fetched from Google Cloud
        """"""
        self.log.info(""Waiting for OPERATION_NAME %s"", operation.name)
        time.sleep(OPERATIONAL_POLL_INTERVAL)
        while operation.status != Operation.Status.DONE:
            if operation.status == Operation.Status.RUNNING or operation.status == \
                    Operation.Status.PENDING:
                time.sleep(OPERATIONAL_POLL_INTERVAL)
            else:
                raise exceptions.GoogleCloudError(
                    ""Operation has failed with status: %s"" % operation.status)
            # To update status of operation
            operation = self.get_operation(operation.name, project_id=project_id or self.project_id)
        return operation",python,"def wait_for_operation(self, operation, project_id=None):
        """"""
        Given an operation, continuously fetches the status from Google Cloud until either
        completion or an error occurring

        :param operation: The Operation to wait for
        :type operation: google.cloud.container_V1.gapic.enums.Operation
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :return: A new, updated operation fetched from Google Cloud
        """"""
        self.log.info(""Waiting for OPERATION_NAME %s"", operation.name)
        time.sleep(OPERATIONAL_POLL_INTERVAL)
        while operation.status != Operation.Status.DONE:
            if operation.status == Operation.Status.RUNNING or operation.status == \
                    Operation.Status.PENDING:
                time.sleep(OPERATIONAL_POLL_INTERVAL)
            else:
                raise exceptions.GoogleCloudError(
                    ""Operation has failed with status: %s"" % operation.status)
            # To update status of operation
            operation = self.get_operation(operation.name, project_id=project_id or self.project_id)
        return operation",def,wait_for_operation,(,self,",",operation,",",project_id,=,None,),:,self,.,log,.,info,(,"""Waiting for OPERATION_NAME %s""",",",operation,.,name,),time,.,sleep,(,OPERATIONAL_POLL_INTERVAL,),while,operation,.,status,!=,Operation,.,Status,.,DONE,:,if,operation,.,status,==,Operation,.,Status,.,RUNNING,or,"Given an operation, continuously fetches the status from Google Cloud until either
        completion or an error occurring

        :param operation: The Operation to wait for
        :type operation: google.cloud.container_V1.gapic.enums.Operation
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :return: A new, updated operation fetched from Google Cloud",Given,an,operation,continuously,fetches,the,status,from,Google,Cloud,until,either,completion,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L72-L94,test,operation,.,status,==,Operation,.,Status,.,PENDING,:,time,.,sleep,(,OPERATIONAL_POLL_INTERVAL,),else,:,raise,exceptions,.,GoogleCloudError,(,"""Operation has failed with status: %s""",%,operation,.,status,),# To update status of operation,operation,=,self,.,get_operation,(,operation,.,name,",",project_id,=,project_id,or,self,.,project_id,),return,operation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,or,an,error,occurring,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_container_hook.py,GKEClusterHook.get_operation,"def get_operation(self, operation_name, project_id=None):
        """"""
        Fetches the operation from Google Cloud

        :param operation_name: Name of operation to fetch
        :type operation_name: str
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :return: The new, updated operation from Google Cloud
        """"""
        return self.get_client().get_operation(project_id=project_id or self.project_id,
                                               zone=self.location,
                                               operation_id=operation_name)",python,"def get_operation(self, operation_name, project_id=None):
        """"""
        Fetches the operation from Google Cloud

        :param operation_name: Name of operation to fetch
        :type operation_name: str
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :return: The new, updated operation from Google Cloud
        """"""
        return self.get_client().get_operation(project_id=project_id or self.project_id,
                                               zone=self.location,
                                               operation_id=operation_name)",def,get_operation,(,self,",",operation_name,",",project_id,=,None,),:,return,self,.,get_client,(,),.,get_operation,(,project_id,=,project_id,or,self,.,project_id,",",zone,=,self,.,location,",",operation_id,=,operation_name,),,,,,,,,,,,,,,"Fetches the operation from Google Cloud

        :param operation_name: Name of operation to fetch
        :type operation_name: str
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :return: The new, updated operation from Google Cloud",Fetches,the,operation,from,Google,Cloud,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L96-L108,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_container_hook.py,GKEClusterHook._append_label,"def _append_label(cluster_proto, key, val):
        """"""
        Append labels to provided Cluster Protobuf

        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current
         airflow version string follows semantic versioning spec: x.y.z).

        :param cluster_proto: The proto to append resource_label airflow
            version to
        :type cluster_proto: google.cloud.container_v1.types.Cluster
        :param key: The key label
        :type key: str
        :param val:
        :type val: str
        :return: The cluster proto updated with new label
        """"""
        val = val.replace('.', '-').replace('+', '-')
        cluster_proto.resource_labels.update({key: val})
        return cluster_proto",python,"def _append_label(cluster_proto, key, val):
        """"""
        Append labels to provided Cluster Protobuf

        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current
         airflow version string follows semantic versioning spec: x.y.z).

        :param cluster_proto: The proto to append resource_label airflow
            version to
        :type cluster_proto: google.cloud.container_v1.types.Cluster
        :param key: The key label
        :type key: str
        :param val:
        :type val: str
        :return: The cluster proto updated with new label
        """"""
        val = val.replace('.', '-').replace('+', '-')
        cluster_proto.resource_labels.update({key: val})
        return cluster_proto",def,_append_label,(,cluster_proto,",",key,",",val,),:,val,=,val,.,replace,(,'.',",",'-',),.,replace,(,'+',",",'-',),cluster_proto,.,resource_labels,.,update,(,{,key,:,val,},),return,cluster_proto,,,,,,,,,,,,"Append labels to provided Cluster Protobuf

        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current
         airflow version string follows semantic versioning spec: x.y.z).

        :param cluster_proto: The proto to append resource_label airflow
            version to
        :type cluster_proto: google.cloud.container_v1.types.Cluster
        :param key: The key label
        :type key: str
        :param val:
        :type val: str
        :return: The cluster proto updated with new label",Append,labels,to,provided,Cluster,Protobuf,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L111-L129,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_container_hook.py,GKEClusterHook.create_cluster,"def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):
        """"""
        Creates a cluster, consisting of the specified number and type of Google Compute
        Engine instances.

        :param cluster: A Cluster protobuf or dict. If dict is provided, it must
            be of the same form as the protobuf message
            :class:`google.cloud.container_v1.types.Cluster`
        :type cluster: dict or google.cloud.container_v1.types.Cluster
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :param retry: A retry object (``google.api_core.retry.Retry``) used to
            retry requests.
            If None is specified, requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to
            complete. Note that if retry is specified, the timeout applies to each
            individual attempt.
        :type timeout: float
        :return: The full url to the new, or existing, cluster
        :raises:
            ParseError: On JSON parsing problems when trying to convert dict
            AirflowException: cluster is not dict type nor Cluster proto type
        """"""

        if isinstance(cluster, dict):
            cluster_proto = Cluster()
            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)
        elif not isinstance(cluster, Cluster):
            raise AirflowException(
                ""cluster is not instance of Cluster proto or python dict"")

        self._append_label(cluster, 'airflow-version', 'v' + version.version)

        self.log.info(
            ""Creating (project_id=%s, zone=%s, cluster_name=%s)"",
            self.project_id, self.location, cluster.name
        )
        try:
            op = self.get_client().create_cluster(project_id=project_id or self.project_id,
                                                  zone=self.location,
                                                  cluster=cluster,
                                                  retry=retry,
                                                  timeout=timeout)
            op = self.wait_for_operation(op)

            return op.target_link
        except AlreadyExists as error:
            self.log.info('Assuming Success: %s', error.message)
            return self.get_cluster(name=cluster.name).self_link",python,"def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):
        """"""
        Creates a cluster, consisting of the specified number and type of Google Compute
        Engine instances.

        :param cluster: A Cluster protobuf or dict. If dict is provided, it must
            be of the same form as the protobuf message
            :class:`google.cloud.container_v1.types.Cluster`
        :type cluster: dict or google.cloud.container_v1.types.Cluster
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :param retry: A retry object (``google.api_core.retry.Retry``) used to
            retry requests.
            If None is specified, requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to
            complete. Note that if retry is specified, the timeout applies to each
            individual attempt.
        :type timeout: float
        :return: The full url to the new, or existing, cluster
        :raises:
            ParseError: On JSON parsing problems when trying to convert dict
            AirflowException: cluster is not dict type nor Cluster proto type
        """"""

        if isinstance(cluster, dict):
            cluster_proto = Cluster()
            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)
        elif not isinstance(cluster, Cluster):
            raise AirflowException(
                ""cluster is not instance of Cluster proto or python dict"")

        self._append_label(cluster, 'airflow-version', 'v' + version.version)

        self.log.info(
            ""Creating (project_id=%s, zone=%s, cluster_name=%s)"",
            self.project_id, self.location, cluster.name
        )
        try:
            op = self.get_client().create_cluster(project_id=project_id or self.project_id,
                                                  zone=self.location,
                                                  cluster=cluster,
                                                  retry=retry,
                                                  timeout=timeout)
            op = self.wait_for_operation(op)

            return op.target_link
        except AlreadyExists as error:
            self.log.info('Assuming Success: %s', error.message)
            return self.get_cluster(name=cluster.name).self_link",def,create_cluster,(,self,",",cluster,",",project_id,=,None,",",retry,=,DEFAULT,",",timeout,=,DEFAULT,),:,if,isinstance,(,cluster,",",dict,),:,cluster_proto,=,Cluster,(,),cluster,=,self,.,_dict_to_proto,(,py_dict,=,cluster,",",proto,=,cluster_proto,),elif,not,isinstance,(,cluster,"Creates a cluster, consisting of the specified number and type of Google Compute
        Engine instances.

        :param cluster: A Cluster protobuf or dict. If dict is provided, it must
            be of the same form as the protobuf message
            :class:`google.cloud.container_v1.types.Cluster`
        :type cluster: dict or google.cloud.container_v1.types.Cluster
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :param retry: A retry object (``google.api_core.retry.Retry``) used to
            retry requests.
            If None is specified, requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to
            complete. Note that if retry is specified, the timeout applies to each
            individual attempt.
        :type timeout: float
        :return: The full url to the new, or existing, cluster
        :raises:
            ParseError: On JSON parsing problems when trying to convert dict
            AirflowException: cluster is not dict type nor Cluster proto type",Creates,a,cluster,consisting,of,the,specified,number,and,type,of,Google,Compute,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L170-L219,test,",",Cluster,),:,raise,AirflowException,(,"""cluster is not instance of Cluster proto or python dict""",),self,.,_append_label,(,cluster,",",'airflow-version',",",'v',+,version,.,version,),self,.,log,.,info,(,"""Creating (project_id=%s, zone=%s, cluster_name=%s)""",",",self,.,project_id,",",self,.,location,",",cluster,.,name,),try,:,op,=,self,.,get_client,(,),.,create_cluster,(,project_id,=,project_id,or,self,.,project_id,",",zone,=,self,.,location,",",cluster,=,cluster,",",retry,=,retry,",",timeout,=,timeout,),op,=,self,.,wait_for_operation,(,op,),return,op,.,target_link,except,AlreadyExists,as,error,:,self,.,log,.,info,(,'Assuming Success: %s',",",error,.,message,),return,self,.,get_cluster,(,name,=,cluster,.,name,),.,self_link,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Engine,instances,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_container_hook.py,GKEClusterHook.get_cluster,"def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):
        """"""
        Gets details of specified cluster

        :param name: The name of the cluster to retrieve
        :type name: str
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :param retry: A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to
            complete. Note that if retry is specified, the timeout applies to each
            individual attempt.
        :type timeout: float
        :return: google.cloud.container_v1.types.Cluster
        """"""
        self.log.info(
            ""Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)"",
            project_id or self.project_id, self.location, name
        )

        return self.get_client().get_cluster(project_id=project_id or self.project_id,
                                             zone=self.location,
                                             cluster_id=name,
                                             retry=retry,
                                             timeout=timeout).self_link",python,"def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):
        """"""
        Gets details of specified cluster

        :param name: The name of the cluster to retrieve
        :type name: str
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :param retry: A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to
            complete. Note that if retry is specified, the timeout applies to each
            individual attempt.
        :type timeout: float
        :return: google.cloud.container_v1.types.Cluster
        """"""
        self.log.info(
            ""Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)"",
            project_id or self.project_id, self.location, name
        )

        return self.get_client().get_cluster(project_id=project_id or self.project_id,
                                             zone=self.location,
                                             cluster_id=name,
                                             retry=retry,
                                             timeout=timeout).self_link",def,get_cluster,(,self,",",name,",",project_id,=,None,",",retry,=,DEFAULT,",",timeout,=,DEFAULT,),:,self,.,log,.,info,(,"""Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)""",",",project_id,or,self,.,project_id,",",self,.,location,",",name,),return,self,.,get_client,(,),.,get_cluster,(,project_id,=,project_id,"Gets details of specified cluster

        :param name: The name of the cluster to retrieve
        :type name: str
        :param project_id: Google Cloud Platform project ID
        :type project_id: str
        :param retry: A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to
            complete. Note that if retry is specified, the timeout applies to each
            individual attempt.
        :type timeout: float
        :return: google.cloud.container_v1.types.Cluster",Gets,details,of,specified,cluster,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L221-L247,test,or,self,.,project_id,",",zone,=,self,.,location,",",cluster_id,=,name,",",retry,=,retry,",",timeout,=,timeout,),.,self_link,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/discord_webhook_hook.py,DiscordWebhookHook._get_webhook_endpoint,"def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint):
        """"""
        Given a Discord http_conn_id, return the default webhook endpoint or override if a
        webhook_endpoint is manually supplied.

        :param http_conn_id: The provided connection ID
        :param webhook_endpoint: The manually provided webhook endpoint
        :return: Webhook endpoint (str) to use
        """"""
        if webhook_endpoint:
            endpoint = webhook_endpoint
        elif http_conn_id:
            conn = self.get_connection(http_conn_id)
            extra = conn.extra_dejson
            endpoint = extra.get('webhook_endpoint', '')
        else:
            raise AirflowException('Cannot get webhook endpoint: No valid Discord '
                                   'webhook endpoint or http_conn_id supplied.')

        # make sure endpoint matches the expected Discord webhook format
        if not re.match('^webhooks/[0-9]+/[a-zA-Z0-9_-]+$', endpoint):
            raise AirflowException('Expected Discord webhook endpoint in the form '
                                   'of ""webhooks/{webhook.id}/{webhook.token}"".')

        return endpoint",python,"def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint):
        """"""
        Given a Discord http_conn_id, return the default webhook endpoint or override if a
        webhook_endpoint is manually supplied.

        :param http_conn_id: The provided connection ID
        :param webhook_endpoint: The manually provided webhook endpoint
        :return: Webhook endpoint (str) to use
        """"""
        if webhook_endpoint:
            endpoint = webhook_endpoint
        elif http_conn_id:
            conn = self.get_connection(http_conn_id)
            extra = conn.extra_dejson
            endpoint = extra.get('webhook_endpoint', '')
        else:
            raise AirflowException('Cannot get webhook endpoint: No valid Discord '
                                   'webhook endpoint or http_conn_id supplied.')

        # make sure endpoint matches the expected Discord webhook format
        if not re.match('^webhooks/[0-9]+/[a-zA-Z0-9_-]+$', endpoint):
            raise AirflowException('Expected Discord webhook endpoint in the form '
                                   'of ""webhooks/{webhook.id}/{webhook.token}"".')

        return endpoint",def,_get_webhook_endpoint,(,self,",",http_conn_id,",",webhook_endpoint,),:,if,webhook_endpoint,:,endpoint,=,webhook_endpoint,elif,http_conn_id,:,conn,=,self,.,get_connection,(,http_conn_id,),extra,=,conn,.,extra_dejson,endpoint,=,extra,.,get,(,'webhook_endpoint',",",'',),else,:,raise,AirflowException,(,'Cannot get webhook endpoint: No valid Discord ','webhook endpoint or http_conn_id supplied.',),# make sure endpoint matches the expected Discord webhook format,if,"Given a Discord http_conn_id, return the default webhook endpoint or override if a
        webhook_endpoint is manually supplied.

        :param http_conn_id: The provided connection ID
        :param webhook_endpoint: The manually provided webhook endpoint
        :return: Webhook endpoint (str) to use",Given,a,Discord,http_conn_id,return,the,default,webhook,endpoint,or,override,if,a,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/discord_webhook_hook.py#L76-L100,test,not,re,.,match,(,'^webhooks/[0-9]+/[a-zA-Z0-9_-]+$',",",endpoint,),:,raise,AirflowException,(,'Expected Discord webhook endpoint in the form ',"'of ""webhooks/{webhook.id}/{webhook.token}"".'",),return,endpoint,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,webhook_endpoint,is,manually,supplied,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/discord_webhook_hook.py,DiscordWebhookHook._build_discord_payload,"def _build_discord_payload(self):
        """"""
        Construct the Discord JSON payload. All relevant parameters are combined here
        to a valid Discord JSON payload.

        :return: Discord payload (str) to send
        """"""
        payload = {}

        if self.username:
            payload['username'] = self.username
        if self.avatar_url:
            payload['avatar_url'] = self.avatar_url

        payload['tts'] = self.tts

        if len(self.message) <= 2000:
            payload['content'] = self.message
        else:
            raise AirflowException('Discord message length must be 2000 or fewer '
                                   'characters.')

        return json.dumps(payload)",python,"def _build_discord_payload(self):
        """"""
        Construct the Discord JSON payload. All relevant parameters are combined here
        to a valid Discord JSON payload.

        :return: Discord payload (str) to send
        """"""
        payload = {}

        if self.username:
            payload['username'] = self.username
        if self.avatar_url:
            payload['avatar_url'] = self.avatar_url

        payload['tts'] = self.tts

        if len(self.message) <= 2000:
            payload['content'] = self.message
        else:
            raise AirflowException('Discord message length must be 2000 or fewer '
                                   'characters.')

        return json.dumps(payload)",def,_build_discord_payload,(,self,),:,payload,=,{,},if,self,.,username,:,payload,[,'username',],=,self,.,username,if,self,.,avatar_url,:,payload,[,'avatar_url',],=,self,.,avatar_url,payload,[,'tts',],=,self,.,tts,if,len,(,self,.,message,),<=,"Construct the Discord JSON payload. All relevant parameters are combined here
        to a valid Discord JSON payload.

        :return: Discord payload (str) to send",Construct,the,Discord,JSON,payload,.,All,relevant,parameters,are,combined,here,to,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/discord_webhook_hook.py#L102-L124,test,2000,:,payload,[,'content',],=,self,.,message,else,:,raise,AirflowException,(,'Discord message length must be 2000 or fewer ','characters.',),return,json,.,dumps,(,payload,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,a,valid,Discord,JSON,payload,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/discord_webhook_hook.py,DiscordWebhookHook.execute,"def execute(self):
        """"""
        Execute the Discord webhook call
        """"""
        proxies = {}
        if self.proxy:
            # we only need https proxy for Discord
            proxies = {'https': self.proxy}

        discord_payload = self._build_discord_payload()

        self.run(endpoint=self.webhook_endpoint,
                 data=discord_payload,
                 headers={'Content-type': 'application/json'},
                 extra_options={'proxies': proxies})",python,"def execute(self):
        """"""
        Execute the Discord webhook call
        """"""
        proxies = {}
        if self.proxy:
            # we only need https proxy for Discord
            proxies = {'https': self.proxy}

        discord_payload = self._build_discord_payload()

        self.run(endpoint=self.webhook_endpoint,
                 data=discord_payload,
                 headers={'Content-type': 'application/json'},
                 extra_options={'proxies': proxies})",def,execute,(,self,),:,proxies,=,{,},if,self,.,proxy,:,# we only need https proxy for Discord,proxies,=,{,'https',:,self,.,proxy,},discord_payload,=,self,.,_build_discord_payload,(,),self,.,run,(,endpoint,=,self,.,webhook_endpoint,",",data,=,discord_payload,",",headers,=,{,'Content-type',:,'application/json',Execute the Discord webhook call,Execute,the,Discord,webhook,call,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/discord_webhook_hook.py#L126-L140,test,},",",extra_options,=,{,'proxies',:,proxies,},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_kms_hook.py,GoogleCloudKMSHook.encrypt,"def encrypt(self, key_name, plaintext, authenticated_data=None):
        """"""
        Encrypts a plaintext message using Google Cloud KMS.

        :param key_name: The Resource Name for the key (or key version)
                         to be used for encyption. Of the form
                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``
        :type key_name: str
        :param plaintext: The message to be encrypted.
        :type plaintext: bytes
        :param authenticated_data: Optional additional authenticated data that
                                   must also be provided to decrypt the message.
        :type authenticated_data: bytes
        :return: The base 64 encoded ciphertext of the original message.
        :rtype: str
        """"""
        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()
        body = {'plaintext': _b64encode(plaintext)}
        if authenticated_data:
            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)

        request = keys.encrypt(name=key_name, body=body)
        response = request.execute(num_retries=self.num_retries)

        ciphertext = response['ciphertext']
        return ciphertext",python,"def encrypt(self, key_name, plaintext, authenticated_data=None):
        """"""
        Encrypts a plaintext message using Google Cloud KMS.

        :param key_name: The Resource Name for the key (or key version)
                         to be used for encyption. Of the form
                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``
        :type key_name: str
        :param plaintext: The message to be encrypted.
        :type plaintext: bytes
        :param authenticated_data: Optional additional authenticated data that
                                   must also be provided to decrypt the message.
        :type authenticated_data: bytes
        :return: The base 64 encoded ciphertext of the original message.
        :rtype: str
        """"""
        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()
        body = {'plaintext': _b64encode(plaintext)}
        if authenticated_data:
            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)

        request = keys.encrypt(name=key_name, body=body)
        response = request.execute(num_retries=self.num_retries)

        ciphertext = response['ciphertext']
        return ciphertext",def,encrypt,(,self,",",key_name,",",plaintext,",",authenticated_data,=,None,),:,keys,=,self,.,get_conn,(,),.,projects,(,),.,locations,(,),.,keyRings,(,),.,cryptoKeys,(,),body,=,{,'plaintext',:,_b64encode,(,plaintext,),},if,authenticated_data,:,body,[,"Encrypts a plaintext message using Google Cloud KMS.

        :param key_name: The Resource Name for the key (or key version)
                         to be used for encyption. Of the form
                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``
        :type key_name: str
        :param plaintext: The message to be encrypted.
        :type plaintext: bytes
        :param authenticated_data: Optional additional authenticated data that
                                   must also be provided to decrypt the message.
        :type authenticated_data: bytes
        :return: The base 64 encoded ciphertext of the original message.
        :rtype: str",Encrypts,a,plaintext,message,using,Google,Cloud,KMS,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_kms_hook.py#L58-L83,test,'additionalAuthenticatedData',],=,_b64encode,(,authenticated_data,),request,=,keys,.,encrypt,(,name,=,key_name,",",body,=,body,),response,=,request,.,execute,(,num_retries,=,self,.,num_retries,),ciphertext,=,response,[,'ciphertext',],return,ciphertext,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sqoop_hook.py,SqoopHook.import_table,"def import_table(self, table, target_dir=None, append=False, file_type=""text"",
                     columns=None, split_by=None, where=None, direct=False,
                     driver=None, extra_import_options=None):
        """"""
        Imports table from remote location to target dir. Arguments are
        copies of direct sqoop command line arguments

        :param table: Table to read
        :param target_dir: HDFS destination dir
        :param append: Append data to an existing dataset in HDFS
        :param file_type: ""avro"", ""sequence"", ""text"" or ""parquet"".
            Imports data to into the specified format. Defaults to text.
        :param columns: <col,col,col…> Columns to import from table
        :param split_by: Column of the table used to split work units
        :param where: WHERE clause to use during import
        :param direct: Use direct connector if exists for the database
        :param driver: Manually specify JDBC driver class to use
        :param extra_import_options: Extra import options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.
        """"""
        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,
                               driver, extra_import_options)

        cmd += [""--table"", table]

        if columns:
            cmd += [""--columns"", columns]
        if where:
            cmd += [""--where"", where]

        self.Popen(cmd)",python,"def import_table(self, table, target_dir=None, append=False, file_type=""text"",
                     columns=None, split_by=None, where=None, direct=False,
                     driver=None, extra_import_options=None):
        """"""
        Imports table from remote location to target dir. Arguments are
        copies of direct sqoop command line arguments

        :param table: Table to read
        :param target_dir: HDFS destination dir
        :param append: Append data to an existing dataset in HDFS
        :param file_type: ""avro"", ""sequence"", ""text"" or ""parquet"".
            Imports data to into the specified format. Defaults to text.
        :param columns: <col,col,col…> Columns to import from table
        :param split_by: Column of the table used to split work units
        :param where: WHERE clause to use during import
        :param direct: Use direct connector if exists for the database
        :param driver: Manually specify JDBC driver class to use
        :param extra_import_options: Extra import options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.
        """"""
        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,
                               driver, extra_import_options)

        cmd += [""--table"", table]

        if columns:
            cmd += [""--columns"", columns]
        if where:
            cmd += [""--where"", where]

        self.Popen(cmd)",def,import_table,(,self,",",table,",",target_dir,=,None,",",append,=,False,",",file_type,=,"""text""",",",columns,=,None,",",split_by,=,None,",",where,=,None,",",direct,=,False,",",driver,=,None,",",extra_import_options,=,None,),:,cmd,=,self,.,_import_cmd,(,target_dir,",","Imports table from remote location to target dir. Arguments are
        copies of direct sqoop command line arguments

        :param table: Table to read
        :param target_dir: HDFS destination dir
        :param append: Append data to an existing dataset in HDFS
        :param file_type: ""avro"", ""sequence"", ""text"" or ""parquet"".
            Imports data to into the specified format. Defaults to text.
        :param columns: <col,col,col…> Columns to import from table
        :param split_by: Column of the table used to split work units
        :param where: WHERE clause to use during import
        :param direct: Use direct connector if exists for the database
        :param driver: Manually specify JDBC driver class to use
        :param extra_import_options: Extra import options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.",Imports,table,from,remote,location,to,target,dir,.,Arguments,are,copies,of,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sqoop_hook.py#L202-L233,test,append,",",file_type,",",split_by,",",direct,",",driver,",",extra_import_options,),cmd,+=,[,"""--table""",",",table,],if,columns,:,cmd,+=,[,"""--columns""",",",columns,],if,where,:,cmd,+=,[,"""--where""",",",where,],self,.,Popen,(,cmd,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,direct,sqoop,command,line,arguments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sqoop_hook.py,SqoopHook.import_query,"def import_query(self, query, target_dir, append=False, file_type=""text"",
                     split_by=None, direct=None, driver=None, extra_import_options=None):
        """"""
        Imports a specific query from the rdbms to hdfs

        :param query: Free format query to run
        :param target_dir: HDFS destination dir
        :param append: Append data to an existing dataset in HDFS
        :param file_type: ""avro"", ""sequence"", ""text"" or ""parquet""
            Imports data to hdfs into the specified format. Defaults to text.
        :param split_by: Column of the table used to split work units
        :param direct: Use direct import fast path
        :param driver: Manually specify JDBC driver class to use
        :param extra_import_options: Extra import options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.
        """"""
        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,
                               driver, extra_import_options)
        cmd += [""--query"", query]

        self.Popen(cmd)",python,"def import_query(self, query, target_dir, append=False, file_type=""text"",
                     split_by=None, direct=None, driver=None, extra_import_options=None):
        """"""
        Imports a specific query from the rdbms to hdfs

        :param query: Free format query to run
        :param target_dir: HDFS destination dir
        :param append: Append data to an existing dataset in HDFS
        :param file_type: ""avro"", ""sequence"", ""text"" or ""parquet""
            Imports data to hdfs into the specified format. Defaults to text.
        :param split_by: Column of the table used to split work units
        :param direct: Use direct import fast path
        :param driver: Manually specify JDBC driver class to use
        :param extra_import_options: Extra import options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.
        """"""
        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,
                               driver, extra_import_options)
        cmd += [""--query"", query]

        self.Popen(cmd)",def,import_query,(,self,",",query,",",target_dir,",",append,=,False,",",file_type,=,"""text""",",",split_by,=,None,",",direct,=,None,",",driver,=,None,",",extra_import_options,=,None,),:,cmd,=,self,.,_import_cmd,(,target_dir,",",append,",",file_type,",",split_by,",",direct,",",driver,",","Imports a specific query from the rdbms to hdfs

        :param query: Free format query to run
        :param target_dir: HDFS destination dir
        :param append: Append data to an existing dataset in HDFS
        :param file_type: ""avro"", ""sequence"", ""text"" or ""parquet""
            Imports data to hdfs into the specified format. Defaults to text.
        :param split_by: Column of the table used to split work units
        :param direct: Use direct import fast path
        :param driver: Manually specify JDBC driver class to use
        :param extra_import_options: Extra import options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.",Imports,a,specific,query,from,the,rdbms,to,hdfs,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sqoop_hook.py#L235-L256,test,extra_import_options,),cmd,+=,[,"""--query""",",",query,],self,.,Popen,(,cmd,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/sqoop_hook.py,SqoopHook.export_table,"def export_table(self, table, export_dir, input_null_string,
                     input_null_non_string, staging_table,
                     clear_staging_table, enclosed_by,
                     escaped_by, input_fields_terminated_by,
                     input_lines_terminated_by,
                     input_optionally_enclosed_by, batch,
                     relaxed_isolation, extra_export_options=None):
        """"""
        Exports Hive table to remote location. Arguments are copies of direct
        sqoop command line Arguments

        :param table: Table remote destination
        :param export_dir: Hive table to export
        :param input_null_string: The string to be interpreted as null for
            string columns
        :param input_null_non_string: The string to be interpreted as null
            for non-string columns
        :param staging_table: The table in which data will be staged before
            being inserted into the destination table
        :param clear_staging_table: Indicate that any data present in the
            staging table can be deleted
        :param enclosed_by: Sets a required field enclosing character
        :param escaped_by: Sets the escape character
        :param input_fields_terminated_by: Sets the field separator character
        :param input_lines_terminated_by: Sets the end-of-line character
        :param input_optionally_enclosed_by: Sets a field enclosing character
        :param batch: Use batch mode for underlying statement execution
        :param relaxed_isolation: Transaction isolation to read uncommitted
            for the mappers
        :param extra_export_options: Extra export options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.
        """"""
        cmd = self._export_cmd(table, export_dir, input_null_string,
                               input_null_non_string, staging_table,
                               clear_staging_table, enclosed_by, escaped_by,
                               input_fields_terminated_by,
                               input_lines_terminated_by,
                               input_optionally_enclosed_by, batch,
                               relaxed_isolation, extra_export_options)

        self.Popen(cmd)",python,"def export_table(self, table, export_dir, input_null_string,
                     input_null_non_string, staging_table,
                     clear_staging_table, enclosed_by,
                     escaped_by, input_fields_terminated_by,
                     input_lines_terminated_by,
                     input_optionally_enclosed_by, batch,
                     relaxed_isolation, extra_export_options=None):
        """"""
        Exports Hive table to remote location. Arguments are copies of direct
        sqoop command line Arguments

        :param table: Table remote destination
        :param export_dir: Hive table to export
        :param input_null_string: The string to be interpreted as null for
            string columns
        :param input_null_non_string: The string to be interpreted as null
            for non-string columns
        :param staging_table: The table in which data will be staged before
            being inserted into the destination table
        :param clear_staging_table: Indicate that any data present in the
            staging table can be deleted
        :param enclosed_by: Sets a required field enclosing character
        :param escaped_by: Sets the escape character
        :param input_fields_terminated_by: Sets the field separator character
        :param input_lines_terminated_by: Sets the end-of-line character
        :param input_optionally_enclosed_by: Sets a field enclosing character
        :param batch: Use batch mode for underlying statement execution
        :param relaxed_isolation: Transaction isolation to read uncommitted
            for the mappers
        :param extra_export_options: Extra export options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.
        """"""
        cmd = self._export_cmd(table, export_dir, input_null_string,
                               input_null_non_string, staging_table,
                               clear_staging_table, enclosed_by, escaped_by,
                               input_fields_terminated_by,
                               input_lines_terminated_by,
                               input_optionally_enclosed_by, batch,
                               relaxed_isolation, extra_export_options)

        self.Popen(cmd)",def,export_table,(,self,",",table,",",export_dir,",",input_null_string,",",input_null_non_string,",",staging_table,",",clear_staging_table,",",enclosed_by,",",escaped_by,",",input_fields_terminated_by,",",input_lines_terminated_by,",",input_optionally_enclosed_by,",",batch,",",relaxed_isolation,",",extra_export_options,=,None,),:,cmd,=,self,.,_export_cmd,(,table,",",export_dir,",",input_null_string,",",input_null_non_string,",",staging_table,",","Exports Hive table to remote location. Arguments are copies of direct
        sqoop command line Arguments

        :param table: Table remote destination
        :param export_dir: Hive table to export
        :param input_null_string: The string to be interpreted as null for
            string columns
        :param input_null_non_string: The string to be interpreted as null
            for non-string columns
        :param staging_table: The table in which data will be staged before
            being inserted into the destination table
        :param clear_staging_table: Indicate that any data present in the
            staging table can be deleted
        :param enclosed_by: Sets a required field enclosing character
        :param escaped_by: Sets the escape character
        :param input_fields_terminated_by: Sets the field separator character
        :param input_lines_terminated_by: Sets the end-of-line character
        :param input_optionally_enclosed_by: Sets a field enclosing character
        :param batch: Use batch mode for underlying statement execution
        :param relaxed_isolation: Transaction isolation to read uncommitted
            for the mappers
        :param extra_export_options: Extra export options to pass as dict.
            If a key doesn't have a value, just pass an empty string to it.
            Don't include prefix of -- for sqoop options.",Exports,Hive,table,to,remote,location,.,Arguments,are,copies,of,direct,sqoop,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/sqoop_hook.py#L314-L355,test,clear_staging_table,",",enclosed_by,",",escaped_by,",",input_fields_terminated_by,",",input_lines_terminated_by,",",input_optionally_enclosed_by,",",batch,",",relaxed_isolation,",",extra_export_options,),self,.,Popen,(,cmd,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,command,line,Arguments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_text_to_speech_hook.py,GCPTextToSpeechHook.get_conn,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Text to Speech.

        :return: Google Cloud Text to Speech client object.
        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient
        """"""
        if not self._client:
            self._client = TextToSpeechClient(credentials=self._get_credentials())
        return self._client",python,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Text to Speech.

        :return: Google Cloud Text to Speech client object.
        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient
        """"""
        if not self._client:
            self._client = TextToSpeechClient(credentials=self._get_credentials())
        return self._client",def,get_conn,(,self,),:,if,not,self,.,_client,:,self,.,_client,=,TextToSpeechClient,(,credentials,=,self,.,_get_credentials,(,),),return,self,.,_client,,,,,,,,,,,,,,,,,,,,,,,"Retrieves connection to Cloud Text to Speech.

        :return: Google Cloud Text to Speech client object.
        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient",Retrieves,connection,to,Cloud,Text,to,Speech,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_text_to_speech_hook.py#L42-L51,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_text_to_speech_hook.py,GCPTextToSpeechHook.synthesize_speech,"def synthesize_speech(self, input_data, voice, audio_config, retry=None, timeout=None):
        """"""
        Synthesizes text input

        :param input_data: text input to be synthesized. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput
        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput
        :param voice: configuration of voice to be used in synthesis. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams
        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams
        :param audio_config: configuration of the synthesized audio. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig
        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig
        :return: SynthesizeSpeechResponse See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse
        :rtype: object
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
                requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        """"""
        client = self.get_conn()
        self.log.info(""Synthesizing input: %s"" % input_data)
        return client.synthesize_speech(
            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout
        )",python,"def synthesize_speech(self, input_data, voice, audio_config, retry=None, timeout=None):
        """"""
        Synthesizes text input

        :param input_data: text input to be synthesized. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput
        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput
        :param voice: configuration of voice to be used in synthesis. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams
        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams
        :param audio_config: configuration of the synthesized audio. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig
        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig
        :return: SynthesizeSpeechResponse See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse
        :rtype: object
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
                requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        """"""
        client = self.get_conn()
        self.log.info(""Synthesizing input: %s"" % input_data)
        return client.synthesize_speech(
            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout
        )",def,synthesize_speech,(,self,",",input_data,",",voice,",",audio_config,",",retry,=,None,",",timeout,=,None,),:,client,=,self,.,get_conn,(,),self,.,log,.,info,(,"""Synthesizing input: %s""",%,input_data,),return,client,.,synthesize_speech,(,input_,=,input_data,",",voice,=,voice,",",audio_config,=,"Synthesizes text input

        :param input_data: text input to be synthesized. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput
        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput
        :param voice: configuration of voice to be used in synthesis. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams
        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams
        :param audio_config: configuration of the synthesized audio. See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig
        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig
        :return: SynthesizeSpeechResponse See more:
            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse
        :rtype: object
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
                requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float",Synthesizes,text,input,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_text_to_speech_hook.py#L53-L80,test,audio_config,",",retry,=,retry,",",timeout,=,timeout,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/log/s3_task_handler.py,S3TaskHandler.close,"def close(self):
        """"""
        Close and upload local log file to remote storage S3.
        """"""
        # When application exit, system shuts down all handlers by
        # calling close method. Here we check if logger is already
        # closed to prevent uploading the log to remote storage multiple
        # times when `logging.shutdown` is called.
        if self.closed:
            return

        super().close()

        if not self.upload_on_close:
            return

        local_loc = os.path.join(self.local_base, self.log_relative_path)
        remote_loc = os.path.join(self.remote_base, self.log_relative_path)
        if os.path.exists(local_loc):
            # read log and remove old logs to get just the latest additions
            with open(local_loc, 'r') as logfile:
                log = logfile.read()
            self.s3_write(log, remote_loc)

        # Mark closed so we don't double write if close is called twice
        self.closed = True",python,"def close(self):
        """"""
        Close and upload local log file to remote storage S3.
        """"""
        # When application exit, system shuts down all handlers by
        # calling close method. Here we check if logger is already
        # closed to prevent uploading the log to remote storage multiple
        # times when `logging.shutdown` is called.
        if self.closed:
            return

        super().close()

        if not self.upload_on_close:
            return

        local_loc = os.path.join(self.local_base, self.log_relative_path)
        remote_loc = os.path.join(self.remote_base, self.log_relative_path)
        if os.path.exists(local_loc):
            # read log and remove old logs to get just the latest additions
            with open(local_loc, 'r') as logfile:
                log = logfile.read()
            self.s3_write(log, remote_loc)

        # Mark closed so we don't double write if close is called twice
        self.closed = True",def,close,(,self,),:,"# When application exit, system shuts down all handlers by",# calling close method. Here we check if logger is already,# closed to prevent uploading the log to remote storage multiple,# times when `logging.shutdown` is called.,if,self,.,closed,:,return,super,(,),.,close,(,),if,not,self,.,upload_on_close,:,return,local_loc,=,os,.,path,.,join,(,self,.,local_base,",",self,.,log_relative_path,),remote_loc,=,os,.,path,.,Close and upload local log file to remote storage S3.,Close,and,upload,local,log,file,to,remote,storage,S3,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/s3_task_handler.py#L62-L87,test,join,(,self,.,remote_base,",",self,.,log_relative_path,),if,os,.,path,.,exists,(,local_loc,),:,# read log and remove old logs to get just the latest additions,with,open,(,local_loc,",",'r',),as,logfile,:,log,=,logfile,.,read,(,),self,.,s3_write,(,log,",",remote_loc,),# Mark closed so we don't double write if close is called twice,self,.,closed,=,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/kubernetes/worker_configuration.py,WorkerConfiguration._get_init_containers,"def _get_init_containers(self):
        """"""When using git to retrieve the DAGs, use the GitSync Init Container""""""
        # If we're using volume claims to mount the dags, no init container is needed
        if self.kube_config.dags_volume_claim or \
           self.kube_config.dags_volume_host or self.kube_config.dags_in_image:
            return []

        # Otherwise, define a git-sync init container
        init_environment = [{
            'name': 'GIT_SYNC_REPO',
            'value': self.kube_config.git_repo
        }, {
            'name': 'GIT_SYNC_BRANCH',
            'value': self.kube_config.git_branch
        }, {
            'name': 'GIT_SYNC_ROOT',
            'value': self.kube_config.git_sync_root
        }, {
            'name': 'GIT_SYNC_DEST',
            'value': self.kube_config.git_sync_dest
        }, {
            'name': 'GIT_SYNC_DEPTH',
            'value': '1'
        }, {
            'name': 'GIT_SYNC_ONE_TIME',
            'value': 'true'
        }]
        if self.kube_config.git_user:
            init_environment.append({
                'name': 'GIT_SYNC_USERNAME',
                'value': self.kube_config.git_user
            })
        if self.kube_config.git_password:
            init_environment.append({
                'name': 'GIT_SYNC_PASSWORD',
                'value': self.kube_config.git_password
            })

        volume_mounts = [{
            'mountPath': self.kube_config.git_sync_root,
            'name': self.dags_volume_name,
            'readOnly': False
        }]
        if self.kube_config.git_ssh_key_secret_name:
            volume_mounts.append({
                'name': self.git_sync_ssh_secret_volume_name,
                'mountPath': '/etc/git-secret/ssh',
                'subPath': 'ssh'
            })
            init_environment.extend([
                {
                    'name': 'GIT_SSH_KEY_FILE',
                    'value': '/etc/git-secret/ssh'
                },
                {
                    'name': 'GIT_SYNC_SSH',
                    'value': 'true'
                }])
        if self.kube_config.git_ssh_known_hosts_configmap_name:
            volume_mounts.append({
                'name': self.git_sync_ssh_known_hosts_volume_name,
                'mountPath': '/etc/git-secret/known_hosts',
                'subPath': 'known_hosts'
            })
            init_environment.extend([
                {
                    'name': 'GIT_KNOWN_HOSTS',
                    'value': 'true'
                },
                {
                    'name': 'GIT_SSH_KNOWN_HOSTS_FILE',
                    'value': '/etc/git-secret/known_hosts'
                }
            ])
        else:
            init_environment.append({
                'name': 'GIT_KNOWN_HOSTS',
                'value': 'false'
            })

        return [{
            'name': self.kube_config.git_sync_init_container_name,
            'image': self.kube_config.git_sync_container,
            'securityContext': {'runAsUser': 65533},  # git-sync user
            'env': init_environment,
            'volumeMounts': volume_mounts
        }]",python,"def _get_init_containers(self):
        """"""When using git to retrieve the DAGs, use the GitSync Init Container""""""
        # If we're using volume claims to mount the dags, no init container is needed
        if self.kube_config.dags_volume_claim or \
           self.kube_config.dags_volume_host or self.kube_config.dags_in_image:
            return []

        # Otherwise, define a git-sync init container
        init_environment = [{
            'name': 'GIT_SYNC_REPO',
            'value': self.kube_config.git_repo
        }, {
            'name': 'GIT_SYNC_BRANCH',
            'value': self.kube_config.git_branch
        }, {
            'name': 'GIT_SYNC_ROOT',
            'value': self.kube_config.git_sync_root
        }, {
            'name': 'GIT_SYNC_DEST',
            'value': self.kube_config.git_sync_dest
        }, {
            'name': 'GIT_SYNC_DEPTH',
            'value': '1'
        }, {
            'name': 'GIT_SYNC_ONE_TIME',
            'value': 'true'
        }]
        if self.kube_config.git_user:
            init_environment.append({
                'name': 'GIT_SYNC_USERNAME',
                'value': self.kube_config.git_user
            })
        if self.kube_config.git_password:
            init_environment.append({
                'name': 'GIT_SYNC_PASSWORD',
                'value': self.kube_config.git_password
            })

        volume_mounts = [{
            'mountPath': self.kube_config.git_sync_root,
            'name': self.dags_volume_name,
            'readOnly': False
        }]
        if self.kube_config.git_ssh_key_secret_name:
            volume_mounts.append({
                'name': self.git_sync_ssh_secret_volume_name,
                'mountPath': '/etc/git-secret/ssh',
                'subPath': 'ssh'
            })
            init_environment.extend([
                {
                    'name': 'GIT_SSH_KEY_FILE',
                    'value': '/etc/git-secret/ssh'
                },
                {
                    'name': 'GIT_SYNC_SSH',
                    'value': 'true'
                }])
        if self.kube_config.git_ssh_known_hosts_configmap_name:
            volume_mounts.append({
                'name': self.git_sync_ssh_known_hosts_volume_name,
                'mountPath': '/etc/git-secret/known_hosts',
                'subPath': 'known_hosts'
            })
            init_environment.extend([
                {
                    'name': 'GIT_KNOWN_HOSTS',
                    'value': 'true'
                },
                {
                    'name': 'GIT_SSH_KNOWN_HOSTS_FILE',
                    'value': '/etc/git-secret/known_hosts'
                }
            ])
        else:
            init_environment.append({
                'name': 'GIT_KNOWN_HOSTS',
                'value': 'false'
            })

        return [{
            'name': self.kube_config.git_sync_init_container_name,
            'image': self.kube_config.git_sync_container,
            'securityContext': {'runAsUser': 65533},  # git-sync user
            'env': init_environment,
            'volumeMounts': volume_mounts
        }]",def,_get_init_containers,(,self,),:,"# If we're using volume claims to mount the dags, no init container is needed",if,self,.,kube_config,.,dags_volume_claim,or,self,.,kube_config,.,dags_volume_host,or,self,.,kube_config,.,dags_in_image,:,return,[,],"# Otherwise, define a git-sync init container",init_environment,=,[,{,'name',:,'GIT_SYNC_REPO',",",'value',:,self,.,kube_config,.,git_repo,},",",{,'name',:,'GIT_SYNC_BRANCH',",","When using git to retrieve the DAGs, use the GitSync Init Container",When,using,git,to,retrieve,the,DAGs,use,the,GitSync,Init,Container,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/kubernetes/worker_configuration.py#L45-L131,test,'value',:,self,.,kube_config,.,git_branch,},",",{,'name',:,'GIT_SYNC_ROOT',",",'value',:,self,.,kube_config,.,git_sync_root,},",",{,'name',:,'GIT_SYNC_DEST',",",'value',:,self,.,kube_config,.,git_sync_dest,},",",{,'name',:,'GIT_SYNC_DEPTH',",",'value',:,'1',},",",{,'name',:,'GIT_SYNC_ONE_TIME',",",'value',:,'true',},],if,self,.,kube_config,.,git_user,:,init_environment,.,append,(,{,'name',:,'GIT_SYNC_USERNAME',",",'value',:,self,.,kube_config,.,git_user,},),if,self,.,kube_config,.,git_password,:,init_environment,.,append,(,{,'name',:,'GIT_SYNC_PASSWORD',",",'value',:,self,.,kube_config,.,git_password,},),volume_mounts,=,[,{,'mountPath',:,self,.,kube_config,.,git_sync_root,",",'name',:,self,.,dags_volume_name,",",'readOnly',:,False,},],if,self,.,kube_config,.,git_ssh_key_secret_name,:,volume_mounts,.,append,(,{,'name',:,self,.,git_sync_ssh_secret_volume_name,",",'mountPath',:,'/etc/git-secret/ssh',",",'subPath',:,'ssh',},),init_environment,.,extend,(,[,{,'name',:,'GIT_SSH_KEY_FILE',",",'value',:,'/etc/git-secret/ssh',},",",{,'name',:,'GIT_SYNC_SSH',",",'value',:,'true',},],),if,self,.,kube_config,.,git_ssh_known_hosts_configmap_name,:,volume_mounts,.,append,(,{,'name',:,self,.,git_sync_ssh_known_hosts_volume_name,",",'mountPath',:,'/etc/git-secret/known_hosts',",",'subPath',:,'known_hosts',},),init_environment,.,extend,(,[,{,'name',:,'GIT_KNOWN_HOSTS',",",'value',:,'true',},",",{,'name',:,'GIT_SSH_KNOWN_HOSTS_FILE',",",'value',:,'/etc/git-secret/known_hosts',},],),else,:,init_environment,.,append,(,{,'name',:,'GIT_KNOWN_HOSTS',",",'value',:,'false',},),return,[,{,'name',:,self,.,kube_config,.,git_sync_init_container_name,",",'image',:,self,.,kube_config,.,git_sync_container,",",'securityContext',:,{,'runAsUser',:,65533,},",",# git-sync user,'env',:,init_environment,",",'volumeMounts',:,volume_mounts,},],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/kubernetes/worker_configuration.py,WorkerConfiguration._get_environment,"def _get_environment(self):
        """"""Defines any necessary environment variables for the pod executor""""""
        env = {}

        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):
            env[env_var_name] = env_var_val

        env[""AIRFLOW__CORE__EXECUTOR""] = ""LocalExecutor""

        if self.kube_config.airflow_configmap:
            env['AIRFLOW_HOME'] = self.worker_airflow_home
            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags
        if (not self.kube_config.airflow_configmap and
                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):
            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(""core"", ""SQL_ALCHEMY_CONN"")
        if self.kube_config.git_dags_folder_mount_point:
            # /root/airflow/dags/repo/dags
            dag_volume_mount_path = os.path.join(
                self.kube_config.git_dags_folder_mount_point,
                self.kube_config.git_sync_dest,  # repo
                self.kube_config.git_subpath     # dags
            )
            env['AIRFLOW__CORE__DAGS_FOLDER'] = dag_volume_mount_path
        return env",python,"def _get_environment(self):
        """"""Defines any necessary environment variables for the pod executor""""""
        env = {}

        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):
            env[env_var_name] = env_var_val

        env[""AIRFLOW__CORE__EXECUTOR""] = ""LocalExecutor""

        if self.kube_config.airflow_configmap:
            env['AIRFLOW_HOME'] = self.worker_airflow_home
            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags
        if (not self.kube_config.airflow_configmap and
                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):
            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(""core"", ""SQL_ALCHEMY_CONN"")
        if self.kube_config.git_dags_folder_mount_point:
            # /root/airflow/dags/repo/dags
            dag_volume_mount_path = os.path.join(
                self.kube_config.git_dags_folder_mount_point,
                self.kube_config.git_sync_dest,  # repo
                self.kube_config.git_subpath     # dags
            )
            env['AIRFLOW__CORE__DAGS_FOLDER'] = dag_volume_mount_path
        return env",def,_get_environment,(,self,),:,env,=,{,},for,env_var_name,",",env_var_val,in,six,.,iteritems,(,self,.,kube_config,.,kube_env_vars,),:,env,[,env_var_name,],=,env_var_val,env,[,"""AIRFLOW__CORE__EXECUTOR""",],=,"""LocalExecutor""",if,self,.,kube_config,.,airflow_configmap,:,env,[,'AIRFLOW_HOME',],=,self,.,Defines any necessary environment variables for the pod executor,Defines,any,necessary,environment,variables,for,the,pod,executor,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/kubernetes/worker_configuration.py#L133-L156,test,worker_airflow_home,env,[,'AIRFLOW__CORE__DAGS_FOLDER',],=,self,.,worker_airflow_dags,if,(,not,self,.,kube_config,.,airflow_configmap,and,'AIRFLOW__CORE__SQL_ALCHEMY_CONN',not,in,self,.,kube_config,.,kube_secrets,),:,env,[,'AIRFLOW__CORE__SQL_ALCHEMY_CONN',],=,conf,.,get,(,"""core""",",","""SQL_ALCHEMY_CONN""",),if,self,.,kube_config,.,git_dags_folder_mount_point,:,# /root/airflow/dags/repo/dags,dag_volume_mount_path,=,os,.,path,.,join,(,self,.,kube_config,.,git_dags_folder_mount_point,",",self,.,kube_config,.,git_sync_dest,",",# repo,self,.,kube_config,.,git_subpath,# dags,),env,[,'AIRFLOW__CORE__DAGS_FOLDER',],=,dag_volume_mount_path,return,env,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/kubernetes/worker_configuration.py,WorkerConfiguration._get_secrets,"def _get_secrets(self):
        """"""Defines any necessary secrets for the pod executor""""""
        worker_secrets = []

        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):
            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')
            worker_secrets.append(
                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)
            )

        if self.kube_config.env_from_secret_ref:
            for secret_ref in self.kube_config.env_from_secret_ref.split(','):
                worker_secrets.append(
                    Secret('env', None, secret_ref)
                )

        return worker_secrets",python,"def _get_secrets(self):
        """"""Defines any necessary secrets for the pod executor""""""
        worker_secrets = []

        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):
            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')
            worker_secrets.append(
                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)
            )

        if self.kube_config.env_from_secret_ref:
            for secret_ref in self.kube_config.env_from_secret_ref.split(','):
                worker_secrets.append(
                    Secret('env', None, secret_ref)
                )

        return worker_secrets",def,_get_secrets,(,self,),:,worker_secrets,=,[,],for,env_var_name,",",obj_key_pair,in,six,.,iteritems,(,self,.,kube_config,.,kube_secrets,),:,k8s_secret_obj,",",k8s_secret_key,=,obj_key_pair,.,split,(,'=',),worker_secrets,.,append,(,Secret,(,'env',",",env_var_name,",",k8s_secret_obj,",",k8s_secret_key,),),if,Defines any necessary secrets for the pod executor,Defines,any,necessary,secrets,for,the,pod,executor,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/kubernetes/worker_configuration.py#L164-L180,test,self,.,kube_config,.,env_from_secret_ref,:,for,secret_ref,in,self,.,kube_config,.,env_from_secret_ref,.,split,(,"','",),:,worker_secrets,.,append,(,Secret,(,'env',",",None,",",secret_ref,),),return,worker_secrets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/kubernetes/worker_configuration.py,WorkerConfiguration._get_security_context,"def _get_security_context(self):
        """"""Defines the security context""""""
        security_context = {}

        if self.kube_config.worker_run_as_user:
            security_context['runAsUser'] = self.kube_config.worker_run_as_user

        if self.kube_config.worker_fs_group:
            security_context['fsGroup'] = self.kube_config.worker_fs_group

        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth
        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:
            security_context['fsGroup'] = 65533

        return security_context",python,"def _get_security_context(self):
        """"""Defines the security context""""""
        security_context = {}

        if self.kube_config.worker_run_as_user:
            security_context['runAsUser'] = self.kube_config.worker_run_as_user

        if self.kube_config.worker_fs_group:
            security_context['fsGroup'] = self.kube_config.worker_fs_group

        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth
        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:
            security_context['fsGroup'] = 65533

        return security_context",def,_get_security_context,(,self,),:,security_context,=,{,},if,self,.,kube_config,.,worker_run_as_user,:,security_context,[,'runAsUser',],=,self,.,kube_config,.,worker_run_as_user,if,self,.,kube_config,.,worker_fs_group,:,security_context,[,'fsGroup',],=,self,.,kube_config,.,worker_fs_group,# set fs_group to 65533 if not explicitly specified and using git ssh keypair auth,if,self,.,kube_config,.,git_ssh_key_secret_name,and,Defines the security context,Defines,the,security,context,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/kubernetes/worker_configuration.py#L188-L202,test,security_context,.,get,(,'fsGroup',),is,None,:,security_context,[,'fsGroup',],=,65533,return,security_context,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/qubole_hook.py,QuboleHook.get_extra_links,"def get_extra_links(self, operator, dttm):
        """"""
        Get link to qubole command result page.

        :param operator: operator
        :param dttm: datetime
        :return: url link
        """"""
        conn = BaseHook.get_connection(operator.kwargs['qubole_conn_id'])
        if conn and conn.host:
            host = re.sub(r'api$', 'v2/analyze?command_id=', conn.host)
        else:
            host = 'https://api.qubole.com/v2/analyze?command_id='

        ti = TaskInstance(task=operator, execution_date=dttm)
        qds_command_id = ti.xcom_pull(task_ids=operator.task_id, key='qbol_cmd_id')
        url = host + str(qds_command_id) if qds_command_id else ''
        return url",python,"def get_extra_links(self, operator, dttm):
        """"""
        Get link to qubole command result page.

        :param operator: operator
        :param dttm: datetime
        :return: url link
        """"""
        conn = BaseHook.get_connection(operator.kwargs['qubole_conn_id'])
        if conn and conn.host:
            host = re.sub(r'api$', 'v2/analyze?command_id=', conn.host)
        else:
            host = 'https://api.qubole.com/v2/analyze?command_id='

        ti = TaskInstance(task=operator, execution_date=dttm)
        qds_command_id = ti.xcom_pull(task_ids=operator.task_id, key='qbol_cmd_id')
        url = host + str(qds_command_id) if qds_command_id else ''
        return url",def,get_extra_links,(,self,",",operator,",",dttm,),:,conn,=,BaseHook,.,get_connection,(,operator,.,kwargs,[,'qubole_conn_id',],),if,conn,and,conn,.,host,:,host,=,re,.,sub,(,r'api$',",",'v2/analyze?command_id=',",",conn,.,host,),else,:,host,=,'https://api.qubole.com/v2/analyze?command_id=',ti,=,TaskInstance,"Get link to qubole command result page.

        :param operator: operator
        :param dttm: datetime
        :return: url link",Get,link,to,qubole,command,result,page,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/qubole_hook.py#L212-L229,test,(,task,=,operator,",",execution_date,=,dttm,),qds_command_id,=,ti,.,xcom_pull,(,task_ids,=,operator,.,task_id,",",key,=,'qbol_cmd_id',),url,=,host,+,str,(,qds_command_id,),if,qds_command_id,else,'',return,url,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BaseJob.heartbeat,"def heartbeat(self):
        """"""
        Heartbeats update the job's entry in the database with a timestamp
        for the latest_heartbeat and allows for the job to be killed
        externally. This allows at the system level to monitor what is
        actually active.

        For instance, an old heartbeat for SchedulerJob would mean something
        is wrong.

        This also allows for any job to be killed externally, regardless
        of who is running it or on which machine it is running.

        Note that if your heartbeat is set to 60 seconds and you call this
        method after 10 seconds of processing since the last heartbeat, it
        will sleep 50 seconds to complete the 60 seconds and keep a steady
        heart rate. If you go over 60 seconds before calling it, it won't
        sleep at all.
        """"""
        try:
            with create_session() as session:
                job = session.query(BaseJob).filter_by(id=self.id).one()
                make_transient(job)
                session.commit()

            if job.state == State.SHUTDOWN:
                self.kill()

            is_unit_test = conf.getboolean('core', 'unit_test_mode')
            if not is_unit_test:
                # Figure out how long to sleep for
                sleep_for = 0
                if job.latest_heartbeat:
                    seconds_remaining = self.heartrate - \
                        (timezone.utcnow() - job.latest_heartbeat)\
                        .total_seconds()
                    sleep_for = max(0, seconds_remaining)

                sleep(sleep_for)

            # Update last heartbeat time
            with create_session() as session:
                job = session.query(BaseJob).filter(BaseJob.id == self.id).first()
                job.latest_heartbeat = timezone.utcnow()
                session.merge(job)
                session.commit()

                self.heartbeat_callback(session=session)
                self.log.debug('[heartbeat]')
        except OperationalError as e:
            self.log.error(""Scheduler heartbeat got an exception: %s"", str(e))",python,"def heartbeat(self):
        """"""
        Heartbeats update the job's entry in the database with a timestamp
        for the latest_heartbeat and allows for the job to be killed
        externally. This allows at the system level to monitor what is
        actually active.

        For instance, an old heartbeat for SchedulerJob would mean something
        is wrong.

        This also allows for any job to be killed externally, regardless
        of who is running it or on which machine it is running.

        Note that if your heartbeat is set to 60 seconds and you call this
        method after 10 seconds of processing since the last heartbeat, it
        will sleep 50 seconds to complete the 60 seconds and keep a steady
        heart rate. If you go over 60 seconds before calling it, it won't
        sleep at all.
        """"""
        try:
            with create_session() as session:
                job = session.query(BaseJob).filter_by(id=self.id).one()
                make_transient(job)
                session.commit()

            if job.state == State.SHUTDOWN:
                self.kill()

            is_unit_test = conf.getboolean('core', 'unit_test_mode')
            if not is_unit_test:
                # Figure out how long to sleep for
                sleep_for = 0
                if job.latest_heartbeat:
                    seconds_remaining = self.heartrate - \
                        (timezone.utcnow() - job.latest_heartbeat)\
                        .total_seconds()
                    sleep_for = max(0, seconds_remaining)

                sleep(sleep_for)

            # Update last heartbeat time
            with create_session() as session:
                job = session.query(BaseJob).filter(BaseJob.id == self.id).first()
                job.latest_heartbeat = timezone.utcnow()
                session.merge(job)
                session.commit()

                self.heartbeat_callback(session=session)
                self.log.debug('[heartbeat]')
        except OperationalError as e:
            self.log.error(""Scheduler heartbeat got an exception: %s"", str(e))",def,heartbeat,(,self,),:,try,:,with,create_session,(,),as,session,:,job,=,session,.,query,(,BaseJob,),.,filter_by,(,id,=,self,.,id,),.,one,(,),make_transient,(,job,),session,.,commit,(,),if,job,.,state,==,State,.,"Heartbeats update the job's entry in the database with a timestamp
        for the latest_heartbeat and allows for the job to be killed
        externally. This allows at the system level to monitor what is
        actually active.

        For instance, an old heartbeat for SchedulerJob would mean something
        is wrong.

        This also allows for any job to be killed externally, regardless
        of who is running it or on which machine it is running.

        Note that if your heartbeat is set to 60 seconds and you call this
        method after 10 seconds of processing since the last heartbeat, it
        will sleep 50 seconds to complete the 60 seconds and keep a steady
        heart rate. If you go over 60 seconds before calling it, it won't
        sleep at all.",Heartbeats,update,the,job,s,entry,in,the,database,with,a,timestamp,for,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L139-L189,test,SHUTDOWN,:,self,.,kill,(,),is_unit_test,=,conf,.,getboolean,(,'core',",",'unit_test_mode',),if,not,is_unit_test,:,# Figure out how long to sleep for,sleep_for,=,0,if,job,.,latest_heartbeat,:,seconds_remaining,=,self,.,heartrate,-,(,timezone,.,utcnow,(,),-,job,.,latest_heartbeat,),.,total_seconds,(,),sleep_for,=,max,(,0,",",seconds_remaining,),sleep,(,sleep_for,),# Update last heartbeat time,with,create_session,(,),as,session,:,job,=,session,.,query,(,BaseJob,),.,filter,(,BaseJob,.,id,==,self,.,id,),.,first,(,),job,.,latest_heartbeat,=,timezone,.,utcnow,(,),session,.,merge,(,job,),session,.,commit,(,),self,.,heartbeat_callback,(,session,=,session,),self,.,log,.,debug,(,'[heartbeat]',),except,OperationalError,as,e,:,self,.,log,.,error,(,"""Scheduler heartbeat got an exception: %s""",",",str,(,e,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,latest_heartbeat,and,allows,for,the,job,to,be,killed,externally,.,This,allows,at,the,system,level,to,monitor,what,is,actually,active,.,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,DagFileProcessor._launch_process,"def _launch_process(result_queue,
                        file_path,
                        pickle_dags,
                        dag_id_white_list,
                        thread_name,
                        zombies):
        """"""
        Launch a process to process the given file.

        :param result_queue: the queue to use for passing back the result
        :type result_queue: multiprocessing.Queue
        :param file_path: the file to process
        :type file_path: unicode
        :param pickle_dags: whether to pickle the DAGs found in the file and
            save them to the DB
        :type pickle_dags: bool
        :param dag_id_white_list: if specified, only examine DAG ID's that are
            in this list
        :type dag_id_white_list: list[unicode]
        :param thread_name: the name to use for the process that is launched
        :type thread_name: unicode
        :return: the process that was launched
        :rtype: multiprocessing.Process
        :param zombies: zombie task instances to kill
        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]
        """"""
        def helper():
            # This helper runs in the newly created process
            log = logging.getLogger(""airflow.processor"")

            stdout = StreamLogWriter(log, logging.INFO)
            stderr = StreamLogWriter(log, logging.WARN)

            set_context(log, file_path)

            try:
                # redirect stdout/stderr to log
                sys.stdout = stdout
                sys.stderr = stderr

                # Re-configure the ORM engine as there are issues with multiple processes
                settings.configure_orm()

                # Change the thread name to differentiate log lines. This is
                # really a separate process, but changing the name of the
                # process doesn't work, so changing the thread name instead.
                threading.current_thread().name = thread_name
                start_time = time.time()

                log.info(""Started process (PID=%s) to work on %s"",
                         os.getpid(), file_path)
                scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
                result = scheduler_job.process_file(file_path,
                                                    zombies,
                                                    pickle_dags)
                result_queue.put(result)
                end_time = time.time()
                log.info(
                    ""Processing %s took %.3f seconds"", file_path, end_time - start_time
                )
            except Exception:
                # Log exceptions through the logging framework.
                log.exception(""Got an exception! Propagating..."")
                raise
            finally:
                sys.stdout = sys.__stdout__
                sys.stderr = sys.__stderr__
                # We re-initialized the ORM within this Process above so we need to
                # tear it down manually here
                settings.dispose_orm()

        p = multiprocessing.Process(target=helper,
                                    args=(),
                                    name=""{}-Process"".format(thread_name))
        p.start()
        return p",python,"def _launch_process(result_queue,
                        file_path,
                        pickle_dags,
                        dag_id_white_list,
                        thread_name,
                        zombies):
        """"""
        Launch a process to process the given file.

        :param result_queue: the queue to use for passing back the result
        :type result_queue: multiprocessing.Queue
        :param file_path: the file to process
        :type file_path: unicode
        :param pickle_dags: whether to pickle the DAGs found in the file and
            save them to the DB
        :type pickle_dags: bool
        :param dag_id_white_list: if specified, only examine DAG ID's that are
            in this list
        :type dag_id_white_list: list[unicode]
        :param thread_name: the name to use for the process that is launched
        :type thread_name: unicode
        :return: the process that was launched
        :rtype: multiprocessing.Process
        :param zombies: zombie task instances to kill
        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]
        """"""
        def helper():
            # This helper runs in the newly created process
            log = logging.getLogger(""airflow.processor"")

            stdout = StreamLogWriter(log, logging.INFO)
            stderr = StreamLogWriter(log, logging.WARN)

            set_context(log, file_path)

            try:
                # redirect stdout/stderr to log
                sys.stdout = stdout
                sys.stderr = stderr

                # Re-configure the ORM engine as there are issues with multiple processes
                settings.configure_orm()

                # Change the thread name to differentiate log lines. This is
                # really a separate process, but changing the name of the
                # process doesn't work, so changing the thread name instead.
                threading.current_thread().name = thread_name
                start_time = time.time()

                log.info(""Started process (PID=%s) to work on %s"",
                         os.getpid(), file_path)
                scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
                result = scheduler_job.process_file(file_path,
                                                    zombies,
                                                    pickle_dags)
                result_queue.put(result)
                end_time = time.time()
                log.info(
                    ""Processing %s took %.3f seconds"", file_path, end_time - start_time
                )
            except Exception:
                # Log exceptions through the logging framework.
                log.exception(""Got an exception! Propagating..."")
                raise
            finally:
                sys.stdout = sys.__stdout__
                sys.stderr = sys.__stderr__
                # We re-initialized the ORM within this Process above so we need to
                # tear it down manually here
                settings.dispose_orm()

        p = multiprocessing.Process(target=helper,
                                    args=(),
                                    name=""{}-Process"".format(thread_name))
        p.start()
        return p",def,_launch_process,(,result_queue,",",file_path,",",pickle_dags,",",dag_id_white_list,",",thread_name,",",zombies,),:,def,helper,(,),:,# This helper runs in the newly created process,log,=,logging,.,getLogger,(,"""airflow.processor""",),stdout,=,StreamLogWriter,(,log,",",logging,.,INFO,),stderr,=,StreamLogWriter,(,log,",",logging,.,WARN,),set_context,(,"Launch a process to process the given file.

        :param result_queue: the queue to use for passing back the result
        :type result_queue: multiprocessing.Queue
        :param file_path: the file to process
        :type file_path: unicode
        :param pickle_dags: whether to pickle the DAGs found in the file and
            save them to the DB
        :type pickle_dags: bool
        :param dag_id_white_list: if specified, only examine DAG ID's that are
            in this list
        :type dag_id_white_list: list[unicode]
        :param thread_name: the name to use for the process that is launched
        :type thread_name: unicode
        :return: the process that was launched
        :rtype: multiprocessing.Process
        :param zombies: zombie task instances to kill
        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]",Launch,a,process,to,process,the,given,file,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L342-L417,test,log,",",file_path,),try,:,# redirect stdout/stderr to log,sys,.,stdout,=,stdout,sys,.,stderr,=,stderr,# Re-configure the ORM engine as there are issues with multiple processes,settings,.,configure_orm,(,),# Change the thread name to differentiate log lines. This is,"# really a separate process, but changing the name of the","# process doesn't work, so changing the thread name instead.",threading,.,current_thread,(,),.,name,=,thread_name,start_time,=,time,.,time,(,),log,.,info,(,"""Started process (PID=%s) to work on %s""",",",os,.,getpid,(,),",",file_path,),scheduler_job,=,SchedulerJob,(,dag_ids,=,dag_id_white_list,",",log,=,log,),result,=,scheduler_job,.,process_file,(,file_path,",",zombies,",",pickle_dags,),result_queue,.,put,(,result,),end_time,=,time,.,time,(,),log,.,info,(,"""Processing %s took %.3f seconds""",",",file_path,",",end_time,-,start_time,),except,Exception,:,# Log exceptions through the logging framework.,log,.,exception,(,"""Got an exception! Propagating...""",),raise,finally,:,sys,.,stdout,=,sys,.,__stdout__,sys,.,stderr,=,sys,.,__stderr__,# We re-initialized the ORM within this Process above so we need to,# tear it down manually here,settings,.,dispose_orm,(,),p,=,multiprocessing,.,Process,(,target,=,helper,",",args,=,(,),",",name,=,"""{}-Process""",.,format,(,thread_name,),),p,.,start,(,),return,p,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,DagFileProcessor.start,"def start(self):
        """"""
        Launch the process and start processing the DAG.
        """"""
        self._process = DagFileProcessor._launch_process(
            self._result_queue,
            self.file_path,
            self._pickle_dags,
            self._dag_id_white_list,
            ""DagFileProcessor{}"".format(self._instance_id),
            self._zombies)
        self._start_time = timezone.utcnow()",python,"def start(self):
        """"""
        Launch the process and start processing the DAG.
        """"""
        self._process = DagFileProcessor._launch_process(
            self._result_queue,
            self.file_path,
            self._pickle_dags,
            self._dag_id_white_list,
            ""DagFileProcessor{}"".format(self._instance_id),
            self._zombies)
        self._start_time = timezone.utcnow()",def,start,(,self,),:,self,.,_process,=,DagFileProcessor,.,_launch_process,(,self,.,_result_queue,",",self,.,file_path,",",self,.,_pickle_dags,",",self,.,_dag_id_white_list,",","""DagFileProcessor{}""",.,format,(,self,.,_instance_id,),",",self,.,_zombies,),self,.,_start_time,=,timezone,.,utcnow,(,),Launch the process and start processing the DAG.,Launch,the,process,and,start,processing,the,DAG,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L419-L430,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,DagFileProcessor.done,"def done(self):
        """"""
        Check if the process launched to process this file is done.

        :return: whether the process is finished running
        :rtype: bool
        """"""
        if self._process is None:
            raise AirflowException(""Tried to see if it's done before starting!"")

        if self._done:
            return True

        # In case result queue is corrupted.
        if self._result_queue and not self._result_queue.empty():
            self._result = self._result_queue.get_nowait()
            self._done = True
            self.log.debug(""Waiting for %s"", self._process)
            self._process.join()
            return True

        # Potential error case when process dies
        if self._result_queue and not self._process.is_alive():
            self._done = True
            # Get the object from the queue or else join() can hang.
            if not self._result_queue.empty():
                self._result = self._result_queue.get_nowait()
            self.log.debug(""Waiting for %s"", self._process)
            self._process.join()
            return True

        return False",python,"def done(self):
        """"""
        Check if the process launched to process this file is done.

        :return: whether the process is finished running
        :rtype: bool
        """"""
        if self._process is None:
            raise AirflowException(""Tried to see if it's done before starting!"")

        if self._done:
            return True

        # In case result queue is corrupted.
        if self._result_queue and not self._result_queue.empty():
            self._result = self._result_queue.get_nowait()
            self._done = True
            self.log.debug(""Waiting for %s"", self._process)
            self._process.join()
            return True

        # Potential error case when process dies
        if self._result_queue and not self._process.is_alive():
            self._done = True
            # Get the object from the queue or else join() can hang.
            if not self._result_queue.empty():
                self._result = self._result_queue.get_nowait()
            self.log.debug(""Waiting for %s"", self._process)
            self._process.join()
            return True

        return False",def,done,(,self,),:,if,self,.,_process,is,None,:,raise,AirflowException,(,"""Tried to see if it's done before starting!""",),if,self,.,_done,:,return,True,# In case result queue is corrupted.,if,self,.,_result_queue,and,not,self,.,_result_queue,.,empty,(,),:,self,.,_result,=,self,.,_result_queue,.,get_nowait,(,),self,"Check if the process launched to process this file is done.

        :return: whether the process is finished running
        :rtype: bool",Check,if,the,process,launched,to,process,this,file,is,done,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L473-L504,test,.,_done,=,True,self,.,log,.,debug,(,"""Waiting for %s""",",",self,.,_process,),self,.,_process,.,join,(,),return,True,# Potential error case when process dies,if,self,.,_result_queue,and,not,self,.,_process,.,is_alive,(,),:,self,.,_done,=,True,# Get the object from the queue or else join() can hang.,if,not,self,.,_result_queue,.,empty,(,),:,self,.,_result,=,self,.,_result_queue,.,get_nowait,(,),self,.,log,.,debug,(,"""Waiting for %s""",",",self,.,_process,),self,.,_process,.,join,(,),return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._exit_gracefully,"def _exit_gracefully(self, signum, frame):
        """"""
        Helper method to clean up processor_agent to avoid leaving orphan processes.
        """"""
        self.log.info(""Exiting gracefully upon receiving signal %s"", signum)
        if self.processor_agent:
            self.processor_agent.end()
        sys.exit(os.EX_OK)",python,"def _exit_gracefully(self, signum, frame):
        """"""
        Helper method to clean up processor_agent to avoid leaving orphan processes.
        """"""
        self.log.info(""Exiting gracefully upon receiving signal %s"", signum)
        if self.processor_agent:
            self.processor_agent.end()
        sys.exit(os.EX_OK)",def,_exit_gracefully,(,self,",",signum,",",frame,),:,self,.,log,.,info,(,"""Exiting gracefully upon receiving signal %s""",",",signum,),if,self,.,processor_agent,:,self,.,processor_agent,.,end,(,),sys,.,exit,(,os,.,EX_OK,),,,,,,,,,,,,,Helper method to clean up processor_agent to avoid leaving orphan processes.,Helper,method,to,clean,up,processor_agent,to,avoid,leaving,orphan,processes,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L599-L606,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob.update_import_errors,"def update_import_errors(session, dagbag):
        """"""
        For the DAGs in the given DagBag, record any associated import errors and clears
        errors for files that no longer have them. These are usually displayed through the
        Airflow UI so that users know that there are issues parsing DAGs.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session
        :param dagbag: DagBag containing DAGs with import errors
        :type dagbag: airflow.models.DagBag
        """"""
        # Clear the errors of the processed files
        for dagbag_file in dagbag.file_last_changed:
            session.query(errors.ImportError).filter(
                errors.ImportError.filename == dagbag_file
            ).delete()

        # Add the errors of the processed files
        for filename, stacktrace in six.iteritems(dagbag.import_errors):
            session.add(errors.ImportError(
                filename=filename,
                stacktrace=stacktrace))
        session.commit()",python,"def update_import_errors(session, dagbag):
        """"""
        For the DAGs in the given DagBag, record any associated import errors and clears
        errors for files that no longer have them. These are usually displayed through the
        Airflow UI so that users know that there are issues parsing DAGs.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session
        :param dagbag: DagBag containing DAGs with import errors
        :type dagbag: airflow.models.DagBag
        """"""
        # Clear the errors of the processed files
        for dagbag_file in dagbag.file_last_changed:
            session.query(errors.ImportError).filter(
                errors.ImportError.filename == dagbag_file
            ).delete()

        # Add the errors of the processed files
        for filename, stacktrace in six.iteritems(dagbag.import_errors):
            session.add(errors.ImportError(
                filename=filename,
                stacktrace=stacktrace))
        session.commit()",def,update_import_errors,(,session,",",dagbag,),:,# Clear the errors of the processed files,for,dagbag_file,in,dagbag,.,file_last_changed,:,session,.,query,(,errors,.,ImportError,),.,filter,(,errors,.,ImportError,.,filename,==,dagbag_file,),.,delete,(,),# Add the errors of the processed files,for,filename,",",stacktrace,in,six,.,iteritems,(,dagbag,.,import_errors,"For the DAGs in the given DagBag, record any associated import errors and clears
        errors for files that no longer have them. These are usually displayed through the
        Airflow UI so that users know that there are issues parsing DAGs.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session
        :param dagbag: DagBag containing DAGs with import errors
        :type dagbag: airflow.models.DagBag",For,the,DAGs,in,the,given,DagBag,record,any,associated,import,errors,and,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L741-L763,test,),:,session,.,add,(,errors,.,ImportError,(,filename,=,filename,",",stacktrace,=,stacktrace,),),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,clears,errors,for,files,that,no,longer,have,them,.,These,are,usually,displayed,through,the,Airflow,UI,so,that,users,know,that,there,are,issues,parsing,DAGs,.,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._process_task_instances,"def _process_task_instances(self, dag, queue, session=None):
        """"""
        This method schedules the tasks for a single DAG by looking at the
        active DAG runs and adding task instances that should run to the
        queue.
        """"""

        # update the state of the previously active dag runs
        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)
        active_dag_runs = []
        for run in dag_runs:
            self.log.info(""Examining DAG run %s"", run)
            # don't consider runs that are executed in the future
            if run.execution_date > timezone.utcnow():
                self.log.error(
                    ""Execution date is in future: %s"",
                    run.execution_date
                )
                continue

            if len(active_dag_runs) >= dag.max_active_runs:
                self.log.info(""Number of active dag runs reached max_active_run."")
                break

            # skip backfill dagruns for now as long as they are not really scheduled
            if run.is_backfill:
                continue

            # todo: run.dag is transient but needs to be set
            run.dag = dag
            # todo: preferably the integrity check happens at dag collection time
            run.verify_integrity(session=session)
            run.update_state(session=session)
            if run.state == State.RUNNING:
                make_transient(run)
                active_dag_runs.append(run)

        for run in active_dag_runs:
            self.log.debug(""Examining active DAG run: %s"", run)
            # this needs a fresh session sometimes tis get detached
            tis = run.get_task_instances(state=(State.NONE,
                                                State.UP_FOR_RETRY,
                                                State.UP_FOR_RESCHEDULE))

            # this loop is quite slow as it uses are_dependencies_met for
            # every task (in ti.is_runnable). This is also called in
            # update_state above which has already checked these tasks
            for ti in tis:
                task = dag.get_task(ti.task_id)

                # fixme: ti.task is transient but needs to be set
                ti.task = task

                if ti.are_dependencies_met(
                        dep_context=DepContext(flag_upstream_failed=True),
                        session=session):
                    self.log.debug('Queuing task: %s', ti)
                    queue.append(ti.key)",python,"def _process_task_instances(self, dag, queue, session=None):
        """"""
        This method schedules the tasks for a single DAG by looking at the
        active DAG runs and adding task instances that should run to the
        queue.
        """"""

        # update the state of the previously active dag runs
        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)
        active_dag_runs = []
        for run in dag_runs:
            self.log.info(""Examining DAG run %s"", run)
            # don't consider runs that are executed in the future
            if run.execution_date > timezone.utcnow():
                self.log.error(
                    ""Execution date is in future: %s"",
                    run.execution_date
                )
                continue

            if len(active_dag_runs) >= dag.max_active_runs:
                self.log.info(""Number of active dag runs reached max_active_run."")
                break

            # skip backfill dagruns for now as long as they are not really scheduled
            if run.is_backfill:
                continue

            # todo: run.dag is transient but needs to be set
            run.dag = dag
            # todo: preferably the integrity check happens at dag collection time
            run.verify_integrity(session=session)
            run.update_state(session=session)
            if run.state == State.RUNNING:
                make_transient(run)
                active_dag_runs.append(run)

        for run in active_dag_runs:
            self.log.debug(""Examining active DAG run: %s"", run)
            # this needs a fresh session sometimes tis get detached
            tis = run.get_task_instances(state=(State.NONE,
                                                State.UP_FOR_RETRY,
                                                State.UP_FOR_RESCHEDULE))

            # this loop is quite slow as it uses are_dependencies_met for
            # every task (in ti.is_runnable). This is also called in
            # update_state above which has already checked these tasks
            for ti in tis:
                task = dag.get_task(ti.task_id)

                # fixme: ti.task is transient but needs to be set
                ti.task = task

                if ti.are_dependencies_met(
                        dep_context=DepContext(flag_upstream_failed=True),
                        session=session):
                    self.log.debug('Queuing task: %s', ti)
                    queue.append(ti.key)",def,_process_task_instances,(,self,",",dag,",",queue,",",session,=,None,),:,# update the state of the previously active dag runs,dag_runs,=,DagRun,.,find,(,dag_id,=,dag,.,dag_id,",",state,=,State,.,RUNNING,",",session,=,session,),active_dag_runs,=,[,],for,run,in,dag_runs,:,self,.,log,.,info,(,"This method schedules the tasks for a single DAG by looking at the
        active DAG runs and adding task instances that should run to the
        queue.",This,method,schedules,the,tasks,for,a,single,DAG,by,looking,at,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L897-L954,test,"""Examining DAG run %s""",",",run,),# don't consider runs that are executed in the future,if,run,.,execution_date,>,timezone,.,utcnow,(,),:,self,.,log,.,error,(,"""Execution date is in future: %s""",",",run,.,execution_date,),continue,if,len,(,active_dag_runs,),>=,dag,.,max_active_runs,:,self,.,log,.,info,(,"""Number of active dag runs reached max_active_run.""",),break,# skip backfill dagruns for now as long as they are not really scheduled,if,run,.,is_backfill,:,continue,# todo: run.dag is transient but needs to be set,run,.,dag,=,dag,# todo: preferably the integrity check happens at dag collection time,run,.,verify_integrity,(,session,=,session,),run,.,update_state,(,session,=,session,),if,run,.,state,==,State,.,RUNNING,:,make_transient,(,run,),active_dag_runs,.,append,(,run,),for,run,in,active_dag_runs,:,self,.,log,.,debug,(,"""Examining active DAG run: %s""",",",run,),# this needs a fresh session sometimes tis get detached,tis,=,run,.,get_task_instances,(,state,=,(,State,.,NONE,",",State,.,UP_FOR_RETRY,",",State,.,UP_FOR_RESCHEDULE,),),# this loop is quite slow as it uses are_dependencies_met for,# every task (in ti.is_runnable). This is also called in,# update_state above which has already checked these tasks,for,ti,in,tis,:,task,=,dag,.,get_task,(,ti,.,task_id,),# fixme: ti.task is transient but needs to be set,ti,.,task,=,task,if,ti,.,are_dependencies_met,(,dep_context,=,DepContext,(,flag_upstream_failed,=,True,),",",session,=,session,),:,self,.,log,.,debug,(,'Queuing task: %s',",",ti,),queue,.,append,(,ti,.,key,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,active,DAG,runs,and,adding,task,instances,that,should,run,to,the,queue,.,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._change_state_for_tis_without_dagrun,"def _change_state_for_tis_without_dagrun(self,
                                             simple_dag_bag,
                                             old_states,
                                             new_state,
                                             session=None):
        """"""
        For all DAG IDs in the SimpleDagBag, look for task instances in the
        old_states and set them to new_state if the corresponding DagRun
        does not exist or exists but is not in the running state. This
        normally should not happen, but it can if the state of DagRuns are
        changed manually.

        :param old_states: examine TaskInstances in this state
        :type old_state: list[airflow.utils.state.State]
        :param new_state: set TaskInstances to this state
        :type new_state: airflow.utils.state.State
        :param simple_dag_bag: TaskInstances associated with DAGs in the
            simple_dag_bag and with states in the old_state will be examined
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        """"""
        tis_changed = 0
        query = session \
            .query(models.TaskInstance) \
            .outerjoin(models.DagRun, and_(
                models.TaskInstance.dag_id == models.DagRun.dag_id,
                models.TaskInstance.execution_date == models.DagRun.execution_date)) \
            .filter(models.TaskInstance.dag_id.in_(simple_dag_bag.dag_ids)) \
            .filter(models.TaskInstance.state.in_(old_states)) \
            .filter(or_(
                models.DagRun.state != State.RUNNING,
                models.DagRun.state.is_(None)))
        if self.using_sqlite:
            tis_to_change = query \
                .with_for_update() \
                .all()
            for ti in tis_to_change:
                ti.set_state(new_state, session=session)
                tis_changed += 1
        else:
            subq = query.subquery()
            tis_changed = session \
                .query(models.TaskInstance) \
                .filter(and_(
                    models.TaskInstance.dag_id == subq.c.dag_id,
                    models.TaskInstance.task_id == subq.c.task_id,
                    models.TaskInstance.execution_date ==
                    subq.c.execution_date)) \
                .update({models.TaskInstance.state: new_state},
                        synchronize_session=False)
            session.commit()

        if tis_changed > 0:
            self.log.warning(
                ""Set %s task instances to state=%s as their associated DagRun was not in RUNNING state"",
                tis_changed, new_state
            )",python,"def _change_state_for_tis_without_dagrun(self,
                                             simple_dag_bag,
                                             old_states,
                                             new_state,
                                             session=None):
        """"""
        For all DAG IDs in the SimpleDagBag, look for task instances in the
        old_states and set them to new_state if the corresponding DagRun
        does not exist or exists but is not in the running state. This
        normally should not happen, but it can if the state of DagRuns are
        changed manually.

        :param old_states: examine TaskInstances in this state
        :type old_state: list[airflow.utils.state.State]
        :param new_state: set TaskInstances to this state
        :type new_state: airflow.utils.state.State
        :param simple_dag_bag: TaskInstances associated with DAGs in the
            simple_dag_bag and with states in the old_state will be examined
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        """"""
        tis_changed = 0
        query = session \
            .query(models.TaskInstance) \
            .outerjoin(models.DagRun, and_(
                models.TaskInstance.dag_id == models.DagRun.dag_id,
                models.TaskInstance.execution_date == models.DagRun.execution_date)) \
            .filter(models.TaskInstance.dag_id.in_(simple_dag_bag.dag_ids)) \
            .filter(models.TaskInstance.state.in_(old_states)) \
            .filter(or_(
                models.DagRun.state != State.RUNNING,
                models.DagRun.state.is_(None)))
        if self.using_sqlite:
            tis_to_change = query \
                .with_for_update() \
                .all()
            for ti in tis_to_change:
                ti.set_state(new_state, session=session)
                tis_changed += 1
        else:
            subq = query.subquery()
            tis_changed = session \
                .query(models.TaskInstance) \
                .filter(and_(
                    models.TaskInstance.dag_id == subq.c.dag_id,
                    models.TaskInstance.task_id == subq.c.task_id,
                    models.TaskInstance.execution_date ==
                    subq.c.execution_date)) \
                .update({models.TaskInstance.state: new_state},
                        synchronize_session=False)
            session.commit()

        if tis_changed > 0:
            self.log.warning(
                ""Set %s task instances to state=%s as their associated DagRun was not in RUNNING state"",
                tis_changed, new_state
            )",def,_change_state_for_tis_without_dagrun,(,self,",",simple_dag_bag,",",old_states,",",new_state,",",session,=,None,),:,tis_changed,=,0,query,=,session,.,query,(,models,.,TaskInstance,),.,outerjoin,(,models,.,DagRun,",",and_,(,models,.,TaskInstance,.,dag_id,==,models,.,DagRun,.,dag_id,",",models,.,"For all DAG IDs in the SimpleDagBag, look for task instances in the
        old_states and set them to new_state if the corresponding DagRun
        does not exist or exists but is not in the running state. This
        normally should not happen, but it can if the state of DagRuns are
        changed manually.

        :param old_states: examine TaskInstances in this state
        :type old_state: list[airflow.utils.state.State]
        :param new_state: set TaskInstances to this state
        :type new_state: airflow.utils.state.State
        :param simple_dag_bag: TaskInstances associated with DAGs in the
            simple_dag_bag and with states in the old_state will be examined
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag",For,all,DAG,IDs,in,the,SimpleDagBag,look,for,task,instances,in,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L957-L1012,test,TaskInstance,.,execution_date,==,models,.,DagRun,.,execution_date,),),.,filter,(,models,.,TaskInstance,.,dag_id,.,in_,(,simple_dag_bag,.,dag_ids,),),.,filter,(,models,.,TaskInstance,.,state,.,in_,(,old_states,),),.,filter,(,or_,(,models,.,DagRun,.,state,!=,State,.,RUNNING,",",models,.,DagRun,.,state,.,is_,(,None,),),),if,self,.,using_sqlite,:,tis_to_change,=,query,.,with_for_update,(,),.,all,(,),for,ti,in,tis_to_change,:,ti,.,set_state,(,new_state,",",session,=,session,),tis_changed,+=,1,else,:,subq,=,query,.,subquery,(,),tis_changed,=,session,.,query,(,models,.,TaskInstance,),.,filter,(,and_,(,models,.,TaskInstance,.,dag_id,==,subq,.,c,.,dag_id,",",models,.,TaskInstance,.,task_id,==,subq,.,c,.,task_id,",",models,.,TaskInstance,.,execution_date,==,subq,.,c,.,execution_date,),),.,update,(,{,models,.,TaskInstance,.,state,:,new_state,},",",synchronize_session,=,False,),session,.,commit,(,),if,tis_changed,>,0,:,self,.,log,.,warning,(,"""Set %s task instances to state=%s as their associated DagRun was not in RUNNING state""",",",tis_changed,",",new_state,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,old_states,and,set,them,to,new_state,if,the,corresponding,DagRun,does,not,exist,or,exists,but,is,not,in,the,running,state,.,This,normally,should,not,happen,but,it,can,if,the,state,of,DagRuns,are,changed,manually,.
apache/airflow,airflow/jobs.py,SchedulerJob.__get_concurrency_maps,"def __get_concurrency_maps(self, states, session=None):
        """"""
        Get the concurrency maps.

        :param states: List of states to query for
        :type states: list[airflow.utils.state.State]
        :return: A map from (dag_id, task_id) to # of task instances and
         a map from (dag_id, task_id) to # of task instances in the given state list
        :rtype: dict[tuple[str, str], int]

        """"""
        TI = models.TaskInstance
        ti_concurrency_query = (
            session
            .query(TI.task_id, TI.dag_id, func.count('*'))
            .filter(TI.state.in_(states))
            .group_by(TI.task_id, TI.dag_id)
        ).all()
        dag_map = defaultdict(int)
        task_map = defaultdict(int)
        for result in ti_concurrency_query:
            task_id, dag_id, count = result
            dag_map[dag_id] += count
            task_map[(dag_id, task_id)] = count
        return dag_map, task_map",python,"def __get_concurrency_maps(self, states, session=None):
        """"""
        Get the concurrency maps.

        :param states: List of states to query for
        :type states: list[airflow.utils.state.State]
        :return: A map from (dag_id, task_id) to # of task instances and
         a map from (dag_id, task_id) to # of task instances in the given state list
        :rtype: dict[tuple[str, str], int]

        """"""
        TI = models.TaskInstance
        ti_concurrency_query = (
            session
            .query(TI.task_id, TI.dag_id, func.count('*'))
            .filter(TI.state.in_(states))
            .group_by(TI.task_id, TI.dag_id)
        ).all()
        dag_map = defaultdict(int)
        task_map = defaultdict(int)
        for result in ti_concurrency_query:
            task_id, dag_id, count = result
            dag_map[dag_id] += count
            task_map[(dag_id, task_id)] = count
        return dag_map, task_map",def,__get_concurrency_maps,(,self,",",states,",",session,=,None,),:,TI,=,models,.,TaskInstance,ti_concurrency_query,=,(,session,.,query,(,TI,.,task_id,",",TI,.,dag_id,",",func,.,count,(,'*',),),.,filter,(,TI,.,state,.,in_,(,states,),),.,"Get the concurrency maps.

        :param states: List of states to query for
        :type states: list[airflow.utils.state.State]
        :return: A map from (dag_id, task_id) to # of task instances and
         a map from (dag_id, task_id) to # of task instances in the given state list
        :rtype: dict[tuple[str, str], int]",Get,the,concurrency,maps,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1015-L1039,test,group_by,(,TI,.,task_id,",",TI,.,dag_id,),),.,all,(,),dag_map,=,defaultdict,(,int,),task_map,=,defaultdict,(,int,),for,result,in,ti_concurrency_query,:,task_id,",",dag_id,",",count,=,result,dag_map,[,dag_id,],+=,count,task_map,[,(,dag_id,",",task_id,),],=,count,return,dag_map,",",task_map,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._change_state_for_executable_task_instances,"def _change_state_for_executable_task_instances(self, task_instances,
                                                    acceptable_states, session=None):
        """"""
        Changes the state of task instances in the list with one of the given states
        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.

        :param task_instances: TaskInstances to change the state of
        :type task_instances: list[airflow.models.TaskInstance]
        :param acceptable_states: Filters the TaskInstances updated to be in these states
        :type acceptable_states: Iterable[State]
        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]
        """"""
        if len(task_instances) == 0:
            session.commit()
            return []

        TI = models.TaskInstance
        filter_for_ti_state_change = (
            [and_(
                TI.dag_id == ti.dag_id,
                TI.task_id == ti.task_id,
                TI.execution_date == ti.execution_date)
                for ti in task_instances])
        ti_query = (
            session
            .query(TI)
            .filter(or_(*filter_for_ti_state_change)))

        if None in acceptable_states:
            ti_query = ti_query.filter(
                or_(TI.state == None, TI.state.in_(acceptable_states))  # noqa: E711
            )
        else:
            ti_query = ti_query.filter(TI.state.in_(acceptable_states))

        tis_to_set_to_queued = (
            ti_query
            .with_for_update()
            .all())
        if len(tis_to_set_to_queued) == 0:
            self.log.info(""No tasks were able to have their state changed to queued."")
            session.commit()
            return []

        # set TIs to queued state
        for task_instance in tis_to_set_to_queued:
            task_instance.state = State.QUEUED
            task_instance.queued_dttm = (timezone.utcnow()
                                         if not task_instance.queued_dttm
                                         else task_instance.queued_dttm)
            session.merge(task_instance)

        # Generate a list of SimpleTaskInstance for the use of queuing
        # them in the executor.
        simple_task_instances = [SimpleTaskInstance(ti) for ti in
                                 tis_to_set_to_queued]

        task_instance_str = ""\n\t"".join(
            [repr(x) for x in tis_to_set_to_queued])

        session.commit()
        self.log.info(""Setting the following %s tasks to queued state:\n\t%s"",
                      len(tis_to_set_to_queued), task_instance_str)
        return simple_task_instances",python,"def _change_state_for_executable_task_instances(self, task_instances,
                                                    acceptable_states, session=None):
        """"""
        Changes the state of task instances in the list with one of the given states
        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.

        :param task_instances: TaskInstances to change the state of
        :type task_instances: list[airflow.models.TaskInstance]
        :param acceptable_states: Filters the TaskInstances updated to be in these states
        :type acceptable_states: Iterable[State]
        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]
        """"""
        if len(task_instances) == 0:
            session.commit()
            return []

        TI = models.TaskInstance
        filter_for_ti_state_change = (
            [and_(
                TI.dag_id == ti.dag_id,
                TI.task_id == ti.task_id,
                TI.execution_date == ti.execution_date)
                for ti in task_instances])
        ti_query = (
            session
            .query(TI)
            .filter(or_(*filter_for_ti_state_change)))

        if None in acceptable_states:
            ti_query = ti_query.filter(
                or_(TI.state == None, TI.state.in_(acceptable_states))  # noqa: E711
            )
        else:
            ti_query = ti_query.filter(TI.state.in_(acceptable_states))

        tis_to_set_to_queued = (
            ti_query
            .with_for_update()
            .all())
        if len(tis_to_set_to_queued) == 0:
            self.log.info(""No tasks were able to have their state changed to queued."")
            session.commit()
            return []

        # set TIs to queued state
        for task_instance in tis_to_set_to_queued:
            task_instance.state = State.QUEUED
            task_instance.queued_dttm = (timezone.utcnow()
                                         if not task_instance.queued_dttm
                                         else task_instance.queued_dttm)
            session.merge(task_instance)

        # Generate a list of SimpleTaskInstance for the use of queuing
        # them in the executor.
        simple_task_instances = [SimpleTaskInstance(ti) for ti in
                                 tis_to_set_to_queued]

        task_instance_str = ""\n\t"".join(
            [repr(x) for x in tis_to_set_to_queued])

        session.commit()
        self.log.info(""Setting the following %s tasks to queued state:\n\t%s"",
                      len(tis_to_set_to_queued), task_instance_str)
        return simple_task_instances",def,_change_state_for_executable_task_instances,(,self,",",task_instances,",",acceptable_states,",",session,=,None,),:,if,len,(,task_instances,),==,0,:,session,.,commit,(,),return,[,],TI,=,models,.,TaskInstance,filter_for_ti_state_change,=,(,[,and_,(,TI,.,dag_id,==,ti,.,dag_id,",",TI,.,task_id,"Changes the state of task instances in the list with one of the given states
        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.

        :param task_instances: TaskInstances to change the state of
        :type task_instances: list[airflow.models.TaskInstance]
        :param acceptable_states: Filters the TaskInstances updated to be in these states
        :type acceptable_states: Iterable[State]
        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]",Changes,the,state,of,task,instances,in,the,list,with,one,of,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1217-L1280,test,==,ti,.,task_id,",",TI,.,execution_date,==,ti,.,execution_date,),for,ti,in,task_instances,],),ti_query,=,(,session,.,query,(,TI,),.,filter,(,or_,(,*,filter_for_ti_state_change,),),),if,None,in,acceptable_states,:,ti_query,=,ti_query,.,filter,(,or_,(,TI,.,state,==,None,",",TI,.,state,.,in_,(,acceptable_states,),),# noqa: E711,),else,:,ti_query,=,ti_query,.,filter,(,TI,.,state,.,in_,(,acceptable_states,),),tis_to_set_to_queued,=,(,ti_query,.,with_for_update,(,),.,all,(,),),if,len,(,tis_to_set_to_queued,),==,0,:,self,.,log,.,info,(,"""No tasks were able to have their state changed to queued.""",),session,.,commit,(,),return,[,],# set TIs to queued state,for,task_instance,in,tis_to_set_to_queued,:,task_instance,.,state,=,State,.,QUEUED,task_instance,.,queued_dttm,=,(,timezone,.,utcnow,(,),if,not,task_instance,.,queued_dttm,else,task_instance,.,queued_dttm,),session,.,merge,(,task_instance,),# Generate a list of SimpleTaskInstance for the use of queuing,# them in the executor.,simple_task_instances,=,[,SimpleTaskInstance,(,ti,),for,ti,in,tis_to_set_to_queued,],task_instance_str,=,"""\n\t""",.,join,(,[,repr,(,x,),for,x,in,tis_to_set_to_queued,],),session,.,commit,(,),self,.,log,.,info,(,"""Setting the following %s tasks to queued state:\n\t%s""",",",len,(,tis_to_set_to_queued,),",",task_instance_str,),return,simple_task_instances,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,given,states,to,QUEUED,atomically,and,returns,the,TIs,changed,in,SimpleTaskInstance,format,.,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._enqueue_task_instances_with_queued_state,"def _enqueue_task_instances_with_queued_state(self, simple_dag_bag,
                                                  simple_task_instances):
        """"""
        Takes task_instances, which should have been set to queued, and enqueues them
        with the executor.

        :param simple_task_instances: TaskInstances to enqueue
        :type simple_task_instances: list[SimpleTaskInstance]
        :param simple_dag_bag: Should contains all of the task_instances' dags
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        """"""
        TI = models.TaskInstance
        # actually enqueue them
        for simple_task_instance in simple_task_instances:
            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)
            command = TI.generate_command(
                simple_task_instance.dag_id,
                simple_task_instance.task_id,
                simple_task_instance.execution_date,
                local=True,
                mark_success=False,
                ignore_all_deps=False,
                ignore_depends_on_past=False,
                ignore_task_deps=False,
                ignore_ti_state=False,
                pool=simple_task_instance.pool,
                file_path=simple_dag.full_filepath,
                pickle_id=simple_dag.pickle_id)

            priority = simple_task_instance.priority_weight
            queue = simple_task_instance.queue
            self.log.info(
                ""Sending %s to executor with priority %s and queue %s"",
                simple_task_instance.key, priority, queue
            )

            self.executor.queue_command(
                simple_task_instance,
                command,
                priority=priority,
                queue=queue)",python,"def _enqueue_task_instances_with_queued_state(self, simple_dag_bag,
                                                  simple_task_instances):
        """"""
        Takes task_instances, which should have been set to queued, and enqueues them
        with the executor.

        :param simple_task_instances: TaskInstances to enqueue
        :type simple_task_instances: list[SimpleTaskInstance]
        :param simple_dag_bag: Should contains all of the task_instances' dags
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        """"""
        TI = models.TaskInstance
        # actually enqueue them
        for simple_task_instance in simple_task_instances:
            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)
            command = TI.generate_command(
                simple_task_instance.dag_id,
                simple_task_instance.task_id,
                simple_task_instance.execution_date,
                local=True,
                mark_success=False,
                ignore_all_deps=False,
                ignore_depends_on_past=False,
                ignore_task_deps=False,
                ignore_ti_state=False,
                pool=simple_task_instance.pool,
                file_path=simple_dag.full_filepath,
                pickle_id=simple_dag.pickle_id)

            priority = simple_task_instance.priority_weight
            queue = simple_task_instance.queue
            self.log.info(
                ""Sending %s to executor with priority %s and queue %s"",
                simple_task_instance.key, priority, queue
            )

            self.executor.queue_command(
                simple_task_instance,
                command,
                priority=priority,
                queue=queue)",def,_enqueue_task_instances_with_queued_state,(,self,",",simple_dag_bag,",",simple_task_instances,),:,TI,=,models,.,TaskInstance,# actually enqueue them,for,simple_task_instance,in,simple_task_instances,:,simple_dag,=,simple_dag_bag,.,get_dag,(,simple_task_instance,.,dag_id,),command,=,TI,.,generate_command,(,simple_task_instance,.,dag_id,",",simple_task_instance,.,task_id,",",simple_task_instance,.,execution_date,",",local,=,True,"Takes task_instances, which should have been set to queued, and enqueues them
        with the executor.

        :param simple_task_instances: TaskInstances to enqueue
        :type simple_task_instances: list[SimpleTaskInstance]
        :param simple_dag_bag: Should contains all of the task_instances' dags
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag",Takes,task_instances,which,should,have,been,set,to,queued,and,enqueues,them,with,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1282-L1322,test,",",mark_success,=,False,",",ignore_all_deps,=,False,",",ignore_depends_on_past,=,False,",",ignore_task_deps,=,False,",",ignore_ti_state,=,False,",",pool,=,simple_task_instance,.,pool,",",file_path,=,simple_dag,.,full_filepath,",",pickle_id,=,simple_dag,.,pickle_id,),priority,=,simple_task_instance,.,priority_weight,queue,=,simple_task_instance,.,queue,self,.,log,.,info,(,"""Sending %s to executor with priority %s and queue %s""",",",simple_task_instance,.,key,",",priority,",",queue,),self,.,executor,.,queue_command,(,simple_task_instance,",",command,",",priority,=,priority,",",queue,=,queue,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,executor,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._execute_task_instances,"def _execute_task_instances(self,
                                simple_dag_bag,
                                states,
                                session=None):
        """"""
        Attempts to execute TaskInstances that should be executed by the scheduler.

        There are three steps:
        1. Pick TIs by priority with the constraint that they are in the expected states
        and that we do exceed max_active_runs or pool limits.
        2. Change the state for the TIs above atomically.
        3. Enqueue the TIs in the executor.

        :param simple_dag_bag: TaskInstances associated with DAGs in the
            simple_dag_bag will be fetched from the DB and executed
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        :param states: Execute TaskInstances in these states
        :type states: tuple[airflow.utils.state.State]
        :return: Number of task instance with state changed.
        """"""
        executable_tis = self._find_executable_task_instances(simple_dag_bag, states,
                                                              session=session)

        def query(result, items):
            simple_tis_with_state_changed = \
                self._change_state_for_executable_task_instances(items,
                                                                 states,
                                                                 session=session)
            self._enqueue_task_instances_with_queued_state(
                simple_dag_bag,
                simple_tis_with_state_changed)
            session.commit()
            return result + len(simple_tis_with_state_changed)

        return helpers.reduce_in_chunks(query, executable_tis, 0, self.max_tis_per_query)",python,"def _execute_task_instances(self,
                                simple_dag_bag,
                                states,
                                session=None):
        """"""
        Attempts to execute TaskInstances that should be executed by the scheduler.

        There are three steps:
        1. Pick TIs by priority with the constraint that they are in the expected states
        and that we do exceed max_active_runs or pool limits.
        2. Change the state for the TIs above atomically.
        3. Enqueue the TIs in the executor.

        :param simple_dag_bag: TaskInstances associated with DAGs in the
            simple_dag_bag will be fetched from the DB and executed
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        :param states: Execute TaskInstances in these states
        :type states: tuple[airflow.utils.state.State]
        :return: Number of task instance with state changed.
        """"""
        executable_tis = self._find_executable_task_instances(simple_dag_bag, states,
                                                              session=session)

        def query(result, items):
            simple_tis_with_state_changed = \
                self._change_state_for_executable_task_instances(items,
                                                                 states,
                                                                 session=session)
            self._enqueue_task_instances_with_queued_state(
                simple_dag_bag,
                simple_tis_with_state_changed)
            session.commit()
            return result + len(simple_tis_with_state_changed)

        return helpers.reduce_in_chunks(query, executable_tis, 0, self.max_tis_per_query)",def,_execute_task_instances,(,self,",",simple_dag_bag,",",states,",",session,=,None,),:,executable_tis,=,self,.,_find_executable_task_instances,(,simple_dag_bag,",",states,",",session,=,session,),def,query,(,result,",",items,),:,simple_tis_with_state_changed,=,self,.,_change_state_for_executable_task_instances,(,items,",",states,",",session,=,session,),self,.,"Attempts to execute TaskInstances that should be executed by the scheduler.

        There are three steps:
        1. Pick TIs by priority with the constraint that they are in the expected states
        and that we do exceed max_active_runs or pool limits.
        2. Change the state for the TIs above atomically.
        3. Enqueue the TIs in the executor.

        :param simple_dag_bag: TaskInstances associated with DAGs in the
            simple_dag_bag will be fetched from the DB and executed
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        :param states: Execute TaskInstances in these states
        :type states: tuple[airflow.utils.state.State]
        :return: Number of task instance with state changed.",Attempts,to,execute,TaskInstances,that,should,be,executed,by,the,scheduler,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1325-L1359,test,_enqueue_task_instances_with_queued_state,(,simple_dag_bag,",",simple_tis_with_state_changed,),session,.,commit,(,),return,result,+,len,(,simple_tis_with_state_changed,),return,helpers,.,reduce_in_chunks,(,query,",",executable_tis,",",0,",",self,.,max_tis_per_query,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._change_state_for_tasks_failed_to_execute,"def _change_state_for_tasks_failed_to_execute(self, session):
        """"""
        If there are tasks left over in the executor,
        we set them back to SCHEDULED to avoid creating hanging tasks.

        :param session: session for ORM operations
        """"""
        if self.executor.queued_tasks:
            TI = models.TaskInstance
            filter_for_ti_state_change = (
                [and_(
                    TI.dag_id == dag_id,
                    TI.task_id == task_id,
                    TI.execution_date == execution_date,
                    # The TI.try_number will return raw try_number+1 since the
                    # ti is not running. And we need to -1 to match the DB record.
                    TI._try_number == try_number - 1,
                    TI.state == State.QUEUED)
                    for dag_id, task_id, execution_date, try_number
                    in self.executor.queued_tasks.keys()])
            ti_query = (session.query(TI)
                        .filter(or_(*filter_for_ti_state_change)))
            tis_to_set_to_scheduled = (ti_query
                                       .with_for_update()
                                       .all())
            if len(tis_to_set_to_scheduled) == 0:
                session.commit()
                return

            # set TIs to queued state
            for task_instance in tis_to_set_to_scheduled:
                task_instance.state = State.SCHEDULED

            task_instance_str = ""\n\t"".join(
                [repr(x) for x in tis_to_set_to_scheduled])

            session.commit()
            self.log.info(""Set the following tasks to scheduled state:\n\t%s"", task_instance_str)",python,"def _change_state_for_tasks_failed_to_execute(self, session):
        """"""
        If there are tasks left over in the executor,
        we set them back to SCHEDULED to avoid creating hanging tasks.

        :param session: session for ORM operations
        """"""
        if self.executor.queued_tasks:
            TI = models.TaskInstance
            filter_for_ti_state_change = (
                [and_(
                    TI.dag_id == dag_id,
                    TI.task_id == task_id,
                    TI.execution_date == execution_date,
                    # The TI.try_number will return raw try_number+1 since the
                    # ti is not running. And we need to -1 to match the DB record.
                    TI._try_number == try_number - 1,
                    TI.state == State.QUEUED)
                    for dag_id, task_id, execution_date, try_number
                    in self.executor.queued_tasks.keys()])
            ti_query = (session.query(TI)
                        .filter(or_(*filter_for_ti_state_change)))
            tis_to_set_to_scheduled = (ti_query
                                       .with_for_update()
                                       .all())
            if len(tis_to_set_to_scheduled) == 0:
                session.commit()
                return

            # set TIs to queued state
            for task_instance in tis_to_set_to_scheduled:
                task_instance.state = State.SCHEDULED

            task_instance_str = ""\n\t"".join(
                [repr(x) for x in tis_to_set_to_scheduled])

            session.commit()
            self.log.info(""Set the following tasks to scheduled state:\n\t%s"", task_instance_str)",def,_change_state_for_tasks_failed_to_execute,(,self,",",session,),:,if,self,.,executor,.,queued_tasks,:,TI,=,models,.,TaskInstance,filter_for_ti_state_change,=,(,[,and_,(,TI,.,dag_id,==,dag_id,",",TI,.,task_id,==,task_id,",",TI,.,execution_date,==,execution_date,",",# The TI.try_number will return raw try_number+1 since the,# ti is not running. And we need to -1 to match the DB record.,TI,.,_try_number,==,try_number,-,"If there are tasks left over in the executor,
        we set them back to SCHEDULED to avoid creating hanging tasks.

        :param session: session for ORM operations",If,there,are,tasks,left,over,in,the,executor,we,set,them,back,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1362-L1399,test,1,",",TI,.,state,==,State,.,QUEUED,),for,dag_id,",",task_id,",",execution_date,",",try_number,in,self,.,executor,.,queued_tasks,.,keys,(,),],),ti_query,=,(,session,.,query,(,TI,),.,filter,(,or_,(,*,filter_for_ti_state_change,),),),tis_to_set_to_scheduled,=,(,ti_query,.,with_for_update,(,),.,all,(,),),if,len,(,tis_to_set_to_scheduled,),==,0,:,session,.,commit,(,),return,# set TIs to queued state,for,task_instance,in,tis_to_set_to_scheduled,:,task_instance,.,state,=,State,.,SCHEDULED,task_instance_str,=,"""\n\t""",.,join,(,[,repr,(,x,),for,x,in,tis_to_set_to_scheduled,],),session,.,commit,(,),self,.,log,.,info,(,"""Set the following tasks to scheduled state:\n\t%s""",",",task_instance_str,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,SCHEDULED,to,avoid,creating,hanging,tasks,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob._process_executor_events,"def _process_executor_events(self, simple_dag_bag, session=None):
        """"""
        Respond to executor events.
        """"""
        # TODO: this shares quite a lot of code with _manage_executor_state

        TI = models.TaskInstance
        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)
                                   .items()):
            dag_id, task_id, execution_date, try_number = key
            self.log.info(
                ""Executor reports execution of %s.%s execution_date=%s ""
                ""exited with status %s for try_number %s"",
                dag_id, task_id, execution_date, state, try_number
            )
            if state == State.FAILED or state == State.SUCCESS:
                qry = session.query(TI).filter(TI.dag_id == dag_id,
                                               TI.task_id == task_id,
                                               TI.execution_date == execution_date)
                ti = qry.first()
                if not ti:
                    self.log.warning(""TaskInstance %s went missing from the database"", ti)
                    continue

                # TODO: should we fail RUNNING as well, as we do in Backfills?
                if ti.try_number == try_number and ti.state == State.QUEUED:
                    msg = (""Executor reports task instance {} finished ({}) ""
                           ""although the task says its {}. Was the task ""
                           ""killed externally?"".format(ti, state, ti.state))
                    self.log.error(msg)
                    try:
                        simple_dag = simple_dag_bag.get_dag(dag_id)
                        dagbag = models.DagBag(simple_dag.full_filepath)
                        dag = dagbag.get_dag(dag_id)
                        ti.task = dag.get_task(task_id)
                        ti.handle_failure(msg)
                    except Exception:
                        self.log.error(""Cannot load the dag bag to handle failure for %s""
                                       "". Setting task to FAILED without callbacks or ""
                                       ""retries. Do you have enough resources?"", ti)
                        ti.state = State.FAILED
                        session.merge(ti)
                        session.commit()",python,"def _process_executor_events(self, simple_dag_bag, session=None):
        """"""
        Respond to executor events.
        """"""
        # TODO: this shares quite a lot of code with _manage_executor_state

        TI = models.TaskInstance
        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)
                                   .items()):
            dag_id, task_id, execution_date, try_number = key
            self.log.info(
                ""Executor reports execution of %s.%s execution_date=%s ""
                ""exited with status %s for try_number %s"",
                dag_id, task_id, execution_date, state, try_number
            )
            if state == State.FAILED or state == State.SUCCESS:
                qry = session.query(TI).filter(TI.dag_id == dag_id,
                                               TI.task_id == task_id,
                                               TI.execution_date == execution_date)
                ti = qry.first()
                if not ti:
                    self.log.warning(""TaskInstance %s went missing from the database"", ti)
                    continue

                # TODO: should we fail RUNNING as well, as we do in Backfills?
                if ti.try_number == try_number and ti.state == State.QUEUED:
                    msg = (""Executor reports task instance {} finished ({}) ""
                           ""although the task says its {}. Was the task ""
                           ""killed externally?"".format(ti, state, ti.state))
                    self.log.error(msg)
                    try:
                        simple_dag = simple_dag_bag.get_dag(dag_id)
                        dagbag = models.DagBag(simple_dag.full_filepath)
                        dag = dagbag.get_dag(dag_id)
                        ti.task = dag.get_task(task_id)
                        ti.handle_failure(msg)
                    except Exception:
                        self.log.error(""Cannot load the dag bag to handle failure for %s""
                                       "". Setting task to FAILED without callbacks or ""
                                       ""retries. Do you have enough resources?"", ti)
                        ti.state = State.FAILED
                        session.merge(ti)
                        session.commit()",def,_process_executor_events,(,self,",",simple_dag_bag,",",session,=,None,),:,# TODO: this shares quite a lot of code with _manage_executor_state,TI,=,models,.,TaskInstance,for,key,",",state,in,list,(,self,.,executor,.,get_event_buffer,(,simple_dag_bag,.,dag_ids,),.,items,(,),),:,dag_id,",",task_id,",",execution_date,",",try_number,=,key,self,.,Respond to executor events.,Respond,to,executor,events,.,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1442-L1484,test,log,.,info,(,"""Executor reports execution of %s.%s execution_date=%s ""","""exited with status %s for try_number %s""",",",dag_id,",",task_id,",",execution_date,",",state,",",try_number,),if,state,==,State,.,FAILED,or,state,==,State,.,SUCCESS,:,qry,=,session,.,query,(,TI,),.,filter,(,TI,.,dag_id,==,dag_id,",",TI,.,task_id,==,task_id,",",TI,.,execution_date,==,execution_date,),ti,=,qry,.,first,(,),if,not,ti,:,self,.,log,.,warning,(,"""TaskInstance %s went missing from the database""",",",ti,),continue,"# TODO: should we fail RUNNING as well, as we do in Backfills?",if,ti,.,try_number,==,try_number,and,ti,.,state,==,State,.,QUEUED,:,msg,=,(,"""Executor reports task instance {} finished ({}) ""","""although the task says its {}. Was the task ""","""killed externally?""",.,format,(,ti,",",state,",",ti,.,state,),),self,.,log,.,error,(,msg,),try,:,simple_dag,=,simple_dag_bag,.,get_dag,(,dag_id,),dagbag,=,models,.,DagBag,(,simple_dag,.,full_filepath,),dag,=,dagbag,.,get_dag,(,dag_id,),ti,.,task,=,dag,.,get_task,(,task_id,),ti,.,handle_failure,(,msg,),except,Exception,:,self,.,log,.,error,(,"""Cannot load the dag bag to handle failure for %s""",""". Setting task to FAILED without callbacks or ""","""retries. Do you have enough resources?""",",",ti,),ti,.,state,=,State,.,FAILED,session,.,merge,(,ti,),session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,SchedulerJob.process_file,"def process_file(self, file_path, zombies, pickle_dags=False, session=None):
        """"""
        Process a Python file containing Airflow DAGs.

        This includes:

        1. Execute the file and look for DAG objects in the namespace.
        2. Pickle the DAG and save it to the DB (if necessary).
        3. For each DAG, see what tasks should run and create appropriate task
        instances in the DB.
        4. Record any errors importing the file into ORM
        5. Kill (in ORM) any task instances belonging to the DAGs that haven't
        issued a heartbeat in a while.

        Returns a list of SimpleDag objects that represent the DAGs found in
        the file

        :param file_path: the path to the Python file that should be executed
        :type file_path: unicode
        :param zombies: zombie task instances to kill.
        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]
        :param pickle_dags: whether serialize the DAGs found in the file and
            save them to the db
        :type pickle_dags: bool
        :return: a list of SimpleDags made from the Dags found in the file
        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]
        """"""
        self.log.info(""Processing file %s for tasks to queue"", file_path)
        # As DAGs are parsed from this file, they will be converted into SimpleDags
        simple_dags = []

        try:
            dagbag = models.DagBag(file_path, include_examples=False)
        except Exception:
            self.log.exception(""Failed at reloading the DAG file %s"", file_path)
            Stats.incr('dag_file_refresh_error', 1, 1)
            return []

        if len(dagbag.dags) > 0:
            self.log.info(""DAG(s) %s retrieved from %s"", dagbag.dags.keys(), file_path)
        else:
            self.log.warning(""No viable dags retrieved from %s"", file_path)
            self.update_import_errors(session, dagbag)
            return []

        # Save individual DAGs in the ORM and update DagModel.last_scheduled_time
        for dag in dagbag.dags.values():
            dag.sync_to_db()

        paused_dag_ids = [dag.dag_id for dag in dagbag.dags.values()
                          if dag.is_paused]

        # Pickle the DAGs (if necessary) and put them into a SimpleDag
        for dag_id in dagbag.dags:
            # Only return DAGs that are not paused
            if dag_id not in paused_dag_ids:
                dag = dagbag.get_dag(dag_id)
                pickle_id = None
                if pickle_dags:
                    pickle_id = dag.pickle(session).id
                simple_dags.append(SimpleDag(dag, pickle_id=pickle_id))

        if len(self.dag_ids) > 0:
            dags = [dag for dag in dagbag.dags.values()
                    if dag.dag_id in self.dag_ids and
                    dag.dag_id not in paused_dag_ids]
        else:
            dags = [dag for dag in dagbag.dags.values()
                    if not dag.parent_dag and
                    dag.dag_id not in paused_dag_ids]

        # Not using multiprocessing.Queue() since it's no longer a separate
        # process and due to some unusual behavior. (empty() incorrectly
        # returns true?)
        ti_keys_to_schedule = []

        self._process_dags(dagbag, dags, ti_keys_to_schedule)

        for ti_key in ti_keys_to_schedule:
            dag = dagbag.dags[ti_key[0]]
            task = dag.get_task(ti_key[1])
            ti = models.TaskInstance(task, ti_key[2])

            ti.refresh_from_db(session=session, lock_for_update=True)
            # We can defer checking the task dependency checks to the worker themselves
            # since they can be expensive to run in the scheduler.
            dep_context = DepContext(deps=QUEUE_DEPS, ignore_task_deps=True)

            # Only schedule tasks that have their dependencies met, e.g. to avoid
            # a task that recently got its state changed to RUNNING from somewhere
            # other than the scheduler from getting its state overwritten.
            # TODO(aoen): It's not great that we have to check all the task instance
            # dependencies twice; once to get the task scheduled, and again to actually
            # run the task. We should try to come up with a way to only check them once.
            if ti.are_dependencies_met(
                    dep_context=dep_context,
                    session=session,
                    verbose=True):
                # Task starts out in the scheduled state. All tasks in the
                # scheduled state will be sent to the executor
                ti.state = State.SCHEDULED

            # Also save this task instance to the DB.
            self.log.info(""Creating / updating %s in ORM"", ti)
            session.merge(ti)
        # commit batch
        session.commit()

        # Record import errors into the ORM
        try:
            self.update_import_errors(session, dagbag)
        except Exception:
            self.log.exception(""Error logging import errors!"")
        try:
            dagbag.kill_zombies(zombies)
        except Exception:
            self.log.exception(""Error killing zombies!"")

        return simple_dags",python,"def process_file(self, file_path, zombies, pickle_dags=False, session=None):
        """"""
        Process a Python file containing Airflow DAGs.

        This includes:

        1. Execute the file and look for DAG objects in the namespace.
        2. Pickle the DAG and save it to the DB (if necessary).
        3. For each DAG, see what tasks should run and create appropriate task
        instances in the DB.
        4. Record any errors importing the file into ORM
        5. Kill (in ORM) any task instances belonging to the DAGs that haven't
        issued a heartbeat in a while.

        Returns a list of SimpleDag objects that represent the DAGs found in
        the file

        :param file_path: the path to the Python file that should be executed
        :type file_path: unicode
        :param zombies: zombie task instances to kill.
        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]
        :param pickle_dags: whether serialize the DAGs found in the file and
            save them to the db
        :type pickle_dags: bool
        :return: a list of SimpleDags made from the Dags found in the file
        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]
        """"""
        self.log.info(""Processing file %s for tasks to queue"", file_path)
        # As DAGs are parsed from this file, they will be converted into SimpleDags
        simple_dags = []

        try:
            dagbag = models.DagBag(file_path, include_examples=False)
        except Exception:
            self.log.exception(""Failed at reloading the DAG file %s"", file_path)
            Stats.incr('dag_file_refresh_error', 1, 1)
            return []

        if len(dagbag.dags) > 0:
            self.log.info(""DAG(s) %s retrieved from %s"", dagbag.dags.keys(), file_path)
        else:
            self.log.warning(""No viable dags retrieved from %s"", file_path)
            self.update_import_errors(session, dagbag)
            return []

        # Save individual DAGs in the ORM and update DagModel.last_scheduled_time
        for dag in dagbag.dags.values():
            dag.sync_to_db()

        paused_dag_ids = [dag.dag_id for dag in dagbag.dags.values()
                          if dag.is_paused]

        # Pickle the DAGs (if necessary) and put them into a SimpleDag
        for dag_id in dagbag.dags:
            # Only return DAGs that are not paused
            if dag_id not in paused_dag_ids:
                dag = dagbag.get_dag(dag_id)
                pickle_id = None
                if pickle_dags:
                    pickle_id = dag.pickle(session).id
                simple_dags.append(SimpleDag(dag, pickle_id=pickle_id))

        if len(self.dag_ids) > 0:
            dags = [dag for dag in dagbag.dags.values()
                    if dag.dag_id in self.dag_ids and
                    dag.dag_id not in paused_dag_ids]
        else:
            dags = [dag for dag in dagbag.dags.values()
                    if not dag.parent_dag and
                    dag.dag_id not in paused_dag_ids]

        # Not using multiprocessing.Queue() since it's no longer a separate
        # process and due to some unusual behavior. (empty() incorrectly
        # returns true?)
        ti_keys_to_schedule = []

        self._process_dags(dagbag, dags, ti_keys_to_schedule)

        for ti_key in ti_keys_to_schedule:
            dag = dagbag.dags[ti_key[0]]
            task = dag.get_task(ti_key[1])
            ti = models.TaskInstance(task, ti_key[2])

            ti.refresh_from_db(session=session, lock_for_update=True)
            # We can defer checking the task dependency checks to the worker themselves
            # since they can be expensive to run in the scheduler.
            dep_context = DepContext(deps=QUEUE_DEPS, ignore_task_deps=True)

            # Only schedule tasks that have their dependencies met, e.g. to avoid
            # a task that recently got its state changed to RUNNING from somewhere
            # other than the scheduler from getting its state overwritten.
            # TODO(aoen): It's not great that we have to check all the task instance
            # dependencies twice; once to get the task scheduled, and again to actually
            # run the task. We should try to come up with a way to only check them once.
            if ti.are_dependencies_met(
                    dep_context=dep_context,
                    session=session,
                    verbose=True):
                # Task starts out in the scheduled state. All tasks in the
                # scheduled state will be sent to the executor
                ti.state = State.SCHEDULED

            # Also save this task instance to the DB.
            self.log.info(""Creating / updating %s in ORM"", ti)
            session.merge(ti)
        # commit batch
        session.commit()

        # Record import errors into the ORM
        try:
            self.update_import_errors(session, dagbag)
        except Exception:
            self.log.exception(""Error logging import errors!"")
        try:
            dagbag.kill_zombies(zombies)
        except Exception:
            self.log.exception(""Error killing zombies!"")

        return simple_dags",def,process_file,(,self,",",file_path,",",zombies,",",pickle_dags,=,False,",",session,=,None,),:,self,.,log,.,info,(,"""Processing file %s for tasks to queue""",",",file_path,),"# As DAGs are parsed from this file, they will be converted into SimpleDags",simple_dags,=,[,],try,:,dagbag,=,models,.,DagBag,(,file_path,",",include_examples,=,False,),except,Exception,:,self,.,"Process a Python file containing Airflow DAGs.

        This includes:

        1. Execute the file and look for DAG objects in the namespace.
        2. Pickle the DAG and save it to the DB (if necessary).
        3. For each DAG, see what tasks should run and create appropriate task
        instances in the DB.
        4. Record any errors importing the file into ORM
        5. Kill (in ORM) any task instances belonging to the DAGs that haven't
        issued a heartbeat in a while.

        Returns a list of SimpleDag objects that represent the DAGs found in
        the file

        :param file_path: the path to the Python file that should be executed
        :type file_path: unicode
        :param zombies: zombie task instances to kill.
        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]
        :param pickle_dags: whether serialize the DAGs found in the file and
            save them to the db
        :type pickle_dags: bool
        :return: a list of SimpleDags made from the Dags found in the file
        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]",Process,a,Python,file,containing,Airflow,DAGs,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1669-L1787,test,log,.,exception,(,"""Failed at reloading the DAG file %s""",",",file_path,),Stats,.,incr,(,'dag_file_refresh_error',",",1,",",1,),return,[,],if,len,(,dagbag,.,dags,),>,0,:,self,.,log,.,info,(,"""DAG(s) %s retrieved from %s""",",",dagbag,.,dags,.,keys,(,),",",file_path,),else,:,self,.,log,.,warning,(,"""No viable dags retrieved from %s""",",",file_path,),self,.,update_import_errors,(,session,",",dagbag,),return,[,],# Save individual DAGs in the ORM and update DagModel.last_scheduled_time,for,dag,in,dagbag,.,dags,.,values,(,),:,dag,.,sync_to_db,(,),paused_dag_ids,=,[,dag,.,dag_id,for,dag,in,dagbag,.,dags,.,values,(,),if,dag,.,is_paused,],# Pickle the DAGs (if necessary) and put them into a SimpleDag,for,dag_id,in,dagbag,.,dags,:,# Only return DAGs that are not paused,if,dag_id,not,in,paused_dag_ids,:,dag,=,dagbag,.,get_dag,(,dag_id,),pickle_id,=,None,if,pickle_dags,:,pickle_id,=,dag,.,pickle,(,session,),.,id,simple_dags,.,append,(,SimpleDag,(,dag,",",pickle_id,=,pickle_id,),),if,len,(,self,.,dag_ids,),>,0,:,dags,=,[,dag,for,dag,in,dagbag,.,dags,.,values,(,),if,dag,.,dag_id,in,self,.,dag_ids,and,dag,.,dag_id,not,in,paused_dag_ids,],else,:,dags,=,[,dag,for,dag,in,dagbag,.,dags,.,values,(,),if,not,dag,.,parent_dag,and,dag,.,dag_id,not,in,paused_dag_ids,],# Not using multiprocessing.Queue() since it's no longer a separate,# process and due to some unusual behavior. (empty() incorrectly,# returns true?),ti_keys_to_schedule,=,[,],self,.,_process_dags,(,dagbag,",",dags,",",ti_keys_to_schedule,),for,ti_key,in,ti_keys_to_schedule,:,dag,=,dagbag,.,dags,[,ti_key,[,0,],],task,=,dag,.,get_task,(,ti_key,[,1,],),ti,=,models,.,TaskInstance,(,task,",",ti_key,[,2,],),ti,.,refresh_from_db,(,session,=,session,",",lock_for_update,=,True,),# We can defer checking the task dependency checks to the worker themselves,# since they can be expensive to run in the scheduler.,dep_context,=,DepContext,(,deps,=,QUEUE_DEPS,",",ignore_task_deps,=,True,),"# Only schedule tasks that have their dependencies met, e.g. to avoid",# a task that recently got its state changed to RUNNING from somewhere,# other than the scheduler from getting its state overwritten.,# TODO(aoen): It's not great that we have to check all the task instance,"# dependencies twice; once to get the task scheduled, and again to actually",# run the task. We should try to come up with a way to only check them once.,if,ti,.,are_dependencies_met,(,dep_context,=,dep_context,",",session,=,session,",",verbose,=,True,),:,# Task starts out in the scheduled state. All tasks in the,# scheduled state will be sent to the executor,ti,.,state,=,State,.,SCHEDULED,# Also save this task instance to the DB.,self,.,log,.,info,(,"""Creating / updating %s in ORM""",",",ti,),session,.,merge,(,ti,),# commit batch,session,.,commit,(,),# Record import errors into the ORM,try,:,self,.,update_import_errors,(,session,",",dagbag,),except,Exception,:,self,.,log,.,exception,(,"""Error logging import errors!""",),try,:,dagbag,.,kill_zombies,(,zombies,),except,Exception,:,self,.,log,.,exception,(,"""Error killing zombies!""",),return,simple_dags,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._update_counters,"def _update_counters(self, ti_status):
        """"""
        Updates the counters per state of the tasks that were running. Can re-add
        to tasks to run in case required.

        :param ti_status: the internal status of the backfill job tasks
        :type ti_status: BackfillJob._DagRunTaskStatus
        """"""
        for key, ti in list(ti_status.running.items()):
            ti.refresh_from_db()
            if ti.state == State.SUCCESS:
                ti_status.succeeded.add(key)
                self.log.debug(""Task instance %s succeeded. Don't rerun."", ti)
                ti_status.running.pop(key)
                continue
            elif ti.state == State.SKIPPED:
                ti_status.skipped.add(key)
                self.log.debug(""Task instance %s skipped. Don't rerun."", ti)
                ti_status.running.pop(key)
                continue
            elif ti.state == State.FAILED:
                self.log.error(""Task instance %s failed"", ti)
                ti_status.failed.add(key)
                ti_status.running.pop(key)
                continue
            # special case: if the task needs to run again put it back
            elif ti.state == State.UP_FOR_RETRY:
                self.log.warning(""Task instance %s is up for retry"", ti)
                ti_status.running.pop(key)
                ti_status.to_run[key] = ti
            # special case: if the task needs to be rescheduled put it back
            elif ti.state == State.UP_FOR_RESCHEDULE:
                self.log.warning(""Task instance %s is up for reschedule"", ti)
                ti_status.running.pop(key)
                ti_status.to_run[key] = ti
            # special case: The state of the task can be set to NONE by the task itself
            # when it reaches concurrency limits. It could also happen when the state
            # is changed externally, e.g. by clearing tasks from the ui. We need to cover
            # for that as otherwise those tasks would fall outside of the scope of
            # the backfill suddenly.
            elif ti.state == State.NONE:
                self.log.warning(
                    ""FIXME: task instance %s state was set to none externally or ""
                    ""reaching concurrency limits. Re-adding task to queue."",
                    ti
                )
                ti.set_state(State.SCHEDULED)
                ti_status.running.pop(key)
                ti_status.to_run[key] = ti",python,"def _update_counters(self, ti_status):
        """"""
        Updates the counters per state of the tasks that were running. Can re-add
        to tasks to run in case required.

        :param ti_status: the internal status of the backfill job tasks
        :type ti_status: BackfillJob._DagRunTaskStatus
        """"""
        for key, ti in list(ti_status.running.items()):
            ti.refresh_from_db()
            if ti.state == State.SUCCESS:
                ti_status.succeeded.add(key)
                self.log.debug(""Task instance %s succeeded. Don't rerun."", ti)
                ti_status.running.pop(key)
                continue
            elif ti.state == State.SKIPPED:
                ti_status.skipped.add(key)
                self.log.debug(""Task instance %s skipped. Don't rerun."", ti)
                ti_status.running.pop(key)
                continue
            elif ti.state == State.FAILED:
                self.log.error(""Task instance %s failed"", ti)
                ti_status.failed.add(key)
                ti_status.running.pop(key)
                continue
            # special case: if the task needs to run again put it back
            elif ti.state == State.UP_FOR_RETRY:
                self.log.warning(""Task instance %s is up for retry"", ti)
                ti_status.running.pop(key)
                ti_status.to_run[key] = ti
            # special case: if the task needs to be rescheduled put it back
            elif ti.state == State.UP_FOR_RESCHEDULE:
                self.log.warning(""Task instance %s is up for reschedule"", ti)
                ti_status.running.pop(key)
                ti_status.to_run[key] = ti
            # special case: The state of the task can be set to NONE by the task itself
            # when it reaches concurrency limits. It could also happen when the state
            # is changed externally, e.g. by clearing tasks from the ui. We need to cover
            # for that as otherwise those tasks would fall outside of the scope of
            # the backfill suddenly.
            elif ti.state == State.NONE:
                self.log.warning(
                    ""FIXME: task instance %s state was set to none externally or ""
                    ""reaching concurrency limits. Re-adding task to queue."",
                    ti
                )
                ti.set_state(State.SCHEDULED)
                ti_status.running.pop(key)
                ti_status.to_run[key] = ti",def,_update_counters,(,self,",",ti_status,),:,for,key,",",ti,in,list,(,ti_status,.,running,.,items,(,),),:,ti,.,refresh_from_db,(,),if,ti,.,state,==,State,.,SUCCESS,:,ti_status,.,succeeded,.,add,(,key,),self,.,log,.,debug,(,"Updates the counters per state of the tasks that were running. Can re-add
        to tasks to run in case required.

        :param ti_status: the internal status of the backfill job tasks
        :type ti_status: BackfillJob._DagRunTaskStatus",Updates,the,counters,per,state,of,the,tasks,that,were,running,.,Can,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1929-L1977,test,"""Task instance %s succeeded. Don't rerun.""",",",ti,),ti_status,.,running,.,pop,(,key,),continue,elif,ti,.,state,==,State,.,SKIPPED,:,ti_status,.,skipped,.,add,(,key,),self,.,log,.,debug,(,"""Task instance %s skipped. Don't rerun.""",",",ti,),ti_status,.,running,.,pop,(,key,),continue,elif,ti,.,state,==,State,.,FAILED,:,self,.,log,.,error,(,"""Task instance %s failed""",",",ti,),ti_status,.,failed,.,add,(,key,),ti_status,.,running,.,pop,(,key,),continue,# special case: if the task needs to run again put it back,elif,ti,.,state,==,State,.,UP_FOR_RETRY,:,self,.,log,.,warning,(,"""Task instance %s is up for retry""",",",ti,),ti_status,.,running,.,pop,(,key,),ti_status,.,to_run,[,key,],=,ti,# special case: if the task needs to be rescheduled put it back,elif,ti,.,state,==,State,.,UP_FOR_RESCHEDULE,:,self,.,log,.,warning,(,"""Task instance %s is up for reschedule""",",",ti,),ti_status,.,running,.,pop,(,key,),ti_status,.,to_run,[,key,],=,ti,# special case: The state of the task can be set to NONE by the task itself,# when it reaches concurrency limits. It could also happen when the state,"# is changed externally, e.g. by clearing tasks from the ui. We need to cover",# for that as otherwise those tasks would fall outside of the scope of,# the backfill suddenly.,elif,ti,.,state,==,State,.,NONE,:,self,.,log,.,warning,(,"""FIXME: task instance %s state was set to none externally or ""","""reaching concurrency limits. Re-adding task to queue.""",",",ti,),ti,.,set_state,(,State,.,SCHEDULED,),ti_status,.,running,.,pop,(,key,),ti_status,.,to_run,[,key,],=,ti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,re,-,add,to,tasks,to,run,in,case,required,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._manage_executor_state,"def _manage_executor_state(self, running):
        """"""
        Checks if the executor agrees with the state of task instances
        that are running

        :param running: dict of key, task to verify
        """"""
        executor = self.executor

        for key, state in list(executor.get_event_buffer().items()):
            if key not in running:
                self.log.warning(
                    ""%s state %s not in running=%s"",
                    key, state, running.values()
                )
                continue

            ti = running[key]
            ti.refresh_from_db()

            self.log.debug(""Executor state: %s task %s"", state, ti)

            if state == State.FAILED or state == State.SUCCESS:
                if ti.state == State.RUNNING or ti.state == State.QUEUED:
                    msg = (""Executor reports task instance {} finished ({}) ""
                           ""although the task says its {}. Was the task ""
                           ""killed externally?"".format(ti, state, ti.state))
                    self.log.error(msg)
                    ti.handle_failure(msg)",python,"def _manage_executor_state(self, running):
        """"""
        Checks if the executor agrees with the state of task instances
        that are running

        :param running: dict of key, task to verify
        """"""
        executor = self.executor

        for key, state in list(executor.get_event_buffer().items()):
            if key not in running:
                self.log.warning(
                    ""%s state %s not in running=%s"",
                    key, state, running.values()
                )
                continue

            ti = running[key]
            ti.refresh_from_db()

            self.log.debug(""Executor state: %s task %s"", state, ti)

            if state == State.FAILED or state == State.SUCCESS:
                if ti.state == State.RUNNING or ti.state == State.QUEUED:
                    msg = (""Executor reports task instance {} finished ({}) ""
                           ""although the task says its {}. Was the task ""
                           ""killed externally?"".format(ti, state, ti.state))
                    self.log.error(msg)
                    ti.handle_failure(msg)",def,_manage_executor_state,(,self,",",running,),:,executor,=,self,.,executor,for,key,",",state,in,list,(,executor,.,get_event_buffer,(,),.,items,(,),),:,if,key,not,in,running,:,self,.,log,.,warning,(,"""%s state %s not in running=%s""",",",key,",",state,",",running,.,values,"Checks if the executor agrees with the state of task instances
        that are running

        :param running: dict of key, task to verify",Checks,if,the,executor,agrees,with,the,state,of,task,instances,that,are,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1979-L2007,test,(,),),continue,ti,=,running,[,key,],ti,.,refresh_from_db,(,),self,.,log,.,debug,(,"""Executor state: %s task %s""",",",state,",",ti,),if,state,==,State,.,FAILED,or,state,==,State,.,SUCCESS,:,if,ti,.,state,==,State,.,RUNNING,or,ti,.,state,==,State,.,QUEUED,:,msg,=,(,"""Executor reports task instance {} finished ({}) ""","""although the task says its {}. Was the task ""","""killed externally?""",.,format,(,ti,",",state,",",ti,.,state,),),self,.,log,.,error,(,msg,),ti,.,handle_failure,(,msg,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,running,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._get_dag_run,"def _get_dag_run(self, run_date, session=None):
        """"""
        Returns a dag run for the given run date, which will be matched to an existing
        dag run if available or create a new dag run otherwise. If the max_active_runs
        limit is reached, this function will return None.

        :param run_date: the execution date for the dag run
        :type run_date: datetime.datetime
        :param session: the database session object
        :type session: sqlalchemy.orm.session.Session
        :return: a DagRun in state RUNNING or None
        """"""
        run_id = BackfillJob.ID_FORMAT_PREFIX.format(run_date.isoformat())

        # consider max_active_runs but ignore when running subdags
        respect_dag_max_active_limit = (True
                                        if (self.dag.schedule_interval and
                                            not self.dag.is_subdag)
                                        else False)

        current_active_dag_count = self.dag.get_num_active_runs(external_trigger=False)

        # check if we are scheduling on top of a already existing dag_run
        # we could find a ""scheduled"" run instead of a ""backfill""
        run = DagRun.find(dag_id=self.dag.dag_id,
                          execution_date=run_date,
                          session=session)

        if run is not None and len(run) > 0:
            run = run[0]
            if run.state == State.RUNNING:
                respect_dag_max_active_limit = False
        else:
            run = None

        # enforce max_active_runs limit for dag, special cases already
        # handled by respect_dag_max_active_limit
        if (respect_dag_max_active_limit and
                current_active_dag_count >= self.dag.max_active_runs):
            return None

        run = run or self.dag.create_dagrun(
            run_id=run_id,
            execution_date=run_date,
            start_date=timezone.utcnow(),
            state=State.RUNNING,
            external_trigger=False,
            session=session,
            conf=self.conf,
        )

        # set required transient field
        run.dag = self.dag

        # explicitly mark as backfill and running
        run.state = State.RUNNING
        run.run_id = run_id
        run.verify_integrity(session=session)
        return run",python,"def _get_dag_run(self, run_date, session=None):
        """"""
        Returns a dag run for the given run date, which will be matched to an existing
        dag run if available or create a new dag run otherwise. If the max_active_runs
        limit is reached, this function will return None.

        :param run_date: the execution date for the dag run
        :type run_date: datetime.datetime
        :param session: the database session object
        :type session: sqlalchemy.orm.session.Session
        :return: a DagRun in state RUNNING or None
        """"""
        run_id = BackfillJob.ID_FORMAT_PREFIX.format(run_date.isoformat())

        # consider max_active_runs but ignore when running subdags
        respect_dag_max_active_limit = (True
                                        if (self.dag.schedule_interval and
                                            not self.dag.is_subdag)
                                        else False)

        current_active_dag_count = self.dag.get_num_active_runs(external_trigger=False)

        # check if we are scheduling on top of a already existing dag_run
        # we could find a ""scheduled"" run instead of a ""backfill""
        run = DagRun.find(dag_id=self.dag.dag_id,
                          execution_date=run_date,
                          session=session)

        if run is not None and len(run) > 0:
            run = run[0]
            if run.state == State.RUNNING:
                respect_dag_max_active_limit = False
        else:
            run = None

        # enforce max_active_runs limit for dag, special cases already
        # handled by respect_dag_max_active_limit
        if (respect_dag_max_active_limit and
                current_active_dag_count >= self.dag.max_active_runs):
            return None

        run = run or self.dag.create_dagrun(
            run_id=run_id,
            execution_date=run_date,
            start_date=timezone.utcnow(),
            state=State.RUNNING,
            external_trigger=False,
            session=session,
            conf=self.conf,
        )

        # set required transient field
        run.dag = self.dag

        # explicitly mark as backfill and running
        run.state = State.RUNNING
        run.run_id = run_id
        run.verify_integrity(session=session)
        return run",def,_get_dag_run,(,self,",",run_date,",",session,=,None,),:,run_id,=,BackfillJob,.,ID_FORMAT_PREFIX,.,format,(,run_date,.,isoformat,(,),),# consider max_active_runs but ignore when running subdags,respect_dag_max_active_limit,=,(,True,if,(,self,.,dag,.,schedule_interval,and,not,self,.,dag,.,is_subdag,),else,False,),current_active_dag_count,=,self,"Returns a dag run for the given run date, which will be matched to an existing
        dag run if available or create a new dag run otherwise. If the max_active_runs
        limit is reached, this function will return None.

        :param run_date: the execution date for the dag run
        :type run_date: datetime.datetime
        :param session: the database session object
        :type session: sqlalchemy.orm.session.Session
        :return: a DagRun in state RUNNING or None",Returns,a,dag,run,for,the,given,run,date,which,will,be,matched,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2010-L2068,test,.,dag,.,get_num_active_runs,(,external_trigger,=,False,),# check if we are scheduling on top of a already existing dag_run,"# we could find a ""scheduled"" run instead of a ""backfill""",run,=,DagRun,.,find,(,dag_id,=,self,.,dag,.,dag_id,",",execution_date,=,run_date,",",session,=,session,),if,run,is,not,None,and,len,(,run,),>,0,:,run,=,run,[,0,],if,run,.,state,==,State,.,RUNNING,:,respect_dag_max_active_limit,=,False,else,:,run,=,None,"# enforce max_active_runs limit for dag, special cases already",# handled by respect_dag_max_active_limit,if,(,respect_dag_max_active_limit,and,current_active_dag_count,>=,self,.,dag,.,max_active_runs,),:,return,None,run,=,run,or,self,.,dag,.,create_dagrun,(,run_id,=,run_id,",",execution_date,=,run_date,",",start_date,=,timezone,.,utcnow,(,),",",state,=,State,.,RUNNING,",",external_trigger,=,False,",",session,=,session,",",conf,=,self,.,conf,",",),# set required transient field,run,.,dag,=,self,.,dag,# explicitly mark as backfill and running,run,.,state,=,State,.,RUNNING,run,.,run_id,=,run_id,run,.,verify_integrity,(,session,=,session,),return,run,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,an,existing,dag,run,if,available,or,create,a,new,dag,run,otherwise,.,If,the,max_active_runs,limit,is,reached,this,function,will,return,None,.,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._task_instances_for_dag_run,"def _task_instances_for_dag_run(self, dag_run, session=None):
        """"""
        Returns a map of task instance key to task instance object for the tasks to
        run in the given dag run.

        :param dag_run: the dag run to get the tasks from
        :type dag_run: airflow.models.DagRun
        :param session: the database session object
        :type session: sqlalchemy.orm.session.Session
        """"""
        tasks_to_run = {}

        if dag_run is None:
            return tasks_to_run

        # check if we have orphaned tasks
        self.reset_state_for_orphaned_tasks(filter_by_dag_run=dag_run, session=session)

        # for some reason if we don't refresh the reference to run is lost
        dag_run.refresh_from_db()
        make_transient(dag_run)

        # TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf
        for ti in dag_run.get_task_instances():
            # all tasks part of the backfill are scheduled to run
            if ti.state == State.NONE:
                ti.set_state(State.SCHEDULED, session=session)
            if ti.state != State.REMOVED:
                tasks_to_run[ti.key] = ti

        return tasks_to_run",python,"def _task_instances_for_dag_run(self, dag_run, session=None):
        """"""
        Returns a map of task instance key to task instance object for the tasks to
        run in the given dag run.

        :param dag_run: the dag run to get the tasks from
        :type dag_run: airflow.models.DagRun
        :param session: the database session object
        :type session: sqlalchemy.orm.session.Session
        """"""
        tasks_to_run = {}

        if dag_run is None:
            return tasks_to_run

        # check if we have orphaned tasks
        self.reset_state_for_orphaned_tasks(filter_by_dag_run=dag_run, session=session)

        # for some reason if we don't refresh the reference to run is lost
        dag_run.refresh_from_db()
        make_transient(dag_run)

        # TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf
        for ti in dag_run.get_task_instances():
            # all tasks part of the backfill are scheduled to run
            if ti.state == State.NONE:
                ti.set_state(State.SCHEDULED, session=session)
            if ti.state != State.REMOVED:
                tasks_to_run[ti.key] = ti

        return tasks_to_run",def,_task_instances_for_dag_run,(,self,",",dag_run,",",session,=,None,),:,tasks_to_run,=,{,},if,dag_run,is,None,:,return,tasks_to_run,# check if we have orphaned tasks,self,.,reset_state_for_orphaned_tasks,(,filter_by_dag_run,=,dag_run,",",session,=,session,),# for some reason if we don't refresh the reference to run is lost,dag_run,.,refresh_from_db,(,),make_transient,(,dag_run,),# TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf,for,ti,in,dag_run,.,"Returns a map of task instance key to task instance object for the tasks to
        run in the given dag run.

        :param dag_run: the dag run to get the tasks from
        :type dag_run: airflow.models.DagRun
        :param session: the database session object
        :type session: sqlalchemy.orm.session.Session",Returns,a,map,of,task,instance,key,to,task,instance,object,for,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2071-L2101,test,get_task_instances,(,),:,# all tasks part of the backfill are scheduled to run,if,ti,.,state,==,State,.,NONE,:,ti,.,set_state,(,State,.,SCHEDULED,",",session,=,session,),if,ti,.,state,!=,State,.,REMOVED,:,tasks_to_run,[,ti,.,key,],=,ti,return,tasks_to_run,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tasks,to,run,in,the,given,dag,run,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._execute_for_run_dates,"def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,
                               start_date, session=None):
        """"""
        Computes the dag runs and their respective task instances for
        the given run dates and executes the task instances.
        Returns a list of execution dates of the dag runs that were executed.

        :param run_dates: Execution dates for dag runs
        :type run_dates: list
        :param ti_status: internal BackfillJob status structure to tis track progress
        :type ti_status: BackfillJob._DagRunTaskStatus
        :param executor: the executor to use, it must be previously started
        :type executor: BaseExecutor
        :param pickle_id: numeric id of the pickled dag, None if not pickled
        :type pickle_id: int
        :param start_date: backfill start date
        :type start_date: datetime.datetime
        :param session: the current session object
        :type session: sqlalchemy.orm.session.Session
        """"""
        for next_run_date in run_dates:
            dag_run = self._get_dag_run(next_run_date, session=session)
            tis_map = self._task_instances_for_dag_run(dag_run,
                                                       session=session)
            if dag_run is None:
                continue

            ti_status.active_runs.append(dag_run)
            ti_status.to_run.update(tis_map or {})

        processed_dag_run_dates = self._process_backfill_task_instances(
            ti_status=ti_status,
            executor=executor,
            pickle_id=pickle_id,
            start_date=start_date,
            session=session)

        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)",python,"def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,
                               start_date, session=None):
        """"""
        Computes the dag runs and their respective task instances for
        the given run dates and executes the task instances.
        Returns a list of execution dates of the dag runs that were executed.

        :param run_dates: Execution dates for dag runs
        :type run_dates: list
        :param ti_status: internal BackfillJob status structure to tis track progress
        :type ti_status: BackfillJob._DagRunTaskStatus
        :param executor: the executor to use, it must be previously started
        :type executor: BaseExecutor
        :param pickle_id: numeric id of the pickled dag, None if not pickled
        :type pickle_id: int
        :param start_date: backfill start date
        :type start_date: datetime.datetime
        :param session: the current session object
        :type session: sqlalchemy.orm.session.Session
        """"""
        for next_run_date in run_dates:
            dag_run = self._get_dag_run(next_run_date, session=session)
            tis_map = self._task_instances_for_dag_run(dag_run,
                                                       session=session)
            if dag_run is None:
                continue

            ti_status.active_runs.append(dag_run)
            ti_status.to_run.update(tis_map or {})

        processed_dag_run_dates = self._process_backfill_task_instances(
            ti_status=ti_status,
            executor=executor,
            pickle_id=pickle_id,
            start_date=start_date,
            session=session)

        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)",def,_execute_for_run_dates,(,self,",",run_dates,",",ti_status,",",executor,",",pickle_id,",",start_date,",",session,=,None,),:,for,next_run_date,in,run_dates,:,dag_run,=,self,.,_get_dag_run,(,next_run_date,",",session,=,session,),tis_map,=,self,.,_task_instances_for_dag_run,(,dag_run,",",session,=,session,),if,dag_run,is,"Computes the dag runs and their respective task instances for
        the given run dates and executes the task instances.
        Returns a list of execution dates of the dag runs that were executed.

        :param run_dates: Execution dates for dag runs
        :type run_dates: list
        :param ti_status: internal BackfillJob status structure to tis track progress
        :type ti_status: BackfillJob._DagRunTaskStatus
        :param executor: the executor to use, it must be previously started
        :type executor: BaseExecutor
        :param pickle_id: numeric id of the pickled dag, None if not pickled
        :type pickle_id: int
        :param start_date: backfill start date
        :type start_date: datetime.datetime
        :param session: the current session object
        :type session: sqlalchemy.orm.session.Session",Computes,the,dag,runs,and,their,respective,task,instances,for,the,given,run,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2405-L2442,test,None,:,continue,ti_status,.,active_runs,.,append,(,dag_run,),ti_status,.,to_run,.,update,(,tis_map,or,{,},),processed_dag_run_dates,=,self,.,_process_backfill_task_instances,(,ti_status,=,ti_status,",",executor,=,executor,",",pickle_id,=,pickle_id,",",start_date,=,start_date,",",session,=,session,),ti_status,.,executed_dag_run_dates,.,update,(,processed_dag_run_dates,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dates,and,executes,the,task,instances,.,Returns,a,list,of,execution,dates,of,the,dag,runs,that,were,executed,.,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._set_unfinished_dag_runs_to_failed,"def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):
        """"""
        Go through the dag_runs and update the state based on the task_instance state.
        Then set DAG runs that are not finished to failed.

        :param dag_runs: DAG runs
        :param session: session
        :return: None
        """"""
        for dag_run in dag_runs:
            dag_run.update_state()
            if dag_run.state not in State.finished():
                dag_run.set_state(State.FAILED)
            session.merge(dag_run)",python,"def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):
        """"""
        Go through the dag_runs and update the state based on the task_instance state.
        Then set DAG runs that are not finished to failed.

        :param dag_runs: DAG runs
        :param session: session
        :return: None
        """"""
        for dag_run in dag_runs:
            dag_run.update_state()
            if dag_run.state not in State.finished():
                dag_run.set_state(State.FAILED)
            session.merge(dag_run)",def,_set_unfinished_dag_runs_to_failed,(,self,",",dag_runs,",",session,=,None,),:,for,dag_run,in,dag_runs,:,dag_run,.,update_state,(,),if,dag_run,.,state,not,in,State,.,finished,(,),:,dag_run,.,set_state,(,State,.,FAILED,),session,.,merge,(,dag_run,),,,,,"Go through the dag_runs and update the state based on the task_instance state.
        Then set DAG runs that are not finished to failed.

        :param dag_runs: DAG runs
        :param session: session
        :return: None",Go,through,the,dag_runs,and,update,the,state,based,on,the,task_instance,state,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2445-L2458,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,Then,set,DAG,runs,that,are,not,finished,to,failed,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,BackfillJob._execute,"def _execute(self, session=None):
        """"""
        Initializes all components required to run a dag for a specified date range and
        calls helper method to execute the tasks.
        """"""
        ti_status = BackfillJob._DagRunTaskStatus()

        start_date = self.bf_start_date

        # Get intervals between the start/end dates, which will turn into dag runs
        run_dates = self.dag.get_run_dates(start_date=start_date,
                                           end_date=self.bf_end_date)
        if self.run_backwards:
            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]
            if tasks_that_depend_on_past:
                raise AirflowException(
                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(
                        "","".join(tasks_that_depend_on_past)))
            run_dates = run_dates[::-1]

        if len(run_dates) == 0:
            self.log.info(""No run dates were found for the given dates and dag interval."")
            return

        # picklin'
        pickle_id = None
        if not self.donot_pickle and self.executor.__class__ not in (
                executors.LocalExecutor, executors.SequentialExecutor):
            pickle = DagPickle(self.dag)
            session.add(pickle)
            session.commit()
            pickle_id = pickle.id

        executor = self.executor
        executor.start()

        ti_status.total_runs = len(run_dates)  # total dag runs in backfill

        try:
            remaining_dates = ti_status.total_runs
            while remaining_dates > 0:
                dates_to_process = [run_date for run_date in run_dates
                                    if run_date not in ti_status.executed_dag_run_dates]

                self._execute_for_run_dates(run_dates=dates_to_process,
                                            ti_status=ti_status,
                                            executor=executor,
                                            pickle_id=pickle_id,
                                            start_date=start_date,
                                            session=session)

                remaining_dates = (
                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)
                )
                err = self._collect_errors(ti_status=ti_status, session=session)
                if err:
                    raise AirflowException(err)

                if remaining_dates > 0:
                    self.log.info(
                        ""max_active_runs limit for dag %s has been reached ""
                        "" - waiting for other dag runs to finish"",
                        self.dag_id
                    )
                    time.sleep(self.delay_on_limit_secs)
        except (KeyboardInterrupt, SystemExit):
            self.log.warning(""Backfill terminated by user."")

            # TODO: we will need to terminate running task instances and set the
            # state to failed.
            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)
        finally:
            session.commit()
            executor.end()

        self.log.info(""Backfill done. Exiting."")",python,"def _execute(self, session=None):
        """"""
        Initializes all components required to run a dag for a specified date range and
        calls helper method to execute the tasks.
        """"""
        ti_status = BackfillJob._DagRunTaskStatus()

        start_date = self.bf_start_date

        # Get intervals between the start/end dates, which will turn into dag runs
        run_dates = self.dag.get_run_dates(start_date=start_date,
                                           end_date=self.bf_end_date)
        if self.run_backwards:
            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]
            if tasks_that_depend_on_past:
                raise AirflowException(
                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(
                        "","".join(tasks_that_depend_on_past)))
            run_dates = run_dates[::-1]

        if len(run_dates) == 0:
            self.log.info(""No run dates were found for the given dates and dag interval."")
            return

        # picklin'
        pickle_id = None
        if not self.donot_pickle and self.executor.__class__ not in (
                executors.LocalExecutor, executors.SequentialExecutor):
            pickle = DagPickle(self.dag)
            session.add(pickle)
            session.commit()
            pickle_id = pickle.id

        executor = self.executor
        executor.start()

        ti_status.total_runs = len(run_dates)  # total dag runs in backfill

        try:
            remaining_dates = ti_status.total_runs
            while remaining_dates > 0:
                dates_to_process = [run_date for run_date in run_dates
                                    if run_date not in ti_status.executed_dag_run_dates]

                self._execute_for_run_dates(run_dates=dates_to_process,
                                            ti_status=ti_status,
                                            executor=executor,
                                            pickle_id=pickle_id,
                                            start_date=start_date,
                                            session=session)

                remaining_dates = (
                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)
                )
                err = self._collect_errors(ti_status=ti_status, session=session)
                if err:
                    raise AirflowException(err)

                if remaining_dates > 0:
                    self.log.info(
                        ""max_active_runs limit for dag %s has been reached ""
                        "" - waiting for other dag runs to finish"",
                        self.dag_id
                    )
                    time.sleep(self.delay_on_limit_secs)
        except (KeyboardInterrupt, SystemExit):
            self.log.warning(""Backfill terminated by user."")

            # TODO: we will need to terminate running task instances and set the
            # state to failed.
            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)
        finally:
            session.commit()
            executor.end()

        self.log.info(""Backfill done. Exiting."")",def,_execute,(,self,",",session,=,None,),:,ti_status,=,BackfillJob,.,_DagRunTaskStatus,(,),start_date,=,self,.,bf_start_date,"# Get intervals between the start/end dates, which will turn into dag runs",run_dates,=,self,.,dag,.,get_run_dates,(,start_date,=,start_date,",",end_date,=,self,.,bf_end_date,),if,self,.,run_backwards,:,tasks_that_depend_on_past,=,[,t,.,task_id,"Initializes all components required to run a dag for a specified date range and
        calls helper method to execute the tasks.",Initializes,all,components,required,to,run,a,dag,for,a,specified,date,range,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2461-L2536,test,for,t,in,self,.,dag,.,task_dict,.,values,(,),if,t,.,depends_on_past,],if,tasks_that_depend_on_past,:,raise,AirflowException,(,'You cannot backfill backwards because one or more tasks depend_on_past: {}',.,format,(,""",""",.,join,(,tasks_that_depend_on_past,),),),run_dates,=,run_dates,[,:,:,-,1,],if,len,(,run_dates,),==,0,:,self,.,log,.,info,(,"""No run dates were found for the given dates and dag interval.""",),return,# picklin',pickle_id,=,None,if,not,self,.,donot_pickle,and,self,.,executor,.,__class__,not,in,(,executors,.,LocalExecutor,",",executors,.,SequentialExecutor,),:,pickle,=,DagPickle,(,self,.,dag,),session,.,add,(,pickle,),session,.,commit,(,),pickle_id,=,pickle,.,id,executor,=,self,.,executor,executor,.,start,(,),ti_status,.,total_runs,=,len,(,run_dates,),# total dag runs in backfill,try,:,remaining_dates,=,ti_status,.,total_runs,while,remaining_dates,>,0,:,dates_to_process,=,[,run_date,for,run_date,in,run_dates,if,run_date,not,in,ti_status,.,executed_dag_run_dates,],self,.,_execute_for_run_dates,(,run_dates,=,dates_to_process,",",ti_status,=,ti_status,",",executor,=,executor,",",pickle_id,=,pickle_id,",",start_date,=,start_date,",",session,=,session,),remaining_dates,=,(,ti_status,.,total_runs,-,len,(,ti_status,.,executed_dag_run_dates,),),err,=,self,.,_collect_errors,(,ti_status,=,ti_status,",",session,=,session,),if,err,:,raise,AirflowException,(,err,),if,remaining_dates,>,0,:,self,.,log,.,info,(,"""max_active_runs limit for dag %s has been reached """,""" - waiting for other dag runs to finish""",",",self,.,dag_id,),time,.,sleep,(,self,.,delay_on_limit_secs,),except,(,KeyboardInterrupt,",",SystemExit,),:,self,.,log,.,warning,(,"""Backfill terminated by user.""",),# TODO: we will need to terminate running task instances and set the,# state to failed.,self,.,_set_unfinished_dag_runs_to_failed,(,ti_status,.,active_runs,),finally,:,session,.,commit,(,),executor,.,end,(,),self,.,log,.,info,(,"""Backfill done. Exiting.""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,and,calls,helper,method,to,execute,the,tasks,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/jobs.py,LocalTaskJob.heartbeat_callback,"def heartbeat_callback(self, session=None):
        """"""Self destruct task if state has been moved away from running externally""""""

        if self.terminating:
            # ensure termination if processes are created later
            self.task_runner.terminate()
            return

        self.task_instance.refresh_from_db()
        ti = self.task_instance

        fqdn = get_hostname()
        same_hostname = fqdn == ti.hostname
        same_process = ti.pid == os.getpid()

        if ti.state == State.RUNNING:
            if not same_hostname:
                self.log.warning(""The recorded hostname %s ""
                                 ""does not match this instance's hostname ""
                                 ""%s"", ti.hostname, fqdn)
                raise AirflowException(""Hostname of job runner does not match"")
            elif not same_process:
                current_pid = os.getpid()
                self.log.warning(""Recorded pid %s does not match ""
                                 ""the current pid %s"", ti.pid, current_pid)
                raise AirflowException(""PID of job runner does not match"")
        elif (
                self.task_runner.return_code() is None and
                hasattr(self.task_runner, 'process')
        ):
            self.log.warning(
                ""State of this instance has been externally set to %s. ""
                ""Taking the poison pill."",
                ti.state
            )
            self.task_runner.terminate()
            self.terminating = True",python,"def heartbeat_callback(self, session=None):
        """"""Self destruct task if state has been moved away from running externally""""""

        if self.terminating:
            # ensure termination if processes are created later
            self.task_runner.terminate()
            return

        self.task_instance.refresh_from_db()
        ti = self.task_instance

        fqdn = get_hostname()
        same_hostname = fqdn == ti.hostname
        same_process = ti.pid == os.getpid()

        if ti.state == State.RUNNING:
            if not same_hostname:
                self.log.warning(""The recorded hostname %s ""
                                 ""does not match this instance's hostname ""
                                 ""%s"", ti.hostname, fqdn)
                raise AirflowException(""Hostname of job runner does not match"")
            elif not same_process:
                current_pid = os.getpid()
                self.log.warning(""Recorded pid %s does not match ""
                                 ""the current pid %s"", ti.pid, current_pid)
                raise AirflowException(""PID of job runner does not match"")
        elif (
                self.task_runner.return_code() is None and
                hasattr(self.task_runner, 'process')
        ):
            self.log.warning(
                ""State of this instance has been externally set to %s. ""
                ""Taking the poison pill."",
                ti.state
            )
            self.task_runner.terminate()
            self.terminating = True",def,heartbeat_callback,(,self,",",session,=,None,),:,if,self,.,terminating,:,# ensure termination if processes are created later,self,.,task_runner,.,terminate,(,),return,self,.,task_instance,.,refresh_from_db,(,),ti,=,self,.,task_instance,fqdn,=,get_hostname,(,),same_hostname,=,fqdn,==,ti,.,hostname,same_process,=,ti,.,Self destruct task if state has been moved away from running externally,Self,destruct,task,if,state,has,been,moved,away,from,running,externally,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2637-L2673,test,pid,==,os,.,getpid,(,),if,ti,.,state,==,State,.,RUNNING,:,if,not,same_hostname,:,self,.,log,.,warning,(,"""The recorded hostname %s ""","""does not match this instance's hostname ""","""%s""",",",ti,.,hostname,",",fqdn,),raise,AirflowException,(,"""Hostname of job runner does not match""",),elif,not,same_process,:,current_pid,=,os,.,getpid,(,),self,.,log,.,warning,(,"""Recorded pid %s does not match ""","""the current pid %s""",",",ti,.,pid,",",current_pid,),raise,AirflowException,(,"""PID of job runner does not match""",),elif,(,self,.,task_runner,.,return_code,(,),is,None,and,hasattr,(,self,.,task_runner,",",'process',),),:,self,.,log,.,warning,(,"""State of this instance has been externally set to %s. ""","""Taking the poison pill.""",",",ti,.,state,),self,.,task_runner,.,terminate,(,),self,.,terminating,=,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook._get_client,"def _get_client(self, project_id):
        """"""
        Provides a client for interacting with the Cloud Spanner API.

        :param project_id: The ID of the  GCP project.
        :type project_id: str
        :return: google.cloud.spanner_v1.client.Client
        :rtype: object
        """"""
        if not self._client:
            self._client = Client(project=project_id, credentials=self._get_credentials())
        return self._client",python,"def _get_client(self, project_id):
        """"""
        Provides a client for interacting with the Cloud Spanner API.

        :param project_id: The ID of the  GCP project.
        :type project_id: str
        :return: google.cloud.spanner_v1.client.Client
        :rtype: object
        """"""
        if not self._client:
            self._client = Client(project=project_id, credentials=self._get_credentials())
        return self._client",def,_get_client,(,self,",",project_id,),:,if,not,self,.,_client,:,self,.,_client,=,Client,(,project,=,project_id,",",credentials,=,self,.,_get_credentials,(,),),return,self,.,_client,,,,,,,,,,,,,,,,,"Provides a client for interacting with the Cloud Spanner API.

        :param project_id: The ID of the  GCP project.
        :type project_id: str
        :return: google.cloud.spanner_v1.client.Client
        :rtype: object",Provides,a,client,for,interacting,with,the,Cloud,Spanner,API,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L41-L52,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.get_instance,"def get_instance(self, instance_id, project_id=None):
        """"""
        Gets information about a particular instance.

        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner
            database.  If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :return: google.cloud.spanner_v1.instance.Instance
        :rtype: object
        """"""
        instance = self._get_client(project_id=project_id).instance(instance_id=instance_id)
        if not instance.exists():
            return None
        return instance",python,"def get_instance(self, instance_id, project_id=None):
        """"""
        Gets information about a particular instance.

        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner
            database.  If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :return: google.cloud.spanner_v1.instance.Instance
        :rtype: object
        """"""
        instance = self._get_client(project_id=project_id).instance(instance_id=instance_id)
        if not instance.exists():
            return None
        return instance",def,get_instance,(,self,",",instance_id,",",project_id,=,None,),:,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,=,instance_id,),if,not,instance,.,exists,(,),:,return,None,return,instance,,,,,,,,,,,,"Gets information about a particular instance.

        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner
            database.  If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :return: google.cloud.spanner_v1.instance.Instance
        :rtype: object",Gets,information,about,a,particular,instance,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L55-L70,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook._apply_to_instance,"def _apply_to_instance(self, project_id, instance_id, configuration_name, node_count,
                           display_name, func):
        """"""
        Invokes a method on a given instance by applying a specified Callable.

        :param project_id: The ID of the  GCP project that owns the Cloud Spanner
            database.
        :type project_id: str
        :param instance_id: The ID of the instance.
        :type instance_id: str
        :param configuration_name: Name of the instance configuration defining how the
            instance will be created. Required for instances which do not yet exist.
        :type configuration_name: str
        :param node_count: (Optional) Number of nodes allocated to the instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the Cloud
            Console UI. (Must be between 4 and 30 characters.) If this value is not set
            in the constructor, will fall back to the instance ID.
        :type display_name: str
        :param func: Method of the instance to be called.
        :type func: Callable
        """"""
        # noinspection PyUnresolvedReferences
        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id, configuration_name=configuration_name,
            node_count=node_count, display_name=display_name)
        try:
            operation = func(instance)  # type: Operation
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e

        if operation:
            result = operation.result()
            self.log.info(result)",python,"def _apply_to_instance(self, project_id, instance_id, configuration_name, node_count,
                           display_name, func):
        """"""
        Invokes a method on a given instance by applying a specified Callable.

        :param project_id: The ID of the  GCP project that owns the Cloud Spanner
            database.
        :type project_id: str
        :param instance_id: The ID of the instance.
        :type instance_id: str
        :param configuration_name: Name of the instance configuration defining how the
            instance will be created. Required for instances which do not yet exist.
        :type configuration_name: str
        :param node_count: (Optional) Number of nodes allocated to the instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the Cloud
            Console UI. (Must be between 4 and 30 characters.) If this value is not set
            in the constructor, will fall back to the instance ID.
        :type display_name: str
        :param func: Method of the instance to be called.
        :type func: Callable
        """"""
        # noinspection PyUnresolvedReferences
        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id, configuration_name=configuration_name,
            node_count=node_count, display_name=display_name)
        try:
            operation = func(instance)  # type: Operation
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e

        if operation:
            result = operation.result()
            self.log.info(result)",def,_apply_to_instance,(,self,",",project_id,",",instance_id,",",configuration_name,",",node_count,",",display_name,",",func,),:,# noinspection PyUnresolvedReferences,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,=,instance_id,",",configuration_name,=,configuration_name,",",node_count,=,node_count,",",display_name,=,display_name,),try,:,operation,=,"Invokes a method on a given instance by applying a specified Callable.

        :param project_id: The ID of the  GCP project that owns the Cloud Spanner
            database.
        :type project_id: str
        :param instance_id: The ID of the instance.
        :type instance_id: str
        :param configuration_name: Name of the instance configuration defining how the
            instance will be created. Required for instances which do not yet exist.
        :type configuration_name: str
        :param node_count: (Optional) Number of nodes allocated to the instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the Cloud
            Console UI. (Must be between 4 and 30 characters.) If this value is not set
            in the constructor, will fall back to the instance ID.
        :type display_name: str
        :param func: Method of the instance to be called.
        :type func: Callable",Invokes,a,method,on,a,given,instance,by,applying,a,specified,Callable,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L72-L106,test,func,(,instance,),# type: Operation,except,GoogleAPICallError,as,e,:,self,.,log,.,error,(,'An error occurred: %s. Exiting.',",",e,.,message,),raise,e,if,operation,:,result,=,operation,.,result,(,),self,.,log,.,info,(,result,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.create_instance,"def create_instance(self, instance_id, configuration_name, node_count,
                        display_name, project_id=None):
        """"""
        Creates a new Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param configuration_name: The name of the instance configuration defining how the
            instance will be created. Possible configuration values can be retrieved via
            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list
        :type configuration_name: str
        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner
            instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the GCP
            Console. Must be between 4 and 30 characters.  If this value is not set in
            the constructor, the name falls back to the instance ID.
        :type display_name: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        self._apply_to_instance(project_id, instance_id, configuration_name,
                                node_count, display_name, lambda x: x.create())",python,"def create_instance(self, instance_id, configuration_name, node_count,
                        display_name, project_id=None):
        """"""
        Creates a new Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param configuration_name: The name of the instance configuration defining how the
            instance will be created. Possible configuration values can be retrieved via
            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list
        :type configuration_name: str
        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner
            instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the GCP
            Console. Must be between 4 and 30 characters.  If this value is not set in
            the constructor, the name falls back to the instance ID.
        :type display_name: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        self._apply_to_instance(project_id, instance_id, configuration_name,
                                node_count, display_name, lambda x: x.create())",def,create_instance,(,self,",",instance_id,",",configuration_name,",",node_count,",",display_name,",",project_id,=,None,),:,self,.,_apply_to_instance,(,project_id,",",instance_id,",",configuration_name,",",node_count,",",display_name,",",lambda,x,:,x,.,create,(,),),,,,,,,,,,,,"Creates a new Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param configuration_name: The name of the instance configuration defining how the
            instance will be created. Possible configuration values can be retrieved via
            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list
        :type configuration_name: str
        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner
            instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the GCP
            Console. Must be between 4 and 30 characters.  If this value is not set in
            the constructor, the name falls back to the instance ID.
        :type display_name: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Creates,a,new,Cloud,Spanner,instance,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L109-L133,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.update_instance,"def update_instance(self, instance_id, configuration_name, node_count,
                        display_name, project_id=None):
        """"""
        Updates an existing Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param configuration_name: The name of the instance configuration defining how the
            instance will be created. Possible configuration values can be retrieved via
            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list
        :type configuration_name: str
        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner
            instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the GCP
            Console. Must be between 4 and 30 characters. If this value is not set in
            the constructor, the name falls back to the instance ID.
        :type display_name: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        return self._apply_to_instance(project_id, instance_id, configuration_name,
                                       node_count, display_name, lambda x: x.update())",python,"def update_instance(self, instance_id, configuration_name, node_count,
                        display_name, project_id=None):
        """"""
        Updates an existing Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param configuration_name: The name of the instance configuration defining how the
            instance will be created. Possible configuration values can be retrieved via
            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list
        :type configuration_name: str
        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner
            instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the GCP
            Console. Must be between 4 and 30 characters. If this value is not set in
            the constructor, the name falls back to the instance ID.
        :type display_name: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        return self._apply_to_instance(project_id, instance_id, configuration_name,
                                       node_count, display_name, lambda x: x.update())",def,update_instance,(,self,",",instance_id,",",configuration_name,",",node_count,",",display_name,",",project_id,=,None,),:,return,self,.,_apply_to_instance,(,project_id,",",instance_id,",",configuration_name,",",node_count,",",display_name,",",lambda,x,:,x,.,update,(,),),,,,,,,,,,,"Updates an existing Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param configuration_name: The name of the instance configuration defining how the
            instance will be created. Possible configuration values can be retrieved via
            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list
        :type configuration_name: str
        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner
            instance.
        :type node_count: int
        :param display_name: (Optional) The display name for the instance in the GCP
            Console. Must be between 4 and 30 characters. If this value is not set in
            the constructor, the name falls back to the instance ID.
        :type display_name: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Updates,an,existing,Cloud,Spanner,instance,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L136-L160,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.delete_instance,"def delete_instance(self, instance_id, project_id=None):
        """"""
        Deletes an existing Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""

        instance = self._get_client(project_id=project_id).instance(instance_id)
        try:
            instance.delete()
            return
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e",python,"def delete_instance(self, instance_id, project_id=None):
        """"""
        Deletes an existing Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""

        instance = self._get_client(project_id=project_id).instance(instance_id)
        try:
            instance.delete()
            return
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e",def,delete_instance,(,self,",",instance_id,",",project_id,=,None,),:,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,),try,:,instance,.,delete,(,),return,except,GoogleAPICallError,as,e,:,self,.,log,.,error,(,'An error occurred: %s. Exiting.',",",e,.,message,),"Deletes an existing Cloud Spanner instance.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None",Deletes,an,existing,Cloud,Spanner,instance,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L163-L181,test,raise,e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.get_database,"def get_database(self, instance_id, database_id, project_id=None):
        """"""
        Retrieves a database in Cloud Spanner. If the database does not exist
        in the specified instance, it returns None.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Database object or None if database does not exist
        :rtype: google.cloud.spanner_v1.database.Database or None
        """"""

        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id)
        if not database.exists():
            return None
        else:
            return database",python,"def get_database(self, instance_id, database_id, project_id=None):
        """"""
        Retrieves a database in Cloud Spanner. If the database does not exist
        in the specified instance, it returns None.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Database object or None if database does not exist
        :rtype: google.cloud.spanner_v1.database.Database or None
        """"""

        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id)
        if not database.exists():
            return None
        else:
            return database",def,get_database,(,self,",",instance_id,",",database_id,",",project_id,=,None,),:,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,=,instance_id,),if,not,instance,.,exists,(,),:,raise,AirflowException,(,"""The instance {} does not exist in project {} !""",.,format,(,instance_id,",",project_id,),),database,"Retrieves a database in Cloud Spanner. If the database does not exist
        in the specified instance, it returns None.

        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Database object or None if database does not exist
        :rtype: google.cloud.spanner_v1.database.Database or None",Retrieves,a,database,in,Cloud,Spanner,.,If,the,database,does,not,exist,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L184-L209,test,=,instance,.,database,(,database_id,=,database_id,),if,not,database,.,exists,(,),:,return,None,else,:,return,database,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,in,the,specified,instance,it,returns,None,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.create_database,"def create_database(self, instance_id, database_id, ddl_statements, project_id=None):
        """"""
        Creates a new database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database to create in Cloud Spanner.
        :type database_id: str
        :param ddl_statements: The string list containing DDL for the new database.
        :type ddl_statements: list[str]
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :return: None
        """"""

        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id,
                                     ddl_statements=ddl_statements)
        try:
            operation = database.create()  # type: Operation
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e

        if operation:
            result = operation.result()
            self.log.info(result)
        return",python,"def create_database(self, instance_id, database_id, ddl_statements, project_id=None):
        """"""
        Creates a new database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database to create in Cloud Spanner.
        :type database_id: str
        :param ddl_statements: The string list containing DDL for the new database.
        :type ddl_statements: list[str]
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :return: None
        """"""

        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id,
                                     ddl_statements=ddl_statements)
        try:
            operation = database.create()  # type: Operation
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e

        if operation:
            result = operation.result()
            self.log.info(result)
        return",def,create_database,(,self,",",instance_id,",",database_id,",",ddl_statements,",",project_id,=,None,),:,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,=,instance_id,),if,not,instance,.,exists,(,),:,raise,AirflowException,(,"""The instance {} does not exist in project {} !""",.,format,(,instance_id,",",project_id,),"Creates a new database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database to create in Cloud Spanner.
        :type database_id: str
        :param ddl_statements: The string list containing DDL for the new database.
        :type ddl_statements: list[str]
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :return: None",Creates,a,new,database,in,Cloud,Spanner,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L212-L244,test,),database,=,instance,.,database,(,database_id,=,database_id,",",ddl_statements,=,ddl_statements,),try,:,operation,=,database,.,create,(,),# type: Operation,except,GoogleAPICallError,as,e,:,self,.,log,.,error,(,'An error occurred: %s. Exiting.',",",e,.,message,),raise,e,if,operation,:,result,=,operation,.,result,(,),self,.,log,.,info,(,result,),return,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.update_database,"def update_database(self, instance_id, database_id, ddl_statements,
                        project_id=None,
                        operation_id=None):
        """"""
        Updates DDL of a database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param ddl_statements: The string list containing DDL for the new database.
        :type ddl_statements: list[str]
        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :param operation_id: (Optional) The unique per database operation ID that can be
            specified to implement idempotency check.
        :type operation_id: str
        :return: None
        """"""

        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id)
        try:
            operation = database.update_ddl(
                ddl_statements=ddl_statements, operation_id=operation_id)
            if operation:
                result = operation.result()
                self.log.info(result)
            return
        except AlreadyExists as e:
            if e.code == 409 and operation_id in e.message:
                self.log.info(""Replayed update_ddl message - the operation id %s ""
                              ""was already done before."", operation_id)
                return
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e",python,"def update_database(self, instance_id, database_id, ddl_statements,
                        project_id=None,
                        operation_id=None):
        """"""
        Updates DDL of a database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param ddl_statements: The string list containing DDL for the new database.
        :type ddl_statements: list[str]
        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :param operation_id: (Optional) The unique per database operation ID that can be
            specified to implement idempotency check.
        :type operation_id: str
        :return: None
        """"""

        instance = self._get_client(project_id=project_id).instance(
            instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id)
        try:
            operation = database.update_ddl(
                ddl_statements=ddl_statements, operation_id=operation_id)
            if operation:
                result = operation.result()
                self.log.info(result)
            return
        except AlreadyExists as e:
            if e.code == 409 and operation_id in e.message:
                self.log.info(""Replayed update_ddl message - the operation id %s ""
                              ""was already done before."", operation_id)
                return
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e",def,update_database,(,self,",",instance_id,",",database_id,",",ddl_statements,",",project_id,=,None,",",operation_id,=,None,),:,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,=,instance_id,),if,not,instance,.,exists,(,),:,raise,AirflowException,(,"""The instance {} does not exist in project {} !""",.,format,(,"Updates DDL of a database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param ddl_statements: The string list containing DDL for the new database.
        :type ddl_statements: list[str]
        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :param operation_id: (Optional) The unique per database operation ID that can be
            specified to implement idempotency check.
        :type operation_id: str
        :return: None",Updates,DDL,of,a,database,in,Cloud,Spanner,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L247-L288,test,instance_id,",",project_id,),),database,=,instance,.,database,(,database_id,=,database_id,),try,:,operation,=,database,.,update_ddl,(,ddl_statements,=,ddl_statements,",",operation_id,=,operation_id,),if,operation,:,result,=,operation,.,result,(,),self,.,log,.,info,(,result,),return,except,AlreadyExists,as,e,:,if,e,.,code,==,409,and,operation_id,in,e,.,message,:,self,.,log,.,info,(,"""Replayed update_ddl message - the operation id %s ""","""was already done before.""",",",operation_id,),return,except,GoogleAPICallError,as,e,:,self,.,log,.,error,(,'An error occurred: %s. Exiting.',",",e,.,message,),raise,e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/gcp_spanner_hook.py,CloudSpannerHook.delete_database,"def delete_database(self, instance_id, database_id, project_id=None):
        """"""
        Drops a database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :return: True if everything succeeded
        :rtype: bool
        """"""

        instance = self._get_client(project_id=project_id).\
            instance(instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id)
        if not database.exists():
            self.log.info(""The database {} is already deleted from instance {}. ""
                          ""Exiting."".format(database_id, instance_id))
            return
        try:
            operation = database.drop()  # type: Operation
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e

        if operation:
            result = operation.result()
            self.log.info(result)
        return",python,"def delete_database(self, instance_id, database_id, project_id=None):
        """"""
        Drops a database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :return: True if everything succeeded
        :rtype: bool
        """"""

        instance = self._get_client(project_id=project_id).\
            instance(instance_id=instance_id)
        if not instance.exists():
            raise AirflowException(""The instance {} does not exist in project {} !"".
                                   format(instance_id, project_id))
        database = instance.database(database_id=database_id)
        if not database.exists():
            self.log.info(""The database {} is already deleted from instance {}. ""
                          ""Exiting."".format(database_id, instance_id))
            return
        try:
            operation = database.drop()  # type: Operation
        except GoogleAPICallError as e:
            self.log.error('An error occurred: %s. Exiting.', e.message)
            raise e

        if operation:
            result = operation.result()
            self.log.info(result)
        return",def,delete_database,(,self,",",instance_id,",",database_id,",",project_id,=,None,),:,instance,=,self,.,_get_client,(,project_id,=,project_id,),.,instance,(,instance_id,=,instance_id,),if,not,instance,.,exists,(,),:,raise,AirflowException,(,"""The instance {} does not exist in project {} !""",.,format,(,instance_id,",",project_id,),),database,"Drops a database in Cloud Spanner.

        :type project_id: str
        :param instance_id: The ID of the Cloud Spanner instance.
        :type instance_id: str
        :param database_id: The ID of the database in Cloud Spanner.
        :type database_id: str
        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner
            database. If set to None or missing, the default project_id from the GCP connection is used.
        :return: True if everything succeeded
        :rtype: bool",Drops,a,database,in,Cloud,Spanner,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L291-L325,test,=,instance,.,database,(,database_id,=,database_id,),if,not,database,.,exists,(,),:,self,.,log,.,info,(,"""The database {} is already deleted from instance {}. ""","""Exiting.""",.,format,(,database_id,",",instance_id,),),return,try,:,operation,=,database,.,drop,(,),# type: Operation,except,GoogleAPICallError,as,e,:,self,.,log,.,error,(,'An error occurred: %s. Exiting.',",",e,.,message,),raise,e,if,operation,:,result,=,operation,.,result,(,),self,.,log,.,info,(,result,),return,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/imap_attachment_sensor.py,ImapAttachmentSensor.poke,"def poke(self, context):
        """"""
        Pokes for a mail attachment on the mail server.

        :param context: The context that is being provided when poking.
        :type context: dict
        :return: True if attachment with the given name is present and False if not.
        :rtype: bool
        """"""
        self.log.info('Poking for %s', self.attachment_name)

        with ImapHook(imap_conn_id=self.conn_id) as imap_hook:
            return imap_hook.has_mail_attachment(
                name=self.attachment_name,
                mail_folder=self.mail_folder,
                check_regex=self.check_regex
            )",python,"def poke(self, context):
        """"""
        Pokes for a mail attachment on the mail server.

        :param context: The context that is being provided when poking.
        :type context: dict
        :return: True if attachment with the given name is present and False if not.
        :rtype: bool
        """"""
        self.log.info('Poking for %s', self.attachment_name)

        with ImapHook(imap_conn_id=self.conn_id) as imap_hook:
            return imap_hook.has_mail_attachment(
                name=self.attachment_name,
                mail_folder=self.mail_folder,
                check_regex=self.check_regex
            )",def,poke,(,self,",",context,),:,self,.,log,.,info,(,'Poking for %s',",",self,.,attachment_name,),with,ImapHook,(,imap_conn_id,=,self,.,conn_id,),as,imap_hook,:,return,imap_hook,.,has_mail_attachment,(,name,=,self,.,attachment_name,",",mail_folder,=,self,.,mail_folder,",",check_regex,=,self,"Pokes for a mail attachment on the mail server.

        :param context: The context that is being provided when poking.
        :type context: dict
        :return: True if attachment with the given name is present and False if not.
        :rtype: bool",Pokes,for,a,mail,attachment,on,the,mail,server,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/imap_attachment_sensor.py#L60-L76,test,.,check_regex,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/gcp_vision_operator.py,prepare_additional_parameters,"def prepare_additional_parameters(additional_properties, language_hints, web_detection_params):
    """"""
    Creates additional_properties parameter based on language_hints, web_detection_params and
    additional_properties parameters specified by the user
    """"""
    if language_hints is None and web_detection_params is None:
        return additional_properties

    if additional_properties is None:
        return {}

    merged_additional_parameters = deepcopy(additional_properties)

    if 'image_context' not in merged_additional_parameters:
        merged_additional_parameters['image_context'] = {}

    merged_additional_parameters['image_context']['language_hints'] = merged_additional_parameters[
        'image_context'
    ].get('language_hints', language_hints)
    merged_additional_parameters['image_context']['web_detection_params'] = merged_additional_parameters[
        'image_context'
    ].get('web_detection_params', web_detection_params)

    return merged_additional_parameters",python,"def prepare_additional_parameters(additional_properties, language_hints, web_detection_params):
    """"""
    Creates additional_properties parameter based on language_hints, web_detection_params and
    additional_properties parameters specified by the user
    """"""
    if language_hints is None and web_detection_params is None:
        return additional_properties

    if additional_properties is None:
        return {}

    merged_additional_parameters = deepcopy(additional_properties)

    if 'image_context' not in merged_additional_parameters:
        merged_additional_parameters['image_context'] = {}

    merged_additional_parameters['image_context']['language_hints'] = merged_additional_parameters[
        'image_context'
    ].get('language_hints', language_hints)
    merged_additional_parameters['image_context']['web_detection_params'] = merged_additional_parameters[
        'image_context'
    ].get('web_detection_params', web_detection_params)

    return merged_additional_parameters",def,prepare_additional_parameters,(,additional_properties,",",language_hints,",",web_detection_params,),:,if,language_hints,is,None,and,web_detection_params,is,None,:,return,additional_properties,if,additional_properties,is,None,:,return,{,},merged_additional_parameters,=,deepcopy,(,additional_properties,),if,'image_context',not,in,merged_additional_parameters,:,merged_additional_parameters,[,'image_context',],=,{,},merged_additional_parameters,[,'image_context',],"Creates additional_properties parameter based on language_hints, web_detection_params and
    additional_properties parameters specified by the user",Creates,additional_properties,parameter,based,on,language_hints,web_detection_params,and,additional_properties,parameters,specified,by,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/gcp_vision_operator.py#L1221-L1244,test,[,'language_hints',],=,merged_additional_parameters,[,'image_context',],.,get,(,'language_hints',",",language_hints,),merged_additional_parameters,[,'image_context',],[,'web_detection_params',],=,merged_additional_parameters,[,'image_context',],.,get,(,'web_detection_params',",",web_detection_params,),return,merged_additional_parameters,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,user,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/cassandra_hook.py,CassandraHook.get_conn,"def get_conn(self):
        """"""
        Returns a cassandra Session object
        """"""
        if self.session and not self.session.is_shutdown:
            return self.session
        self.session = self.cluster.connect(self.keyspace)
        return self.session",python,"def get_conn(self):
        """"""
        Returns a cassandra Session object
        """"""
        if self.session and not self.session.is_shutdown:
            return self.session
        self.session = self.cluster.connect(self.keyspace)
        return self.session",def,get_conn,(,self,),:,if,self,.,session,and,not,self,.,session,.,is_shutdown,:,return,self,.,session,self,.,session,=,self,.,cluster,.,connect,(,self,.,keyspace,),return,self,.,session,,,,,,,,,,,,,Returns a cassandra Session object,Returns,a,cassandra,Session,object,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/cassandra_hook.py#L108-L115,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/cassandra_hook.py,CassandraHook.table_exists,"def table_exists(self, table):
        """"""
        Checks if a table exists in Cassandra

        :param table: Target Cassandra table.
                      Use dot notation to target a specific keyspace.
        :type table: str
        """"""
        keyspace = self.keyspace
        if '.' in table:
            keyspace, table = table.split('.', 1)
        cluster_metadata = self.get_conn().cluster.metadata
        return (keyspace in cluster_metadata.keyspaces and
                table in cluster_metadata.keyspaces[keyspace].tables)",python,"def table_exists(self, table):
        """"""
        Checks if a table exists in Cassandra

        :param table: Target Cassandra table.
                      Use dot notation to target a specific keyspace.
        :type table: str
        """"""
        keyspace = self.keyspace
        if '.' in table:
            keyspace, table = table.split('.', 1)
        cluster_metadata = self.get_conn().cluster.metadata
        return (keyspace in cluster_metadata.keyspaces and
                table in cluster_metadata.keyspaces[keyspace].tables)",def,table_exists,(,self,",",table,),:,keyspace,=,self,.,keyspace,if,'.',in,table,:,keyspace,",",table,=,table,.,split,(,'.',",",1,),cluster_metadata,=,self,.,get_conn,(,),.,cluster,.,metadata,return,(,keyspace,in,cluster_metadata,.,keyspaces,and,table,in,cluster_metadata,"Checks if a table exists in Cassandra

        :param table: Target Cassandra table.
                      Use dot notation to target a specific keyspace.
        :type table: str",Checks,if,a,table,exists,in,Cassandra,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/cassandra_hook.py#L164-L177,test,.,keyspaces,[,keyspace,],.,tables,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/cassandra_hook.py,CassandraHook.record_exists,"def record_exists(self, table, keys):
        """"""
        Checks if a record exists in Cassandra

        :param table: Target Cassandra table.
                      Use dot notation to target a specific keyspace.
        :type table: str
        :param keys: The keys and their values to check the existence.
        :type keys: dict
        """"""
        keyspace = self.keyspace
        if '.' in table:
            keyspace, table = table.split('.', 1)
        ks = "" AND "".join(""{}=%({})s"".format(key, key) for key in keys.keys())
        cql = ""SELECT * FROM {keyspace}.{table} WHERE {keys}"".format(
            keyspace=keyspace, table=table, keys=ks)

        try:
            rs = self.get_conn().execute(cql, keys)
            return rs.one() is not None
        except Exception:
            return False",python,"def record_exists(self, table, keys):
        """"""
        Checks if a record exists in Cassandra

        :param table: Target Cassandra table.
                      Use dot notation to target a specific keyspace.
        :type table: str
        :param keys: The keys and their values to check the existence.
        :type keys: dict
        """"""
        keyspace = self.keyspace
        if '.' in table:
            keyspace, table = table.split('.', 1)
        ks = "" AND "".join(""{}=%({})s"".format(key, key) for key in keys.keys())
        cql = ""SELECT * FROM {keyspace}.{table} WHERE {keys}"".format(
            keyspace=keyspace, table=table, keys=ks)

        try:
            rs = self.get_conn().execute(cql, keys)
            return rs.one() is not None
        except Exception:
            return False",def,record_exists,(,self,",",table,",",keys,),:,keyspace,=,self,.,keyspace,if,'.',in,table,:,keyspace,",",table,=,table,.,split,(,'.',",",1,),ks,=,""" AND """,.,join,(,"""{}=%({})s""",.,format,(,key,",",key,),for,key,in,keys,.,keys,"Checks if a record exists in Cassandra

        :param table: Target Cassandra table.
                      Use dot notation to target a specific keyspace.
        :type table: str
        :param keys: The keys and their values to check the existence.
        :type keys: dict",Checks,if,a,record,exists,in,Cassandra,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/cassandra_hook.py#L179-L200,test,(,),),cql,=,"""SELECT * FROM {keyspace}.{table} WHERE {keys}""",.,format,(,keyspace,=,keyspace,",",table,=,table,",",keys,=,ks,),try,:,rs,=,self,.,get_conn,(,),.,execute,(,cql,",",keys,),return,rs,.,one,(,),is,not,None,except,Exception,:,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/spark_submit_hook.py,SparkSubmitHook._build_track_driver_status_command,"def _build_track_driver_status_command(self):
        """"""
        Construct the command to poll the driver status.

        :return: full command to be executed
        """"""
        connection_cmd = self._get_spark_binary_path()

        # The url ot the spark master
        connection_cmd += [""--master"", self._connection['master']]

        # The driver id so we can poll for its status
        if self._driver_id:
            connection_cmd += [""--status"", self._driver_id]
        else:
            raise AirflowException(
                ""Invalid status: attempted to poll driver "" +
                ""status but no driver id is known. Giving up."")

        self.log.debug(""Poll driver status cmd: %s"", connection_cmd)

        return connection_cmd",python,"def _build_track_driver_status_command(self):
        """"""
        Construct the command to poll the driver status.

        :return: full command to be executed
        """"""
        connection_cmd = self._get_spark_binary_path()

        # The url ot the spark master
        connection_cmd += [""--master"", self._connection['master']]

        # The driver id so we can poll for its status
        if self._driver_id:
            connection_cmd += [""--status"", self._driver_id]
        else:
            raise AirflowException(
                ""Invalid status: attempted to poll driver "" +
                ""status but no driver id is known. Giving up."")

        self.log.debug(""Poll driver status cmd: %s"", connection_cmd)

        return connection_cmd",def,_build_track_driver_status_command,(,self,),:,connection_cmd,=,self,.,_get_spark_binary_path,(,),# The url ot the spark master,connection_cmd,+=,[,"""--master""",",",self,.,_connection,[,'master',],],# The driver id so we can poll for its status,if,self,.,_driver_id,:,connection_cmd,+=,[,"""--status""",",",self,.,_driver_id,],else,:,raise,AirflowException,(,"""Invalid status: attempted to poll driver """,+,"""status but no driver id is known. Giving up.""",),self,.,"Construct the command to poll the driver status.

        :return: full command to be executed",Construct,the,command,to,poll,the,driver,status,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/spark_submit_hook.py#L299-L320,test,log,.,debug,(,"""Poll driver status cmd: %s""",",",connection_cmd,),return,connection_cmd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/spark_submit_hook.py,SparkSubmitHook.submit,"def submit(self, application="""", **kwargs):
        """"""
        Remote Popen to execute the spark-submit job

        :param application: Submitted application, jar or py file
        :type application: str
        :param kwargs: extra arguments to Popen (see subprocess.Popen)
        """"""
        spark_submit_cmd = self._build_spark_submit_command(application)

        if hasattr(self, '_env'):
            env = os.environ.copy()
            env.update(self._env)
            kwargs[""env""] = env

        self._submit_sp = subprocess.Popen(spark_submit_cmd,
                                           stdout=subprocess.PIPE,
                                           stderr=subprocess.STDOUT,
                                           bufsize=-1,
                                           universal_newlines=True,
                                           **kwargs)

        self._process_spark_submit_log(iter(self._submit_sp.stdout.readline, ''))
        returncode = self._submit_sp.wait()

        # Check spark-submit return code. In Kubernetes mode, also check the value
        # of exit code in the log, as it may differ.
        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):
            raise AirflowException(
                ""Cannot execute: {}. Error code is: {}."".format(
                    spark_submit_cmd, returncode
                )
            )

        self.log.debug(""Should track driver: {}"".format(self._should_track_driver_status))

        # We want the Airflow job to wait until the Spark driver is finished
        if self._should_track_driver_status:
            if self._driver_id is None:
                raise AirflowException(
                    ""No driver id is known: something went wrong when executing "" +
                    ""the spark submit command""
                )

            # We start with the SUBMITTED status as initial status
            self._driver_status = ""SUBMITTED""

            # Start tracking the driver status (blocking function)
            self._start_driver_status_tracking()

            if self._driver_status != ""FINISHED"":
                raise AirflowException(
                    ""ERROR : Driver {} badly exited with status {}""
                    .format(self._driver_id, self._driver_status)
                )",python,"def submit(self, application="""", **kwargs):
        """"""
        Remote Popen to execute the spark-submit job

        :param application: Submitted application, jar or py file
        :type application: str
        :param kwargs: extra arguments to Popen (see subprocess.Popen)
        """"""
        spark_submit_cmd = self._build_spark_submit_command(application)

        if hasattr(self, '_env'):
            env = os.environ.copy()
            env.update(self._env)
            kwargs[""env""] = env

        self._submit_sp = subprocess.Popen(spark_submit_cmd,
                                           stdout=subprocess.PIPE,
                                           stderr=subprocess.STDOUT,
                                           bufsize=-1,
                                           universal_newlines=True,
                                           **kwargs)

        self._process_spark_submit_log(iter(self._submit_sp.stdout.readline, ''))
        returncode = self._submit_sp.wait()

        # Check spark-submit return code. In Kubernetes mode, also check the value
        # of exit code in the log, as it may differ.
        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):
            raise AirflowException(
                ""Cannot execute: {}. Error code is: {}."".format(
                    spark_submit_cmd, returncode
                )
            )

        self.log.debug(""Should track driver: {}"".format(self._should_track_driver_status))

        # We want the Airflow job to wait until the Spark driver is finished
        if self._should_track_driver_status:
            if self._driver_id is None:
                raise AirflowException(
                    ""No driver id is known: something went wrong when executing "" +
                    ""the spark submit command""
                )

            # We start with the SUBMITTED status as initial status
            self._driver_status = ""SUBMITTED""

            # Start tracking the driver status (blocking function)
            self._start_driver_status_tracking()

            if self._driver_status != ""FINISHED"":
                raise AirflowException(
                    ""ERROR : Driver {} badly exited with status {}""
                    .format(self._driver_id, self._driver_status)
                )",def,submit,(,self,",",application,=,"""""",",",*,*,kwargs,),:,spark_submit_cmd,=,self,.,_build_spark_submit_command,(,application,),if,hasattr,(,self,",",'_env',),:,env,=,os,.,environ,.,copy,(,),env,.,update,(,self,.,_env,),kwargs,[,"""env""",],=,"Remote Popen to execute the spark-submit job

        :param application: Submitted application, jar or py file
        :type application: str
        :param kwargs: extra arguments to Popen (see subprocess.Popen)",Remote,Popen,to,execute,the,spark,-,submit,job,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/spark_submit_hook.py#L322-L376,test,env,self,.,_submit_sp,=,subprocess,.,Popen,(,spark_submit_cmd,",",stdout,=,subprocess,.,PIPE,",",stderr,=,subprocess,.,STDOUT,",",bufsize,=,-,1,",",universal_newlines,=,True,",",*,*,kwargs,),self,.,_process_spark_submit_log,(,iter,(,self,.,_submit_sp,.,stdout,.,readline,",",'',),),returncode,=,self,.,_submit_sp,.,wait,(,),"# Check spark-submit return code. In Kubernetes mode, also check the value","# of exit code in the log, as it may differ.",if,returncode,or,(,self,.,_is_kubernetes,and,self,.,_spark_exit_code,!=,0,),:,raise,AirflowException,(,"""Cannot execute: {}. Error code is: {}.""",.,format,(,spark_submit_cmd,",",returncode,),),self,.,log,.,debug,(,"""Should track driver: {}""",.,format,(,self,.,_should_track_driver_status,),),# We want the Airflow job to wait until the Spark driver is finished,if,self,.,_should_track_driver_status,:,if,self,.,_driver_id,is,None,:,raise,AirflowException,(,"""No driver id is known: something went wrong when executing """,+,"""the spark submit command""",),# We start with the SUBMITTED status as initial status,self,.,_driver_status,=,"""SUBMITTED""",# Start tracking the driver status (blocking function),self,.,_start_driver_status_tracking,(,),if,self,.,_driver_status,!=,"""FINISHED""",:,raise,AirflowException,(,"""ERROR : Driver {} badly exited with status {}""",.,format,(,self,.,_driver_id,",",self,.,_driver_status,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/spark_submit_hook.py,SparkSubmitHook._process_spark_submit_log,"def _process_spark_submit_log(self, itr):
        """"""
        Processes the log files and extracts useful information out of it.

        If the deploy-mode is 'client', log the output of the submit command as those
        are the output logs of the Spark worker directly.

        Remark: If the driver needs to be tracked for its status, the log-level of the
        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)

        :param itr: An iterator which iterates over the input of the subprocess
        """"""
        # Consume the iterator
        for line in itr:
            line = line.strip()
            # If we run yarn cluster mode, we want to extract the application id from
            # the logs so we can kill the application when we stop it unexpectedly
            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':
                match = re.search('(application[0-9_]+)', line)
                if match:
                    self._yarn_application_id = match.groups()[0]
                    self.log.info(""Identified spark driver id: %s"",
                                  self._yarn_application_id)

            # If we run Kubernetes cluster mode, we want to extract the driver pod id
            # from the logs so we can kill the application when we stop it unexpectedly
            elif self._is_kubernetes:
                match = re.search(r'\s*pod name: ((.+?)-([a-z0-9]+)-driver)', line)
                if match:
                    self._kubernetes_driver_pod = match.groups()[0]
                    self.log.info(""Identified spark driver pod: %s"",
                                  self._kubernetes_driver_pod)

                # Store the Spark Exit code
                match_exit_code = re.search(r'\s*Exit code: (\d+)', line)
                if match_exit_code:
                    self._spark_exit_code = int(match_exit_code.groups()[0])

            # if we run in standalone cluster mode and we want to track the driver status
            # we need to extract the driver id from the logs. This allows us to poll for
            # the status using the driver id. Also, we can kill the driver when needed.
            elif self._should_track_driver_status and not self._driver_id:
                match_driver_id = re.search(r'(driver-[0-9\-]+)', line)
                if match_driver_id:
                    self._driver_id = match_driver_id.groups()[0]
                    self.log.info(""identified spark driver id: {}""
                                  .format(self._driver_id))

            else:
                self.log.info(line)

            self.log.debug(""spark submit log: {}"".format(line))",python,"def _process_spark_submit_log(self, itr):
        """"""
        Processes the log files and extracts useful information out of it.

        If the deploy-mode is 'client', log the output of the submit command as those
        are the output logs of the Spark worker directly.

        Remark: If the driver needs to be tracked for its status, the log-level of the
        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)

        :param itr: An iterator which iterates over the input of the subprocess
        """"""
        # Consume the iterator
        for line in itr:
            line = line.strip()
            # If we run yarn cluster mode, we want to extract the application id from
            # the logs so we can kill the application when we stop it unexpectedly
            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':
                match = re.search('(application[0-9_]+)', line)
                if match:
                    self._yarn_application_id = match.groups()[0]
                    self.log.info(""Identified spark driver id: %s"",
                                  self._yarn_application_id)

            # If we run Kubernetes cluster mode, we want to extract the driver pod id
            # from the logs so we can kill the application when we stop it unexpectedly
            elif self._is_kubernetes:
                match = re.search(r'\s*pod name: ((.+?)-([a-z0-9]+)-driver)', line)
                if match:
                    self._kubernetes_driver_pod = match.groups()[0]
                    self.log.info(""Identified spark driver pod: %s"",
                                  self._kubernetes_driver_pod)

                # Store the Spark Exit code
                match_exit_code = re.search(r'\s*Exit code: (\d+)', line)
                if match_exit_code:
                    self._spark_exit_code = int(match_exit_code.groups()[0])

            # if we run in standalone cluster mode and we want to track the driver status
            # we need to extract the driver id from the logs. This allows us to poll for
            # the status using the driver id. Also, we can kill the driver when needed.
            elif self._should_track_driver_status and not self._driver_id:
                match_driver_id = re.search(r'(driver-[0-9\-]+)', line)
                if match_driver_id:
                    self._driver_id = match_driver_id.groups()[0]
                    self.log.info(""identified spark driver id: {}""
                                  .format(self._driver_id))

            else:
                self.log.info(line)

            self.log.debug(""spark submit log: {}"".format(line))",def,_process_spark_submit_log,(,self,",",itr,),:,# Consume the iterator,for,line,in,itr,:,line,=,line,.,strip,(,),"# If we run yarn cluster mode, we want to extract the application id from",# the logs so we can kill the application when we stop it unexpectedly,if,self,.,_is_yarn,and,self,.,_connection,[,'deploy_mode',],==,'cluster',:,match,=,re,.,search,(,'(application[0-9_]+)',",",line,),if,match,:,self,.,"Processes the log files and extracts useful information out of it.

        If the deploy-mode is 'client', log the output of the submit command as those
        are the output logs of the Spark worker directly.

        Remark: If the driver needs to be tracked for its status, the log-level of the
        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)

        :param itr: An iterator which iterates over the input of the subprocess",Processes,the,log,files,and,extracts,useful,information,out,of,it,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/spark_submit_hook.py#L378-L429,test,_yarn_application_id,=,match,.,groups,(,),[,0,],self,.,log,.,info,(,"""Identified spark driver id: %s""",",",self,.,_yarn_application_id,),"# If we run Kubernetes cluster mode, we want to extract the driver pod id",# from the logs so we can kill the application when we stop it unexpectedly,elif,self,.,_is_kubernetes,:,match,=,re,.,search,(,r'\s*pod name: ((.+?)-([a-z0-9]+)-driver)',",",line,),if,match,:,self,.,_kubernetes_driver_pod,=,match,.,groups,(,),[,0,],self,.,log,.,info,(,"""Identified spark driver pod: %s""",",",self,.,_kubernetes_driver_pod,),# Store the Spark Exit code,match_exit_code,=,re,.,search,(,r'\s*Exit code: (\d+)',",",line,),if,match_exit_code,:,self,.,_spark_exit_code,=,int,(,match_exit_code,.,groups,(,),[,0,],),# if we run in standalone cluster mode and we want to track the driver status,# we need to extract the driver id from the logs. This allows us to poll for,"# the status using the driver id. Also, we can kill the driver when needed.",elif,self,.,_should_track_driver_status,and,not,self,.,_driver_id,:,match_driver_id,=,re,.,search,(,r'(driver-[0-9\-]+)',",",line,),if,match_driver_id,:,self,.,_driver_id,=,match_driver_id,.,groups,(,),[,0,],self,.,log,.,info,(,"""identified spark driver id: {}""",.,format,(,self,.,_driver_id,),),else,:,self,.,log,.,info,(,line,),self,.,log,.,debug,(,"""spark submit log: {}""",.,format,(,line,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/spark_submit_hook.py,SparkSubmitHook._process_spark_status_log,"def _process_spark_status_log(self, itr):
        """"""
        parses the logs of the spark driver status query process

        :param itr: An iterator which iterates over the input of the subprocess
        """"""
        # Consume the iterator
        for line in itr:
            line = line.strip()

            # Check if the log line is about the driver status and extract the status.
            if ""driverState"" in line:
                self._driver_status = line.split(' : ')[1] \
                    .replace(',', '').replace('\""', '').strip()

            self.log.debug(""spark driver status log: {}"".format(line))",python,"def _process_spark_status_log(self, itr):
        """"""
        parses the logs of the spark driver status query process

        :param itr: An iterator which iterates over the input of the subprocess
        """"""
        # Consume the iterator
        for line in itr:
            line = line.strip()

            # Check if the log line is about the driver status and extract the status.
            if ""driverState"" in line:
                self._driver_status = line.split(' : ')[1] \
                    .replace(',', '').replace('\""', '').strip()

            self.log.debug(""spark driver status log: {}"".format(line))",def,_process_spark_status_log,(,self,",",itr,),:,# Consume the iterator,for,line,in,itr,:,line,=,line,.,strip,(,),# Check if the log line is about the driver status and extract the status.,if,"""driverState""",in,line,:,self,.,_driver_status,=,line,.,split,(,' : ',),[,1,],.,replace,(,"','",",",'',),.,replace,(,"'\""'",",","parses the logs of the spark driver status query process

        :param itr: An iterator which iterates over the input of the subprocess",parses,the,logs,of,the,spark,driver,status,query,process,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/spark_submit_hook.py#L431-L446,test,'',),.,strip,(,),self,.,log,.,debug,(,"""spark driver status log: {}""",.,format,(,line,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/task/task_runner/__init__.py,get_task_runner,"def get_task_runner(local_task_job):
    """"""
    Get the task runner that can be used to run the given job.

    :param local_task_job: The LocalTaskJob associated with the TaskInstance
        that needs to be executed.
    :type local_task_job: airflow.jobs.LocalTaskJob
    :return: The task runner to use to run the task.
    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner
    """"""
    if _TASK_RUNNER == ""StandardTaskRunner"":
        return StandardTaskRunner(local_task_job)
    elif _TASK_RUNNER == ""CgroupTaskRunner"":
        from airflow.contrib.task_runner.cgroup_task_runner import CgroupTaskRunner
        return CgroupTaskRunner(local_task_job)
    else:
        raise AirflowException(""Unknown task runner type {}"".format(_TASK_RUNNER))",python,"def get_task_runner(local_task_job):
    """"""
    Get the task runner that can be used to run the given job.

    :param local_task_job: The LocalTaskJob associated with the TaskInstance
        that needs to be executed.
    :type local_task_job: airflow.jobs.LocalTaskJob
    :return: The task runner to use to run the task.
    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner
    """"""
    if _TASK_RUNNER == ""StandardTaskRunner"":
        return StandardTaskRunner(local_task_job)
    elif _TASK_RUNNER == ""CgroupTaskRunner"":
        from airflow.contrib.task_runner.cgroup_task_runner import CgroupTaskRunner
        return CgroupTaskRunner(local_task_job)
    else:
        raise AirflowException(""Unknown task runner type {}"".format(_TASK_RUNNER))",def,get_task_runner,(,local_task_job,),:,if,_TASK_RUNNER,==,"""StandardTaskRunner""",:,return,StandardTaskRunner,(,local_task_job,),elif,_TASK_RUNNER,==,"""CgroupTaskRunner""",:,from,airflow,.,contrib,.,task_runner,.,cgroup_task_runner,import,CgroupTaskRunner,return,CgroupTaskRunner,(,local_task_job,),else,:,raise,AirflowException,(,"""Unknown task runner type {}""",.,format,(,_TASK_RUNNER,),),,,,,"Get the task runner that can be used to run the given job.

    :param local_task_job: The LocalTaskJob associated with the TaskInstance
        that needs to be executed.
    :type local_task_job: airflow.jobs.LocalTaskJob
    :return: The task runner to use to run the task.
    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner",Get,the,task,runner,that,can,be,used,to,run,the,given,job,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/task/task_runner/__init__.py#L27-L43,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/awsbatch_operator.py,AWSBatchOperator._wait_for_task_ended,"def _wait_for_task_ended(self):
        """"""
        Try to use a waiter from the below pull request

            * https://github.com/boto/botocore/pull/1307

        If the waiter is not available apply a exponential backoff

            * docs.aws.amazon.com/general/latest/gr/api-retries.html
        """"""
        try:
            waiter = self.client.get_waiter('job_execution_complete')
            waiter.config.max_attempts = sys.maxsize  # timeout is managed by airflow
            waiter.wait(jobs=[self.jobId])
        except ValueError:
            # If waiter not available use expo
            retry = True
            retries = 0

            while retries < self.max_retries and retry:
                self.log.info('AWS Batch retry in the next %s seconds', retries)
                response = self.client.describe_jobs(
                    jobs=[self.jobId]
                )
                if response['jobs'][-1]['status'] in ['SUCCEEDED', 'FAILED']:
                    retry = False

                sleep(1 + pow(retries * 0.1, 2))
                retries += 1",python,"def _wait_for_task_ended(self):
        """"""
        Try to use a waiter from the below pull request

            * https://github.com/boto/botocore/pull/1307

        If the waiter is not available apply a exponential backoff

            * docs.aws.amazon.com/general/latest/gr/api-retries.html
        """"""
        try:
            waiter = self.client.get_waiter('job_execution_complete')
            waiter.config.max_attempts = sys.maxsize  # timeout is managed by airflow
            waiter.wait(jobs=[self.jobId])
        except ValueError:
            # If waiter not available use expo
            retry = True
            retries = 0

            while retries < self.max_retries and retry:
                self.log.info('AWS Batch retry in the next %s seconds', retries)
                response = self.client.describe_jobs(
                    jobs=[self.jobId]
                )
                if response['jobs'][-1]['status'] in ['SUCCEEDED', 'FAILED']:
                    retry = False

                sleep(1 + pow(retries * 0.1, 2))
                retries += 1",def,_wait_for_task_ended,(,self,),:,try,:,waiter,=,self,.,client,.,get_waiter,(,'job_execution_complete',),waiter,.,config,.,max_attempts,=,sys,.,maxsize,# timeout is managed by airflow,waiter,.,wait,(,jobs,=,[,self,.,jobId,],),except,ValueError,:,# If waiter not available use expo,retry,=,True,retries,=,0,while,retries,"Try to use a waiter from the below pull request

            * https://github.com/boto/botocore/pull/1307

        If the waiter is not available apply a exponential backoff

            * docs.aws.amazon.com/general/latest/gr/api-retries.html",Try,to,use,a,waiter,from,the,below,pull,request,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/awsbatch_operator.py#L117-L145,test,<,self,.,max_retries,and,retry,:,self,.,log,.,info,(,'AWS Batch retry in the next %s seconds',",",retries,),response,=,self,.,client,.,describe_jobs,(,jobs,=,[,self,.,jobId,],),if,response,[,'jobs',],[,-,1,],[,'status',],in,[,'SUCCEEDED',",",'FAILED',],:,retry,=,False,sleep,(,1,+,pow,(,retries,*,0.1,",",2,),),retries,+=,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mysql_to_gcs.py,MySqlToGoogleCloudStorageOperator._query_mysql,"def _query_mysql(self):
        """"""
        Queries mysql and returns a cursor to the results.
        """"""
        mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)
        conn = mysql.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql)
        return cursor",python,"def _query_mysql(self):
        """"""
        Queries mysql and returns a cursor to the results.
        """"""
        mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)
        conn = mysql.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql)
        return cursor",def,_query_mysql,(,self,),:,mysql,=,MySqlHook,(,mysql_conn_id,=,self,.,mysql_conn_id,),conn,=,mysql,.,get_conn,(,),cursor,=,conn,.,cursor,(,),cursor,.,execute,(,self,.,sql,),return,cursor,,,,,,,,,,,,,Queries mysql and returns a cursor to the results.,Queries,mysql,and,returns,a,cursor,to,the,results,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L134-L142,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mysql_to_gcs.py,MySqlToGoogleCloudStorageOperator._configure_csv_file,"def _configure_csv_file(self, file_handle, schema):
        """"""Configure a csv writer with the file_handle and write schema
        as headers for the new file.
        """"""
        csv_writer = csv.writer(file_handle, encoding='utf-8',
                                delimiter=self.field_delimiter)
        csv_writer.writerow(schema)
        return csv_writer",python,"def _configure_csv_file(self, file_handle, schema):
        """"""Configure a csv writer with the file_handle and write schema
        as headers for the new file.
        """"""
        csv_writer = csv.writer(file_handle, encoding='utf-8',
                                delimiter=self.field_delimiter)
        csv_writer.writerow(schema)
        return csv_writer",def,_configure_csv_file,(,self,",",file_handle,",",schema,),:,csv_writer,=,csv,.,writer,(,file_handle,",",encoding,=,'utf-8',",",delimiter,=,self,.,field_delimiter,),csv_writer,.,writerow,(,schema,),return,csv_writer,,,,,,,,,,,,,,,,,"Configure a csv writer with the file_handle and write schema
        as headers for the new file.",Configure,a,csv,writer,with,the,file_handle,and,write,schema,as,headers,for,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L201-L208,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,new,file,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mysql_to_gcs.py,MySqlToGoogleCloudStorageOperator._write_local_schema_file,"def _write_local_schema_file(self, cursor):
        """"""
        Takes a cursor, and writes the BigQuery schema in .json format for the
        results to a local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.
        """"""
        schema_str = None
        schema_file_mime_type = 'application/json'
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)
        if self.schema is not None and isinstance(self.schema, string_types):
            schema_str = self.schema.encode('utf-8')
        elif self.schema is not None and isinstance(self.schema, list):
            schema_str = json.dumps(self.schema).encode('utf-8')
        else:
            schema = []
            for field in cursor.description:
                # See PEP 249 for details about the description tuple.
                field_name = field[0]
                field_type = self.type_map(field[1])
                # Always allow TIMESTAMP to be nullable. MySQLdb returns None types
                # for required fields because some MySQL timestamps can't be
                # represented by Python's datetime (e.g. 0000-00-00 00:00:00).
                if field[6] or field_type == 'TIMESTAMP':
                    field_mode = 'NULLABLE'
                else:
                    field_mode = 'REQUIRED'
                schema.append({
                    'name': field_name,
                    'type': field_type,
                    'mode': field_mode,
                })
            schema_str = json.dumps(schema, sort_keys=True).encode('utf-8')
        tmp_schema_file_handle.write(schema_str)

        self.log.info('Using schema for %s: %s', self.schema_filename, schema_str)
        schema_file_to_upload = {
            'file_name': self.schema_filename,
            'file_handle': tmp_schema_file_handle,
            'file_mime_type': schema_file_mime_type
        }
        return schema_file_to_upload",python,"def _write_local_schema_file(self, cursor):
        """"""
        Takes a cursor, and writes the BigQuery schema in .json format for the
        results to a local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.
        """"""
        schema_str = None
        schema_file_mime_type = 'application/json'
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)
        if self.schema is not None and isinstance(self.schema, string_types):
            schema_str = self.schema.encode('utf-8')
        elif self.schema is not None and isinstance(self.schema, list):
            schema_str = json.dumps(self.schema).encode('utf-8')
        else:
            schema = []
            for field in cursor.description:
                # See PEP 249 for details about the description tuple.
                field_name = field[0]
                field_type = self.type_map(field[1])
                # Always allow TIMESTAMP to be nullable. MySQLdb returns None types
                # for required fields because some MySQL timestamps can't be
                # represented by Python's datetime (e.g. 0000-00-00 00:00:00).
                if field[6] or field_type == 'TIMESTAMP':
                    field_mode = 'NULLABLE'
                else:
                    field_mode = 'REQUIRED'
                schema.append({
                    'name': field_name,
                    'type': field_type,
                    'mode': field_mode,
                })
            schema_str = json.dumps(schema, sort_keys=True).encode('utf-8')
        tmp_schema_file_handle.write(schema_str)

        self.log.info('Using schema for %s: %s', self.schema_filename, schema_str)
        schema_file_to_upload = {
            'file_name': self.schema_filename,
            'file_handle': tmp_schema_file_handle,
            'file_mime_type': schema_file_mime_type
        }
        return schema_file_to_upload",def,_write_local_schema_file,(,self,",",cursor,),:,schema_str,=,None,schema_file_mime_type,=,'application/json',tmp_schema_file_handle,=,NamedTemporaryFile,(,delete,=,True,),if,self,.,schema,is,not,None,and,isinstance,(,self,.,schema,",",string_types,),:,schema_str,=,self,.,schema,.,encode,(,'utf-8',),elif,self,.,"Takes a cursor, and writes the BigQuery schema in .json format for the
        results to a local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.",Takes,a,cursor,and,writes,the,BigQuery,schema,in,.,json,format,for,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L210-L253,test,schema,is,not,None,and,isinstance,(,self,.,schema,",",list,),:,schema_str,=,json,.,dumps,(,self,.,schema,),.,encode,(,'utf-8',),else,:,schema,=,[,],for,field,in,cursor,.,description,:,# See PEP 249 for details about the description tuple.,field_name,=,field,[,0,],field_type,=,self,.,type_map,(,field,[,1,],),# Always allow TIMESTAMP to be nullable. MySQLdb returns None types,# for required fields because some MySQL timestamps can't be,# represented by Python's datetime (e.g. 0000-00-00 00:00:00).,if,field,[,6,],or,field_type,==,'TIMESTAMP',:,field_mode,=,'NULLABLE',else,:,field_mode,=,'REQUIRED',schema,.,append,(,{,'name',:,field_name,",",'type',:,field_type,",",'mode',:,field_mode,",",},),schema_str,=,json,.,dumps,(,schema,",",sort_keys,=,True,),.,encode,(,'utf-8',),tmp_schema_file_handle,.,write,(,schema_str,),self,.,log,.,info,(,'Using schema for %s: %s',",",self,.,schema_filename,",",schema_str,),schema_file_to_upload,=,{,'file_name',:,self,.,schema_filename,",",'file_handle',:,tmp_schema_file_handle,",",'file_mime_type',:,schema_file_mime_type,},return,schema_file_to_upload,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,results,to,a,local,file,system,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mysql_to_gcs.py,MySqlToGoogleCloudStorageOperator._get_col_type_dict,"def _get_col_type_dict(self):
        """"""
        Return a dict of column name and column type based on self.schema if not None.
        """"""
        schema = []
        if isinstance(self.schema, string_types):
            schema = json.loads(self.schema)
        elif isinstance(self.schema, list):
            schema = self.schema
        elif self.schema is not None:
            self.log.warn('Using default schema due to unexpected type.'
                          'Should be a string or list.')

        col_type_dict = {}
        try:
            col_type_dict = {col['name']: col['type'] for col in schema}
        except KeyError:
            self.log.warn('Using default schema due to missing name or type. Please '
                          'refer to: https://cloud.google.com/bigquery/docs/schemas'
                          '#specifying_a_json_schema_file')
        return col_type_dict",python,"def _get_col_type_dict(self):
        """"""
        Return a dict of column name and column type based on self.schema if not None.
        """"""
        schema = []
        if isinstance(self.schema, string_types):
            schema = json.loads(self.schema)
        elif isinstance(self.schema, list):
            schema = self.schema
        elif self.schema is not None:
            self.log.warn('Using default schema due to unexpected type.'
                          'Should be a string or list.')

        col_type_dict = {}
        try:
            col_type_dict = {col['name']: col['type'] for col in schema}
        except KeyError:
            self.log.warn('Using default schema due to missing name or type. Please '
                          'refer to: https://cloud.google.com/bigquery/docs/schemas'
                          '#specifying_a_json_schema_file')
        return col_type_dict",def,_get_col_type_dict,(,self,),:,schema,=,[,],if,isinstance,(,self,.,schema,",",string_types,),:,schema,=,json,.,loads,(,self,.,schema,),elif,isinstance,(,self,.,schema,",",list,),:,schema,=,self,.,schema,elif,self,.,schema,is,not,None,Return a dict of column name and column type based on self.schema if not None.,Return,a,dict,of,column,name,and,column,type,based,on,self,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L290-L310,test,:,self,.,log,.,warn,(,'Using default schema due to unexpected type.','Should be a string or list.',),col_type_dict,=,{,},try,:,col_type_dict,=,{,col,[,'name',],:,col,[,'type',],for,col,in,schema,},except,KeyError,:,self,.,log,.,warn,(,'Using default schema due to missing name or type. Please ','refer to: https://cloud.google.com/bigquery/docs/schemas','#specifying_a_json_schema_file',),return,col_type_dict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,schema,if,not,None,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/mysql_to_gcs.py,MySqlToGoogleCloudStorageOperator.type_map,"def type_map(cls, mysql_type):
        """"""
        Helper function that maps from MySQL fields to BigQuery fields. Used
        when a schema_filename is set.
        """"""
        d = {
            FIELD_TYPE.INT24: 'INTEGER',
            FIELD_TYPE.TINY: 'INTEGER',
            FIELD_TYPE.BIT: 'INTEGER',
            FIELD_TYPE.DATETIME: 'TIMESTAMP',
            FIELD_TYPE.DATE: 'TIMESTAMP',
            FIELD_TYPE.DECIMAL: 'FLOAT',
            FIELD_TYPE.NEWDECIMAL: 'FLOAT',
            FIELD_TYPE.DOUBLE: 'FLOAT',
            FIELD_TYPE.FLOAT: 'FLOAT',
            FIELD_TYPE.LONG: 'INTEGER',
            FIELD_TYPE.LONGLONG: 'INTEGER',
            FIELD_TYPE.SHORT: 'INTEGER',
            FIELD_TYPE.TIMESTAMP: 'TIMESTAMP',
            FIELD_TYPE.YEAR: 'INTEGER',
        }
        return d[mysql_type] if mysql_type in d else 'STRING'",python,"def type_map(cls, mysql_type):
        """"""
        Helper function that maps from MySQL fields to BigQuery fields. Used
        when a schema_filename is set.
        """"""
        d = {
            FIELD_TYPE.INT24: 'INTEGER',
            FIELD_TYPE.TINY: 'INTEGER',
            FIELD_TYPE.BIT: 'INTEGER',
            FIELD_TYPE.DATETIME: 'TIMESTAMP',
            FIELD_TYPE.DATE: 'TIMESTAMP',
            FIELD_TYPE.DECIMAL: 'FLOAT',
            FIELD_TYPE.NEWDECIMAL: 'FLOAT',
            FIELD_TYPE.DOUBLE: 'FLOAT',
            FIELD_TYPE.FLOAT: 'FLOAT',
            FIELD_TYPE.LONG: 'INTEGER',
            FIELD_TYPE.LONGLONG: 'INTEGER',
            FIELD_TYPE.SHORT: 'INTEGER',
            FIELD_TYPE.TIMESTAMP: 'TIMESTAMP',
            FIELD_TYPE.YEAR: 'INTEGER',
        }
        return d[mysql_type] if mysql_type in d else 'STRING'",def,type_map,(,cls,",",mysql_type,),:,d,=,{,FIELD_TYPE,.,INT24,:,'INTEGER',",",FIELD_TYPE,.,TINY,:,'INTEGER',",",FIELD_TYPE,.,BIT,:,'INTEGER',",",FIELD_TYPE,.,DATETIME,:,'TIMESTAMP',",",FIELD_TYPE,.,DATE,:,'TIMESTAMP',",",FIELD_TYPE,.,DECIMAL,:,'FLOAT',",",FIELD_TYPE,.,NEWDECIMAL,:,'FLOAT',"Helper function that maps from MySQL fields to BigQuery fields. Used
        when a schema_filename is set.",Helper,function,that,maps,from,MySQL,fields,to,BigQuery,fields,.,Used,when,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L313-L334,test,",",FIELD_TYPE,.,DOUBLE,:,'FLOAT',",",FIELD_TYPE,.,FLOAT,:,'FLOAT',",",FIELD_TYPE,.,LONG,:,'INTEGER',",",FIELD_TYPE,.,LONGLONG,:,'INTEGER',",",FIELD_TYPE,.,SHORT,:,'INTEGER',",",FIELD_TYPE,.,TIMESTAMP,:,'TIMESTAMP',",",FIELD_TYPE,.,YEAR,:,'INTEGER',",",},return,d,[,mysql_type,],if,mysql_type,in,d,else,'STRING',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,a,schema_filename,is,set,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/sqoop_operator.py,SqoopOperator.execute,"def execute(self, context):
        """"""
        Execute sqoop job
        """"""
        self.hook = SqoopHook(
            conn_id=self.conn_id,
            verbose=self.verbose,
            num_mappers=self.num_mappers,
            hcatalog_database=self.hcatalog_database,
            hcatalog_table=self.hcatalog_table,
            properties=self.properties
        )

        if self.cmd_type == 'export':
            self.hook.export_table(
                table=self.table,
                export_dir=self.export_dir,
                input_null_string=self.input_null_string,
                input_null_non_string=self.input_null_non_string,
                staging_table=self.staging_table,
                clear_staging_table=self.clear_staging_table,
                enclosed_by=self.enclosed_by,
                escaped_by=self.escaped_by,
                input_fields_terminated_by=self.input_fields_terminated_by,
                input_lines_terminated_by=self.input_lines_terminated_by,
                input_optionally_enclosed_by=self.input_optionally_enclosed_by,
                batch=self.batch,
                relaxed_isolation=self.relaxed_isolation,
                extra_export_options=self.extra_export_options)
        elif self.cmd_type == 'import':
            # add create hcatalog table to extra import options if option passed
            # if new params are added to constructor can pass them in here
            # so don't modify sqoop_hook for each param
            if self.create_hcatalog_table:
                self.extra_import_options['create-hcatalog-table'] = ''

            if self.table and self.query:
                raise AirflowException(
                    'Cannot specify query and table together. Need to specify either or.'
                )

            if self.table:
                self.hook.import_table(
                    table=self.table,
                    target_dir=self.target_dir,
                    append=self.append,
                    file_type=self.file_type,
                    columns=self.columns,
                    split_by=self.split_by,
                    where=self.where,
                    direct=self.direct,
                    driver=self.driver,
                    extra_import_options=self.extra_import_options)
            elif self.query:
                self.hook.import_query(
                    query=self.query,
                    target_dir=self.target_dir,
                    append=self.append,
                    file_type=self.file_type,
                    split_by=self.split_by,
                    direct=self.direct,
                    driver=self.driver,
                    extra_import_options=self.extra_import_options)
            else:
                raise AirflowException(
                    ""Provide query or table parameter to import using Sqoop""
                )
        else:
            raise AirflowException(""cmd_type should be 'import' or 'export'"")",python,"def execute(self, context):
        """"""
        Execute sqoop job
        """"""
        self.hook = SqoopHook(
            conn_id=self.conn_id,
            verbose=self.verbose,
            num_mappers=self.num_mappers,
            hcatalog_database=self.hcatalog_database,
            hcatalog_table=self.hcatalog_table,
            properties=self.properties
        )

        if self.cmd_type == 'export':
            self.hook.export_table(
                table=self.table,
                export_dir=self.export_dir,
                input_null_string=self.input_null_string,
                input_null_non_string=self.input_null_non_string,
                staging_table=self.staging_table,
                clear_staging_table=self.clear_staging_table,
                enclosed_by=self.enclosed_by,
                escaped_by=self.escaped_by,
                input_fields_terminated_by=self.input_fields_terminated_by,
                input_lines_terminated_by=self.input_lines_terminated_by,
                input_optionally_enclosed_by=self.input_optionally_enclosed_by,
                batch=self.batch,
                relaxed_isolation=self.relaxed_isolation,
                extra_export_options=self.extra_export_options)
        elif self.cmd_type == 'import':
            # add create hcatalog table to extra import options if option passed
            # if new params are added to constructor can pass them in here
            # so don't modify sqoop_hook for each param
            if self.create_hcatalog_table:
                self.extra_import_options['create-hcatalog-table'] = ''

            if self.table and self.query:
                raise AirflowException(
                    'Cannot specify query and table together. Need to specify either or.'
                )

            if self.table:
                self.hook.import_table(
                    table=self.table,
                    target_dir=self.target_dir,
                    append=self.append,
                    file_type=self.file_type,
                    columns=self.columns,
                    split_by=self.split_by,
                    where=self.where,
                    direct=self.direct,
                    driver=self.driver,
                    extra_import_options=self.extra_import_options)
            elif self.query:
                self.hook.import_query(
                    query=self.query,
                    target_dir=self.target_dir,
                    append=self.append,
                    file_type=self.file_type,
                    split_by=self.split_by,
                    direct=self.direct,
                    driver=self.driver,
                    extra_import_options=self.extra_import_options)
            else:
                raise AirflowException(
                    ""Provide query or table parameter to import using Sqoop""
                )
        else:
            raise AirflowException(""cmd_type should be 'import' or 'export'"")",def,execute,(,self,",",context,),:,self,.,hook,=,SqoopHook,(,conn_id,=,self,.,conn_id,",",verbose,=,self,.,verbose,",",num_mappers,=,self,.,num_mappers,",",hcatalog_database,=,self,.,hcatalog_database,",",hcatalog_table,=,self,.,hcatalog_table,",",properties,=,self,.,properties,),if,self,Execute sqoop job,Execute,sqoop,job,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/sqoop_operator.py#L166-L234,test,.,cmd_type,==,'export',:,self,.,hook,.,export_table,(,table,=,self,.,table,",",export_dir,=,self,.,export_dir,",",input_null_string,=,self,.,input_null_string,",",input_null_non_string,=,self,.,input_null_non_string,",",staging_table,=,self,.,staging_table,",",clear_staging_table,=,self,.,clear_staging_table,",",enclosed_by,=,self,.,enclosed_by,",",escaped_by,=,self,.,escaped_by,",",input_fields_terminated_by,=,self,.,input_fields_terminated_by,",",input_lines_terminated_by,=,self,.,input_lines_terminated_by,",",input_optionally_enclosed_by,=,self,.,input_optionally_enclosed_by,",",batch,=,self,.,batch,",",relaxed_isolation,=,self,.,relaxed_isolation,",",extra_export_options,=,self,.,extra_export_options,),elif,self,.,cmd_type,==,'import',:,# add create hcatalog table to extra import options if option passed,# if new params are added to constructor can pass them in here,# so don't modify sqoop_hook for each param,if,self,.,create_hcatalog_table,:,self,.,extra_import_options,[,'create-hcatalog-table',],=,'',if,self,.,table,and,self,.,query,:,raise,AirflowException,(,'Cannot specify query and table together. Need to specify either or.',),if,self,.,table,:,self,.,hook,.,import_table,(,table,=,self,.,table,",",target_dir,=,self,.,target_dir,",",append,=,self,.,append,",",file_type,=,self,.,file_type,",",columns,=,self,.,columns,",",split_by,=,self,.,split_by,",",where,=,self,.,where,",",direct,=,self,.,direct,",",driver,=,self,.,driver,",",extra_import_options,=,self,.,extra_import_options,),elif,self,.,query,:,self,.,hook,.,import_query,(,query,=,self,.,query,",",target_dir,=,self,.,target_dir,",",append,=,self,.,append,",",file_type,=,self,.,file_type,",",split_by,=,self,.,split_by,",",direct,=,self,.,direct,",",driver,=,self,.,driver,",",extra_import_options,=,self,.,extra_import_options,),else,:,raise,AirflowException,(,"""Provide query or table parameter to import using Sqoop""",),else,:,raise,AirflowException,(,"""cmd_type should be 'import' or 'export'""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/lineage/__init__.py,apply_lineage,"def apply_lineage(func):
    """"""
    Saves the lineage to XCom and if configured to do so sends it
    to the backend.
    """"""
    backend = _get_backend()

    @wraps(func)
    def wrapper(self, context, *args, **kwargs):
        self.log.debug(""Backend: %s, Lineage called with inlets: %s, outlets: %s"",
                       backend, self.inlets, self.outlets)
        ret_val = func(self, context, *args, **kwargs)

        outlets = [x.as_dict() for x in self.outlets]
        inlets = [x.as_dict() for x in self.inlets]

        if len(self.outlets) > 0:
            self.xcom_push(context,
                           key=PIPELINE_OUTLETS,
                           value=outlets,
                           execution_date=context['ti'].execution_date)

        if len(self.inlets) > 0:
            self.xcom_push(context,
                           key=PIPELINE_INLETS,
                           value=inlets,
                           execution_date=context['ti'].execution_date)

        if backend:
            backend.send_lineage(operator=self, inlets=self.inlets,
                                 outlets=self.outlets, context=context)

        return ret_val

    return wrapper",python,"def apply_lineage(func):
    """"""
    Saves the lineage to XCom and if configured to do so sends it
    to the backend.
    """"""
    backend = _get_backend()

    @wraps(func)
    def wrapper(self, context, *args, **kwargs):
        self.log.debug(""Backend: %s, Lineage called with inlets: %s, outlets: %s"",
                       backend, self.inlets, self.outlets)
        ret_val = func(self, context, *args, **kwargs)

        outlets = [x.as_dict() for x in self.outlets]
        inlets = [x.as_dict() for x in self.inlets]

        if len(self.outlets) > 0:
            self.xcom_push(context,
                           key=PIPELINE_OUTLETS,
                           value=outlets,
                           execution_date=context['ti'].execution_date)

        if len(self.inlets) > 0:
            self.xcom_push(context,
                           key=PIPELINE_INLETS,
                           value=inlets,
                           execution_date=context['ti'].execution_date)

        if backend:
            backend.send_lineage(operator=self, inlets=self.inlets,
                                 outlets=self.outlets, context=context)

        return ret_val

    return wrapper",def,apply_lineage,(,func,),:,backend,=,_get_backend,(,),@,wraps,(,func,),def,wrapper,(,self,",",context,",",*,args,",",*,*,kwargs,),:,self,.,log,.,debug,(,"""Backend: %s, Lineage called with inlets: %s, outlets: %s""",",",backend,",",self,.,inlets,",",self,.,outlets,),ret_val,=,func,"Saves the lineage to XCom and if configured to do so sends it
    to the backend.",Saves,the,lineage,to,XCom,and,if,configured,to,do,so,sends,it,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/lineage/__init__.py#L48-L82,test,(,self,",",context,",",*,args,",",*,*,kwargs,),outlets,=,[,x,.,as_dict,(,),for,x,in,self,.,outlets,],inlets,=,[,x,.,as_dict,(,),for,x,in,self,.,inlets,],if,len,(,self,.,outlets,),>,0,:,self,.,xcom_push,(,context,",",key,=,PIPELINE_OUTLETS,",",value,=,outlets,",",execution_date,=,context,[,'ti',],.,execution_date,),if,len,(,self,.,inlets,),>,0,:,self,.,xcom_push,(,context,",",key,=,PIPELINE_INLETS,",",value,=,inlets,",",execution_date,=,context,[,'ti',],.,execution_date,),if,backend,:,backend,.,send_lineage,(,operator,=,self,",",inlets,=,self,.,inlets,",",outlets,=,self,.,outlets,",",context,=,context,),return,ret_val,return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,the,backend,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/connection.py,Connection.extra_dejson,"def extra_dejson(self):
        """"""Returns the extra property by deserializing json.""""""
        obj = {}
        if self.extra:
            try:
                obj = json.loads(self.extra)
            except Exception as e:
                self.log.exception(e)
                self.log.error(""Failed parsing the json for conn_id %s"", self.conn_id)

        return obj",python,"def extra_dejson(self):
        """"""Returns the extra property by deserializing json.""""""
        obj = {}
        if self.extra:
            try:
                obj = json.loads(self.extra)
            except Exception as e:
                self.log.exception(e)
                self.log.error(""Failed parsing the json for conn_id %s"", self.conn_id)

        return obj",def,extra_dejson,(,self,),:,obj,=,{,},if,self,.,extra,:,try,:,obj,=,json,.,loads,(,self,.,extra,),except,Exception,as,e,:,self,.,log,.,exception,(,e,),self,.,log,.,error,(,"""Failed parsing the json for conn_id %s""",",",self,.,conn_id,),Returns the extra property by deserializing json.,Returns,the,extra,property,by,deserializing,json,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/connection.py#L286-L296,test,return,obj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dates.py,date_range,"def date_range(start_date, end_date=None, num=None, delta=None):
    """"""
    Get a set of dates as a list based on a start, end and delta, delta
    can be something that can be added to `datetime.datetime`
    or a cron expression as a `str`

    :Example::

        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),
            datetime.datetime(2016, 1, 3, 0, 0)]
        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),
            datetime.datetime(2016, 1, 3, 0, 0)]
        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=""0 0 0 * *"")
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),
            datetime.datetime(2016, 3, 1, 0, 0)]

    :param start_date: anchor date to start the series from
    :type start_date: datetime.datetime
    :param end_date: right boundary for the date range
    :type end_date: datetime.datetime
    :param num: alternatively to end_date, you can specify the number of
        number of entries you want in the range. This number can be negative,
        output will always be sorted regardless
    :type num: int
    """"""
    if not delta:
        return []
    if end_date and start_date > end_date:
        raise Exception(""Wait. start_date needs to be before end_date"")
    if end_date and num:
        raise Exception(""Wait. Either specify end_date OR num"")
    if not end_date and not num:
        end_date = timezone.utcnow()

    delta_iscron = False
    tz = start_date.tzinfo
    if isinstance(delta, six.string_types):
        delta_iscron = True
        start_date = timezone.make_naive(start_date, tz)
        cron = croniter(delta, start_date)
    elif isinstance(delta, timedelta):
        delta = abs(delta)
    dates = []
    if end_date:
        if timezone.is_naive(start_date):
            end_date = timezone.make_naive(end_date, tz)
        while start_date <= end_date:
            if timezone.is_naive(start_date):
                dates.append(timezone.make_aware(start_date, tz))
            else:
                dates.append(start_date)

            if delta_iscron:
                start_date = cron.get_next(datetime)
            else:
                start_date += delta
    else:
        for _ in range(abs(num)):
            if timezone.is_naive(start_date):
                dates.append(timezone.make_aware(start_date, tz))
            else:
                dates.append(start_date)

            if delta_iscron:
                if num > 0:
                    start_date = cron.get_next(datetime)
                else:
                    start_date = cron.get_prev(datetime)
            else:
                if num > 0:
                    start_date += delta
                else:
                    start_date -= delta
    return sorted(dates)",python,"def date_range(start_date, end_date=None, num=None, delta=None):
    """"""
    Get a set of dates as a list based on a start, end and delta, delta
    can be something that can be added to `datetime.datetime`
    or a cron expression as a `str`

    :Example::

        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),
            datetime.datetime(2016, 1, 3, 0, 0)]
        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),
            datetime.datetime(2016, 1, 3, 0, 0)]
        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=""0 0 0 * *"")
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),
            datetime.datetime(2016, 3, 1, 0, 0)]

    :param start_date: anchor date to start the series from
    :type start_date: datetime.datetime
    :param end_date: right boundary for the date range
    :type end_date: datetime.datetime
    :param num: alternatively to end_date, you can specify the number of
        number of entries you want in the range. This number can be negative,
        output will always be sorted regardless
    :type num: int
    """"""
    if not delta:
        return []
    if end_date and start_date > end_date:
        raise Exception(""Wait. start_date needs to be before end_date"")
    if end_date and num:
        raise Exception(""Wait. Either specify end_date OR num"")
    if not end_date and not num:
        end_date = timezone.utcnow()

    delta_iscron = False
    tz = start_date.tzinfo
    if isinstance(delta, six.string_types):
        delta_iscron = True
        start_date = timezone.make_naive(start_date, tz)
        cron = croniter(delta, start_date)
    elif isinstance(delta, timedelta):
        delta = abs(delta)
    dates = []
    if end_date:
        if timezone.is_naive(start_date):
            end_date = timezone.make_naive(end_date, tz)
        while start_date <= end_date:
            if timezone.is_naive(start_date):
                dates.append(timezone.make_aware(start_date, tz))
            else:
                dates.append(start_date)

            if delta_iscron:
                start_date = cron.get_next(datetime)
            else:
                start_date += delta
    else:
        for _ in range(abs(num)):
            if timezone.is_naive(start_date):
                dates.append(timezone.make_aware(start_date, tz))
            else:
                dates.append(start_date)

            if delta_iscron:
                if num > 0:
                    start_date = cron.get_next(datetime)
                else:
                    start_date = cron.get_prev(datetime)
            else:
                if num > 0:
                    start_date += delta
                else:
                    start_date -= delta
    return sorted(dates)",def,date_range,(,start_date,",",end_date,=,None,",",num,=,None,",",delta,=,None,),:,if,not,delta,:,return,[,],if,end_date,and,start_date,>,end_date,:,raise,Exception,(,"""Wait. start_date needs to be before end_date""",),if,end_date,and,num,:,raise,Exception,(,"""Wait. Either specify end_date OR num""",),if,not,end_date,and,not,"Get a set of dates as a list based on a start, end and delta, delta
    can be something that can be added to `datetime.datetime`
    or a cron expression as a `str`

    :Example::

        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),
            datetime.datetime(2016, 1, 3, 0, 0)]
        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),
            datetime.datetime(2016, 1, 3, 0, 0)]
        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=""0 0 0 * *"")
            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),
            datetime.datetime(2016, 3, 1, 0, 0)]

    :param start_date: anchor date to start the series from
    :type start_date: datetime.datetime
    :param end_date: right boundary for the date range
    :type end_date: datetime.datetime
    :param num: alternatively to end_date, you can specify the number of
        number of entries you want in the range. This number can be negative,
        output will always be sorted regardless
    :type num: int",Get,a,set,of,dates,as,a,list,based,on,a,start,end,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dates.py#L36-L111,test,num,:,end_date,=,timezone,.,utcnow,(,),delta_iscron,=,False,tz,=,start_date,.,tzinfo,if,isinstance,(,delta,",",six,.,string_types,),:,delta_iscron,=,True,start_date,=,timezone,.,make_naive,(,start_date,",",tz,),cron,=,croniter,(,delta,",",start_date,),elif,isinstance,(,delta,",",timedelta,),:,delta,=,abs,(,delta,),dates,=,[,],if,end_date,:,if,timezone,.,is_naive,(,start_date,),:,end_date,=,timezone,.,make_naive,(,end_date,",",tz,),while,start_date,<=,end_date,:,if,timezone,.,is_naive,(,start_date,),:,dates,.,append,(,timezone,.,make_aware,(,start_date,",",tz,),),else,:,dates,.,append,(,start_date,),if,delta_iscron,:,start_date,=,cron,.,get_next,(,datetime,),else,:,start_date,+=,delta,else,:,for,_,in,range,(,abs,(,num,),),:,if,timezone,.,is_naive,(,start_date,),:,dates,.,append,(,timezone,.,make_aware,(,start_date,",",tz,),),else,:,dates,.,append,(,start_date,),if,delta_iscron,:,if,num,>,0,:,start_date,=,cron,.,get_next,(,datetime,),else,:,start_date,=,cron,.,get_prev,(,datetime,),else,:,if,num,>,0,:,start_date,+=,delta,else,:,start_date,-=,delta,return,sorted,(,dates,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,and,delta,delta,can,be,something,that,can,be,added,to,datetime,.,datetime,or,a,cron,expression,as,a,str,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dates.py,scale_time_units,"def scale_time_units(time_seconds_arr, unit):
    """"""
    Convert an array of time durations in seconds to the specified time unit.
    """"""
    if unit == 'minutes':
        return list(map(lambda x: x * 1.0 / 60, time_seconds_arr))
    elif unit == 'hours':
        return list(map(lambda x: x * 1.0 / (60 * 60), time_seconds_arr))
    elif unit == 'days':
        return list(map(lambda x: x * 1.0 / (24 * 60 * 60), time_seconds_arr))
    return time_seconds_arr",python,"def scale_time_units(time_seconds_arr, unit):
    """"""
    Convert an array of time durations in seconds to the specified time unit.
    """"""
    if unit == 'minutes':
        return list(map(lambda x: x * 1.0 / 60, time_seconds_arr))
    elif unit == 'hours':
        return list(map(lambda x: x * 1.0 / (60 * 60), time_seconds_arr))
    elif unit == 'days':
        return list(map(lambda x: x * 1.0 / (24 * 60 * 60), time_seconds_arr))
    return time_seconds_arr",def,scale_time_units,(,time_seconds_arr,",",unit,),:,if,unit,==,'minutes',:,return,list,(,map,(,lambda,x,:,x,*,1.0,/,60,",",time_seconds_arr,),),elif,unit,==,'hours',:,return,list,(,map,(,lambda,x,:,x,*,1.0,/,(,60,*,60,),Convert an array of time durations in seconds to the specified time unit.,Convert,an,array,of,time,durations,in,seconds,to,the,specified,time,unit,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dates.py#L214-L224,test,",",time_seconds_arr,),),elif,unit,==,'days',:,return,list,(,map,(,lambda,x,:,x,*,1.0,/,(,24,*,60,*,60,),",",time_seconds_arr,),),return,time_seconds_arr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/utils/dates.py,days_ago,"def days_ago(n, hour=0, minute=0, second=0, microsecond=0):
    """"""
    Get a datetime object representing `n` days ago. By default the time is
    set to midnight.
    """"""
    today = timezone.utcnow().replace(
        hour=hour,
        minute=minute,
        second=second,
        microsecond=microsecond)
    return today - timedelta(days=n)",python,"def days_ago(n, hour=0, minute=0, second=0, microsecond=0):
    """"""
    Get a datetime object representing `n` days ago. By default the time is
    set to midnight.
    """"""
    today = timezone.utcnow().replace(
        hour=hour,
        minute=minute,
        second=second,
        microsecond=microsecond)
    return today - timedelta(days=n)",def,days_ago,(,n,",",hour,=,0,",",minute,=,0,",",second,=,0,",",microsecond,=,0,),:,today,=,timezone,.,utcnow,(,),.,replace,(,hour,=,hour,",",minute,=,minute,",",second,=,second,",",microsecond,=,microsecond,),return,today,-,timedelta,"Get a datetime object representing `n` days ago. By default the time is
    set to midnight.",Get,a,datetime,object,representing,n,days,ago,.,By,default,the,time,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dates.py#L227-L237,test,(,days,=,n,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,is,set,to,midnight,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.init_role,"def init_role(self, role_name, role_vms, role_perms):
        """"""
        Initialize the role with the permissions and related view-menus.

        :param role_name:
        :param role_vms:
        :param role_perms:
        :return:
        """"""
        pvms = self.get_session.query(sqla_models.PermissionView).all()
        pvms = [p for p in pvms if p.permission and p.view_menu]

        role = self.find_role(role_name)
        if not role:
            role = self.add_role(role_name)

        if len(role.permissions) == 0:
            self.log.info('Initializing permissions for role:%s in the database.', role_name)
            role_pvms = set()
            for pvm in pvms:
                if pvm.view_menu.name in role_vms and pvm.permission.name in role_perms:
                    role_pvms.add(pvm)
            role.permissions = list(role_pvms)
            self.get_session.merge(role)
            self.get_session.commit()
        else:
            self.log.debug('Existing permissions for the role:%s '
                           'within the database will persist.', role_name)",python,"def init_role(self, role_name, role_vms, role_perms):
        """"""
        Initialize the role with the permissions and related view-menus.

        :param role_name:
        :param role_vms:
        :param role_perms:
        :return:
        """"""
        pvms = self.get_session.query(sqla_models.PermissionView).all()
        pvms = [p for p in pvms if p.permission and p.view_menu]

        role = self.find_role(role_name)
        if not role:
            role = self.add_role(role_name)

        if len(role.permissions) == 0:
            self.log.info('Initializing permissions for role:%s in the database.', role_name)
            role_pvms = set()
            for pvm in pvms:
                if pvm.view_menu.name in role_vms and pvm.permission.name in role_perms:
                    role_pvms.add(pvm)
            role.permissions = list(role_pvms)
            self.get_session.merge(role)
            self.get_session.commit()
        else:
            self.log.debug('Existing permissions for the role:%s '
                           'within the database will persist.', role_name)",def,init_role,(,self,",",role_name,",",role_vms,",",role_perms,),:,pvms,=,self,.,get_session,.,query,(,sqla_models,.,PermissionView,),.,all,(,),pvms,=,[,p,for,p,in,pvms,if,p,.,permission,and,p,.,view_menu,],role,=,self,.,find_role,(,role_name,"Initialize the role with the permissions and related view-menus.

        :param role_name:
        :param role_vms:
        :param role_perms:
        :return:",Initialize,the,role,with,the,permissions,and,related,view,-,menus,.,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L175-L202,test,),if,not,role,:,role,=,self,.,add_role,(,role_name,),if,len,(,role,.,permissions,),==,0,:,self,.,log,.,info,(,'Initializing permissions for role:%s in the database.',",",role_name,),role_pvms,=,set,(,),for,pvm,in,pvms,:,if,pvm,.,view_menu,.,name,in,role_vms,and,pvm,.,permission,.,name,in,role_perms,:,role_pvms,.,add,(,pvm,),role,.,permissions,=,list,(,role_pvms,),self,.,get_session,.,merge,(,role,),self,.,get_session,.,commit,(,),else,:,self,.,log,.,debug,(,'Existing permissions for the role:%s ','within the database will persist.',",",role_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.delete_role,"def delete_role(self, role_name):
        """"""Delete the given Role

        :param role_name: the name of a role in the ab_role table
        """"""
        session = self.get_session
        role = session.query(sqla_models.Role)\
                      .filter(sqla_models.Role.name == role_name)\
                      .first()
        if role:
            self.log.info(""Deleting role '%s'"", role_name)
            session.delete(role)
            session.commit()
        else:
            raise AirflowException(""Role named '{}' does not exist"".format(
                role_name))",python,"def delete_role(self, role_name):
        """"""Delete the given Role

        :param role_name: the name of a role in the ab_role table
        """"""
        session = self.get_session
        role = session.query(sqla_models.Role)\
                      .filter(sqla_models.Role.name == role_name)\
                      .first()
        if role:
            self.log.info(""Deleting role '%s'"", role_name)
            session.delete(role)
            session.commit()
        else:
            raise AirflowException(""Role named '{}' does not exist"".format(
                role_name))",def,delete_role,(,self,",",role_name,),:,session,=,self,.,get_session,role,=,session,.,query,(,sqla_models,.,Role,),.,filter,(,sqla_models,.,Role,.,name,==,role_name,),.,first,(,),if,role,:,self,.,log,.,info,(,"""Deleting role '%s'""",",",role_name,),session,"Delete the given Role

        :param role_name: the name of a role in the ab_role table",Delete,the,given,Role,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L204-L219,test,.,delete,(,role,),session,.,commit,(,),else,:,raise,AirflowException,(,"""Role named '{}' does not exist""",.,format,(,role_name,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.get_user_roles,"def get_user_roles(self, user=None):
        """"""
        Get all the roles associated with the user.

        :param user: the ab_user in FAB model.
        :return: a list of roles associated with the user.
        """"""
        if user is None:
            user = g.user
        if user.is_anonymous:
            public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')
            return [appbuilder.security_manager.find_role(public_role)] \
                if public_role else []
        return user.roles",python,"def get_user_roles(self, user=None):
        """"""
        Get all the roles associated with the user.

        :param user: the ab_user in FAB model.
        :return: a list of roles associated with the user.
        """"""
        if user is None:
            user = g.user
        if user.is_anonymous:
            public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')
            return [appbuilder.security_manager.find_role(public_role)] \
                if public_role else []
        return user.roles",def,get_user_roles,(,self,",",user,=,None,),:,if,user,is,None,:,user,=,g,.,user,if,user,.,is_anonymous,:,public_role,=,appbuilder,.,config,.,get,(,'AUTH_ROLE_PUBLIC',),return,[,appbuilder,.,security_manager,.,find_role,(,public_role,),],if,public_role,else,[,],return,"Get all the roles associated with the user.

        :param user: the ab_user in FAB model.
        :return: a list of roles associated with the user.",Get,all,the,roles,associated,with,the,user,.,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L221-L234,test,user,.,roles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.get_all_permissions_views,"def get_all_permissions_views(self):
        """"""
        Returns a set of tuples with the perm name and view menu name
        """"""
        perms_views = set()
        for role in self.get_user_roles():
            perms_views.update({(perm_view.permission.name, perm_view.view_menu.name)
                                for perm_view in role.permissions})
        return perms_views",python,"def get_all_permissions_views(self):
        """"""
        Returns a set of tuples with the perm name and view menu name
        """"""
        perms_views = set()
        for role in self.get_user_roles():
            perms_views.update({(perm_view.permission.name, perm_view.view_menu.name)
                                for perm_view in role.permissions})
        return perms_views",def,get_all_permissions_views,(,self,),:,perms_views,=,set,(,),for,role,in,self,.,get_user_roles,(,),:,perms_views,.,update,(,{,(,perm_view,.,permission,.,name,",",perm_view,.,view_menu,.,name,),for,perm_view,in,role,.,permissions,},),return,perms_views,,,,,Returns a set of tuples with the perm name and view menu name,Returns,a,set,of,tuples,with,the,perm,name,and,view,menu,name,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L236-L244,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager._has_role,"def _has_role(self, role_name_or_list):
        """"""
        Whether the user has this role name
        """"""
        if not isinstance(role_name_or_list, list):
            role_name_or_list = [role_name_or_list]
        return any(
            [r.name in role_name_or_list for r in self.get_user_roles()])",python,"def _has_role(self, role_name_or_list):
        """"""
        Whether the user has this role name
        """"""
        if not isinstance(role_name_or_list, list):
            role_name_or_list = [role_name_or_list]
        return any(
            [r.name in role_name_or_list for r in self.get_user_roles()])",def,_has_role,(,self,",",role_name_or_list,),:,if,not,isinstance,(,role_name_or_list,",",list,),:,role_name_or_list,=,[,role_name_or_list,],return,any,(,[,r,.,name,in,role_name_or_list,for,r,in,self,.,get_user_roles,(,),],),,,,,,,,,,,,Whether the user has this role name,Whether,the,user,has,this,role,name,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L294-L301,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager._has_perm,"def _has_perm(self, permission_name, view_menu_name):
        """"""
        Whether the user has this perm
        """"""
        if hasattr(self, 'perms'):
            if (permission_name, view_menu_name) in self.perms:
                return True
        # rebuild the permissions set
        self._get_and_cache_perms()
        return (permission_name, view_menu_name) in self.perms",python,"def _has_perm(self, permission_name, view_menu_name):
        """"""
        Whether the user has this perm
        """"""
        if hasattr(self, 'perms'):
            if (permission_name, view_menu_name) in self.perms:
                return True
        # rebuild the permissions set
        self._get_and_cache_perms()
        return (permission_name, view_menu_name) in self.perms",def,_has_perm,(,self,",",permission_name,",",view_menu_name,),:,if,hasattr,(,self,",",'perms',),:,if,(,permission_name,",",view_menu_name,),in,self,.,perms,:,return,True,# rebuild the permissions set,self,.,_get_and_cache_perms,(,),return,(,permission_name,",",view_menu_name,),in,self,.,perms,,,,,,Whether the user has this perm,Whether,the,user,has,this,perm,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L303-L312,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.clean_perms,"def clean_perms(self):
        """"""
        FAB leaves faulty permissions that need to be cleaned up
        """"""
        self.log.debug('Cleaning faulty perms')
        sesh = self.get_session
        pvms = (
            sesh.query(sqla_models.PermissionView)
            .filter(or_(
                sqla_models.PermissionView.permission == None,  # NOQA
                sqla_models.PermissionView.view_menu == None,  # NOQA
            ))
        )
        deleted_count = pvms.delete()
        sesh.commit()
        if deleted_count:
            self.log.info('Deleted %s faulty permissions', deleted_count)",python,"def clean_perms(self):
        """"""
        FAB leaves faulty permissions that need to be cleaned up
        """"""
        self.log.debug('Cleaning faulty perms')
        sesh = self.get_session
        pvms = (
            sesh.query(sqla_models.PermissionView)
            .filter(or_(
                sqla_models.PermissionView.permission == None,  # NOQA
                sqla_models.PermissionView.view_menu == None,  # NOQA
            ))
        )
        deleted_count = pvms.delete()
        sesh.commit()
        if deleted_count:
            self.log.info('Deleted %s faulty permissions', deleted_count)",def,clean_perms,(,self,),:,self,.,log,.,debug,(,'Cleaning faulty perms',),sesh,=,self,.,get_session,pvms,=,(,sesh,.,query,(,sqla_models,.,PermissionView,),.,filter,(,or_,(,sqla_models,.,PermissionView,.,permission,==,None,",",# NOQA,sqla_models,.,PermissionView,.,view_menu,==,None,",",FAB leaves faulty permissions that need to be cleaned up,FAB,leaves,faulty,permissions,that,need,to,be,cleaned,up,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L326-L342,test,# NOQA,),),),deleted_count,=,pvms,.,delete,(,),sesh,.,commit,(,),if,deleted_count,:,self,.,log,.,info,(,'Deleted %s faulty permissions',",",deleted_count,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager._merge_perm,"def _merge_perm(self, permission_name, view_menu_name):
        """"""
        Add the new permission , view_menu to ab_permission_view_role if not exists.
        It will add the related entry to ab_permission
        and ab_view_menu two meta tables as well.

        :param permission_name: Name of the permission.
        :type permission_name: str
        :param view_menu_name: Name of the view-menu
        :type view_menu_name: str
        :return:
        """"""
        permission = self.find_permission(permission_name)
        view_menu = self.find_view_menu(view_menu_name)
        pv = None
        if permission and view_menu:
            pv = self.get_session.query(self.permissionview_model).filter_by(
                permission=permission, view_menu=view_menu).first()
        if not pv and permission_name and view_menu_name:
            self.add_permission_view_menu(permission_name, view_menu_name)",python,"def _merge_perm(self, permission_name, view_menu_name):
        """"""
        Add the new permission , view_menu to ab_permission_view_role if not exists.
        It will add the related entry to ab_permission
        and ab_view_menu two meta tables as well.

        :param permission_name: Name of the permission.
        :type permission_name: str
        :param view_menu_name: Name of the view-menu
        :type view_menu_name: str
        :return:
        """"""
        permission = self.find_permission(permission_name)
        view_menu = self.find_view_menu(view_menu_name)
        pv = None
        if permission and view_menu:
            pv = self.get_session.query(self.permissionview_model).filter_by(
                permission=permission, view_menu=view_menu).first()
        if not pv and permission_name and view_menu_name:
            self.add_permission_view_menu(permission_name, view_menu_name)",def,_merge_perm,(,self,",",permission_name,",",view_menu_name,),:,permission,=,self,.,find_permission,(,permission_name,),view_menu,=,self,.,find_view_menu,(,view_menu_name,),pv,=,None,if,permission,and,view_menu,:,pv,=,self,.,get_session,.,query,(,self,.,permissionview_model,),.,filter_by,(,permission,=,permission,"Add the new permission , view_menu to ab_permission_view_role if not exists.
        It will add the related entry to ab_permission
        and ab_view_menu two meta tables as well.

        :param permission_name: Name of the permission.
        :type permission_name: str
        :param view_menu_name: Name of the view-menu
        :type view_menu_name: str
        :return:",Add,the,new,permission,view_menu,to,ab_permission_view_role,if,not,exists,.,It,will,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L344-L363,test,",",view_menu,=,view_menu,),.,first,(,),if,not,pv,and,permission_name,and,view_menu_name,:,self,.,add_permission_view_menu,(,permission_name,",",view_menu_name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,add,the,related,entry,to,ab_permission,and,ab_view_menu,two,meta,tables,as,well,.,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.update_admin_perm_view,"def update_admin_perm_view(self):
        """"""
        Admin should have all the permission-views.
        Add the missing ones to the table for admin.

        :return: None.
        """"""
        pvms = self.get_session.query(sqla_models.PermissionView).all()
        pvms = [p for p in pvms if p.permission and p.view_menu]

        admin = self.find_role('Admin')
        admin.permissions = list(set(admin.permissions) | set(pvms))

        self.get_session.commit()",python,"def update_admin_perm_view(self):
        """"""
        Admin should have all the permission-views.
        Add the missing ones to the table for admin.

        :return: None.
        """"""
        pvms = self.get_session.query(sqla_models.PermissionView).all()
        pvms = [p for p in pvms if p.permission and p.view_menu]

        admin = self.find_role('Admin')
        admin.permissions = list(set(admin.permissions) | set(pvms))

        self.get_session.commit()",def,update_admin_perm_view,(,self,),:,pvms,=,self,.,get_session,.,query,(,sqla_models,.,PermissionView,),.,all,(,),pvms,=,[,p,for,p,in,pvms,if,p,.,permission,and,p,.,view_menu,],admin,=,self,.,find_role,(,'Admin',),admin,.,permissions,=,list,"Admin should have all the permission-views.
        Add the missing ones to the table for admin.

        :return: None.",Admin,should,have,all,the,permission,-,views,.,Add,the,missing,ones,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L440-L453,test,(,set,(,admin,.,permissions,),|,set,(,pvms,),),self,.,get_session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,the,table,for,admin,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager._sync_dag_view_permissions,"def _sync_dag_view_permissions(self, dag_id, access_control):
        """"""Set the access policy on the given DAG's ViewModel.

        :param dag_id: the ID of the DAG whose permissions should be updated
        :type dag_id: string
        :param access_control: a dict where each key is a rolename and
            each value is a set() of permission names (e.g.,
            {'can_dag_read'}
        :type access_control: dict
        """"""
        def _get_or_create_dag_permission(perm_name):
            dag_perm = self.find_permission_view_menu(perm_name, dag_id)
            if not dag_perm:
                self.log.info(
                    ""Creating new permission '%s' on view '%s'"",
                    perm_name, dag_id
                )
                dag_perm = self.add_permission_view_menu(perm_name, dag_id)

            return dag_perm

        def _revoke_stale_permissions(dag_view):
            existing_dag_perms = self.find_permissions_view_menu(dag_view)
            for perm in existing_dag_perms:
                non_admin_roles = [role for role in perm.role
                                   if role.name != 'Admin']
                for role in non_admin_roles:
                    target_perms_for_role = access_control.get(role.name, {})
                    if perm.permission.name not in target_perms_for_role:
                        self.log.info(
                            ""Revoking '%s' on DAG '%s' for role '%s'"",
                            perm.permission, dag_id, role.name
                        )
                        self.del_permission_role(role, perm)

        dag_view = self.find_view_menu(dag_id)
        if dag_view:
            _revoke_stale_permissions(dag_view)

        for rolename, perms in access_control.items():
            role = self.find_role(rolename)
            if not role:
                raise AirflowException(
                    ""The access_control mapping for DAG '{}' includes a role ""
                    ""named '{}', but that role does not exist"".format(
                        dag_id,
                        rolename))

            perms = set(perms)
            invalid_perms = perms - self.DAG_PERMS
            if invalid_perms:
                raise AirflowException(
                    ""The access_control map for DAG '{}' includes the following ""
                    ""invalid permissions: {}; The set of valid permissions ""
                    ""is: {}"".format(dag_id,
                                    (perms - self.DAG_PERMS),
                                    self.DAG_PERMS))

            for perm_name in perms:
                dag_perm = _get_or_create_dag_permission(perm_name)
                self.add_permission_role(role, dag_perm)",python,"def _sync_dag_view_permissions(self, dag_id, access_control):
        """"""Set the access policy on the given DAG's ViewModel.

        :param dag_id: the ID of the DAG whose permissions should be updated
        :type dag_id: string
        :param access_control: a dict where each key is a rolename and
            each value is a set() of permission names (e.g.,
            {'can_dag_read'}
        :type access_control: dict
        """"""
        def _get_or_create_dag_permission(perm_name):
            dag_perm = self.find_permission_view_menu(perm_name, dag_id)
            if not dag_perm:
                self.log.info(
                    ""Creating new permission '%s' on view '%s'"",
                    perm_name, dag_id
                )
                dag_perm = self.add_permission_view_menu(perm_name, dag_id)

            return dag_perm

        def _revoke_stale_permissions(dag_view):
            existing_dag_perms = self.find_permissions_view_menu(dag_view)
            for perm in existing_dag_perms:
                non_admin_roles = [role for role in perm.role
                                   if role.name != 'Admin']
                for role in non_admin_roles:
                    target_perms_for_role = access_control.get(role.name, {})
                    if perm.permission.name not in target_perms_for_role:
                        self.log.info(
                            ""Revoking '%s' on DAG '%s' for role '%s'"",
                            perm.permission, dag_id, role.name
                        )
                        self.del_permission_role(role, perm)

        dag_view = self.find_view_menu(dag_id)
        if dag_view:
            _revoke_stale_permissions(dag_view)

        for rolename, perms in access_control.items():
            role = self.find_role(rolename)
            if not role:
                raise AirflowException(
                    ""The access_control mapping for DAG '{}' includes a role ""
                    ""named '{}', but that role does not exist"".format(
                        dag_id,
                        rolename))

            perms = set(perms)
            invalid_perms = perms - self.DAG_PERMS
            if invalid_perms:
                raise AirflowException(
                    ""The access_control map for DAG '{}' includes the following ""
                    ""invalid permissions: {}; The set of valid permissions ""
                    ""is: {}"".format(dag_id,
                                    (perms - self.DAG_PERMS),
                                    self.DAG_PERMS))

            for perm_name in perms:
                dag_perm = _get_or_create_dag_permission(perm_name)
                self.add_permission_role(role, dag_perm)",def,_sync_dag_view_permissions,(,self,",",dag_id,",",access_control,),:,def,_get_or_create_dag_permission,(,perm_name,),:,dag_perm,=,self,.,find_permission_view_menu,(,perm_name,",",dag_id,),if,not,dag_perm,:,self,.,log,.,info,(,"""Creating new permission '%s' on view '%s'""",",",perm_name,",",dag_id,),dag_perm,=,self,.,add_permission_view_menu,(,perm_name,",",dag_id,),"Set the access policy on the given DAG's ViewModel.

        :param dag_id: the ID of the DAG whose permissions should be updated
        :type dag_id: string
        :param access_control: a dict where each key is a rolename and
            each value is a set() of permission names (e.g.,
            {'can_dag_read'}
        :type access_control: dict",Set,the,access,policy,on,the,given,DAG,s,ViewModel,.,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L500-L560,test,return,dag_perm,def,_revoke_stale_permissions,(,dag_view,),:,existing_dag_perms,=,self,.,find_permissions_view_menu,(,dag_view,),for,perm,in,existing_dag_perms,:,non_admin_roles,=,[,role,for,role,in,perm,.,role,if,role,.,name,!=,'Admin',],for,role,in,non_admin_roles,:,target_perms_for_role,=,access_control,.,get,(,role,.,name,",",{,},),if,perm,.,permission,.,name,not,in,target_perms_for_role,:,self,.,log,.,info,(,"""Revoking '%s' on DAG '%s' for role '%s'""",",",perm,.,permission,",",dag_id,",",role,.,name,),self,.,del_permission_role,(,role,",",perm,),dag_view,=,self,.,find_view_menu,(,dag_id,),if,dag_view,:,_revoke_stale_permissions,(,dag_view,),for,rolename,",",perms,in,access_control,.,items,(,),:,role,=,self,.,find_role,(,rolename,),if,not,role,:,raise,AirflowException,(,"""The access_control mapping for DAG '{}' includes a role ""","""named '{}', but that role does not exist""",.,format,(,dag_id,",",rolename,),),perms,=,set,(,perms,),invalid_perms,=,perms,-,self,.,DAG_PERMS,if,invalid_perms,:,raise,AirflowException,(,"""The access_control map for DAG '{}' includes the following ""","""invalid permissions: {}; The set of valid permissions ""","""is: {}""",.,format,(,dag_id,",",(,perms,-,self,.,DAG_PERMS,),",",self,.,DAG_PERMS,),),for,perm_name,in,perms,:,dag_perm,=,_get_or_create_dag_permission,(,perm_name,),self,.,add_permission_role,(,role,",",dag_perm,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/www/security.py,AirflowSecurityManager.create_perm_vm_for_all_dag,"def create_perm_vm_for_all_dag(self):
        """"""
        Create perm-vm if not exist and insert into FAB security model for all-dags.
        """"""
        # create perm for global logical dag
        for dag_vm in self.DAG_VMS:
            for perm in self.DAG_PERMS:
                self._merge_perm(permission_name=perm,
                                 view_menu_name=dag_vm)",python,"def create_perm_vm_for_all_dag(self):
        """"""
        Create perm-vm if not exist and insert into FAB security model for all-dags.
        """"""
        # create perm for global logical dag
        for dag_vm in self.DAG_VMS:
            for perm in self.DAG_PERMS:
                self._merge_perm(permission_name=perm,
                                 view_menu_name=dag_vm)",def,create_perm_vm_for_all_dag,(,self,),:,# create perm for global logical dag,for,dag_vm,in,self,.,DAG_VMS,:,for,perm,in,self,.,DAG_PERMS,:,self,.,_merge_perm,(,permission_name,=,perm,",",view_menu_name,=,dag_vm,),,,,,,,,,,,,,,,,,,,,Create perm-vm if not exist and insert into FAB security model for all-dags.,Create,perm,-,vm,if,not,exist,and,insert,into,FAB,security,model,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/security.py#L562-L570,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,for,all,-,dags,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/models/crypto.py,get_fernet,"def get_fernet():
    """"""
    Deferred load of Fernet key.

    This function could fail either because Cryptography is not installed
    or because the Fernet key is invalid.

    :return: Fernet object
    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet
    """"""
    global _fernet
    log = LoggingMixin().log

    if _fernet:
        return _fernet
    try:
        from cryptography.fernet import Fernet, MultiFernet, InvalidToken
        global InvalidFernetToken
        InvalidFernetToken = InvalidToken

    except BuiltinImportError:
        log.warning(
            ""cryptography not found - values will not be stored encrypted.""
        )
        _fernet = NullFernet()
        return _fernet

    try:
        fernet_key = configuration.conf.get('core', 'FERNET_KEY')
        if not fernet_key:
            log.warning(
                ""empty cryptography key - values will not be stored encrypted.""
            )
            _fernet = NullFernet()
        else:
            _fernet = MultiFernet([
                Fernet(fernet_part.encode('utf-8'))
                for fernet_part in fernet_key.split(',')
            ])
            _fernet.is_encrypted = True
    except (ValueError, TypeError) as ve:
        raise AirflowException(""Could not create Fernet object: {}"".format(ve))

    return _fernet",python,"def get_fernet():
    """"""
    Deferred load of Fernet key.

    This function could fail either because Cryptography is not installed
    or because the Fernet key is invalid.

    :return: Fernet object
    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet
    """"""
    global _fernet
    log = LoggingMixin().log

    if _fernet:
        return _fernet
    try:
        from cryptography.fernet import Fernet, MultiFernet, InvalidToken
        global InvalidFernetToken
        InvalidFernetToken = InvalidToken

    except BuiltinImportError:
        log.warning(
            ""cryptography not found - values will not be stored encrypted.""
        )
        _fernet = NullFernet()
        return _fernet

    try:
        fernet_key = configuration.conf.get('core', 'FERNET_KEY')
        if not fernet_key:
            log.warning(
                ""empty cryptography key - values will not be stored encrypted.""
            )
            _fernet = NullFernet()
        else:
            _fernet = MultiFernet([
                Fernet(fernet_part.encode('utf-8'))
                for fernet_part in fernet_key.split(',')
            ])
            _fernet.is_encrypted = True
    except (ValueError, TypeError) as ve:
        raise AirflowException(""Could not create Fernet object: {}"".format(ve))

    return _fernet",def,get_fernet,(,),:,global,_fernet,log,=,LoggingMixin,(,),.,log,if,_fernet,:,return,_fernet,try,:,from,cryptography,.,fernet,import,Fernet,",",MultiFernet,",",InvalidToken,global,InvalidFernetToken,InvalidFernetToken,=,InvalidToken,except,BuiltinImportError,:,log,.,warning,(,"""cryptography not found - values will not be stored encrypted.""",),_fernet,=,NullFernet,(,),return,_fernet,"Deferred load of Fernet key.

    This function could fail either because Cryptography is not installed
    or because the Fernet key is invalid.

    :return: Fernet object
    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet",Deferred,load,of,Fernet,key,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/crypto.py#L54-L97,test,try,:,fernet_key,=,configuration,.,conf,.,get,(,'core',",",'FERNET_KEY',),if,not,fernet_key,:,log,.,warning,(,"""empty cryptography key - values will not be stored encrypted.""",),_fernet,=,NullFernet,(,),else,:,_fernet,=,MultiFernet,(,[,Fernet,(,fernet_part,.,encode,(,'utf-8',),),for,fernet_part,in,fernet_key,.,split,(,"','",),],),_fernet,.,is_encrypted,=,True,except,(,ValueError,",",TypeError,),as,ve,:,raise,AirflowException,(,"""Could not create Fernet object: {}""",.,format,(,ve,),),return,_fernet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/aws_glue_catalog_partition_sensor.py,AwsGlueCatalogPartitionSensor.poke,"def poke(self, context):
        """"""
        Checks for existence of the partition in the AWS Glue Catalog table
        """"""
        if '.' in self.table_name:
            self.database_name, self.table_name = self.table_name.split('.')
        self.log.info(
            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression
        )

        return self.get_hook().check_for_partition(
            self.database_name, self.table_name, self.expression)",python,"def poke(self, context):
        """"""
        Checks for existence of the partition in the AWS Glue Catalog table
        """"""
        if '.' in self.table_name:
            self.database_name, self.table_name = self.table_name.split('.')
        self.log.info(
            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression
        )

        return self.get_hook().check_for_partition(
            self.database_name, self.table_name, self.expression)",def,poke,(,self,",",context,),:,if,'.',in,self,.,table_name,:,self,.,database_name,",",self,.,table_name,=,self,.,table_name,.,split,(,'.',),self,.,log,.,info,(,"'Poking for table %s. %s, expression %s'",",",self,.,database_name,",",self,.,table_name,",",self,.,expression,),return,Checks for existence of the partition in the AWS Glue Catalog table,Checks,for,existence,of,the,partition,in,the,AWS,Glue,Catalog,table,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/aws_glue_catalog_partition_sensor.py#L70-L81,test,self,.,get_hook,(,),.,check_for_partition,(,self,.,database_name,",",self,.,table_name,",",self,.,expression,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/aws_glue_catalog_partition_sensor.py,AwsGlueCatalogPartitionSensor.get_hook,"def get_hook(self):
        """"""
        Gets the AwsGlueCatalogHook
        """"""
        if not hasattr(self, 'hook'):
            from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook
            self.hook = AwsGlueCatalogHook(
                aws_conn_id=self.aws_conn_id,
                region_name=self.region_name)

        return self.hook",python,"def get_hook(self):
        """"""
        Gets the AwsGlueCatalogHook
        """"""
        if not hasattr(self, 'hook'):
            from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook
            self.hook = AwsGlueCatalogHook(
                aws_conn_id=self.aws_conn_id,
                region_name=self.region_name)

        return self.hook",def,get_hook,(,self,),:,if,not,hasattr,(,self,",",'hook',),:,from,airflow,.,contrib,.,hooks,.,aws_glue_catalog_hook,import,AwsGlueCatalogHook,self,.,hook,=,AwsGlueCatalogHook,(,aws_conn_id,=,self,.,aws_conn_id,",",region_name,=,self,.,region_name,),return,self,.,hook,,,,,,Gets the AwsGlueCatalogHook,Gets,the,AwsGlueCatalogHook,,,,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/aws_glue_catalog_partition_sensor.py#L83-L93,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/sensors/aws_sqs_sensor.py,SQSSensor.poke,"def poke(self, context):
        """"""
        Check for message on subscribed queue and write to xcom the message with key ``messages``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message is available or ``False``
        """"""

        sqs_hook = SQSHook(aws_conn_id=self.aws_conn_id)
        sqs_conn = sqs_hook.get_conn()

        self.log.info('SQSSensor checking for message on queue: %s', self.sqs_queue)

        messages = sqs_conn.receive_message(QueueUrl=self.sqs_queue,
                                            MaxNumberOfMessages=self.max_messages,
                                            WaitTimeSeconds=self.wait_time_seconds)

        self.log.info(""reveived message %s"", str(messages))

        if 'Messages' in messages and len(messages['Messages']) > 0:

            entries = [{'Id': message['MessageId'], 'ReceiptHandle': message['ReceiptHandle']}
                       for message in messages['Messages']]

            result = sqs_conn.delete_message_batch(QueueUrl=self.sqs_queue,
                                                   Entries=entries)

            if 'Successful' in result:
                context['ti'].xcom_push(key='messages', value=messages)
                return True
            else:
                raise AirflowException(
                    'Delete SQS Messages failed ' + str(result) + ' for messages ' + str(messages))

        return False",python,"def poke(self, context):
        """"""
        Check for message on subscribed queue and write to xcom the message with key ``messages``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message is available or ``False``
        """"""

        sqs_hook = SQSHook(aws_conn_id=self.aws_conn_id)
        sqs_conn = sqs_hook.get_conn()

        self.log.info('SQSSensor checking for message on queue: %s', self.sqs_queue)

        messages = sqs_conn.receive_message(QueueUrl=self.sqs_queue,
                                            MaxNumberOfMessages=self.max_messages,
                                            WaitTimeSeconds=self.wait_time_seconds)

        self.log.info(""reveived message %s"", str(messages))

        if 'Messages' in messages and len(messages['Messages']) > 0:

            entries = [{'Id': message['MessageId'], 'ReceiptHandle': message['ReceiptHandle']}
                       for message in messages['Messages']]

            result = sqs_conn.delete_message_batch(QueueUrl=self.sqs_queue,
                                                   Entries=entries)

            if 'Successful' in result:
                context['ti'].xcom_push(key='messages', value=messages)
                return True
            else:
                raise AirflowException(
                    'Delete SQS Messages failed ' + str(result) + ' for messages ' + str(messages))

        return False",def,poke,(,self,",",context,),:,sqs_hook,=,SQSHook,(,aws_conn_id,=,self,.,aws_conn_id,),sqs_conn,=,sqs_hook,.,get_conn,(,),self,.,log,.,info,(,'SQSSensor checking for message on queue: %s',",",self,.,sqs_queue,),messages,=,sqs_conn,.,receive_message,(,QueueUrl,=,self,.,sqs_queue,",",MaxNumberOfMessages,=,self,"Check for message on subscribed queue and write to xcom the message with key ``messages``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message is available or ``False``",Check,for,message,on,subscribed,queue,and,write,to,xcom,the,message,with,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/aws_sqs_sensor.py#L58-L93,test,.,max_messages,",",WaitTimeSeconds,=,self,.,wait_time_seconds,),self,.,log,.,info,(,"""reveived message %s""",",",str,(,messages,),),if,'Messages',in,messages,and,len,(,messages,[,'Messages',],),>,0,:,entries,=,[,{,'Id',:,message,[,'MessageId',],",",'ReceiptHandle',:,message,[,'ReceiptHandle',],},for,message,in,messages,[,'Messages',],],result,=,sqs_conn,.,delete_message_batch,(,QueueUrl,=,self,.,sqs_queue,",",Entries,=,entries,),if,'Successful',in,result,:,context,[,'ti',],.,xcom_push,(,key,=,'messages',",",value,=,messages,),return,True,else,:,raise,AirflowException,(,'Delete SQS Messages failed ',+,str,(,result,),+,' for messages ',+,str,(,messages,),),return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,key,messages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/hdfs_hook.py,HDFSHook.get_conn,"def get_conn(self):
        """"""
        Returns a snakebite HDFSClient object.
        """"""
        # When using HAClient, proxy_user must be the same, so is ok to always
        # take the first.
        effective_user = self.proxy_user
        autoconfig = self.autoconfig
        use_sasl = configuration.conf.get('core', 'security') == 'kerberos'

        try:
            connections = self.get_connections(self.hdfs_conn_id)

            if not effective_user:
                effective_user = connections[0].login
            if not autoconfig:
                autoconfig = connections[0].extra_dejson.get('autoconfig',
                                                             False)
            hdfs_namenode_principal = connections[0].extra_dejson.get(
                'hdfs_namenode_principal')
        except AirflowException:
            if not autoconfig:
                raise

        if autoconfig:
            # will read config info from $HADOOP_HOME conf files
            client = AutoConfigClient(effective_user=effective_user,
                                      use_sasl=use_sasl)
        elif len(connections) == 1:
            client = Client(connections[0].host, connections[0].port,
                            effective_user=effective_user, use_sasl=use_sasl,
                            hdfs_namenode_principal=hdfs_namenode_principal)
        elif len(connections) > 1:
            nn = [Namenode(conn.host, conn.port) for conn in connections]
            client = HAClient(nn, effective_user=effective_user,
                              use_sasl=use_sasl,
                              hdfs_namenode_principal=hdfs_namenode_principal)
        else:
            raise HDFSHookException(""conn_id doesn't exist in the repository ""
                                    ""and autoconfig is not specified"")

        return client",python,"def get_conn(self):
        """"""
        Returns a snakebite HDFSClient object.
        """"""
        # When using HAClient, proxy_user must be the same, so is ok to always
        # take the first.
        effective_user = self.proxy_user
        autoconfig = self.autoconfig
        use_sasl = configuration.conf.get('core', 'security') == 'kerberos'

        try:
            connections = self.get_connections(self.hdfs_conn_id)

            if not effective_user:
                effective_user = connections[0].login
            if not autoconfig:
                autoconfig = connections[0].extra_dejson.get('autoconfig',
                                                             False)
            hdfs_namenode_principal = connections[0].extra_dejson.get(
                'hdfs_namenode_principal')
        except AirflowException:
            if not autoconfig:
                raise

        if autoconfig:
            # will read config info from $HADOOP_HOME conf files
            client = AutoConfigClient(effective_user=effective_user,
                                      use_sasl=use_sasl)
        elif len(connections) == 1:
            client = Client(connections[0].host, connections[0].port,
                            effective_user=effective_user, use_sasl=use_sasl,
                            hdfs_namenode_principal=hdfs_namenode_principal)
        elif len(connections) > 1:
            nn = [Namenode(conn.host, conn.port) for conn in connections]
            client = HAClient(nn, effective_user=effective_user,
                              use_sasl=use_sasl,
                              hdfs_namenode_principal=hdfs_namenode_principal)
        else:
            raise HDFSHookException(""conn_id doesn't exist in the repository ""
                                    ""and autoconfig is not specified"")

        return client",def,get_conn,(,self,),:,"# When using HAClient, proxy_user must be the same, so is ok to always",# take the first.,effective_user,=,self,.,proxy_user,autoconfig,=,self,.,autoconfig,use_sasl,=,configuration,.,conf,.,get,(,'core',",",'security',),==,'kerberos',try,:,connections,=,self,.,get_connections,(,self,.,hdfs_conn_id,),if,not,effective_user,:,effective_user,=,connections,[,Returns a snakebite HDFSClient object.,Returns,a,snakebite,HDFSClient,object,.,,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hdfs_hook.py#L57-L98,test,0,],.,login,if,not,autoconfig,:,autoconfig,=,connections,[,0,],.,extra_dejson,.,get,(,'autoconfig',",",False,),hdfs_namenode_principal,=,connections,[,0,],.,extra_dejson,.,get,(,'hdfs_namenode_principal',),except,AirflowException,:,if,not,autoconfig,:,raise,if,autoconfig,:,# will read config info from $HADOOP_HOME conf files,client,=,AutoConfigClient,(,effective_user,=,effective_user,",",use_sasl,=,use_sasl,),elif,len,(,connections,),==,1,:,client,=,Client,(,connections,[,0,],.,host,",",connections,[,0,],.,port,",",effective_user,=,effective_user,",",use_sasl,=,use_sasl,",",hdfs_namenode_principal,=,hdfs_namenode_principal,),elif,len,(,connections,),>,1,:,nn,=,[,Namenode,(,conn,.,host,",",conn,.,port,),for,conn,in,connections,],client,=,HAClient,(,nn,",",effective_user,=,effective_user,",",use_sasl,=,use_sasl,",",hdfs_namenode_principal,=,hdfs_namenode_principal,),else,:,raise,HDFSHookException,(,"""conn_id doesn't exist in the repository ""","""and autoconfig is not specified""",),return,client,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/webhdfs_hook.py,WebHDFSHook.get_conn,"def get_conn(self):
        """"""
        Establishes a connection depending on the security mode set via config or environment variable.

        :return: a hdfscli InsecureClient or KerberosClient object.
        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient
        """"""
        connections = self.get_connections(self.webhdfs_conn_id)

        for connection in connections:
            try:
                self.log.debug('Trying namenode %s', connection.host)
                client = self._get_client(connection)
                client.status('/')
                self.log.debug('Using namenode %s for hook', connection.host)
                return client
            except HdfsError as hdfs_error:
                self.log.debug('Read operation on namenode %s failed with error: %s',
                               connection.host, hdfs_error)

        hosts = [connection.host for connection in connections]
        error_message = 'Read operations failed on the namenodes below:\n{hosts}'.format(
            hosts='\n'.join(hosts))
        raise AirflowWebHDFSHookException(error_message)",python,"def get_conn(self):
        """"""
        Establishes a connection depending on the security mode set via config or environment variable.

        :return: a hdfscli InsecureClient or KerberosClient object.
        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient
        """"""
        connections = self.get_connections(self.webhdfs_conn_id)

        for connection in connections:
            try:
                self.log.debug('Trying namenode %s', connection.host)
                client = self._get_client(connection)
                client.status('/')
                self.log.debug('Using namenode %s for hook', connection.host)
                return client
            except HdfsError as hdfs_error:
                self.log.debug('Read operation on namenode %s failed with error: %s',
                               connection.host, hdfs_error)

        hosts = [connection.host for connection in connections]
        error_message = 'Read operations failed on the namenodes below:\n{hosts}'.format(
            hosts='\n'.join(hosts))
        raise AirflowWebHDFSHookException(error_message)",def,get_conn,(,self,),:,connections,=,self,.,get_connections,(,self,.,webhdfs_conn_id,),for,connection,in,connections,:,try,:,self,.,log,.,debug,(,'Trying namenode %s',",",connection,.,host,),client,=,self,.,_get_client,(,connection,),client,.,status,(,'/',),self,.,log,"Establishes a connection depending on the security mode set via config or environment variable.

        :return: a hdfscli InsecureClient or KerberosClient object.
        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient",Establishes,a,connection,depending,on,the,security,mode,set,via,config,or,environment,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/webhdfs_hook.py#L56-L79,test,.,debug,(,'Using namenode %s for hook',",",connection,.,host,),return,client,except,HdfsError,as,hdfs_error,:,self,.,log,.,debug,(,'Read operation on namenode %s failed with error: %s',",",connection,.,host,",",hdfs_error,),hosts,=,[,connection,.,host,for,connection,in,connections,],error_message,=,'Read operations failed on the namenodes below:\n{hosts}',.,format,(,hosts,=,'\n',.,join,(,hosts,),),raise,AirflowWebHDFSHookException,(,error_message,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,variable,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/webhdfs_hook.py,WebHDFSHook.check_for_path,"def check_for_path(self, hdfs_path):
        """"""
        Check for the existence of a path in HDFS by querying FileStatus.

        :param hdfs_path: The path to check.
        :type hdfs_path: str
        :return: True if the path exists and False if not.
        :rtype: bool
        """"""
        conn = self.get_conn()

        status = conn.status(hdfs_path, strict=False)
        return bool(status)",python,"def check_for_path(self, hdfs_path):
        """"""
        Check for the existence of a path in HDFS by querying FileStatus.

        :param hdfs_path: The path to check.
        :type hdfs_path: str
        :return: True if the path exists and False if not.
        :rtype: bool
        """"""
        conn = self.get_conn()

        status = conn.status(hdfs_path, strict=False)
        return bool(status)",def,check_for_path,(,self,",",hdfs_path,),:,conn,=,self,.,get_conn,(,),status,=,conn,.,status,(,hdfs_path,",",strict,=,False,),return,bool,(,status,),,,,,,,,,,,,,,,,,,,,,"Check for the existence of a path in HDFS by querying FileStatus.

        :param hdfs_path: The path to check.
        :type hdfs_path: str
        :return: True if the path exists and False if not.
        :rtype: bool",Check,for,the,existence,of,a,path,in,HDFS,by,querying,FileStatus,.,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/webhdfs_hook.py#L92-L104,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/hooks/webhdfs_hook.py,WebHDFSHook.load_file,"def load_file(self, source, destination, overwrite=True, parallelism=1, **kwargs):
        r""""""
        Uploads a file to HDFS.

        :param source: Local path to file or folder.
            If it's a folder, all the files inside of it will be uploaded.
            .. note:: This implies that folders empty of files will not be created remotely.

        :type source: str
        :param destination: PTarget HDFS path.
            If it already exists and is a directory, files will be uploaded inside.
        :type destination: str
        :param overwrite: Overwrite any existing file or directory.
        :type overwrite: bool
        :param parallelism: Number of threads to use for parallelization.
            A value of `0` (or negative) uses as many threads as there are files.
        :type parallelism: int
        :param \**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.
        """"""
        conn = self.get_conn()

        conn.upload(hdfs_path=destination,
                    local_path=source,
                    overwrite=overwrite,
                    n_threads=parallelism,
                    **kwargs)
        self.log.debug(""Uploaded file %s to %s"", source, destination)",python,"def load_file(self, source, destination, overwrite=True, parallelism=1, **kwargs):
        r""""""
        Uploads a file to HDFS.

        :param source: Local path to file or folder.
            If it's a folder, all the files inside of it will be uploaded.
            .. note:: This implies that folders empty of files will not be created remotely.

        :type source: str
        :param destination: PTarget HDFS path.
            If it already exists and is a directory, files will be uploaded inside.
        :type destination: str
        :param overwrite: Overwrite any existing file or directory.
        :type overwrite: bool
        :param parallelism: Number of threads to use for parallelization.
            A value of `0` (or negative) uses as many threads as there are files.
        :type parallelism: int
        :param \**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.
        """"""
        conn = self.get_conn()

        conn.upload(hdfs_path=destination,
                    local_path=source,
                    overwrite=overwrite,
                    n_threads=parallelism,
                    **kwargs)
        self.log.debug(""Uploaded file %s to %s"", source, destination)",def,load_file,(,self,",",source,",",destination,",",overwrite,=,True,",",parallelism,=,1,",",*,*,kwargs,),:,conn,=,self,.,get_conn,(,),conn,.,upload,(,hdfs_path,=,destination,",",local_path,=,source,",",overwrite,=,overwrite,",",n_threads,=,parallelism,",",*,*,kwargs,"r""""""
        Uploads a file to HDFS.

        :param source: Local path to file or folder.
            If it's a folder, all the files inside of it will be uploaded.
            .. note:: This implies that folders empty of files will not be created remotely.

        :type source: str
        :param destination: PTarget HDFS path.
            If it already exists and is a directory, files will be uploaded inside.
        :type destination: str
        :param overwrite: Overwrite any existing file or directory.
        :type overwrite: bool
        :param parallelism: Number of threads to use for parallelization.
            A value of `0` (or negative) uses as many threads as there are files.
        :type parallelism: int
        :param \**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.",r,Uploads,a,file,to,HDFS,.,,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/webhdfs_hook.py#L106-L132,test,),self,.,log,.,debug,(,"""Uploaded file %s to %s""",",",source,",",destination,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/pinot_hook.py,PinotDbApiHook.get_conn,"def get_conn(self):
        """"""
        Establish a connection to pinot broker through pinot dbqpi.
        """"""
        conn = self.get_connection(self.pinot_broker_conn_id)
        pinot_broker_conn = connect(
            host=conn.host,
            port=conn.port,
            path=conn.extra_dejson.get('endpoint', '/pql'),
            scheme=conn.extra_dejson.get('schema', 'http')
        )
        self.log.info('Get the connection to pinot '
                      'broker on {host}'.format(host=conn.host))
        return pinot_broker_conn",python,"def get_conn(self):
        """"""
        Establish a connection to pinot broker through pinot dbqpi.
        """"""
        conn = self.get_connection(self.pinot_broker_conn_id)
        pinot_broker_conn = connect(
            host=conn.host,
            port=conn.port,
            path=conn.extra_dejson.get('endpoint', '/pql'),
            scheme=conn.extra_dejson.get('schema', 'http')
        )
        self.log.info('Get the connection to pinot '
                      'broker on {host}'.format(host=conn.host))
        return pinot_broker_conn",def,get_conn,(,self,),:,conn,=,self,.,get_connection,(,self,.,pinot_broker_conn_id,),pinot_broker_conn,=,connect,(,host,=,conn,.,host,",",port,=,conn,.,port,",",path,=,conn,.,extra_dejson,.,get,(,'endpoint',",",'/pql',),",",scheme,=,conn,.,extra_dejson,.,get,Establish a connection to pinot broker through pinot dbqpi.,Establish,a,connection,to,pinot,broker,through,pinot,dbqpi,.,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/pinot_hook.py#L36-L49,test,(,'schema',",",'http',),),self,.,log,.,info,(,'Get the connection to pinot ','broker on {host}',.,format,(,host,=,conn,.,host,),),return,pinot_broker_conn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/hooks/pinot_hook.py,PinotDbApiHook.get_uri,"def get_uri(self):
        """"""
        Get the connection uri for pinot broker.

        e.g: http://localhost:9000/pql
        """"""
        conn = self.get_connection(getattr(self, self.conn_name_attr))
        host = conn.host
        if conn.port is not None:
            host += ':{port}'.format(port=conn.port)
        conn_type = 'http' if not conn.conn_type else conn.conn_type
        endpoint = conn.extra_dejson.get('endpoint', 'pql')
        return '{conn_type}://{host}/{endpoint}'.format(
            conn_type=conn_type, host=host, endpoint=endpoint)",python,"def get_uri(self):
        """"""
        Get the connection uri for pinot broker.

        e.g: http://localhost:9000/pql
        """"""
        conn = self.get_connection(getattr(self, self.conn_name_attr))
        host = conn.host
        if conn.port is not None:
            host += ':{port}'.format(port=conn.port)
        conn_type = 'http' if not conn.conn_type else conn.conn_type
        endpoint = conn.extra_dejson.get('endpoint', 'pql')
        return '{conn_type}://{host}/{endpoint}'.format(
            conn_type=conn_type, host=host, endpoint=endpoint)",def,get_uri,(,self,),:,conn,=,self,.,get_connection,(,getattr,(,self,",",self,.,conn_name_attr,),),host,=,conn,.,host,if,conn,.,port,is,not,None,:,host,+=,':{port}',.,format,(,port,=,conn,.,port,),conn_type,=,'http',if,not,conn,"Get the connection uri for pinot broker.

        e.g: http://localhost:9000/pql",Get,the,connection,uri,for,pinot,broker,.,,,,,,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/pinot_hook.py#L51-L64,test,.,conn_type,else,conn,.,conn_type,endpoint,=,conn,.,extra_dejson,.,get,(,'endpoint',",",'pql',),return,'{conn_type}://{host}/{endpoint}',.,format,(,conn_type,=,conn_type,",",host,=,host,",",endpoint,=,endpoint,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache/airflow,airflow/contrib/operators/gcp_transfer_operator.py,TransferJobPreprocessor._convert_date_to_dict,"def _convert_date_to_dict(field_date):
        """"""
        Convert native python ``datetime.date`` object  to a format supported by the API
        """"""
        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}",python,"def _convert_date_to_dict(field_date):
        """"""
        Convert native python ``datetime.date`` object  to a format supported by the API
        """"""
        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}",def,_convert_date_to_dict,(,field_date,),:,return,{,DAY,:,field_date,.,day,",",MONTH,:,field_date,.,month,",",YEAR,:,field_date,.,year,},,,,,,,,,,,,,,,,,,,,,,,,,,,Convert native python ``datetime.date`` object  to a format supported by the API,Convert,native,python,datetime,.,date,object,to,a,format,supported,by,the,b69c686ad8a0c89b9136bb4b31767257eb7b2597,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/gcp_transfer_operator.py#L106-L110,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
