code,docstring,generated_comment
"def foo(p, level=""s""):
  level = level+""__""
 
  result = p.split(level)
 
  return result[0]+level+result[1].split("";"")[0]","Return either the full or truncated version of a QIIME-formatted taxonomy string.
 
 
 
  :type p: str
 
  :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...
 
 
 
  :type level: str
 
  :param level: The different level of identification are kingdom (k), phylum (p),
 
  class (c),order (o), family (f), genus (g) and species (s). If level is
 
  not provided, the default level of identification is species.
 
 
 
  :rtype: str
 
  :return: A QIIME-formatted taxonomy string up to the classification given
 
  by param level.","This function, named `foo`, takes in two parameters: `p` and `level`. The default value for `level` is set to ""s"". 

Inside the function, the `level` parameter is concatenated with two underscore characters to create a modified level. 

The input string `p` is then split using the modified level as the delimiter. This splits the string into two parts and stores them in the `result` variable.

The function then returns the concatenation of the first part of `result`, the modified level, and the first part of the second part of `result` (which is obtained by splitting the second part of `result` using a semicolon as the delimiter).

In simpler terms, this function takes a string `p` and splits it into two parts based on the provided level. It then returns a new string consisting of the first part, the modified level, and the first part of the second part (before a semicolon, if present)."
"def foo(d):

  if not os.path.exists(d):
 
  try:
 
  os.makedirs(d)
 
  except OSError as oe:
 
  # should not happen with os.makedirs
 
  # ENOENT: No such file or directory
 
  if os.errno == errno.ENOENT:
 
  msg = twdd(""""""One or more directories in the path ({}) do not exist. If
 
  you are specifying a new directory for output, please ensure
 
  all other directories in the path currently exist."""""")
 
  return msg.format(d)
 
  else:
 
  msg = twdd(""""""An error occurred trying to create the output directory
 
  ({}) with message: {}"""""")
 
  return msg.format(d, oe.strerror)","Check to make sure the supplied directory path does not exist, if so, create it. The
 
  method catches OSError exceptions and returns a descriptive message instead of
 
  re-raising the error.
 
 
 
  :type d: str
 
  :param d: It is the full path to a directory.
 
 
 
  :return: Does not return anything, but creates a directory path if it doesn't exist
 
  already.","The function `foo` is used to create a directory `d`, if it does not already exist. It takes a parameter `d`, which represents the desired directory path.

First, the function checks if the directory does not exist using the `os.path.exists` function. If this condition is true, it proceeds to create the directory.

Inside the `try` block, the function attempts to create the directory using the `os.makedirs` function. If the directory creation is successful, the function completes and does not return anything.

If an exception occurs while creating the directory, the code enters the `except` block. Within this block, it checks the specific type of error that occurred. In this case, it is checking for an `OSError` with the `errno.ENOENT` error number, which indicates that the parent directory of the desired directory does not exist.

If the error condition matches, the function generates a message explaining that one or more directories in the path do not exist. It suggests that if the output directory is intended to be new, all other directories in the path should already exist. The function then returns this error message, formatted with the directory path.

If the error condition does not match, it generates a message indicating that an error occurred while creating the output directory. It includes the directory path and the specific error message generated by the `OSError`. The function then returns this error message, formatted with the directory path and error message."
"def foo(fnh, mode=""rU""):
 
  handle = None
 
  if isinstance(fnh, file):
 
  if fnh.closed:
 
  raise ValueError(""Input file is closed."")
 
  handle = fnh
 
  elif isinstance(fnh, str):
 
  handle = open(fnh, mode)
 
 
 
  return handle","Takes either a file path or an open file handle, checks validity and returns an open
 
  file handle or raises an appropriate Exception.
 
 
 
  :type fnh: str
 
  :param fnh: It is the full path to a file, or open file handle
 
 
 
  :type mode: str
 
  :param mode: The way in which this file will be used, for example to read or write or
 
  both. By default, file will be opened in rU mode.
 
 
 
  :return: Returns an opened file for appropriate usage.","This function, named ""foo"", takes two arguments: ""fnh"" and ""mode"", with a default value of ""rU"". The purpose of this function is to open a file and return a file handle. 

First, a variable named ""handle"" is initialized to None. 

Then, the function checks if the ""fnh"" argument is of type ""file"" using the ""isinstance()"" function. If it is, the function checks if the file is closed using the ""closed"" attribute. If the file is closed, a ValueError is raised with the message ""Input file is closed."". If the file is not closed, the ""handle"" variable is assigned the value of ""fnh"". 

If the ""fnh"" argument is not of type ""file"", the function checks if it is of type ""str"" using the ""isinstance()"" function. If it is, the function opens the file specified by ""fnh"" with the given mode (""rU"" by default) using the ""open()"" function, and assigns the resulting file handle to the ""handle"" variable. 

Finally, the function returns the file handle stored in the ""handle"" variable."
"def foo(imap, header, categories=None):

 
  if categories is None:
 
  return {""default"": DataCategory(set(imap.keys()), {})}
 
 
 
  cat_ids = [header.index(cat)
 
  for cat in categories if cat in header and ""="" not in cat]
 
 
 
  table = OrderedDict()
 
  conditions = defaultdict(set)
 
  for i, cat in enumerate(categories):
 
  if ""="" in cat and cat.split(""="")[0] in header:
 
  cat_name = header[header.index(cat.split(""="")[0])]
 
  conditions[cat_name].add(cat.split(""="")[1])

  if not cat_ids and not conditions:
 
  return {""default"": DataCategory(set(imap.keys()), {})}
 

  if cat_ids and not conditions:
 
  for sid, row in imap.items():
 
  cat_name = ""_"".join([row[cid] for cid in cat_ids])
 
  if cat_name not in table:
 
  table[cat_name] = DataCategory(set(), {})
 
  table[cat_name].sids.add(sid)
 
  return table
 
ct all condition names
 
  cond_ids = set()
 
  for k in conditions:
 
  try:
 
  cond_ids.add(header.index(k))
 
  except ValueError:
 
  continue
 
  idx_to_test = set(cat_ids).union(cond_ids)

 
  for sid, row in imap.items():
 
  if all([row[header.index(c)] in conditions[c] for c in conditions]):
 
  key = ""_"".join([row[idx] for idx in idx_to_test])
 
  try:
 
  assert key in table.keys()
 
  except AssertionError:
 
  table[key] = DataCategory(set(), {})
 
  table[key].sids.add(sid)
 
  try:
 
  assert len(table) > 0
 
  except AssertionError:
 
  return {""default"": DataCategory(set(imap.keys()), {})}
 
  else:
 
  return table","Find the user specified categories in the map and create a dictionary to contain the
 
  relevant data for each type within the categories. Multiple categories will have their
 
  types combined such that each possible combination will have its own entry in the
 
  dictionary.
 
 
 
  :type imap: dict
 
  :param imap: The input mapping file data keyed by SampleID
 
  :type header: list
 
  :param header: The header line from the input mapping file. This will be searched for
 
  the user-specified categories
 
  :type categories: list
 
  :param categories: The list of user-specified category column name from mapping file
 
  :rtype: dict
 
  :return: A sorted dictionary keyed on the combinations of all the types found within
 
  the user-specified categories. Each entry will contain an empty DataCategory
 
  namedtuple. If no categories are specified, a single entry with the key
 
  'default' will be returned","The `foo` function takes in an `imap` (dictionary representing some data), `header` (list of column names), and `categories` (optional list of categories).

If `categories` is not provided, a default data category is created with all the keys from `imap` and an empty category dictionary.

If `categories` is provided, the function checks for valid category names (column names in `header` without ""="") and stores their indices in `cat_ids`. It also creates an empty ordered dictionary called `table` to store the resulting data categories.

If there are no valid category names (`cat_ids`) and no conditions (`conditions`), the function returns the default data category.

If there are valid category names but no conditions, the function iterates through each row in `imap` and creates a category name by joining the values from the valid category indices in `cat_ids`. It then adds the row to the corresponding category in the `table` dictionary. Finally, it returns the `table`.

If there are both valid category names and conditions, the function determines the indices of the condition names in `header`. It creates a set `idx_to_test` which contains both the valid category indices and the condition indices. It then iterates through each row in `imap` and checks if all the conditions are satisfied. If they are, it creates a key by joining the values from the indices in `idx_to_test`, and adds the row to the corresponding category in the `table` dictionary. If the `table` dictionary is empty, the function returns the default data category. Otherwise, it returns the `table`."
"def foo(unifracFN):
 
  with open(unifracFN, ""rU"") as uF:
 
  first = uF.next().split(""\t"")
 
  lines = [line.strip() for line in uF]
 
 
 
  unifrac = {""pcd"": OrderedDict(), ""eigvals"": [], ""varexp"": []}
 
  if first[0] == ""pc vector number"":
 
  return parse_unifrac_v1_8(unifrac, lines)
 
  elif first[0] == ""Eigvals"":
 
  return parse_unifrac_v1_9(unifrac, lines)
 
  else:
 
  raise ValueError(""File format not supported/recognized. Please check input ""
 
  ""unifrac file."")","Parses the unifrac results file into a dictionary
 
 
 
  :type unifracFN: str
 
  :param unifracFN: The path to the unifrac results file
 
 
 
  :rtype: dict
 
  :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a
 
  dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and
 
  'varexp' (variation explained)","This function takes a filename as input and reads the contents of the specified file. 
The file is expected to be in a specific format, either version 1.8 or 1.9 of the unifrac file format. 
The function then parses the contents of the file and extracts relevant data, including PC vectors, eigenvalues, and variance explained values. 
The parsed data is stored in a dictionary called 'unifrac', with the PC vectors being stored in an ordered dictionary. 
If the file format is not recognized or supported, a ValueError is raised."
"def foo(unifrac, file_data):
  for line in file_data:
 
  if line == """":
 
  break
 
  line = line.split(""\t"")
 
  unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]
 
 
 
  unifrac[""eigvals""] = [float(entry) for entry in file_data[-2].split(""\t"")[1:]]
 
  unifrac[""varexp""] = [float(entry) for entry in file_data[-1].split(""\t"")[1:]]
 
  return unifrac","Function to parse data from older version of unifrac file obtained from Qiime version
 
  1.8 and earlier.
 
 
 
  :type unifrac: dict
 
  :param unifracFN: The path to the unifrac results file
 
 
 
  :type file_data: list
 
  :param file_data: Unifrac data lines after stripping whitespace characters.","This function takes two parameters: 'unifrac' and 'file_data'.

The function iterates over each line in the 'file_data' parameter.

If the line is an empty string, the iteration is stopped and the function returns.

The line is then split into a list using the tab character as the delimiter.

The values in the line, starting from index 1, are converted to float and assigned to the 'unifrac' dictionary under the key 'pcd' with the corresponding line[0] value as the sub-key.

The 'unifrac' dictionary is then updated with two additional keys: 'eigvals' and 'varexp'. These keys are assigned lists of floats, obtained by splitting the second-to-last and last lines of 'file_data' into lists using the tab character as the delimiter.

Finally, the 'unifrac' dictionary is returned."
"def foo(unifrac, file_data):
  unifrac[""eigvals""] = [float(entry) for entry in file_data[0].split(""\t"")]
 
  unifrac[""varexp""] = [float(entry)*100 for entry in file_data[3].split(""\t"")]
 
 
 
  for line in file_data[8:]:
 
  if line == """":
 
  break
 
  line = line.split(""\t"")
 
  unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]
 
  return unifrac","Function to parse data from newer version of unifrac file obtained from Qiime version
 
  1.9 and later.
 
 
 
  :type unifracFN: str
 
  :param unifracFN: The path to the unifrac results file
 
 
 
  :type file_data: list
 
  :param file_data: Unifrac data lines after stripping whitespace characters.","This function takes in two parameters: ""unifrac"" and ""file_data"". The ""unifrac"" parameter is a dictionary that will store the results obtained from processing the ""file_data"" parameter.

The first line of code converts the first element of ""file_data"" into a list of floating-point numbers and assigns it to the ""eigvals"" key in the ""unifrac"" dictionary.

The second line of code does a similar conversion, but it also multiplies each element by 100. The resulting list is assigned to the ""varexp"" key in the ""unifrac"" dictionary.

The for loop iterates over the elements in ""file_data"" starting from the 8th element. It checks if the line is an empty string and if so, breaks out of the loop.

If the line is not empty, it is split into a list of values using the tab (\t) as the delimiter.

The first element of the split line is used as the key in the ""pcd"" dictionary within the ""unifrac"" dictionary. The remaining elements are converted to floating-point numbers and stored as the values corresponding to the key.

Finally, the updated ""unifrac"" dictionary is returned as the output of the function."
"def foo(sample_map, header, group_column, color_column=None):
 
  group_colors = OrderedDict()
 
  group_gather = gather_categories(sample_map, header, [group_column])
 
 
 
  if color_column is not None:
 
  color_gather = gather_categories(sample_map, header, [color_column])
 
  # match sample IDs between color_gather and group_gather
 
  for group in group_gather:
 
  for color in color_gather:
 
  # allow incomplete assignment of colors, if group sids overlap at
 
  # all with the color sids, consider it a match
 
  if group_gather[group].sids.intersection(color_gather[color].sids):
 
  group_colors[group] = color
 
  else:
 
  bcolors = itertools.cycle(Set3_12.hex_colors)
 
  for group in group_gather:
 
  group_colors[group] = bcolors.next()
 
 
 
  return group_colors","Determine color-category mapping. If color_column was specified, then map the category
 
  names to color values. Otherwise, use the palettable colors to automatically generate
 
  a set of colors for the group values.
 
 
 
  :type sample_map: dict
 
  :param unifracFN: Map associating each line of the mapping file with the appropriate
 
  sample ID (each value of the map also contains the sample ID)
 
 
 
  :type header: tuple
 
  :param A tuple of header line for mapping file
 
 
 
  :type group_column: str
 
  :param group_column: String denoting the column name for sample groups.
 
 
 
  :type color_column: str
 
  :param color_column: String denoting the column name for sample colors.
 
 
 
  :type return: dict
 
  :param return: {SampleID: Color}","This code defines a function called ""foo"" that takes in a ""sample_map"", a ""header"", a ""group_column"", and an optional ""color_column"" as input arguments. 

The function first creates an empty ordered dictionary called ""group_colors"" to store the grouping information. 

Then, it calls a helper function called ""gather_categories"" to gather the unique categories in the ""group_column"" of the ""sample_map"" and store it in a variable called ""group_gather"".

If a ""color_column"" is provided, the function also calls the ""gather_categories"" function to gather the unique categories in the ""color_column"" and store it in a variable called ""color_gather"". 

Next, the code iterates over the unique groups in ""group_gather"" and checks if there is a corresponding group in ""color_gather"" that intersects with the group in ""group_gather"". If there is a match, it assigns the color of that group in ""color_gather"" to the corresponding group in ""group_gather"" and adds it to the ""group_colors"" dictionary.

If there is no match, the code assigns a unique color from the ""Set3_12.hex_colors"" list to each group in ""group_gather"" using the ""itertools.cycle"" function.

Finally, the function returns the ""group_colors"" dictionary, which contains the assigned colors for each group.

Overall, this function takes in a sample map, along with grouping and color columns, and assigns colors to each group in the grouping column based on the colors found in the color column (if provided) or assigns unique colors if there is no match."
"def foo(read):
 
  rc = []
 
  rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}
 
  for base in read:
 
  rc.extend(rc_nucs[base.upper()])
 
  return rc[::-1]",return reverse completment of read,"This code defines a function called ""foo"" that takes in a DNA sequence as input. The function's purpose is to generate the reverse complement of the input sequence. 

The function starts by initializing an empty list called ""rc"" to store the reverse complement nucleotides. 

Next, a dictionary called ""rc_nucs"" is created, mapping each nucleotide to its corresponding complement. 

A for loop is then used to iterate through each base in the input sequence. Within the loop, the function retrieves the complement of the current base using the dictionary ""rc_nucs"" and appends it to the ""rc"" list. 

Finally, the function returns the reverse of the ""rc"" list, which effectively gives the reverse complement of the input DNA sequence."
"def foo(genome, cat, fraction = float(100), plot = True, \
 
  alpha = 0.1, beta = 100000, \
 
  min_length = 1000, max_length = 200000):
 
  header = '>randomized_%s' % (genome.name)
 
  sequence = list(''.join([i[1] for i in parse_fasta(genome)]))
 
  length = len(sequence)
 
  shuffled = []
 
  # break genome into pieces
 
  while sequence is not False:
 
  s = int(random.gammavariate(alpha, beta))
 
  if s <= min_length or s >= max_length:
 
  continue
 
  if len(sequence) < s:
 
  seq = sequence[0:]
 
  else:
 
  seq = sequence[0:s]
 
  sequence = sequence[s:]
 
 # if bool(random.getrandbits(1)) is True:
 
 # seq = rev_c(seq)
 
 # print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)
 
 # else:
 
 # print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)
 
  shuffled.append(''.join(seq))
 
  if sequence == []:
 
  break
 
  # shuffle pieces
 
  random.shuffle(shuffled)
 
  # subset fragments
 
  if fraction == float(100):
 
  subset = shuffled
 
  else:
 
  max_pieces = int(length * fraction/100)
 
  subset, total = [], 0
 
  for fragment in shuffled:
 
  length = len(fragment)
 
  if total + length <= max_pieces:
 
  subset.append(fragment)
 
  total += length
 
  else:
 
  diff = max_pieces - total
 
  subset.append(fragment[0:diff])
 
  break
 
  # combine sequences, if requested
 
  if cat is True:
 
  yield [header, ''.join(subset)]
 
  else:
 
  for i, seq in enumerate(subset):
 
  yield ['%s fragment:%s' % (header, i), seq]",randomly shuffle genome,"The function `foo` takes several input parameters: `genome` (which represents a genome object), `cat` (a boolean value indicating whether to combine the generated sequences into one), `fraction` (a float value representing the percentage of the genome to retain), `plot` (a boolean value indicating whether to plot the results), `alpha` and `beta` (parameters for the gamma distribution used to generate fragment lengths), and `min_length` and `max_length` (to set the minimum and maximum fragment lengths).

The function starts by creating a header string for the output sequences, using the name of the input genome.

Next, it converts the genome sequence into a list of characters.

Then, it loops until the sequence is exhausted. It randomly selects a fragment length using the gamma distribution parameters and checks if it falls within the specified min_length and max_length criteria. If not, it continues to the next iteration. If the remaining sequence is shorter than the selected fragment length, it takes the remaining sequence as the fragment. Otherwise, it selects the sequence from the beginning to the selected length and updates the remaining sequence by removing that portion.

The code then shuffles the generated fragments using the `random.shuffle` function.

If the `fraction` parameter is set to 100, it sets the `subset` variable to contain all the shuffled fragments. Otherwise, it calculates the maximum number of pieces allowed based on the fraction and selects fragments accordingly. If the total length of the selected fragments exceeds the maximum, it trims the last fragment to fit the specified maximum.

Finally, if the `cat` parameter is True, it yields a single sequence with the header and combined fragments. Otherwise, it yields individual sequences with headers representing the original genome name and fragment number.

If the `plot` parameter is True, the code also plots the results."
"def foo(self, fit, p_max):
  def remove_from_model_desc(x, model_desc):
 
  rhs_termlist = []
 
  for t in model_desc.rhs_termlist:
 
  if not t.factors:
 
  # intercept, add anyway
 
  rhs_termlist.append(t)
 
  elif not x == t.factors[0]._varname:
 
  # this is not the term with x
 
  rhs_termlist.append(t)
 
 
 
  md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)
 
  return md
 
 
 
  corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])
 
  pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
 
  try:
 
  pars_to_prune.remove('Intercept')
 
  except:
 
  pass
 
  while pars_to_prune:
 
  corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)
 
  fit = fm.ols(corrected_model_desc, data=self.df).fit()
 
  pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
 
  try:
 
  pars_to_prune.remove('Intercept')
 
  except:
 
  pass
 
  return fit","If the fit contains statistically insignificant parameters, remove them.
 
  Returns a pruned fit where all parameters have p-values of the t-statistic below p_max
 
 
 
  Parameters
 
  ----------
 
  fit: fm.ols fit object
 
  Can contain insignificant parameters
 
  p_max : float
 
  Maximum allowed probability of the t-statistic
 
 
 
  Returns
 
  -------
 
  fit: fm.ols fit object
 
  Won't contain any insignificant parameters","The ""foo"" function takes in three parameters: self, fit, and p_max. 

Inside the ""foo"" function, there is a nested function called ""remove_from_model_desc"" which takes in two parameters: x and model_desc. 

There is a variable called ""rhs_termlist"" which is an empty list. 

A for loop iterates through the ""rhs_termlist"" in the ""model_desc"" object. 

If the term's factors list is empty, it means it is an intercept term, so it is added to the ""rhs_termlist"". 

If the term's factors list is not empty and the variable ""x"" is not equal to the first factor's variable name, the term is added to the ""rhs_termlist"". 

A new ""ModelDesc"" object called ""md"" is created with the modified ""lhs_termlist"" and ""rhs_termlist"". 

The ""md"" object is then returned. 

A new ""ModelDesc"" object called ""corrected_model_desc"" is created, which is a copy of the input ""fit"" model formula. 

A list called ""pars_to_prune"" is populated with the names of parameters that have p-values higher than ""p_max"". 

If ""Intercept"" is in the ""pars_to_prune"" list, it is removed. 

A loop is started that continues until the ""pars_to_prune"" list is empty. 

Inside the loop, the ""remove_from_model_desc"" function is called to remove the first parameter in the ""pars_to_prune"" list from the ""corrected_model_desc"". The result is stored back into ""corrected_model_desc"". 

A new regression model is fitted using the updated ""corrected_model_desc"" and the ""fit"" data. 

The ""pars_to_prune"" list is updated with the names of parameters that have p-values higher than ""p_max"" in the new fit. 

If ""Intercept"" is in the ""pars_to_prune"" list, it is removed. 

After the loop finishes, the final fitted model is returned."
"def find_best_rsquared(list_of_fits):
 
  """"""Return the best fit, based on rsquared""""""
 
  res = sorted(list_of_fits, key=lambda x: x.rsquared)
 
  return res[-1]","Return the best fit, based on rsquared","This code defines a function called ""find_best_rsquared"" that takes in a list of fits as input. It then returns the fit with the highest r-squared value, which is a measure of how well the fit predicts the data. The code accomplishes this by sorting the list of fits in ascending order based on the rsquared attribute of each fit, and then returning the last fit in the sorted list, which will have the highest rsquared value."
"def foo(self, fit, df):
 
  # Add model results to data as column 'predictions'
 
  df_res = df.copy()
 
  if 'Intercept' in fit.model.exog_names:
 
  df_res['Intercept'] = 1.0
 
  df_res['predicted'] = fit.predict(df_res)
 
  if not self.allow_negative_predictions:
 
  df_res.loc[df_res['predicted'] < 0, 'predicted'] = 0
 
 
 
  prstd, interval_l, interval_u = wls_prediction_std(fit,
 
  df_res[fit.model.exog_names],
 
  alpha=1 - self.confint)
 
  df_res['interval_l'] = interval_l
 
  df_res['interval_u'] = interval_u
 
 
 
  if 'Intercept' in df_res:
 
  df_res.drop(labels=['Intercept'], axis=1, inplace=True)
 
 
 
  return df_res","Return a df with predictions and confidence interval
 
 
 
  Notes
 
  -----
 
  The df will contain the following columns:
 
  - 'predicted': the model output
 
  - 'interval_u', 'interval_l': upper and lower confidence bounds.
 
 
 
  The result will depend on the following attributes of self:
 
  confint : float (default=0.95)
 
  Confidence level for two-sided hypothesis
 
  allow_negative_predictions : bool (default=True)
 
  If False, correct negative predictions to zero (typically for energy consumption predictions)
 
 
 
  Parameters
 
  ----------
 
  fit : Statsmodels fit
 
  df : pandas DataFrame or None (default)
 
  If None, use self.df
 
 
 
 
 
  Returns
 
  -------
 
  df_res : pandas DataFrame
 
  Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'","The given code defines a method called ""foo"" that takes in parameters ""self"", ""fit"", and ""df"". 
This method is used to add model results to a dataframe as a new column called ""predictions"". 

First, a copy of the input dataframe is created and stored in a variable called ""df_res"".

Next, it checks if the model used for fitting has an intercept term. If it does, a new column called ""Intercept"" is added to the dataframe with all values set to 1. This is done to ensure compatibility with the model's exog variables.

Then, the model is used to make predictions on the dataframe and the predicted values are stored in the ""predicted"" column of the dataframe.

If the option to allow negative predictions is not enabled, any predicted values that are less than 0 are set to 0.

After that, the code calculates the prediction standard error, lower interval, and upper interval using the ""wls_prediction_std"" function. These values are then added to the dataframe as ""interval_l"" and ""interval_u"" columns.

If the ""Intercept"" column exists in the dataframe, it is dropped to clean up the dataframe.

Finally, the modified dataframe is returned as the output of the method."
"def foo(biomf, sampleIDs=None):
 
  if sampleIDs is None:
 
  sampleIDs = biomf.ids()
 
  else:
 
  try:
 
  for sid in sampleIDs:
 
  assert sid in biomf.ids()
 
  except AssertionError:
 
  raise ValueError(
 
  ""\nError while calculating relative abundances: The sampleIDs provided do""
 
  "" not match the sampleIDs in biom file. Please double check the sampleIDs""
 
  "" provided.\n"")
 
  otuIDs = biomf.ids(axis=""observation"")
 
  norm_biomf = biomf.norm(inplace=False)
 
 
 
  return {sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample)
 
  for otuID in otuIDs} for sample in sampleIDs}","Calculate the relative abundance of each OTUID in a Sample.
 
 
 
  :type biomf: A BIOM file.
 
  :param biomf: OTU table format.
 
 
 
  :type sampleIDs: list
 
  :param sampleIDs: A list of sample id's from BIOM format OTU table.
 
 
 
  :rtype: dict
 
  :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on
 
  OTUID's and their values represent the relative abundance of that OTUID in
 
  that SampleID.","This function takes a biom file object (biomf) and a list of sample IDs (sampleIDs) as input arguments. If no sampleIDs are provided, it retrieves all the sample IDs from the biom file. 

Next, it checks if the provided sampleIDs match the sample IDs in the biom file. If any of the sampleIDs do not match, it raises a ValueError with a descriptive error message.

Then, it retrieves the OTU (Operational Taxonomic Unit) IDs from the biom file.

After that, it creates a normalized version of the biom file (norm_biomf) by applying normalization to the abundance values. The original biom file is not modified due to the inplace=False option.

Finally, it returns a dictionary where each sample ID is mapped to a dictionary of OTU IDs and their corresponding normalized abundance values. The normalization is performed using the get_value_by_ids method of the norm_biomf object."
"def foo(ra, otuIDs):
  sids = ra.keys()
 
  otumeans = defaultdict(int)
 
 
 
  for oid in otuIDs:
 
  otumeans[oid] = sum([ra[sid][oid] for sid in sids
 
  if oid in ra[sid]]) / len(sids) * 100
 
  return otumeans","Calculate the mean OTU abundance percentage.
 
 
 
  :type ra: Dict
 
  :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are
 
  dictionaries keyed on OTUID's and their values represent the relative
 
  abundance of that OTUID in that SampleID. 'ra' is the output of
 
  relative_abundance() function.
 
 
 
  :type otuIDs: List
 
  :param otuIDs: A list of OTUID's for which the percentage abundance needs to be
 
  measured.
 
 
 
  :rtype: dict
 
  :return: A dictionary of OTUID and their percent relative abundance as key/value pair.","This code defines a function called ""foo"" which takes two arguments: 'ra' and 'otuIDs'. 

Inside the function, it creates an empty list called 'sids' to store some values.

It also creates an empty dictionary called 'otumeans' using the defaultdict(int) method, which sets the default value for each key to zero.

Next, the code loops through each 'oid' in 'otuIDs'. 

Inside the loop, it calculates the mean value for each 'oid' by summing up the values of 'oid' for all the 'sids' present in the 'ra' dictionary. It checks if 'oid' is present in each 'sid' using the ""if oid in ra[sid]"" condition. It then divides the sum by the total number of 'sids' and multiplies it by 100.

The mean value for each 'oid' is stored in the 'otumeans' dictionary, with the 'oid' as the key and the mean value as the value.

Finally, the function returns the 'otumeans' dictionary containing the mean values for each 'oid'."
"def MRA(biomf, sampleIDs=None, transform=None):
 
  ra = relative_abundance(biomf, sampleIDs)
 
  if transform is not None:
 
  ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}
 
  for sample in ra.keys()}
 
  otuIDs = biomf.ids(axis=""observation"")
 
  return mean_otu_pct_abundance(ra, otuIDs)","Calculate the mean relative abundance percentage.
 
 
 
  :type biomf: A BIOM file.
 
  :param biomf: OTU table format.
 
 
 
  :type sampleIDs: list
 
  :param sampleIDs: A list of sample id's from BIOM format OTU table.
 
 
 
  :param transform: Mathematical function which is used to transform smax to another
 
  format. By default, the function has been set to None.
 
 
 
  :rtype: dict
 
  :return: A dictionary keyed on OTUID's and their mean relative abundance for a given
 
  number of sampleIDs.","This function, MRA, calculates the mean relative abundance (MRA) of OTUs (Operational Taxonomic Units) in a given BIOM file. The function takes in the BIOM file (biomf) as input, along with optional arguments sampleIDs and transform. 

First, the function calls the relative_abundance function to calculate the relative abundance of OTUs in each sample of the BIOM file. It stores the result in a variable called ""ra"".

Next, there is a conditional statement that checks if the transform argument is not None. If it is not None, the function applies a transformation to each abundance value in the ""ra"" variable. The transform argument is a function that takes an abundance value and returns a transformed value. It is expected that the transform function will be supplied by the user when calling the MRA function.

After applying the transformation (if specified), the function calculates the mean OTU percent abundance by calling the mean_otu_pct_abundance function. This function takes the transformed relative abundance values and the OTU IDs as input. The OTU IDs are extracted from the BIOM file using the biomf.ids(axis=""observation"") method.

Finally, the mean OTU percent abundance values are returned as the result of the MRA function."
"def raw_abundance(biomf, sampleIDs=None, sample_abd=True):
 
  results = defaultdict(int)
 
  if sampleIDs is None:
 
  sampleIDs = biomf.ids()
 
  else:
 
  try:
 
  for sid in sampleIDs:
 
  assert sid in biomf.ids()
 
  except AssertionError:
 
  raise ValueError(
 
  ""\nError while calculating raw total abundances: The sampleIDs provided ""
 
  ""do not match the sampleIDs in biom file. Please double check the ""
 
  ""sampleIDs provided.\n"")
 
  otuIDs = biomf.ids(axis=""observation"")
 
 
 
  for sampleID in sampleIDs:
 
  for otuID in otuIDs:
 
  abd = biomf.get_value_by_ids(otuID, sampleID)
 
  if sample_abd:
 
  results[sampleID] += abd
 
  else:
 
  results[otuID] += abd
 
  return results","Calculate the total number of sequences in each OTU or SampleID.
 
 
 
  :type biomf: A BIOM file.
 
  :param biomf: OTU table format.
 
 
 
  :type sampleIDs: List
 
  :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the
 
  list has been set to None.
 
 
 
  :type sample_abd: Boolean
 
  :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By
 
  default, the output will be provided for SampleID's.
 
 
 
  :rtype: dict
 
  :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their
 
  respective abundance as values.","This function calculates the raw abundance of OTUs (operational taxonomic units) in a biom file. The biom file contains data on the abundance of different OTUs in various samples. 

Here's an explanation of the code:

1. The function takes two parameters: biomf (the biom file object) and sampleIDs (optional list of sample IDs).
2. It creates an empty dictionary called ""results"" to store the calculated abundances.
3. If the sampleIDs parameter is not provided, it assigns the list of all sample IDs from the biom file to the sampleIDs variable.
4. If the sampleIDs are provided, it checks if each provided sample ID exists in the biom file. If any of the provided sample IDs don't exist, it raises a ValueError.
5. It retrieves the list of OTU IDs from the biom file.
6. It loops over each sample ID and OTU ID combination.
7. For each combination, it retrieves the abundance value from the biom file using the get_value_by_ids() method.
8. If the sample_abd parameter is set to True (default), it adds the abundance value to the corresponding sample ID in the results dictionary.
9. If the sample_abd parameter is set to False, it adds the abundance value to the corresponding OTU ID in the results dictionary.
10. Finally, it returns the results dictionary containing the calculated raw abundances."
"def foo(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):
  totals = raw_abundance(biomf, sampleIDs, sample_abd)
 
  return {sid: fn(abd) for sid, abd in totals.items()}","Function to transform the total abundance calculation for each sample ID to another
 
  format based on user given transformation function.
 
 
 
  :type biomf: A BIOM file.
 
  :param biomf: OTU table format.
 
 
 
  :param fn: Mathematical function which is used to transform smax to another format.
 
  By default, the function has been given as base 10 logarithm.
 
 
 
  :rtype: dict
 
  :return: Returns a dictionary similar to output of raw_abundance function but with
 
  the abundance values modified by the mathematical operation. By default, the
 
  operation performed on the abundances is base 10 logarithm.","The function `foo` takes in several parameters: `biomf`, which represents the biom file, `fn` which is a function to be applied to the abundance values, `sampleIDs` which is a list of sample IDs to consider, and `sample_abd` which indicates whether to use raw abundance values or normalized ones.

Within the function, the `raw_abundance` function is called with the `biomf`, `sampleIDs`, and `sample_abd` as arguments. This function calculates the total abundance for each sample and returns it as a dictionary where the keys are sample IDs and the values are the corresponding abundances.

The function then proceeds to create a new dictionary using a dictionary comprehension. For each key-value pair in the `totals` dictionary, the function applies the function `fn` to the abundance value and assigns the result as the value in the new dictionary, with the same sample ID as the key.

Finally, the new dictionary is returned from the function. The purpose of this function is to apply a specified function to the abundances of each sample, and return the results in a dictionary format."
"def foo(div_calc):
 
  try:
 
  x = div_calc.values()[0].values()
 
  y = div_calc.values()[1].values()
 
  except:
 
  return ""Error setting up input arrays for Mann-Whitney U Test. Skipping ""\
 
  ""significance testing.""
 
  T, p = stats.mannwhitneyu(x, y)
 
  print ""\nMann-Whitney U test statistic:"", T
 
  print ""Two-tailed p-value: {}"".format(2 * p)",Compute the Mann-Whitney U test for unequal group sample sizes.,"The `foo` function takes in a dictionary called `div_calc` as an argument. 

It tries to access the values of the first and second items in the dictionary. If the values are successfully accessed, they are stored in variables `x` and `y` respectively.

If there is an error in accessing the values, an error message is returned, indicating that the input arrays for the Mann-Whitney U Test could not be set up properly.

If there are no errors, the Mann-Whitney U Test is performed using the `stats.mannwhitneyu` function from the `stats` module. The test statistic is assigned to variable `T` and the p-value is assigned to variable `p`. 

Finally, the test statistic and the two-tailed p-value are printed to the console."
"def foo(div_calc):
 
  calc = defaultdict(list)
 
  try:
 
  for k1, v1 in div_calc.iteritems():
 
  for k2, v2 in v1.iteritems():
 
  calc[k1].append(v2)
 
  except:
 
  return ""Error setting up input arrays for Kruskal-Wallis H-Test. Skipping ""\
 
  ""significance testing.""
 
  h, p = stats.kruskal(*calc.values())
 
  print ""\nKruskal-Wallis H-test statistic for {} groups: {}"".format(str(len(div_calc)), h)
 
  print ""p-value: {}"".format(p)","Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that
 
  each group must have at least 5 measurements.","This code defines a function named ""foo"" that takes a parameter called ""div_calc"". The function performs a Kruskal-Wallis H-test on the values in the input dictionary.

The code first creates an empty defaultdict called ""calc"", which will store the values for each group in the input dictionary.

Then, the code tries to iterate over the items in the input dictionary using two nested loops. The outer loop iterates over the keys and values of the dictionary, while the inner loop iterates over the keys and values of the inner dictionary.

Inside the inner loop, the code appends the inner value (""v2"") to the ""calc"" dictionary under the corresponding outer key (""k1"").

If any error occurs during the iteration, an error message is returned indicating that the input arrays for the Kruskal-Wallis H-test could not be set up correctly, and the significance testing is skipped.

If the iteration is successful, the code proceeds to perform the Kruskal-Wallis H-test using the values from the ""calc"" dictionary. The resulting H-statistic and p-value are stored in the variables ""h"" and ""p"" respectively.

Finally, the code prints the H-statistic and p-value to the console, indicating the results of the Kruskal-Wallis H-test."
"def handle_program_options():
  parser = argparse.ArgumentParser(description=""Calculate the alpha diversity\
 
  of a set of samples using one or more \
 
  metrics and output a kernal density \
 
  estimator-smoothed histogram of the \
 
  results."")
 
  parser.add_argument(""-m"", ""--map_file"",
 
  help=""QIIME mapping file."")
 
  parser.add_argument(""-i"", ""--biom_fp"",
 
  help=""Path to the BIOM table"")
 
  parser.add_argument(""-c"", ""--category"",
 
  help=""Specific category from the mapping file."")
 
  parser.add_argument(""-d"", ""--diversity"", default=[""shannon""], nargs=""+"",
 
  help=""The alpha diversity metric. Default \
 
  value is 'shannon', which will calculate the Shannon\
 
  entropy. Multiple metrics can be specified (space separated).\
 
  The full list of metrics is available at:\
 
  http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\
 
  Beta diversity metrics will be supported in the future."")
 
  parser.add_argument(""--x_label"", default=[None], nargs=""+"",
 
  help=""The name of the diversity metric to be displayed on the\
 
  plot as the X-axis label. If multiple metrics are specified,\
 
  then multiple entries for the X-axis label should be given."")
 
  parser.add_argument(""--color_by"",
 
  help=""A column name in the mapping file containing\
 
  hexadecimal (#FF0000) color values that will\
 
  be used to color the groups. Each sample ID must\
 
  have a color entry."")
 
  parser.add_argument(""--plot_title"", default="""",
 
  help=""A descriptive title that will appear at the top \
 
  of the output plot. Surround with quotes if there are\
 
  spaces in the title."")
 
  parser.add_argument(""-o"", ""--output_dir"", default=""."",
 
  help=""The directory plots will be saved to."")
 
  parser.add_argument(""--image_type"", default=""png"",
 
  help=""The type of image to save: png, svg, pdf, eps, etc..."")
 
  parser.add_argument(""--save_calculations"",
 
  help=""Path and name of text file to store the calculated ""
 
  ""diversity metrics."")
 
  parser.add_argument(""--suppress_stats"", action=""store_true"", help=""Do not display ""
 
  ""significance testing results which are shown by default."")
 
  parser.add_argument(""--show_available_metrics"", action=""store_true"",
 
  help=""Supply this parameter to see which alpha diversity metrics ""
 
  "" are available for usage. No calculations will be performed""
 
  "" if this parameter is provided."")
 
  return parser.parse_args()",Parses the given options passed in at the command line.,"The `handle_program_options` function is used to handle the program options or command line arguments for a script that calculates alpha diversity of a set of samples. 

The function defines various command line options using the `argparse` module to allow users to provide necessary information for the diversity calculation and plot generation.

The available options include:

- `map_file`: Specifies the path to a QIIME mapping file.
- `biom_fp`: Specifies the path to the BIOM table.
- `category`: Specifies a specific category from the mapping file.
- `diversity`: Specifies the alpha diversity metric(s) to be calculated. The default metric is Shannon entropy, but multiple metrics can be specified. The full list of metrics is provided in a link.
- `x_label`: Specifies the name(s) of the diversity metric(s) to be displayed on the plot as the X-axis label. If multiple metrics are specified, multiple entries for the X-axis label should be given.
- `color_by`: Specifies a column name in the mapping file containing hexadecimal color values that will be used to color the groups in the plot.
- `plot_title`: Specifies a descriptive title that will appear at the top of the output plot.
- `output_dir`: Specifies the directory where the plots will be saved.
- `image_type`: Specifies the type of image to save (e.g., png, svg, pdf, eps).
- `save_calculations`: Specifies the path and name of a text file to store the calculated diversity metrics.
- `suppress_stats`: If provided, significance testing results will not be displayed (they are shown by default).
- `show_available_metrics`: If provided, a list of available alpha diversity metrics will be displayed. No calculations will be performed if this option is used.

The function returns the parsed command line arguments."
"def blastdb(fasta, maxfile = 10000000):
 
  db = fasta.rsplit('.', 1)[0]
 
  type = check_type(fasta)
 
  if type == 'nucl':
 
  type = ['nhr', type]
 
  else:
 
  type = ['phr', type]
 
  if os.path.exists('%s.%s' % (db, type[0])) is False \
 
  and os.path.exists('%s.00.%s' % (db, type[0])) is False:
 
  print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)
 
  os.system('makeblastdb \
 
  -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \
 
  % (fasta, db, type[1], maxfile))
 
  else:
 
  print('# ... database found for: %s' % (fasta), file=sys.stderr)
 
  return db",make blast db,"The `blastdb` function is used to create a BLAST database from a given fasta file. It takes two arguments: `fasta` (the name of the fasta file) and `maxfile` (the maximum file size for the database).

First, the function extracts the name of the database from the fasta file by removing the file extension. It then checks the type of the fasta file (nucleotide or protein).

If the fasta file is a nucleotide sequence, it sets the type to be ['nhr', 'nucl']. Otherwise, it sets the type to be ['phr', <fasta_type>], where <fasta_type> can be 'prot' or any other protein type.

The function then checks if the blast database files already exist for the given fasta file. If the files do not exist, it proceeds to create the database by calling the `makeblastdb` command. The command takes the fasta file, database name, database type, and maximum file size as arguments.

Finally, if the blast database files already exist, the function prints a message stating that the database files were found. The function returns the name of the database.

Note: The function assumes that the `check_type` function is defined elsewhere in the code and is used to determine the type of the fasta file."
"def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):
 
  if '.udb' in fasta:
 
  print('# ... database found: %s' % (fasta), file=sys.stderr)
 
  return fasta
 
  type = check_type(fasta)
 
  db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)
 
  if os.path.exists(db) is False:
 
  print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)
 
  if alignment == 'local':
 
  os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))
 
  elif alignment == 'global':
 
  os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))
 
  else:
 
  print('# ... database found for: %s' % (fasta), file=sys.stderr)
 
  return db",make usearch db,"This function is used to create or utilize a USEARCH database from a given FASTA file.

The function takes three parameters: 
1. fasta - the input FASTA file name or the database file name with "".udb"" extension. 
2. alignment - the type of alignment to perform, either 'local' or 'global'. 
3. usearch_loc - the location of the USEARCH tool on the system.

The function starts by checking if the input is already a USEARCH database file. If it is, it returns the filename.

If the input is not a database file, it determines the type of the input fasta file using the 'check_type' function and creates the corresponding database file by appending "".udb"" to the fasta filename.

If the database file does not already exist, it prints a message indicating that it is creating the USEARCH database for the fasta file.

If the alignment type is 'local', it uses the USEARCH tool to create the database by running the command 'usearch -makeudb_ublast fasta -output db'. The output is redirected to a log file.

If the alignment type is 'global', it uses the USEARCH tool to create the database by running the command 'usearch -makeudb_usearch fasta -output db'. The output is redirected to a log file.

Finally, if the database file already exists, it prints a message indicating that the database is found, and returns the database file name."
"def _pp(dict_data):
  for key, val in dict_data.items():
 
  # pylint: disable=superfluous-parens
 
  print('{0:<11}: {1}'.format(key, val))",Pretty print.,"This code defines a function called `_pp` which takes a dictionary as input. The function is used to pretty-print the contents of the dictionary, where each key-value pair is displayed on a separate line. 

The function uses a for loop to iterate through the items in the dictionary. Within the loop, each key-value pair is printed using the `print` function. The format string used for printing includes placeholders for the key and value. The `""<""` symbol is used to left-align the key, and a width of 11 is specified to ensure consistency in the output format.

However, there is a line with a comment that says `# pylint: disable=superfluous-parens`. This is a comment that is used to disable a specific pylint warning. The warning being disabled in this case is related to superfluous parentheses, which means that pylint may warn about unnecessary parentheses in the code."
"def foo(params, metadata):
 
  if hasattr(params, 'licenses'):
 
  if params.licenses:
 
  _pp(metadata.licenses_desc())
 
  sys.exit(0)","Print licenses.
 
 
 
  :param argparse.Namespace params: parameter
 
  :param bootstrap_py.classifier.Classifiers metadata: package metadata","The `foo` function takes in two parameters: `params` and `metadata`. 

This function checks if the `params` object has an attribute called `licenses`. If it does, it then checks if the value of `params.licenses` is truthy (i.e., not False, 0, an empty string, etc.). 

If `params.licenses` is truthy, the function will print out a description of the licenses stored in the `metadata` object by calling `metadata.licenses_desc()` and then exit the program with a status code of 0."
"def foo(params):
 
  repodir = os.path.join(params.outdir, params.name)
 
  if os.path.isdir(repodir):
 
  raise Conflict(
 
  'Package repository ""{0}"" has already exists.'.format(repodir))","Check repository existence.
 
 
 
  :param argparse.Namespace params: parameters","This function, `foo`, checks if a package repository already exists in a given directory. It takes in a `params` object as input, which should have an `outdir` attribute indicating the directory path and a `name` attribute indicating the name of the package repository. 

The function first creates a path by joining the `outdir` with the `name` attribute using `os.path.join()`. 

Then, it checks if the `repodir` path already exists as a directory using `os.path.isdir()`. If it does exist, it raises a `Conflict` exception with a specific error message indicating that the package repository already exists."
"def foo(params):
 
  pkg_data = package.PackageData(params)
 
  pkg_tree = package.PackageTree(pkg_data)
 
  pkg_tree.generate()
 
  pkg_tree.move()
 
  VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)","Generate package repository.
 
 
 
  :param argparse.Namespace params: parameters","This function takes in parameters and performs the following steps:

1. Creates a PackageData object using the params provided.
2. Creates a PackageTree object using the PackageData object.
3. Generates the package tree.
4. Moves the package tree to a specified output directory.
5. Uses a VCS (Version Control System) to track changes in the package tree."
"def print_single(line, rev):
 
  if rev is True:
 
  seq = rc(['', line[9]])[1]
 
  qual = line[10][::-1]
 
  else:
 
  seq = line[9]
 
  qual = line[10]
 
  fq = ['@%s' % line[0], seq, '+%s' % line[0], qual]
 
  print('\n'.join(fq), file = sys.stderr)",print single reads to stderr,"This function, named `print_single`, takes in two parameters: `line` and `rev`. 

The purpose of this function is to generate and print a single sequence entry from a FastQ file. 

If the `rev` parameter is True, it means that the sequence needs to be reversed. In this case, the function obtains the reverse complement of the sequence in `line[9]` using the `rc()` function and assigns it to the variable `seq`. The quality score string in `line[10]` is also reversed and assigned to the variable `qual`. 

If `rev` is not True, the function assigns `line[9]` to the variable `seq` and `line[10]` to the variable `qual`. 

After that, a list named `fq` is created with the sequence ID, the sequence itself (`seq`), the ""+"", and the quality score string (`qual`). 

Finally, the function prints each element of the `fq` list on a separate line, using the `print()` function. The output is redirected to `sys.stderr`."
"def foo(sam, singles = False, force = False):
 
  L, R = None, None
 
  for line in sam:
 
  if line.startswith('@') is True:
 
  continue
 
  line = line.strip().split()
 
  bit = [True if i == '1' else False \
 
  for i in bin(int(line[1])).split('b')[1][::-1]]
 
  while len(bit) < 8:
 
  bit.append(False)
 
  pair, proper, na, nap, rev, mrev, left, right = bit
 
  # make sure read is paired
 
  if pair is False:
 
  if singles is True:
 
  print_single(line, rev)
 
  continue
 
  # check if sequence is reverse-complemented
 
  if rev is True:
 
  seq = rc(['', line[9]])[1]
 
  qual = line[10][::-1]
 
  else:
 
  seq = line[9]
 
  qual = line[10]
 
  # check if read is forward or reverse, return when both have been found
 
  if left is True:
 
  if L is not None and force is False:
 
  print('sam file is not sorted', file = sys.stderr)
 
  print('\te.g.: %s' % (line[0]), file = sys.stderr)
 
  exit()
 
  if L is not None:
 
  L = None
 
  continue
 
  L = ['@%s' % line[0], seq, '+%s' % line[0], qual]
 
  if R is not None:
 
  yield L
 
  yield R
 
  L, R = None, None
 
  if right is True:
 
  if R is not None and force is False:
 
  print('sam file is not sorted', file = sys.stderr)
 
  print('\te.g.: %s' % (line[0]), file = sys.stderr)
 
  exit()
 
  if R is not None:
 
  R = None
 
  continue
 
  R = ['@%s' % line[0], seq, '+%s' % line[0], qual]
 
  if L is not None:
 
  yield L
 
  yield R
 
  L, R = None, None",convert sam to fastq,"The given code is a function called ""foo"" which takes a variable ""sam"" as input, along with two optional parameters ""singles"" and ""force"". 

The function reads a file line by line, skipping lines that start with '@'. 
It then processes each line and extracts a binary representation of a number from a specific position in the line. This binary representation is converted to a list of boolean values representing different properties of the read. The properties include whether the read is paired, whether it is a proper pair, whether it is aligned, whether it is aligned to a primer, whether it is in a reverse orientation, whether it is in a mate-reverse orientation, whether it is a left read or a right read.

If the read is not paired and the ""singles"" parameter is True, then a function called ""print_single"" is called with the line and the boolean value representing reverse orientation as arguments, and the loop moves on to the next line.

If the read is reverse-oriented, the sequence and quality values are reversed and complemented.

The function then checks if the read is a left or right read and keeps track of the first occurrence of each read. If a read has already been found and the ""force"" parameter is False, an error message is printed, and the program exits. If a read of the opposite orientation has been found, the current read (L or R) along with the opposite read is yielded. If the current read is not the opposite, it is stored as the opposite read (L or R).

Once the loop finishes, any remaining reads are yielded if they exist.

In summary, this function processes a SAM file, which is a file format used for storing biological sequence alignment data. It extracts information about reads from each line of the file and distinguishes between left and right reads. It also checks for paired reads, reverse orientation, and proper alignment. The function can optionally handle single reads and enforce sorting of the SAM file."
"def foo(sam, sort):
 
  tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])
 
  if sort is True:
 
  mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])
 
  if sam != '-':
 
  if os.path.exists(mapping) is False:
 
  os.system(""\
 
  sort -k1 --buffer-size=%sG -T %s -o %s %s\
 
  "" % (sbuffer, tempdir, mapping, sam)) 
 
  else:
 
  mapping = 'stdin-sam.sorted.sam'
 
  p = Popen(""sort -k1 --buffer-size=%sG -T %s -o %s"" \
 
  % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True) 
 
  p.communicate()
 
  mapping = open(mapping)
 
  else:
 
  if sam == '-':
 
  mapping = sys.stdin
 
  else:
 
  mapping = open(sam)
 
  return mapping",sort sam file,"This function takes in two arguments: 'sam' and 'sort'. 

The 'sam' argument represents a file path, which is checked to determine if the file exists. If the file does not exist, a mapping file is created by sorting the 'sam' file based on the first column of data. The sorted data is stored in a new file with the extension '.sorted.sam'. The sorting is done using the 'sort' command in the shell, with a buffer size of 'sbuffer' and a temporary directory specified.

If the 'sam' file does exist, the sorting operation is performed using the standard input stream. The sorted data is stored in a file named 'stdin-sam.sorted.sam'.

The result of the sorting operation, either from the file or the standard input, is opened and returned as the final output of the function."
"def sub_sam(sam, percent, sort = True, sbuffer = False):
  mapping = sort_sam(sam, sort)
 
  pool = [1 for i in range(0, percent)] + [0 for i in range(0, 100 - percent)]
 
  c = cycle([1, 2])
 
  for line in mapping:
 
  line = line.strip().split()
 
  if line[0].startswith('@'): # get the sam header
 
  yield line
 
  continue
 
  if int(line[1]) <= 20: # is this from a single read?
 
  if random.choice(pool) == 1:
 
  yield line
 
  else:
 
  n = next(c)
 
  if n == 1:
 
  prev = line
 
  if n == 2 and random.choice(pool) == 1:
 
  yield prev
 
  yield line",randomly subset sam file,"This function takes in a SAM (Sequence Alignment/Map) file, a percentage value, and optional parameters. It returns a subset of the SAM file based on the given percentage.

The function first calls another function called ""sort_sam"" to sort the SAM file based on some criteria specified by the ""sort"" parameter.

Next, it creates a pool of integers with the given percentage of 1s and the remaining 0s. This pool is used for random selection later.

Then, it uses a cycle object to alternate between values 1 and 2.

The function iterates over each line in the sorted SAM file. If the line starts with ""@"" indicating it is a SAM header, it is yielded as is.

If the ""NH"" field in the SAM line (line[1]) is less than or equal to 20, it means it is from a single read. If a random choice from the pool is 1, the line is yielded.

If the line is from a paired-end read, the function checks the cycle value. If the value is 1, the current line is stored in a variable called ""prev"". If the value is 2 and the random choice from the pool is 1, both the previous line and the current line are yielded.

In summary, the function selects a subset of the SAM file based on the given percentage and sorts the SAM file before doing so."
"def fq2fa(fq):
 
  c = cycle([1, 2, 3, 4])
 
  for line in fq:
 
  n = next(c)
 
  if n == 1:
 
  seq = ['>%s' % (line.strip().split('@', 1)[1])]
 
  if n == 2:
 
  seq.append(line.strip())
 
  yield seq",convert fq to fa,"This code defines a function named ""fq2fa"" that takes in a list of sequences in fastq format. The code then converts each fastq sequence into fasta format and yields it. 

Inside the function, the code creates a cycle object with the numbers 1, 2, 3, and 4. 

Then, for each line in the fastq sequence list, the code retrieves the next number from the cycle object.

If the number is 1, it means that the line represents the sequence identifier, so it is extracted and stored in a list called ""seq"" with a "">"" character added at the beginning.

If the number is 2, it means that the line contains the sequence itself, so it is stripped of any leading or trailing spaces and then appended to the ""seq"" list.

The ""seq"" list is then yielded as a fasta sequence containing the identifier line with "">"" and the sequence line.

By repeatedly yielding fasta sequences, the code effectively converts the input fastq sequences into fasta format."
"def foo(f):
 
  @wraps(f)
 
  def wrapper(*args, **kwargs):
 
  if kwargs.has_key('return_type'):
 
  return_type = kwargs['return_type']
 
  kwargs.pop('return_type')
 
  return return_type(f(*args, **kwargs))
 
  elif len(args) > 0:
 
  return_type = type(args[0])
 
  return return_type(f(*args, **kwargs))
 
  else:
 
  return f(*args, **kwargs)
 
  return wrapper","Converts the returned value of wrapped function to the type of the
 
  first arg or to the type specified by a kwarg key return_type's value.","This code defines a decorator function called ""foo"" that can be used to wrap other functions. The decorator is used to modify the behavior of the wrapped function.

The ""foo"" function uses the ""wraps"" decorator from the ""functools"" module to preserve the original metadata of the wrapped function.

The ""foo"" function itself takes a single argument, which is the function to be wrapped. It then defines an inner function called ""wrapper"" that will be returned as the decorated function.

The ""wrapper"" function accepts any number of positional arguments (*args) and keyword arguments (**kwargs). It checks if the ""kwargs"" dictionary contains a key called ""return_type"". If it does, it extracts the value of ""return_type"" and removes it from the ""kwargs"" dictionary.

The ""wrapper"" function then calls the wrapped function with the supplied arguments and keyword arguments. If the ""return_type"" key was present in the ""kwargs"" dictionary, it converts the return value of the wrapped function to the specified return type and returns it. If the ""return_type"" key was not present, it returns the return value of the wrapped function as is.

If there are no positional arguments (*args) provided to the ""wrapper"" function, it determines the return type based on the type of the first positional argument. This is done by using the ""type"" function.

After defining the ""wrapper"" function, the ""foo"" function returns it as the decorated function.

Overall, this code allows the decorated function to specify a return type, and if provided, the return value of the wrapped function is converted to that type. If no return type is specified, the return value is returned as is."
"def foo(f):
 
  @wraps(f)
 
  def wrapper(*args, **kwargs):
 
  args = (setify(x) for x in args)
 
  return f(*args, **kwargs)
 
  return wrapper",Converts all args to 'set' type via self.setify function.,"This code defines a decorator called 'foo' that can be used to wrap around functions. The decorator uses the 'wraps' function from the 'functools' module to preserve the original name and documentation of the wrapped function.

The 'foo' decorator takes a function 'f' as its argument. It defines another function called 'wrapper' that will be returned as the new decorated function. 

The 'wrapper' function takes any number of arguments and keyword arguments specified by '*args' and '**kwargs', respectively. It applies the 'setify' function to each argument in '*args' using a generator comprehension. The 'setify' function converts each argument into a set, removing any duplicate elements. 

Finally, the 'wrapper' function returns the result of calling the original function 'f', passing in the modified arguments and keyword arguments."
"def _init_entri(self, laman):
 
 
  sup = BeautifulSoup(laman.text, 'html.parser')
 
  estr = ''
 
  for label in sup.find('hr').next_siblings:
 
  if label.name == 'hr':
 
  self.entri.append(Entri(estr))
 
  break
 
  if label.name == 'h2':
 
  if estr:
 
  self.entri.append(Entri(estr))
 
  estr = ''
 
  estr += str(label).strip()","Membuat objek-objek entri dari laman yang diambil.
 
 
 
  :param laman: Laman respons yang dikembalikan oleh KBBI daring.
 
  :type laman: Response","The `_init_entri` function initializes the `entri` list by parsing the given HTML page (`laman`) using BeautifulSoup. 

It starts by creating a BeautifulSoup object, `sup`, using the `laman` text and specifying the parser as `html.parser`. 

Then, an empty string, `estr`, is created. 

The function iterates over the siblings of the `<hr>` tag found in the `sup` object. 

If the current sibling's tag name is `<hr>`, it means that we have reached the end of a section, so the current `estr` string is appended to the `entri` list as an `Entri` object, and the loop is terminated with `break`. 

If the current sibling's tag name is `<h2>`, it means that a new section is starting. In this case, if the `estr` string is not empty, it means that there was a previous section, so that section is appended to the `entri` list. Then, the `estr` string is reset to an empty string. 

For each sibling, the string representation of the sibling (excluding any leading or trailing whitespace) is concatenated to the `estr` string. This allows us to accumulate the content of each section in the `estr` variable."
"def _init_kata_dasar(self, dasar):
 
 
  for tiap in dasar:
 
  kata = tiap.find('a')
 
  dasar_no = kata.find('sup')
 
  kata = ambil_teks_dalam_label(kata)
 
  self.kata_dasar.append(
 
  kata + ' [{}]'.format(dasar_no.text.strip()) if dasar_no else kata
 
  )","Memproses kata dasar yang ada dalam nama entri.
 
 
 
  :param dasar: ResultSet untuk label HTML dengan class=""rootword""
 
  :type dasar: ResultSet","The ""_init_kata_dasar"" function initializes the ""kata_dasar"" attribute of the class using a list of ""dasar"" elements as input.
Within the function, a loop iterates over each ""dasar"" element.
Within each iteration, the function extracts the first anchor element (""a"") within the ""dasar"" element.
Then, the function searches for a ""sup"" element within the anchor element.
The text content of the anchor element is retrieved using the ""ambil_teks_dalam_label"" function.
If a ""dasar_no"" element (superscript) is found, the text content of the anchor element is appended to the ""kata_dasar"" list, along with the text content inside the ""dasar_no"" element (in square brackets). If no ""dasar_no"" element is found, only the text content of the anchor element is appended to the ""kata_dasar"" list.
Overall, this function extracts the base words from the ""dasar"" elements and appends them to the ""kata_dasar"" list along with an optional base number, if available."
"def serialisasi(self):
  return {
 
  ""nama"": self.nama,
 
  ""nomor"": self.nomor,
 
  ""kata_dasar"": self.kata_dasar,
 
  ""pelafalan"": self.pelafalan,
 
  ""bentuk_tidak_baku"": self.bentuk_tidak_baku,
 
  ""varian"": self.varian,
 
  ""makna"": [makna.serialisasi() for makna in self.makna]
 
  }","Mengembalikan hasil serialisasi objek Entri ini.
 
 
 
  :returns: Dictionary hasil serialisasi
 
  :rtype: dict","The `serialisasi` method is used to convert an object of a class into a serialized format, which can be easily stored, transmitted, or persisted. 

The method returns a dictionary that represents the serialized version of the object. The dictionary contains various attributes of the object, such as name, number, base word, pronunciation, non-standard form, variants, and meanings. 

The object's attributes are accessed using their respective instance variables, and their values are added to the dictionary. The `nama` attribute is added with the value `self.nama`, `nomor` with `self.nomor`, `kata_dasar` with `self.kata_dasar`, and so on. 

The `makna` attribute, which is a list of objects, is handled in a slightly different way. The list comprehension `[makna.serialisasi() for makna in self.makna]` is used to iterate over each object in the `makna` list and call their `serialisasi` method. This returns the serialized version of each `makna` object, which is then added to the dictionary under the key ""makna"".

Finally, the serialized dictionary is returned as the output of the `serialisasi` method."
"def _makna(self):
 
 
  if len(self.makna) > 1:
 
  return '\n'.join(
 
  str(i) + "". "" + str(makna)
 
  for i, makna in enumerate(self.makna, 1)
 
  )
 
  return str(self.makna[0])","Mengembalikan representasi string untuk semua makna entri ini.
 
 
 
  :returns: String representasi makna-makna
 
  :rtype: str","This code defines a method called `_makna` that is a part of a class. The purpose of this method is to return the meaning(s) of a word. 

The method first checks if the `makna` attribute of the instance has more than one value. If it does, it means that the word has multiple meanings. 

In that case, the method uses a loop to iterate over each meaning in the `makna` attribute and concatenate it with its corresponding number. The result is a formatted string that lists each meaning with its index number. 

If the `makna` attribute has only one value, the method directly returns that value as a string."
"def _nama(self):
 
 
  hasil = self.nama
 
  if self.nomor:
 
  hasil += "" [{}]"".format(self.nomor)
 
  if self.kata_dasar:
 
  hasil = "" ¬ª "".join(self.kata_dasar) + "" ¬ª "" + hasil
 
  return hasil","Mengembalikan representasi string untuk nama entri ini.
 
 
 
  :returns: String representasi nama entri
 
  :rtype: str","The code defines a function `_nama` which takes no arguments. 

Inside the function, it first assigns the value of the `nama` attribute of the object to the variable `hasil`. 

Then, it checks if the `nomor` attribute of the object exists. If it does, it appends the value enclosed in square brackets to the `hasil` variable using string formatting. 

Next, it checks if the `kata_dasar` attribute of the object exists. If it does, it joins the values of the `kata_dasar` attribute using the string "" ¬ª "" as a separator, and appends it with "" ¬ª "" and the `hasil` variable. 

Finally, it returns the `hasil` variable as the result of the function."
"def _varian(self, varian):
 
 
  if varian == self.bentuk_tidak_baku:
 
  nama = ""Bentuk tidak baku""
 
  elif varian == self.varian:
 
  nama = ""Varian""
 
  else:
 
  return ''
 
  return nama + ': ' + ', '.join(varian)","Mengembalikan representasi string untuk varian entri ini.
 
  Dapat digunakan untuk ""Varian"" maupun ""Bentuk tidak baku"".
 
 
 
  :param varian: List bentuk tidak baku atau varian
 
  :type varian: list
 
  :returns: String representasi varian atau bentuk tidak baku
 
  :rtype: str","This function is used to generate a description based on the input parameter ""varian"". The function takes in a variable called ""varian"" as input.

The function checks if the value of ""varian"" is equal to the value of ""self.bentuk_tidak_baku"". If it is, then the variable ""nama"" is assigned the string ""Bentuk tidak baku"".

If the value of ""varian"" is equal to the value of ""self.varian"", then the variable ""nama"" is assigned the string ""Varian"".

If neither of these conditions are met, the function returns an empty string.

Finally, the function returns a concatenated string, consisting of the value of ""nama"", followed by "": "", and then the elements of the ""varian"" variable joined by commas."
"def _init_kelas(self, makna_label):
  kelas = makna_label.find(color='red')
 
  lain = makna_label.find(color='darkgreen')
 
  info = makna_label.find(color='green')
 
  if kelas:
 
  kelas = kelas.find_all('span')
 
  if lain:
 
  self.kelas = {lain.text.strip(): lain['title'].strip()}
 
  self.submakna = lain.next_sibling.strip()
 
  self.submakna += ' ' + makna_label.find(color='grey').text.strip()
 
  else:
 
  self.kelas = {
 
  k.text.strip(): k['title'].strip() for k in kelas
 
  } if kelas else {}
 
  self.info = info.text.strip() if info else ''","Memproses kelas kata yang ada dalam makna.
 
 
 
  :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
 
  :type makna_label: BeautifulSoup","This code defines a private method called `_init_kelas` that takes a parameter called `makna_label`. The purpose of this method is to initialize the properties of a class object.

Inside the method, the code first searches for a span element with the color attribute set to 'red' within the `makna_label`. This element represents the class of the object. If such an element is found, it retrieves all the span elements within it and assigns them to the `kelas` variable.

Next, the code searches for a span element with the color attribute set to 'darkgreen' within the `makna_label`. This element represents additional information or sub-class of the object. If found, it assigns the text and the 'title' attribute value of that span element to the `self.kelas` dictionary. It also retrieves the sibling element of the 'darkgreen' span and assigns it to the `self.submakna` variable. Additionally, it concatenates the text of a span element with the color attribute set to 'grey' to the `self.submakna`.

If no 'darkgreen' span element is found, the code proceeds to search for span elements with the color attribute set to 'green'. These elements represent more information about the object. If found, the code assigns the text of the first span element to the `self.info` variable.

Finally, the method finishes executing, and the object's properties (`self.kelas`, `self.submakna`, and `self.info`) are initialized based on the results of the code execution."
"def _init_contoh(self, makna_label):  
 
  indeks = makna_label.text.find(': ')
 
  if indeks != -1:
 
  contoh = makna_label.text[indeks + 2:].strip()
 
  self.contoh = contoh.split('; ')
 
  else:
 
  self.contoh = []","Memproses contoh yang ada dalam makna.
 
 
 
  :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
 
  :type makna_label: BeautifulSoup","This function initializes the 'contoh' attribute of an object. It takes in a 'makna_label' parameter, which is expected to be a label containing a string.

The function first finds the index position of the substring ': ' within the 'makna_label' text. If this substring is found, it means that there is an example part in the label.

The function then extracts the example part by getting the substring starting from the index position after ': ' and removes any leading or trailing whitespace characters. 

If no example part is found, the 'contoh' attribute is set to an empty list.

The extracted example part is then split by the ';' character and stored in the 'contoh' attribute.

Overall, this function is responsible for initializing the 'contoh' attribute of an object, which contains a list of examples extracted from a given label."
"def serialisasi(self):
 
  return {
 
  ""kelas"": self.kelas,
 
  ""submakna"": self.submakna,
 
  ""info"": self.info,
 
  ""contoh"": self.contoh
 
  }","Mengembalikan hasil serialisasi objek Makna ini.
 
 
 
  :returns: Dictionary hasil serialisasi
 
  :rtype: dict","This code defines a method named `serialisasi()` within a class. The purpose of this method is to serialize the object data into a dictionary format. It takes no parameters and returns a dictionary containing four key-value pairs.

The keys in the dictionary are 'kelas', 'submakna', 'info', and 'contoh'. The values of these keys are obtained from the corresponding attributes of the object.

- The value of the 'kelas' key is obtained from the 'kelas' attribute of the object.
- The value of the 'submakna' key is obtained from the 'submakna' attribute of the object.
- The value of the 'info' key is obtained from the 'info' attribute of the object.
- The value of the 'contoh' key is obtained from the 'contoh' attribute of the object.

Finally, the method returns the dictionary representation of the object data."
"def foo(pkg_data, projectdir):
  try:
 
  version, _minor_version = pkg_data.version.rsplit('.', 1)
 
  except ValueError:
 
  version = pkg_data.version
 
  args = ' '.join(('sphinx-quickstart',
 
  '--sep',
 
  '-q',
 
  '-p ""{name}""',
 
  '-a ""{author}""',
 
  '-v ""{version}""',
 
  '-r ""{release}""',
 
  '-l en',
 
  '--suffix=.rst',
 
  '--master=index',
 
  '--ext-autodoc',
 
  '--ext-viewcode',
 
  '--makefile',
 
  '{projectdir}')).format(name=pkg_data.name,
 
  author=pkg_data.author,
 
  version=version,
 
  release=pkg_data.version,
 
  projectdir=projectdir)
 
  if subprocess.call(shlex.split(args)) == 0:
 
  _touch_gitkeep(projectdir)","Build sphinx documentation.
 
 
 
  :rtype: int
 
  :return: subprocess.call return code
 
 
 
  :param `bootstrap_py.control.PackageData` pkg_data: package meta data
 
  :param str projectdir: project root directory","The `foo` function takes two parameters: `pkg_data` and `projectdir`. 

It starts by trying to split the `version` from `pkg_data` into two parts using the dot as the separator. If successful, it assigns the first part to the `version` variable, discarding the second part. If splitting fails, it simply assigns the `version` from `pkg_data` to the `version` variable.

Next, it creates a string of arguments to be passed to the `subprocess.call` function. These arguments are generated using various options for the `sphinx-quickstart` command, such as setting the project name, author name, version, release, language, and file extension. The `projectdir` parameter is included in the argument string.

The `subprocess.call` function is then called with the argument string as a list of command line arguments. If the return value of this function call is 0 (indicating success), the `_touch_gitkeep` function is called with the `projectdir` parameter."
"def bowtiedb(fa, keepDB):
  btdir = '%s/bt2' % (os.getcwd())
 
  # make directory for
 
  if not os.path.exists(btdir):
 
  os.mkdir(btdir)
 
  btdb = '%s/%s' % (btdir, fa.rsplit('/', 1)[-1])
 
  if keepDB is True:
 
  if os.path.exists('%s.1.bt2' % (btdb)):
 
  return btdb
 
  p = subprocess.Popen('bowtie2-build -q %s %s' \
 
  % (fa, btdb), shell = True)
 
  p.communicate()
 
  return btdb",make bowtie db,"This code defines a function called ""bowtiedb"" that takes two parameters: ""fa"" and ""keepDB"". The purpose of this function is to build a Bowtie2 database using a given FASTA file.

The function starts by creating a directory called ""bt2"" within the current working directory, if it doesn't already exist. Then, it constructs the path for the Bowtie2 database by appending the name of the FASTA file to the ""bt2"" directory.

If the ""keepDB"" parameter is set to True and the Bowtie2 database files already exist, the function simply returns the path of the existing database.

If the database files do not exist or the ""keepDB"" parameter is set to False, the function uses the ""bowtie2-build"" command-line tool to build the Bowtie2 database. It executes this tool as a subprocess, passing the path of the FASTA file and the desired output path of the database as arguments.

Once the execution of the ""bowtie2-build"" command is completed, the function returns the path of the newly created Bowtie2 database.

Overall, this code automates the process of creating a Bowtie2 database for a given FASTA file and allows for reusing an existing database if specified."
"def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):
 
  bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)
 
  if f is not False:
 
  bt2 += '-1 %s -2 %s ' % (f, r)
 
  if u is not False:
 
  bt2 += '-U %s ' % (u)
 
  bt2 += opt
 
  if no_shrink is False:
 
  if f is False:
 
  bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)
 
  else:
 
  bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)
 
  else:
 
  bt2 += ' > %s.sam' % (sam)
 
  return bt2",generate bowtie2 command,"This code defines a function called ""bowtie"" which takes several parameters. The purpose of this function is to generate a string command to run the Bowtie2 aligner. 

The function takes the following parameters:
- ""sam"": a string representing the name of the output SAM file.
- ""btd"": a string representing the name of the Bowtie2 index to be used for alignment.
- ""f"": a string representing the name of the forward input FASTQ file.
- ""r"": a string representing the name of the reverse input FASTQ file.
- ""u"": a string representing the name of the unpaired input FASTQ file.
- ""opt"": a string representing any additional options or parameters for the Bowtie2 command.
- ""no_shrink"": a boolean indicating whether to use the ""shrinksam"" tool to reduce the size of the SAM output file.
- ""threads"": an integer representing the number of threads to use for alignment.

The function starts by constructing the initial part of the Bowtie2 command, using the ""-x"" option to specify the Bowtie2 index and the ""-p"" option to specify the number of threads.

Then, depending on the values of the ""f"", ""r"", and ""u"" parameters, appropriate options are added to the Bowtie2 command. If both ""f"" and ""r"" are provided (i.e. paired-end alignment), the ""-1"" and ""-2"" options are used to specify the input FASTQ files. If only ""u"" is provided (i.e. unpaired alignment), the ""-U"" option is used.

After adding the options, the function checks the value of the ""no_shrink"" parameter. If it is False, it appends a command to use the ""shrinksam"" tool to reduce the size of the SAM output file. The ""-u"" option is used if the ""f"" parameter is False, indicating an unpaired alignment. Otherwise, the ""-k"" option is used.

Finally, depending on the value of the ""no_shrink"" parameter, the function appends the appropriate output redirection to either save the output to a SAM file or to a shrunk SAM file.

The final Bowtie2 command string is returned as the output of the function."
"def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):
 
  if cluster is True:
 
  threads = '48'
 
  btc = []
 
  for fa in fas:
 
  btd = bowtiedb(fa, keepDB)
 
  F, R, U = reads
 
  if F is not False:
 
  if U is False:
 
  u = False
 
  for i, f in enumerate(F):
 
  r = R[i]
 
  if U is not False:
 
  u = U[i]
 
  sam = '%s/%s-vs-%s' % (os.getcwd(), \
 
  fa.rsplit('/', 1)[-1], f.rsplit('/', 1)[-1].rsplit('.', 3)[0])
 
  btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))
 
  else:
 
  f = False
 
  r = False
 
  for u in U:
 
  sam = '%s/%s-vs-%s' % (os.getcwd(), \
 
  fa.rsplit('/', 1)[-1], u.rsplit('/', 1)[-1].rsplit('.', 3)[0])
 
  btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))
 
  if cluster is False:
 
  for i in btc:
 
  p = subprocess.Popen(i, shell = True)
 
  p.communicate()
 
  else:
 
  ID = ''.join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))
 
  for node, commands in enumerate(chunks(btc, nodes), 1):
 
  bs = open('%s/crossmap-qsub.%s.%s.sh' % (os.getcwd(), ID, node), 'w')
 
  print('\n'.join(commands), file=bs)
 
  bs.close()
 
  p = subprocess.Popen(\
 
  'qsub -V -N crossmap %s' \
 
  % (bs.name), \
 
  shell = True)
 
  p.communicate()",map all read sets against all fasta files,"The `crossmap` function takes a set of input fasta files (`fas`), a set of read files (`reads`), and some optional parameters (`options`, `no_shrink`, `keepDB`, `threads`, `cluster`, `nodes`).

If the `cluster` parameter is set to `True`, the number of threads is set to 48. Otherwise, the value of `threads` is used as is.

The function creates an empty list called `btc`.

For each fasta file in the input set:
- It creates a bowtie database (`btd`) for the fasta file and adds it to `btc`.

If forward read files (`F`) are provided:
- If there are no unpaired read files (`U`), the value of `u` is set to `False`.
- For each forward read file `f` and its corresponding reverse read file `r`:
  - If unpaired read files are provided (`U`), the value of `u` is set to the corresponding `U` file.
  - A sam file path is created based on the current fasta file and read file names.
  - A Bowtie command is generated by calling the `bowtie` function with the sam file path, bowtie database, read files (`f`, `r`, and `u`), optional parameters, and the `threads` value.
  - The Bowtie command is added to `btc`.

If unpaired read files are provided (`U`):
- The value of `f` and `r` are set to `False`.
- For each unpaired read file `u`:
  - A sam file path is created based on the current fasta file and unpaired read file name.
  - A Bowtie command is generated by calling the `bowtie` function with the sam file path, bowtie database, read files (`f`, `r`, and `u`), optional parameters, and the `threads` value.
  - The Bowtie command is added to `btc`.

If the `cluster` parameter is `False`:
- For each Bowtie command in `btc`, a subprocess is created and executed.

If the `cluster` parameter is `True`:
- Generates a unique ID by randomly selecting five digits.
- Iterates over the chunks of Bowtie commands in `btc` based on the number of `nodes`.
- For each node and its corresponding commands:
  - Creates a shell script file named `crossmap-qsub.ID.node.sh` in the current working directory.
  - Writes the commands to the shell script file.
  - Executes a subprocess to submit the shell script to the cluster using the `qsub` command.

Overall, the `crossmap` function prepares and executes Bowtie commands for mapping reads from the input read files to the input fasta files. The function handles both paired-end and unpaired reads, and can be run in parallel on a cluster if specified."
"def get_conn(self, *args, **kwargs):
  connections = self.__connections_for('get_conn', args=args, kwargs=kwargs)
 
 
 
  if len(connections) is 1:
 
  return connections[0]
 
  else:
 
  return connections","Returns a connection object from the router given ``args``.
 
 
 
  Useful in cases where a connection cannot be automatically determined
 
  during all steps of the process. An example of this would be
 
  Redis pipelines.","This function is used to get a single database connection object. It accepts any number of arguments and keyword arguments. It first retrieves a list of connections using the '__connections_for' method, passing the 'get_conn' function name and the provided arguments. 

If the length of the connections list is equal to 1, it means there is only one connection object available. In this case, the function returns that single connection object.

If there are multiple connections available (length of the connections list greater than 1), the function returns the entire list of connections."
"def __get_nondirect_init(self, init):
 
  crc = init
 
  for i in range(self.Width):
 
  bit = crc & 0x01
 
  if bit:
 
  crc^= self.Poly
 
  crc >>= 1
 
  if bit:
 
  crc |= self.MSB_Mask
 
  return crc & self.Mask",return the non-direct init if the direct algorithm has been selected.,"This code snippet defines a method called ""__get_nondirect_init"" that takes in an input parameter called ""init"". This method is part of a larger class implementation.

The purpose of this method is to calculate and return the non-direct initialization value for a cyclic redundancy check (CRC) calculation. The CRC is a popular error checking mechanism in computer networks.

The method starts by assigning the ""init"" value to a variable called ""crc"". Then, it enters a loop that runs for ""self.Width"" number of iterations. Within the loop, the least significant bit of the ""crc"" value is extracted and stored in a variable called ""bit"".

If the ""bit"" is True (i.e., 1), it performs a bitwise XOR operation between ""crc"" and ""self.Poly"". This is done to apply the polynomial division algorithm in CRC calculations.

Next, the ""crc"" value is shifted right by one bit position. This is equivalent to dividing the ""crc"" value by 2.

If the ""bit"" was initially True, a bitwise OR operation is performed between ""crc"" and ""self.MSB_Mask"". This is done to set the most significant bit (MSB) of ""crc"" to 1.

Finally, the calculated ""crc"" value is bitwise ANDed with ""self.Mask"" to ensure it remains within the desired range of values. The result is then returned as the non-direct initialization value."
"def reflect(self, data, width):
  x = data & 0x01
 
  for i in range(width - 1):
 
  data >>= 1
 
  x = (x << 1) | (data & 0x01)
 
  return x","reflect a data word, i.e. reverts the bit order.","The `reflect` function takes in two parameters: `data` and `width`. 

It first extracts the least significant bit of `data` by performing a bitwise AND with the value `0x01` (which is the hexadecimal representation of 1). This is stored in the variable `x`.

Next, a loop is initiated for `width - 1` iterations. In each iteration, `data` is shifted right by 1 bit using the right bit shift operator. This effectively removes the least significant bit from `data`.

The variable `x` is then updated by shifting its value left by 1 bit and OR'ing it with the least significant bit of `data`. This effectively appends the new least significant bit to the left of the previous bits in `x`.

After the loop completes, the final value of `x` is returned."
"def bit_by_bit(self, in_data):
 
  # If the input data is a string, convert to bytes.
 
  if isinstance(in_data, str):
 
  in_data = [ord(c) for c in in_data]
 
 
 
  register = self.NonDirectInit
 
  for octet in in_data:
 
  if self.ReflectIn:
 
  octet = self.reflect(octet, 8)
 
  for i in range(8):
 
  topbit = register & self.MSB_Mask
 
  register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)
 
  if topbit:
 
  register ^= self.Poly
 
 
 
  for i in range(self.Width):
 
  topbit = register & self.MSB_Mask
 
  register = ((register << 1) & self.Mask)
 
  if topbit:
 
  register ^= self.Poly
 
 
 
  if self.ReflectOut:
 
  register = self.reflect(register, self.Width)
 
  return register ^ self.XorOut","Classic simple and slow CRC implementation. This function iterates bit
 
  by bit over the augmented input message and returns the calculated CRC
 
  value at the end.","This code defines a method called ""bit_by_bit"" that takes in a data input. The code first checks if the input data is a string. If it is, the code converts it to a list of Unicode character codes. 

The code then initializes a register variable with some initial value. 

Next, the code iterates over each octet (an 8-bit chunk of data) in the input data. If the ""ReflectIn"" flag is set, the code reflects the bits in the octet. 

For each bit in the reflected octet, the code checks the top bit of the register. If the top bit is set, the code performs a bitwise XOR operation between the register and a polynomial value. 

After processing all the bits in the octet, the code then performs a similar processing for the register itself, shifting its bits to the left and performing a bitwise XOR operation if the top bit is set. This is repeated for a number of times equal to the ""Width"" of the register. 

If the ""ReflectOut"" flag is set, the code reflects the bits in the final register value. 

Finally, the code performs a bitwise XOR operation between the final register value and the ""XorOut"" value, and returns the result."
"def gen_table(self):
 
  table_length = 1 << self.TableIdxWidth
 
  tbl = [0] * table_length
 
  for i in range(table_length):
 
  register = i
 
  if self.ReflectIn:
 
  register = self.reflect(register, self.TableIdxWidth)
 
  register = register << (self.Width - self.TableIdxWidth + self.CrcShift)
 
  for j in range(self.TableIdxWidth):
 
  if register & (self.MSB_Mask << self.CrcShift) != 0:
 
  register = (register << 1) ^ (self.Poly << self.CrcShift)
 
  else:
 
  register = (register << 1)
 
  if self.ReflectIn:
 
  register = self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift
 
  tbl[i] = register & (self.Mask << self.CrcShift)
 
  return tbl","This function generates the CRC table used for the table_driven CRC
 
  algorithm. The Python version cannot handle tables of an index width
 
  other than 8. See the generated C code for tables with different sizes
 
  instead.","This code generates a lookup table for a CRC (Cyclic Redundancy Check) algorithm. 

The length of the table is determined by shifting the binary value 1 by the width of the table index. 

The table is initially filled with zeros. 

Then, for each entry in the table, the algorithm performs the following steps: 
- The current entry value is stored in a register. 
- If the ""ReflectIn"" flag is enabled, the contents of the register are reflected (the bits are reversed) based on the width of the table index. 
- The register is then shifted to the left by the difference between the algorithm's width and the table index width plus the CRC shift value. 
- For each bit in the table index, starting from the most significant bit: 
  - If the bit in the register, after shifting, is set to 1, the register is left-shifted by 1 and the polynomial value (poly) is XOR-ed with the register after being shifted by the CRC shift value. 
  - If the bit in the register, after shifting, is set to 0, the register is only left-shifted by 1. 
- If the ""ReflectIn"" flag is enabled, the register is reflected by shifting it to the right by the CRC shift value and then reflecting it again based on the algorithm's width. 
- Finally, the entry value in the table is updated with the result of AND operation between the register and a mask shifted by the CRC shift value. 

After iterating through all the entries, the generated table is returned as the output of the function."
"def table_driven(self, in_data):
 
  # If the input data is a string, convert to bytes.
 
  if isinstance(in_data, str):
 
  in_data = [ord(c) for c in in_data]
 
 
 
  tbl = self.gen_table()
 
 
 
  register = self.DirectInit << self.CrcShift
 
  if not self.ReflectIn:
 
  for octet in in_data:
 
  tblidx = ((register >> (self.Width - self.TableIdxWidth + self.CrcShift)) ^ octet) & 0xff
 
  register = ((register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)
 
  register = register >> self.CrcShift
 
  else:
 
  register = self.reflect(register, self.Width + self.CrcShift) << self.CrcShift
 
  for octet in in_data:
 
  tblidx = ((register >> self.CrcShift) ^ octet) & 0xff
 
  register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)
 
  register = self.reflect(register, self.Width + self.CrcShift) & self.Mask
 
 
 
  if self.ReflectOut:
 
  register = self.reflect(register, self.Width)
 
  return register ^ self.XorOut",The Standard table_driven CRC algorithm.,"This code implements a table-driven CRC (Cyclic Redundancy Check) algorithm. CRC is a method used to detect errors in data transmission by adding a checksum to the data. 

The function takes two arguments: 
- self: represents the instance of a class, although this code doesn't seem to be a part of any class.
- in_data: the input data to be checked for errors. It can be either a string or a list of bytes.

If the input data is a string, it is converted to a list of bytes by using the ord() function to get the ASCII value of each character. 

The function initializes a table called ""tbl"" by calling the gen_table() function. This table is used for lookup operations during the calculation of the CRC.

Then, the function initializes a variable called ""register"" with a value based on the ""DirectInit"" attribute shifted by the ""CrcShift"" value.

If the ""ReflectIn"" attribute is False, the function iterates over each octet (byte) in the input data. For each octet, it calculates an index into the lookup table by performing a bitwise operation on the register and the octet. It then updates the register by shifting it and XORing it with a value from the lookup table. Finally, it applies a bitwise mask to ensure the register is within the expected range. If ""ReflectIn"" is True, the function first reflects the register using the reflect() function and then performs similar operations as before.

After processing all the input data, the function performs some additional operations depending on the attributes ""ReflectOut"" and ""XorOut"". If ""ReflectOut"" is True, the register is reflected again. Finally, the register is XORed with the ""XorOut"" attribute to get the final CRC value.

The calculated CRC value is returned as the output of the function."
"def parse_masked(seq, min_len):
 
  nm, masked = [], [[]]
 
  prev = None
 
  for base in seq[1]:
 
  if base.isupper():
 
  nm.append(base)
 
  if masked != [[]] and len(masked[-1]) < min_len:
 
  nm.extend(masked[-1])
 
  del masked[-1]
 
  prev = False
 
  elif base.islower():
 
  if prev is False:
 
  masked.append([])
 
  masked[-1].append(base)
 
  prev = True
 
  return nm, masked",parse masked sequence into non-masked and masked regions,"The given code defines a function named ""parse_masked"" that takes two parameters: ""seq"" and ""min_len"". This function is used to parse a masked sequence and split it into a list of nucleotides and a list of masked regions.

The function initializes two empty lists: ""nm"" to store the nucleotides and ""masked"" to store the masked regions. 

Then, a variable named ""prev"" is initialized with a value of None. This variable is used to keep track of the previous character in the sequence.

The code iterates through each character in the ""seq"" parameter. If the character is uppercase, it means it is a nucleotide, so it is added to the ""nm"" list.

If the character is lowercase, it means it is part of a masked region. The code checks the value of the ""prev"" variable. If it is False, it means the previous character was also lowercase, indicating that the current lowercase character is consecutive to the previous one. In this case, the current lowercase character is appended to the last sublist in the ""masked"" list.

If the ""prev"" variable is False, it means the previous character was uppercase, indicating that a new masked region is starting. In this case, a new sublist is appended to the ""masked"" list and the current lowercase character is added to this new sublist.

Finally, the function returns the ""nm"" list containing all the nucleotides and the ""masked"" list containing all the masked regions."
"def strip_masked(fasta, min_len, print_masked):
 
 
  for seq in parse_fasta(fasta):
 
  nm, masked = parse_masked(seq, min_len)
 
  nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]
 
  yield [0, nm]
 
  if print_masked is True:
 
  for i, m in enumerate([i for i in masked if i != []], 1):
 
  m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]
 
  yield [1, m]","remove masked regions from fasta file as long as
 
  they are longer than min_len","This function takes in a FASTA file and parameters for minimum length and whether or not to print masked sequences. It parses the FASTA file and extracts sequences. It then removes any masked sequences that are shorter than the specified minimum length. 

If the print_masked parameter is True, it generates natural language comments for each masked sequence encountered, indicating the sequence name and the position of the insertion. 

The function returns a generator object with lists consisting of integer flags indicating whether the sequence is a removed masked sequence (0) or an inserted masked sequence (1), along with the corresponding natural language comments."
"def get_relative_abundance(biomfile):
  biomf = biom.load_table(biomfile)
 
  norm_biomf = biomf.norm(inplace=False)
 
  rel_abd = {}
 
  for sid in norm_biomf.ids():
 
  rel_abd[sid] = {}
 
  for otuid in norm_biomf.ids(""observation""):
 
  otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=""observation"")[""taxonomy""])
 
  otuname = "" "".join(otuname.split(""_""))
 
  abd = norm_biomf.get_value_by_ids(otuid, sid)
 
  rel_abd[sid][otuname] = abd
 
  ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)
 
  return ast_rel_abd","Return arcsine transformed relative abundance from a BIOM format file.
 
 
 
  :type biomfile: BIOM format file
 
  :param biomfile: BIOM format file used to obtain relative abundances for each OTU in
 
  a SampleID, which are used as node sizes in network plots.
 
 
 
  :type return: Dictionary of dictionaries.
 
  :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name
 
  whose value is the arc sine tranfsormed relative abundance value for that
 
  SampleID-OTU Name pair.","The function `get_relative_abundance` takes as input a biom file and calculates the relative abundance of each observation in the input file. 

First, the function loads the biom file using the `biom.load_table` function and assigns it to the variable `biomf`. 

Next, it calculates the normalized biom table by calling the `norm` function on `biomf` and assigns the result to the variable `norm_biomf`. 

Then, it initializes an empty dictionary called `rel_abd` to store the relative abundance values. 

The code then iterates through each sample ID in `norm_biomf` using a for loop. Inside the loop, it initializes an empty dictionary `rel_abd[sid]` for each sample ID.

Within the inner loop, the code iterates through each observation ID in `norm_biomf` using another for loop. For each observation ID, it retrieves the taxonomy metadata for that observation and joins the taxonomy strings by replacing underscores with spaces. 

Next, it gets the abundance value for the current observation and sample ID using the `get_value_by_ids` function, and stores it in the `rel_abd` dictionary with the sample ID as the key and the joined taxonomy string as the key in the nested dictionary. 

Finally, the code applies the arcsine square root transformation to the `rel_abd` dictionary using the `arcsine_sqrt_transform` function from an imported module `bc`. The transformed relative abundance data is then assigned to the variable `ast_rel_abd`. 

The function returns the `ast_rel_abd` dictionary as the output."
"def find_otu(otuid, tree):
 
 
  for m in re.finditer(otuid, tree):
 
  before, after = tree[m.start()-1], tree[m.start()+len(otuid)]
 
  if before in [""("", "","", "")""] and after in ["":"", "";""]:
 
  return m.start()
 
  return None","Find an OTU ID in a Newick-format tree.
 
  Return the starting position of the ID or None if not found.","This code defines a function called find_otu, which takes two parameters: otuid (a string) and tree (also a string). The purpose of this function is to find the position of the otuid within the tree string. 

The function uses a for loop and the re.finditer() method to search for all occurrences of otuid within the tree string. The re.finditer() method returns an iterator that produces Match objects. Each Match object contains information about the start and end positions of the matched substring.

Inside the for loop, the code retrieves the characters before and after the matched otuid using the indices returned by the Match object. 

The code then checks if the character before the otuid is either ""("", "","", or "")"" and if the character after the otuid is either "":"" or "";"". This is done to ensure that the otuid is being matched as a whole and not as part of a larger word or string.

If both conditions are satisfied, the code returns the start position of the otuid. If none of the occurrences of otuid satisfy the conditions, the code returns None."
"def newick_replace_otuids(tree, biomf):
 
  for val, id_, md in biomf.iter(axis=""observation""):
 
  otu_loc = find_otu(id_, tree)
 
  if otu_loc is not None:
 
  tree = tree[:otu_loc] + \
 
  oc.otu_name(md[""taxonomy""]) + \
 
  tree[otu_loc + len(id_):]
 
  return tree","Replace the OTU ids in the Newick phylogenetic tree format with truncated
 
  OTU names","The given code is a function named ""newick_replace_otuids"" that takes two parameters: ""tree"" and ""biomf"". 

The function is intended to replace the OTU IDs in the provided ""tree"" with the corresponding OTU names from a given biom file (""biomf""). 

It iterates through all the observations in the biom file, retrieving the ID, value, and metadata for each observation. 

For each observation, the function attempts to find the OTU ID in the ""tree"". If a matching OTU ID is found, the function replaces it with the corresponding OTU name from the metadata information. 

Finally, the function returns the modified ""tree"" with the replaced OTU IDs."
"def genome_info(genome, info):
 
  try:
 
  scg = info['#SCGs']
 
  dups = info['#SCG duplicates']
 
  length = info['genome size (bp)']
 
  return [scg - dups, length, genome]
 
  except:
 
  return [False, False, info['genome size (bp)'], genome]","return genome info for choosing representative
 
 
 
  if ggKbase table provided - choose rep based on SCGs and genome length
 
  - priority for most SCGs - extra SCGs, then largest genome
 
 
 
  otherwise, based on largest genome","This code defines a function called genome_info. The function takes in two arguments: genome and info. The purpose of this function is to extract certain information from the 'info' dictionary and return it in a specific format. 

The function begins with a try-except block, which is used for error handling. Inside the try block, the function extracts three pieces of information from the 'info' dictionary using specific keys: the number of single copy genes ('#SCGs'), the number of duplicate genes ('#SCG duplicates'), and the size of the genome in base pairs ('genome size (bp)'). 

Next, the function calculates the difference between the number of single copy genes and the number of duplicate genes, and assigns it to the variable 'scg'. The function also assigns the genome size to the variable 'length'.

Finally, the function returns a list with three elements: the difference between single copy genes and duplicate genes, the genome size, and the original input genome. This list provides information about the genome, such as the number of unique genes, the size of the genome, and the original genome itself.

If there is an error while extracting the information from the 'info' dictionary, such as a missing key, the except block will be executed. In this case, the function returns a list with two elements: False and the genome size. This indicates that there was an error in retrieving the information and the function couldn't provide the desired output."
"def print_clusters(fastas, info, ANI):
 
  header = ['#cluster', 'num. genomes', 'rep.', 'genome', '#SCGs', '#SCG duplicates', \
 
  'genome size (bp)', 'fragments', 'list']
 
  yield header
 
  in_cluster = []
 
  for cluster_num, cluster in enumerate(connected_components(ANI)):
 
  cluster = sorted([genome_info(genome, info[genome]) \
 
  for genome in cluster], \
 
  key = lambda x: x[0:], reverse = True)
 
  rep = cluster[0][-1]
 
  cluster = [i[-1] for i in cluster]
 
  size = len(cluster)
 
  for genome in cluster:
 
  in_cluster.append(genome)
 
  try:
 
  stats = [size, rep, genome, \
 
  info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \
 
  info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]
 
  except:
 
  stats = [size, rep, genome, \
 
  'n/a', 'n/a', \
 
  info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]
 
  if rep == genome:
 
  stats = ['*%s' % (cluster_num)] + stats
 
  else:
 
  stats = [cluster_num] + stats
 
  yield stats
 
  # print singletons
 
  try:
 
  start = cluster_num + 1
 
  except:
 
  start = 0
 
  fastas = set([i.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0] for i in fastas])
 
  for cluster_num, genome in \
 
  enumerate(fastas.difference(set(in_cluster)), start):
 
  try:
 
  stats = ['*%s' % (cluster_num), 1, genome, genome, \
 
  info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \
 
  info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]
 
  except:
 
  stats = ['*%s' % (cluster_num), 1, genome, genome, \
 
  'n/a', 'n/a', \
 
  info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]
 
  yield stats","choose represenative genome and
 
  print cluster information
 
 
 
  *if ggKbase table is provided, use SCG info to choose best genome","The function `print_clusters` takes in three parameters: `fastas`, `info`, and `ANI`. This function is designed to print information about clusters of genomes based on their connectedness determined by the ANI (Average Nucleotide Identity) values.

First, the function initializes a header that specifies the information that will be printed for each cluster. This header is yielded as the first output.

Next, the variable `in_cluster` is initialized as an empty list. This list will keep track of the genomes that have already been processed in a cluster.

The function then loops through each cluster in the connected components of the ANI graph. Each cluster is sorted based on the information of the genomes within it, in descending order. The representative genome for the cluster is determined as the one with the highest score. The cluster is then rearranged to only contain the genome names, discarding the additional information.

For each genome in the cluster, its information is retrieved from the `info` dictionary. If the information is not available, appropriate 'n/a' values are used instead. The relevant statistics, such as the size of the cluster, the representative genome, the number of SCGs (Single Copy Genes), the number of SCG duplicates, the genome size, the number of contigs, and the list of genomes in the cluster, are stored in the `stats` list.

If the genome is the representative genome, it is labeled with an asterisk and the cluster number concatenated with it. Otherwise, only the cluster number is used. The `stats` list is then yielded as the output.

After processing all the clusters, the function moves on to handling the singletons, i.e., the genomes that are not part of any cluster. It starts by determining the index for labeling the singletons based on the number of clusters encountered in the loop. The set of `fastas` is created by extracting the genome names from the file paths given in `fastas`. 

For each singleton genome, its information is retrieved from the `info` dictionary. If the information is not available, appropriate 'n/a' values are used instead. The relevant statistics are stored in the `stats` list.

The `stats` list is then yielded as the output."
"def parse_ggKbase_tables(tables, id_type):
 
 
  g2info = {}
 
  for table in tables:
 
  for line in open(table):
 
  line = line.strip().split('\t')
 
  if line[0].startswith('name'):
 
  header = line
 
  header[4] = 'genome size (bp)'
 
  header[12] = '#SCGs'
 
  header[13] = '#SCG duplicates'
 
  continue
 
  name, code, info = line[0], line[1], line
 
  info = [to_int(i) for i in info]
 
  if id_type is False: # try to use name and code ID
 
  if 'UNK' in code or 'unknown' in code:
 
  code = name
 
  if (name != code) and (name and code in g2info):
 
  print('# duplicate name or code in table(s)', file=sys.stderr)
 
  print('# %s and/or %s' % (name, code), file=sys.stderr)
 
  exit()
 
  if name not in g2info:
 
  g2info[name] = {item:stat for item, stat in zip(header, info)}
 
  if code not in g2info:
 
  g2info[code] = {item:stat for item, stat in zip(header, info)}
 
  else:
 
  if id_type == 'name':
 
  ID = name
 
  elif id_type == 'code':
 
  ID = code
 
  else:
 
  print('# specify name or code column using -id', file=sys.stderr)
 
  exit()
 
  ID = ID.replace(' ', '')
 
  g2info[ID] = {item:stat for item, stat in zip(header, info)}
 
  if g2info[ID]['genome size (bp)'] == '':
 
  g2info[ID]['genome size (bp)'] = 0
 
  return g2info",convert ggKbase genome info tables to dictionary,"This code defines a function called ""parse_ggKbase_tables"" which takes in two parameters: ""tables"" (a list of table files) and ""id_type"" (a string indicating whether to use the name or code as the ID). 

Inside the function, a dictionary called ""g2info"" is initialized. This dictionary will store information about each entry in the tables.

The function then iterates through each table file. For each line in the file, it removes leading and trailing whitespaces and splits the line into a list of elements based on tab separation.

If the first element of the line starts with 'name', it means it is the header line of the table. In this case, the function updates certain values in the header list (at index 4, 12, and 13) to make it more informative. Then, it continues to the next line.

For all lines that are not header lines, the function extracts the name, code, and info from the line. The info list is converted to integers using a helper function called ""to_int"". 

If the id_type is set to False, the function attempts to use both the name and code as the ID. If the code contains 'UNK' or 'unknown', it is replaced with the name. If both the name and code already exist in the g2info dictionary, it outputs an error message and exits the function.

If the name does not exist in the g2info dictionary, it creates a new entry in the dictionary with the name as the key and a sub-dictionary containing the header items as the value. Similarly, if the code does not exist in the g2info dictionary, it creates a new entry with the code as the key and the sub-dictionary as the value.

If the id_type is specified as either 'name' or 'code', the appropriate ID is chosen based on the id_type. The spaces are removed from the ID and it is used as the key in the g2info dictionary, with the sub-dictionary as the value.

If the 'genome size (bp)' value in the sub-dictionary is empty, it is set to 0.

Finally, the function returns the g2info dictionary containing the parsed information from the table files."
"def foo(tables):
 
 
  g2info = {}
 
  for table in tables:
 
  for line in open(table):
 
  line = line.strip().split('\t')
 
  if line[0].startswith('Bin Id'):
 
  header = line
 
  header[8] = 'genome size (bp)'
 
  header[5] = '#SCGs'
 
  header[6] = '#SCG duplicates'
 
  continue
 
  ID, info = line[0], line
 
  info = [to_int(i) for i in info]
 
  ID = ID.replace(' ', '')
 
  g2info[ID] = {item:stat for item, stat in zip(header, info)}
 
  if g2info[ID]['genome size (bp)'] == '':
 
  g2info[ID]['genome size (bp)'] = 0
 
  return g2info",convert checkM genome info tables to dictionary,"This code defines a function called ""foo"" that takes a list of table files as input. The purpose of the function is to read and process the information in these tables and return a dictionary containing the extracted information. 

The function starts by creating an empty dictionary called ""g2info"" that will store the extracted information. 

Next, it iterates over each table file in the input list. 

For each table file, the code opens the file and iterates over each line. 

Each line is stripped of any leading or trailing white spaces and then split by tab ('\t') delimiter to separate the values. 

If the first value of a line starts with 'Bin Id', it means this line contains the header information. The code updates the column names to make them more meaningful by replacing some of the values in the list called ""header"". 

After handling the header, the code retrieves the ID from the first value of the line and assigns the rest of the values to a list called ""info"". 

The values in ""info"" are converted to integers using the ""to_int"" function. 

The ID is then stripped of any white spaces and used as the key to store the information in the ""g2info"" dictionary. Each column name from the header is paired with the corresponding value from ""info"" and stored as a key-value pair in a nested dictionary under the ID key. 

If the 'genome size (bp)' value is empty, it is set to 0. 

Finally, the function returns the populated ""g2info"" dictionary."
"def foo(fastas, info):
  if info is False:
 
  info = {}
 
  for genome in fastas:
 
  name = genome.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]
 
  if name in info:
 
  continue
 
  length = 0
 
  fragments = 0
 
  for seq in parse_fasta(genome):
 
  length += len(seq[1])
 
  fragments += 1
 
  info[name] = {'genome size (bp)':length, '# contigs':fragments}
 
  return info",get genome lengths,"This function takes in a list of DNA sequence files and an info flag as input. If the info flag is set to False, an empty dictionary called ""info"" is initialized. 

For each genome in the list of sequence files, the function extracts the name of the genome by removing the file extension and directory information. If the name already exists in the info dictionary, the function moves on to the next genome. 

Next, the function calculates the length of the genome by summing up the lengths of each sequence in the genome. It also counts the number of sequences in the genome as the number of fragments. 

Finally, the function adds an entry to the info dictionary for the current genome, with the genome name as the key and a dictionary containing the genome size and number of contigs as the value. 

The function returns the updated info dictionary."
"def get_dbs(self, attr, args, kwargs, **fkwargs):
 
 
  if not self._ready:
 
  if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):
 
  raise self.UnableToSetupRouter()
 
 
 
  retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)
 
  if retval is not None:
 
  args, kwargs = retval
 
 
 
  if not (args or kwargs):
 
  return self.cluster.hosts.keys()
 
 
 
  try:
 
  db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)
 
  except Exception as e:
 
  self._handle_exception(e)
 
  db_nums = []
 
 
 
  return self._post_routing(attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs)","Returns a list of db keys to route the given call to.
 
 
 
  :param attr: Name of attribute being called on the connection.
 
  :param args: List of arguments being passed to ``attr``.
 
  :param kwargs: Dictionary of keyword arguments being passed to ``attr``.
 
 
 
  >>> redis = Cluster(router=BaseRouter)
 
  >>> router = redis.router
 
  >>> router.get_dbs('incr', args=('key name', 1))
 
  [0,1,2]","This code is a method that retrieves the databases to which an operation should be routed in a distributed database system. It takes in the attribute, arguments, and keyword arguments of the operation, as well as any additional keyword arguments. 

First, it checks if the router is ready. If not, it tries to set up the router using the provided arguments and keyword arguments. If the router setup fails, it raises an exception indicating that it was unable to set up the router. 

Next, it performs some pre-routing operations, checking if there are any modifications required for the arguments or keyword arguments. If there are modifications, it updates the corresponding variables. 

If there are no arguments or keyword arguments, it returns the list of keys representing the available databases in the cluster. 

If there are arguments or keyword arguments, it tries to route the operation based on the provided attributes, arguments, and keyword arguments. If any exception occurs during the routing process, it handles the exception and sets the list of database numbers to an empty list. 

Finally, it performs any required post-routing operations, passing the attribute, database numbers, arguments, keyword arguments, and any additional keyword arguments. The method then returns the list of database numbers to which the operation is to be routed."
"def setup_router(self, args, kwargs, **fkwargs):
 
  self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)
 
 
 
  return self._ready",Call method to perform any setup,"The `setup_router` function is used to set up the router for the current object. It takes in arguments (`args`), keyword arguments (`kwargs`), and additional keyword arguments (`fkwargs`). 

The `_ready` attribute is initially set to False. 

The `_setup_router` function is called with the given arguments and keyword arguments, along with the additional keyword arguments. The result of this function is assigned to the `_ready` attribute. 

Finally, the function returns the value of the `_ready` attribute."
"def _route(self, attr, args, kwargs, **fkwargs):
 
 
  return self.cluster.hosts.keys()",Perform routing and return db_nums,"This code is defining a private method called `_route` within a class. The purpose of this method is to return a list of keys from a dictionary.

The method takes in four parameters: `attr`, `args`, `kwargs`, and `fkwargs`. The `attr` parameter is not used in the method and is probably meant for future use. The `args` and `kwargs` parameters are also not utilized within the method.

The method uses the `**fkwargs` parameter, which is used to pass any additional keyword arguments that are not explicitly defined in the method signature. These additional keyword arguments are packed into a dictionary named `fkwargs`.

The method body consists of a single line of code. It returns the keys from a dictionary called `self.cluster.hosts`. The `self.cluster.hosts` dictionary is accessed using the `.keys()` method, which returns a list of all the keys in the dictionary.

In summary, this method returns a list of keys from the `self.cluster.hosts` dictionary."
"def check_down_connections(self):
  now = time.time()
 
 
 
  for db_num, marked_down_at in self._down_connections.items():
 
  if marked_down_at + self.retry_timeout <= now:
 
  self.mark_connection_up(db_num)","Iterates through all connections which were previously listed as unavailable
 
  and marks any that have expired their retry_timeout as being up.","The `check_down_connections` method is responsible for checking if any database connections that were previously marked as down can be reestablished. 

First, it retrieves the current time using the `time.time()` function. 

Then, it iterates over each database number and the timestamp when the connection was marked as down, stored in the `_down_connections` dictionary. 

If the marked down time plus the retry timeout is less than or equal to the current time, it means that the retry timeout has passed and the connection can be retried. In this case, the `mark_connection_up` method is called to mark the connection as up again."
"def flush_down_connections(self):
 
 
  self._get_db_attempts = 0
 
  for db_num in self._down_connections.keys():
 
  self.mark_connection_up(db_num)",Marks all connections which were previously listed as unavailable as being up.,"This code is defining a method called ""flush_down_connections"" within a class. The purpose of this method is to iterate through a dictionary of ""down_connections"" and mark each connection as ""up"".

The method starts by initializing a variable called ""_get_db_attempts"" to 0.

Then, a for loop is used to iterate through each key in the dictionary of ""down_connections"". For each key (which represents a database number), the method calls another method called ""mark_connection_up"" and passes the database number as an argument.

The ""mark_connection_up"" method is responsible for marking a given database connection as ""up"".

Overall, this ""flush_down_connections"" method is used to reset any down connections in the dictionary and ensures that all connections are marked as ""up""."
"def standby(df, resolution='24h', time_window=None): 
 
 
  if df.empty:
 
  raise EmptyDataFrame()
 
 
 
  df = pd.DataFrame(df) # if df was a pd.Series, convert to DataFrame
 
  def parse_time(t):
 
  if isinstance(t, numbers.Number):
 
  return pd.Timestamp.utcfromtimestamp(t).time()
 
  else:
 
  return pd.Timestamp(t).time()
 
 
 
 
 
  # first filter based on the time-window
 
  if time_window is not None:
 
  t_start = parse_time(time_window[0])
 
  t_end = parse_time(time_window[1])
 
  if t_start > t_end:
 
  # start before midnight
 
  df = df[(df.index.time >= t_start) | (df.index.time < t_end)]
 
  else:
 
  df = df[(df.index.time >= t_start) & (df.index.time < t_end)]
 
 
 
  return df.resample(resolution).min()","Compute standby power
 
 
 
  Parameters
 
  ----------
 
  df : pandas.DataFrame or pandas.Series
 
  Electricity Power
 
  resolution : str, default='d'
 
  Resolution of the computation. Data will be resampled to this resolution (as mean) before computation
 
  of the minimum.
 
  String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
 
  time_window : tuple with start-hour and end-hour, default=None
 
  Specify the start-time and end-time for the analysis.
 
  Only data within this time window will be considered.
 
  Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects
 
 
 
  Returns
 
  -------
 
  df : pandas.Series with DateTimeIndex in the given resolution","This code defines a function called ""standby"" that is used to filter and resample a given DataFrame or Series based on a specific time window and resolution.

The function takes in a DataFrame or Series object named ""df"" as its input. If the DataFrame is empty, it raises an EmptyDataFrame exception.

The function then converts the input to a DataFrame if it is a Series.

There is a helper function called ""parse_time"" defined within the ""standby"" function. This function is used to parse timestamps or time values. If the input is a number, it converts it to a timestamp and returns the time component. If the input is a string, it converts it to a timestamp and returns the time component.

Next, the code checks if a time window is specified. If a time window is provided, it parses the start and end times using the ""parse_time"" function. If the start time is later than the end time, it filters the DataFrame to include rows where the time is greater than or equal to the start time or less than the end time. Otherwise, it filters the DataFrame to include rows where the time is greater than or equal to the start time and less than the end time.

Finally, the function resamples the filtered DataFrame using the specified resolution. It calculates the minimum value within each resampled time interval and returns the resampled DataFrame."
"def share_of_standby(df, resolution='24h', time_window=None): 
 
 
  p_sb = standby(df, resolution, time_window)
 
  df = df.resample(resolution).mean()
 
  p_tot = df.sum()
 
  p_standby = p_sb.sum()
 
  share_standby = p_standby / p_tot
 
  res = share_standby.iloc[0]
 
  return res","Compute the share of the standby power in the total consumption.
 
 
 
  Parameters
 
  ----------
 
  df : pandas.DataFrame or pandas.Series
 
  Power (typically electricity, can be anything)
 
  resolution : str, default='d'
 
  Resolution of the computation. Data will be resampled to this resolution (as mean) before computation
 
  of the minimum.
 
  String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
 
  time_window : tuple with start-hour and end-hour, default=None
 
  Specify the start-time and end-time for the analysis.
 
  Only data within this time window will be considered.
 
  Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects
 
 
 
  Returns
 
  -------
 
  fraction : float between 0-1 with the share of the standby consumption","This function calculates the share of standby power consumption in a given dataset. The input dataframe 'df' represents power consumption data over a certain period of time. 

The 'standby' function is called to calculate the standby power consumption. The 'resolution' parameter determines the time interval for which the standby power consumption is calculated (default is 24 hours). The 'time_window' parameter is optional and can be used to specify a specific time period within the dataset for the calculation of standby power consumption.

The given dataframe 'df' is then resampled to match the specified resolution. The mean power consumption for each resampled interval is calculated.

The total power consumption 'p_tot' is obtained by summing up the mean power consumption values from the resampled dataframe.

The standby power consumption 'p_standby' is obtained by summing up the standby power values calculated by the 'standby' function.

The share of standby power consumption 'share_standby' is obtained by dividing 'p_standby' by 'p_tot'.

The result is then returned as the share of standby power consumption."
"def count_peaks(ts):
 
  on_toggles = ts.diff() > 3000
 
  shifted = np.logical_not(on_toggles.shift(1))
 
  result = on_toggles & shifted
 
  count = result.sum()
 
  return count","Toggle counter for gas boilers
 
 
 
  Counts the number of times the gas consumption increases with more than 3kW
 
 
 
  Parameters
 
  ----------
 
  ts: Pandas Series
 
  Gas consumption in minute resolution
 
 
 
  Returns
 
  -------
 
  int","This function counts the number of peaks in a time series data. 

The input to the function is a time series data, represented as a one-dimensional array (ts). 

First, the function calculates the differences between consecutive elements in the time series using the diff() function. It then checks if the differences are greater than a threshold value of 3000, indicating a change in value. 

The on_toggles variable stores the resulting Boolean values (True or False) indicating whether there is a change in value or not. 

The shifted variable is created by shifting the on_toggles variable by one position. 

The result variable is then created by performing a logical AND operation between on_toggles and shifted. This will be True only when there is a change in value followed by no change or vice versa, representing a peak. 

Finally, the count variable calculates the sum of True values in the resulting array. 

The count of peaks is then returned as the output of the function."
"def load_factor(ts, resolution=None, norm=None):
 
  if norm is None:
 
  norm = ts.max()
 
 
 
  if resolution is not None:
 
  ts = ts.resample(rule=resolution).mean()
 
 
 
  lf = ts / norm
 
 
 
  return lf","Calculate the ratio of input vs. norm over a given interval.
 
 
 
  Parameters
 
  ----------
 
  ts : pandas.Series
 
  timeseries
 
  resolution : str, optional
 
  interval over which to calculate the ratio
 
  default: resolution of the input timeseries
 
  norm : int | float, optional
 
  denominator of the ratio
 
  default: the maximum of the input timeseries
 
 
 
  Returns
 
  -------
 
  pandas.Series","This function calculates the load factor of a time series data. The load factor is a measure of the proportion of data compared to a maximum value. 

The function takes three optional arguments: 
- 'ts' which represents the input time series data
- 'resolution' which represents the desired time resolution for the time series data after resampling (if provided)
- 'norm' which represents the normalization value for the time series data (if provided)

If the 'norm' argument is not provided, the function sets the normalization value to the maximum value of the time series data.

If the 'resolution' argument is provided, the function resamples the time series at the specified resolution by taking the mean of the data within each time interval.

The load factor is then calculated by dividing the time series data (or the resampled data if 'resolution' was provided) by the normalization value.

Finally, the function returns the load factor."
"def top_hits(hits, num, column, reverse):
  hits.sort(key = itemgetter(column), reverse = reverse)
 
  for hit in hits[0:num]:
 
  yield hit",get top hits after sorting by column number,"This code defines a function called ""top_hits"" that takes in four parameters: ""hits"" (a list of items), ""num"" (the number of top hits to return), ""column"" (the column of the items to sort by), and ""reverse"" (a boolean indicating whether to sort in reverse order). 

The function first sorts the ""hits"" list based on the specified ""column"" value using the ""itemgetter"" function from the ""operator"" module. The ""reverse"" parameter determines whether the sorting order is ascending or descending.

Next, the function iterates over the sorted list and yields (returns) each hit one by one. Only the first ""num"" hits are returned.

Overall, this function is used to retrieve the top hits from a list based on a given column and sorting order."
"def numBlast_sort(blast, numHits, evalueT, bitT):
 
  header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',
 
  'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']
 
  yield header
 
  hmm = {h:[] for h in header}
 
  for line in blast:
 
  if line.startswith('#'):
 
  continue
 
  line = line.strip().split('\t')
 
  # Evalue and Bitscore thresholds
 
  line[10], line[11] = float(line[10]), float(line[11])
 
  evalue, bit = line[10], line[11]
 
  if evalueT is not False and evalue > evalueT:
 
  continue
 
  if bitT is not False and bit < bitT:
 
  continue
 
  for i, h in zip(line, header):
 
  hmm[h].append(i)
 
  hmm = pd.DataFrame(hmm)
 
  for query, df in hmm.groupby(by = ['#query']):
 
  df = df.sort_values(by = ['bitscore'], ascending = False)
 
  for hit in df[header].values[0:numHits]:
 
  yield hit",parse b6 output with sorting,"The function `numBlast_sort` takes in four parameters:
1. `blast`: A list containing the blast results.
2. `numHits`: The number of top hits to return.
3. `evalueT`: The threshold value for the e-value. If `False`, there is no e-value threshold.
4. `bitT`: The threshold value for the bit score. If `False`, there is no bit score threshold.

The function begins by defining the header for the blast results. This header contains the names of the columns in the blast data.

Then, the function initializes an empty dictionary called `hmm` with the keys from the header.

Next, the function iterates through each line in the blast results. If a line starts with ""#"" (indicating a comment), the function skips to the next line. Otherwise, it splits the line into a list of values.

The function then checks if the e-value and bit score exceed the specified thresholds. If so, the loop continues to the next line. If not, the values are added to the `hmm` dictionary corresponding to their respective header key.

After iterating through all the lines, the `hmm` dictionary is converted into a pandas DataFrame.

The DataFrame is then grouped by the query column (`#query`) and sorted based on the bit score in descending order.

Finally, the function iterates through each query and returns the top `numHits` hits for each query. Each hit is yielded as a list of values corresponding to the header."
"def numBlast(blast, numHits, evalueT = False, bitT = False, sort = False):
 
 
  if sort is True:
 
  for hit in numBlast_sort(blast, numHits, evalueT, bitT):
 
  yield hit
 
  return
 
  header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',
 
  'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']
 
  yield header
 
  prev, hits = None, []
 
  for line in blast:
 
  line = line.strip().split('\t')
 
  ID = line[0]
 
  line[10], line[11] = float(line[10]), float(line[11])
 
  evalue, bit = line[10], line[11]
 
  if ID != prev:
 
  if len(hits) > 0:
 
  # column is 1 + line index
 
  for hit in top_hits(hits, numHits, 11, True):
 
  yield hit
 
  hits = []
 
  if evalueT == False and bitT == False:
 
  hits.append(line)
 
  elif evalue <= evalueT and bitT == False:
 
  hits.append(line)
 
  elif evalue <= evalueT and bit >= bitT:
 
  hits.append(line)
 
  elif evalueT == False and bit >= bitT:
 
  hits.append(line)
 
  prev = ID
 
  for hit in top_hits(hits, numHits, 11, True):
 
  yield hit",parse b6 output,"This function takes in a blast file, the number of hits to retrieve, and optional thresholds for evalue and bitscores. It generates a sorted list of hits based on the specified criteria and yields each hit one by one.

If the 'sort' parameter is set to True, the function uses the 'numBlast_sort' function to sort the hits. Otherwise, it proceeds with the default order.

The function starts by defining a header with column names and yields it as the first output.

Then, it iterates over each line in the blast file. Each line is stripped and split into a list of values. The ID of the query is extracted from the first element of the list.

The evalue and bit values are converted to floats from their original string format.

Next, the function checks if the ID of the current line is different from the previous line. If so, it means that a new query is being processed. In this case, it checks if any hits were stored in the 'hits' list. If there are hits present, it passes them to the 'top_hits' function along with the number of hits to retrieve, the index of the evalue column (11), and a flag indicating that the list should be sorted in descending order. The function then yields each hit one by one.

After processing the hits, the 'hits' list is reset to an empty list.

The function then checks the specified thresholds for evalue and bitscores. If both thresholds are set to False (i.e., not provided), it adds the current line as a hit to the 'hits' list.

If the evalue threshold is provided and the current line's evalue is less than or equal to the threshold, it adds the line as a hit.

If both the evalue and bit thresholds are provided, it adds the line as a hit only if the evalue is less than or equal to the evalue threshold and the bit value is greater than or equal to the bit threshold.

Finally, if the evalue threshold is not provided but the bit threshold is provided, it adds the line as a hit only if the bit value is greater than or equal to the bit threshold.

After processing all the lines, the function passes the remaining hits in the 'hits' list to the 'top_hits' function and yields each hit one by one.

In summary, this function reads a blast file, filters the hits based on the specified thresholds, and yields the filtered hits in the desired order."
"def numDomtblout(domtblout, numHits, evalueT, bitT, sort):
  if sort is True:
 
  for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):
 
  yield hit
 
  return
 
  header = ['#target name', 'target accession', 'tlen',
 
  'query name', 'query accession', 'qlen',
 
  'full E-value', 'full score', 'full bias',
 
  'domain #', '# domains',
 
  'domain c-Evalue', 'domain i-Evalue', 'domain score', 'domain bias',
 
  'hmm from', 'hmm to', 'seq from', 'seq to', 'env from', 'env to',
 
  'acc', 'target description']
 
  yield header
 
  prev, hits = None, []
 
  for line in domtblout:
 
  if line.startswith('#'):
 
  continue
 
  # parse line and get description
 
  line = line.strip().split()
 
  desc = ' '.join(line[18:])
 
  line = line[0:18]
 
  line.append(desc)
 
  # create ID based on query name and domain number
 
  ID = line[0] + line[9]
 
  # domain c-Evalue and domain score thresholds
 
  line[11], line[13] = float(line[11]), float(line[13])
 
  evalue, bitscore = line[11], line[13]
 
  line[11], line[13] = evalue, bitscore
 
  if ID != prev:
 
  if len(hits) > 0:
 
  for hit in top_hits(hits, numHits, 13, True):
 
  yield hit
 
  hits = []
 
  if evalueT == False and bitT == False:
 
  hits.append(line)
 
  elif evalue <= evalueT and bitT == False:
 
  hits.append(line)
 
  elif evalue <= evalueT and bit >= bitT:
 
  hits.append(line)
 
  elif evalueT == False and bit >= bitT:
 
  hits.append(line)
 
  prev = ID
 
  for hit in top_hits(hits, numHits, 13, True):
 
  yield hit","parse hmm domain table output
 
  this version is faster but does not work unless the table is sorted","The function `numDomtblout` takes in a file `domtblout`, the number of hits to include (`numHits`), an E-value threshold (`evalueT`), a bitscore threshold (`bitT`), and a flag for sorting the output (`sort`). 

If `sort` is True, the function sorts the hits in the `domtblout` file. 

The function iterates through the sorted or unsorted hits in the `domtblout` file and yields each hit. 

The function starts by defining a header for the output and yields it. 

It then goes through each line in the `domtblout` file. If a line starts with a '#' symbol, it is skipped. 

The line is then split into a list of values. The description of the hit from index 18 onwards is joined back together and added to the list. 

The ID for the hit is created using the target name and domain number. 

The thresholds for the domain c-Evalue and domain score are converted to floats. 

If the current ID is not the same as the previous ID, the function checks if there are any hits in the list `hits`. If there are, it yields the top hits based on the number of hits to include (`numHits`) and the domain score. 

The `hits` list is then reset. 

If there are no threshold values set, or if the current hit meets the threshold conditions, it is added to the `hits` list. 

Finally, the function yields the top hits based on the number of hits to include (`numHits`) and the domain score."
"def stock2fa(stock):
  
  seqs = {}
 
  for line in stock:
 
  if line.startswith('#') is False and line.startswith(' ') is False and len(line) > 3:
 
  id, seq = line.strip().split()
 
  id = id.rsplit('/', 1)[0]
 
  id = re.split('[0-9]\|', id, 1)[-1]
 
  if id not in seqs:
 
  seqs[id] = []
 
  seqs[id].append(seq)
 
  if line.startswith('//'):
 
  break
 
  return seqs",convert stockholm to fasta,"This code defines a function called stock2fa that takes in a list called stock as input. The function performs the following steps:

1. It initializes an empty dictionary called seqs that will be used to store the sequences.
2. It iterates over each line in the stock list.
3. If a line does not start with '#' and does not start with a space, and if the length of the line is greater than 3, the code proceeds.
4. It splits the line into two parts using whitespace and assigns the first part to the variable id and the second part to the variable seq.
5. It removes the last part of the id if it is separated by a forward slash ""/"" using rsplit.
6. It removes any numeric followed by a ""|"" in the id using a regular expression.
7. If the id is not already present in the seqs dictionary, it adds it as a key and initializes an empty list as its value.
8. It appends the seq to the list associated with the id in the seqs dictionary.
9. If a line starts with '//', indicating the end of the data, the loop breaks.
10. Finally, it returns the seqs dictionary containing the ids as keys and their respective sequences as values."
"def week_schedule(index, on_time=None, off_time=None, off_days=None):
 
  if on_time is None:
 
  on_time = '9:00'
 
  if off_time is None:
 
  off_time = '17:00'
 
  if off_days is None:
 
  off_days = ['Sunday', 'Monday']
 
  if not isinstance(on_time, datetime.time):
 
  on_time = pd.to_datetime(on_time, format='%H:%M').time()
 
  if not isinstance(off_time, datetime.time):
 
  off_time = pd.to_datetime(off_time, format='%H:%M').time()
 
  times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))
 
  return pd.Series(times, index=index)","Return boolean time series following given week schedule.
 
 
 
  Parameters
 
  ----------
 
  index : pandas.DatetimeIndex
 
  Datetime index
 
  on_time : str or datetime.time
 
  Daily opening time. Default: '09:00'
 
  off_time : str or datetime.time
 
  Daily closing time. Default: '17:00'
 
  off_days : list of str
 
  List of weekdays. Default: ['Sunday', 'Monday']
 
 
 
  Returns
 
  -------
 
  pandas.Series of bool
 
  True when on, False otherwise for given datetime index
 
 
 
  Examples
 
  --------
 
  >>> import pandas as pd
 
  >>> from opengrid.library.utils import week_schedule
 
  >>> index = pd.date_range('20170701', '20170710', freq='H')
 
  >>> week_schedule(index)","The `week_schedule` function is used to generate a schedule based on a specified time range and days off.

The function takes in four parameters:
- `index`: A pandas DataFrame index representing the time range for the schedule.
- `on_time`: A string representing the starting time of work. If not provided, the default value is set to '9:00'.
- `off_time`: A string representing the ending time of work. If not provided, the default value is set to '17:00'.
- `off_days`: A list of strings representing the days off. If not provided, the default value is set to ['Sunday', 'Monday'].

The function first checks if the `on_time`, `off_time`, and `off_days` variables are None. If any of them are None, the default values will be used.

Next, the function checks if the `on_time` and `off_time` variables are not already in the format `datetime.time`. If they are not, the function converts them to `datetime.time` format using the `pd.to_datetime` function.

Then, the function creates a boolean Series called `times` with the same length as the input index. The Series represents whether each time in the index falls within the specified time range and is not on any of the specified off days. This is done using logical comparisons with the `on_time`, `off_time`, and `off_days` variables.

Finally, the function returns the boolean Series as a pandas Series object, with the same index as the input index. This Series can be used to filter the original data based on the generated schedule."
"def carpet(timeseries, **kwargs):
  
 
 
  # define optional input parameters
 
  cmap = kwargs.pop('cmap', cm.coolwarm)
 
  norm = kwargs.pop('norm', LogNorm())
 
  interpolation = kwargs.pop('interpolation', 'nearest')
 
  cblabel = kwargs.pop('zlabel', timeseries.name if timeseries.name else '')
 
  title = kwargs.pop('title', 'carpet plot: ' + timeseries.name if timeseries.name else '')
 
 
 
  # data preparation
 
  if timeseries.dropna().empty:
 
  print('skipped {} - no data'.format(title))
 
  return
 
  ts = timeseries.resample('15min').interpolate()
 
  vmin = max(0.1, kwargs.pop('vmin', ts[ts > 0].min()))
 
  vmax = max(vmin, kwargs.pop('vmax', ts.quantile(.999)))
 
 
 
  # convert to dataframe with date as index and time as columns by
 
  # first replacing the index by a MultiIndex
 
  mpldatetimes = date2num(ts.index.to_pydatetime())
 
  ts.index = pd.MultiIndex.from_arrays(
 
  [np.floor(mpldatetimes), 2 + mpldatetimes % 1]) # '2 +': matplotlib bug workaround.
 
  # and then unstacking the second index level to columns
 
  df = ts.unstack()
 
 
 
  # data plotting
 
 
 
  fig, ax = plt.subplots()
 
  # define the extent of the axes (remark the +- 0.5 for the y axis in order to obtain aligned date ticks)
 
  extent = [df.columns[0], df.columns[-1], df.index[-1] + 0.5, df.index[0] - 0.5]
 
  im = plt.imshow(df, vmin=vmin, vmax=vmax, extent=extent, cmap=cmap, aspect='auto', norm=norm,
 
  interpolation=interpolation, **kwargs)
 
 
 
  # figure formatting
 
 
 
  # x axis
 
  ax.xaxis_date()
 
  ax.xaxis.set_major_locator(HourLocator(interval=2))
 
  ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))
 
  ax.xaxis.grid(True)
 
  plt.xlabel('UTC Time')
 
 
 
  # y axis
 
  ax.yaxis_date()
 
  dmin, dmax = ax.yaxis.get_data_interval()
 
  number_of_days = (num2date(dmax) - num2date(dmin)).days
 
  # AutoDateLocator is not suited in case few data is available
 
  if abs(number_of_days) <= 35:
 
  ax.yaxis.set_major_locator(DayLocator())
 
  else:
 
  ax.yaxis.set_major_locator(AutoDateLocator())
 
  ax.yaxis.set_major_formatter(DateFormatter(""%a, %d %b %Y""))
 
 
 
  # plot colorbar
 
  cbticks = np.logspace(np.log10(vmin), np.log10(vmax), 11, endpoint=True)
 
  cb = plt.colorbar(format='%.0f', ticks=cbticks)
 
  cb.set_label(cblabel)
 
 
 
  # plot title
 
  plt.title(title)
 
 
 
  return im","Draw a carpet plot of a pandas timeseries.
 
 
 
  The carpet plot reads like a letter. Every day one line is added to the
 
  bottom of the figure, minute for minute moving from left (morning) to right
 
  (evening).
 
  The color denotes the level of consumption and is scaled logarithmically.
 
  If vmin and vmax are not provided as inputs, the minimum and maximum of the
 
  colorbar represent the minimum and maximum of the (resampled) timeseries.
 
 
 
  Parameters
 
  ----------
 
  timeseries : pandas.Series
 
  vmin, vmax : If not None, either or both of these values determine the range
 
  of the z axis. If None, the range is given by the minimum and/or maximum
 
  of the (resampled) timeseries.
 
  zlabel, title : If not None, these determine the labels of z axis and/or
 
  title. If None, the name of the timeseries is used if defined.
 
  cmap : matplotlib.cm instance, default coolwarm
 
 
 
  Examples
 
  --------
 
  >>> import numpy as np
 
  >>> import pandas as pd
 
  >>> from opengrid.library import plotting
 
  >>> plt = plotting.plot_style()
 
  >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')
 
  >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')
 
  >>> im = plotting.carpet(ser)","The ""carpet"" function is used to create a carpet plot for a given time series data.
The function takes a time series as input and optional parameters like colormap, normalization, interpolation, label for colorbar, and plot title.

First, the function checks if the input time series has any valid data. If it is empty, the function will skip further processing and return.

Next, the function resamples the time series to 15-minute intervals and performs linear interpolation to fill any missing values.

Then, the function determines the minimum and maximum values for the color mapping. The minimum value is set to either 0.1 or the minimum value in the time series (whichever is higher), and the maximum value is set to the 99.9th percentile of the time series values.

The time series data is converted into a dataframe, where the date is used as the index and the time is used as the columns. This is done by replacing the original index with a MultiIndex, where the first level contains the floor of the date as days and the second level contains the fractional part of the date as time. Finally, the second level of the MultiIndex is unstacked to convert it into columns.

Now, the data is ready for plotting. The function creates a new figure and axes using matplotlib and defines the extent of the axes. The extent is determined by the first and last columns of the dataframe for the x-axis and the last and first index values for the y-axis.

The time series data is displayed as an image using the 'imshow' function, with the colormap, normalization, and interpolation specified as parameters.

The x-axis is formatted to show UTC time in hours and minutes, with a grid displayed. The y-axis is formatted to show dates in the format ""day of week, day month year"". The spacing of the y-axis ticks depends on the number of days in the time series.

A colorbar is added to the plot, with the tick values ranging from the minimum to maximum value of the time series, divided logarithmically into 11 ticks. The label of the colorbar is set to the provided zlabel or the name of the time series.

Finally, the title of the plot is set to the provided title or a default title using the name of the time series.

The function returns the plotted image."
"def calc_pident_ignore_gaps(a, b):
 
  m = 0 # matches
 
  mm = 0 # mismatches
 
  for A, B in zip(list(a), list(b)):
 
  if A == '-' or A == '.' or B == '-' or B == '.':
 
  continue
 
  if A == B:
 
  m += 1
 
  else:
 
  mm += 1
 
  try:
 
  return float(float(m)/float((m + mm))) * 100
 
  except:
 
  return 0",calculate percent identity,"This code defines a function called calc_pident_ignore_gaps that calculates the percentage identity (pident) between two sequences, ignoring any '-' or '.' characters. The function takes in two sequences, a and b, as input.

The function initializes two variables, m and mm, to keep track of the number of matches and mismatches between the sequences, respectively.

The function then iterates through the characters of the sequences using the zip function. It checks if either character is a '-' or '.', and if so, it skips that position and continues to the next one.

If the characters at the current position are not '-' or '.', the function compares them. If they are equal, it increments the matches variable, m, by 1. Otherwise, it increments the mismatches variable, mm, by 1.

Finally, the function calculates the percentage identity by dividing the total number of matches (m) by the sum of matches and mismatches (m + mm), and then multiplying by 100 to get the percentage. This value is returned by the function.

If there is any error in the calculation, such as dividing by zero, the function returns 0."
"def foo(A, B):
 
  a_seq, b_seq = [], []
 
  for a, b in zip(list(A), list(B)):
 
  if a == '-' or a == '.' or b == '-' or b == '.':
 
  continue
 
  a_seq.append(a)
 
  b_seq.append(b)
 
  return ''.join(a_seq), ''.join(b_seq)",skip column if either is a gap,"This code defines a function named `foo` that takes two input arguments, `A` and `B`. The purpose of this function is to process the input sequences `A` and `B` and return modified versions of the sequences.

Inside the function, two empty lists, `a_seq` and `b_seq`, are initialized.

The `zip()` function is used to iterate over the elements of `A` and `B` simultaneously. This allows the code to process corresponding elements of both sequences together.

For each pair of elements `a` and `b`, the code checks if either `a` or `b` is equal to `'-'` or `'.'`. If any of these conditions is true, the code continues to the next iteration of the loop without performing any further actions.

If none of the conditions above are true, the code appends `a` to the `a_seq` list and `b` to the `b_seq` list.

Finally, the code joins the elements of `a_seq` into a single string and the elements of `b_seq` into another string using the `join()` method. These two strings are then returned as the output of the function."
"def foo(seqs):
  
  A, B, ignore_gaps = seqs
 
  a, b = A[1], B[1] # actual sequences
 
  if len(a) != len(b):
 
  print('# reads are not the same length', file=sys.stderr)
 
  exit()
 
  if ignore_gaps is True:
 
  pident = calc_pident_ignore_gaps(a, b)
 
  else:
 
  pident = calc_pident(a, b)
 
  return A[0], B[0], pident",compare pairs of sequences,"This code defines a function called ""foo"" that takes in three sequences as input: A, B, and ignore_gaps. 

The code first extracts the actual sequences from A and B and assigns them to variables a and b. 

Next, it checks if the lengths of the sequences are equal. If they are not equal, an error message is printed and the program exits. 

If the ignore_gaps flag is set to True, the code calculates the percentage identity between the sequences by calling the ""calc_pident_ignore_gaps"" function. If the flag is set to False, the code calculates the percentage identity using the ""calc_pident"" function. 

Finally, the code returns the names of sequences A and B, along with the calculated percentage identity."
"def foo(seqs):
 
  A, B, ignore_gaps = seqs
 
  a, b = remove_gaps(A[1], B[1]) # actual sequences
 
  if len(a) != len(b):
 
  print('# reads are not the same length', file=sys.stderr)
 
  exit()
 
  pident = lr(a, b) * 100
 
  return A[0], B[0], pident",calculate Levenshtein ratio of sequences,"This function, foo, takes in three arguments: seqs, A, B, and ignore_gaps. 

Seqs is a list containing the sequences A, B, and a boolean value ignore_gaps.

The function first extracts the actual sequences from A and B by calling the remove_gaps function on A[1] and B[1]. The remove_gaps function removes any gaps from the sequences.

Next, the function checks if the lengths of the extracted sequences are equal. If they are not equal, it prints an error message and exits the program.

If the lengths of the sequences are equal, the function calculates the percent identity (pident) between the two sequences using the lr function and multiplies it by 100.

Finally, the function returns the first elements of A and B (A[0] and B[0], respectively) along with the calculated pident."
"def foo(afa, leven, threads, print_list, ignore_gaps):
 
 
  # load sequences into dictionary
 
  seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index = True)}
 
  num_seqs = len(seqs)
 
  # define all pairs
 
  pairs = ((i[0], i[1], ignore_gaps) for i in itertools.combinations(list(seqs.values()), 2))
 
  pool = multithread(threads)
 
  # calc percent identity between all pairs - parallelize
 
  if leven is True:
 
  pident = pool.map(compare_seqs_leven, pairs)
 
  else:
 
  compare = pool.imap_unordered(compare_seqs, pairs)
 
  pident = [i for i in tqdm(compare, total = (num_seqs*num_seqs)/2)]
 
  pool.close()
 
  pool.terminate()
 
  pool.join()
 
  return to_dictionary(pident, print_list)",make pairwise sequence comparisons between aligned sequences,"This function, ""foo"", takes in several parameters including ""afa"" (a file path to a FASTA file), ""leven"" (a boolean indicating whether to use Levenshtein distance to calculate percent identity), ""threads"" (the number of threads to use for parallelization), ""print_list"" (a boolean indicating whether to print the results as a list), and ""ignore_gaps"" (a boolean indicating whether to ignore gaps when calculating percent identity).

The function first loads the sequences from the FASTA file into a dictionary, where the sequence names are the keys and the sequences are the values.

Next, it generates all pairs of sequences from the dictionary using the ""combinations"" function from the ""itertools"" module. Each pair is represented as a tuple containing the two sequences and the ""ignore_gaps"" parameter.

Then, it initializes a thread pool with the specified number of threads.

If the ""leven"" parameter is True, the function uses the ""compare_seqs_leven"" function to calculate the percent identity between each pair of sequences. This calculation is done in parallel using the thread pool, and the results are stored in the ""pident"" list.

If the ""leven"" parameter is False, the function uses the ""compare_seqs"" function to calculate the percent identity between each pair of sequences. This calculation is also done in parallel using the thread pool, and the results are stored in the ""pident"" list. The tqdm library is used to display a progress bar during the calculation.

After the percent identity calculations are complete, the thread pool is closed, terminated, and joined.

Finally, the function returns the results as a dictionary using the ""to_dictionary"" function, with the option to print the results as a list depending on the value of the ""print_list"" parameter."
"def print_pairwise(pw, median = False):
 
 
  names = sorted(set([i for i in pw]))
 
  if len(names) != 0:
 
  if '>' in names[0]:
 
  yield ['#'] + [i.split('>')[1] for i in names if '>' in i]
 
  else:
 
  yield ['#'] + names
 
  for a in names:
 
  if '>' in a:
 
  yield [a.split('>')[1]] + [pw[a][b] for b in names]
 
  else:
 
  out = []
 
  for b in names:
 
  if b in pw[a]:
 
  if median is False:
 
  out.append(max(pw[a][b]))
 
  else:
 
  out.append(np.median(pw[a][b]))
 
  else:
 
  out.append('-')
 
  yield [a] + out",print matrix of pidents to stdout,"This code defines a function called ""print_pairwise"" that takes a dictionary called ""pw"" as input. It also has an optional parameter called ""median"" which is set to False by default. 

The function first creates a list of unique names by extracting keys from the ""pw"" dictionary, and then sorts them in ascending order. 

If the length of the names list is not zero, the function continues execution. 

If any of the names in the names list contain the '>' character, a new list is generated with the '>' character removed from those names. This list is then yielded. 

Otherwise, the original names list is yielded. 

After that, the function enters a loop to iterate over the names. 

If a name contains the '>' character, a new list is generated by splitting the name at the '>' character and taking the second part. This new list is then concatenated with a list comprehension that iterates over the names and retrieves the corresponding value from the ""pw"" dictionary. This new list is then yielded. 

If a name does not contain the '>' character, an empty list called ""out"" is created. 

Another loop is used to iterate over the names again. 

If the current name is present as a key in the sub-dictionary associated with the current name in the outer loop, the function checks if the ""median"" parameter is set to False. If so, it appends the maximum value from the list of values associated with the current key in the ""pw"" dictionary to the ""out"" list. If the ""median"" parameter is True, it appends the median value calculated using numpy to the ""out"" list. 

If the current name is not present in the sub-dictionary, the function appends '-' to the ""out"" list. 

Finally, the function yields a new list consisting of the current name followed by the values in the ""out"" list."
"def print_comps(comps):
 
  if comps == []:
 
  print('n/a')
 
  else:
 
  print('# min: %s, max: %s, mean: %s' % \
 
  (min(comps), max(comps), np.mean(comps)))",print stats for comparisons,"This function accepts a list of numbers as an input parameter called ""comps"". It then checks if the list is empty. If the list is empty, it prints ""n/a"" to indicate that there are no values in the list.
If the list is not empty, it calculates the minimum value, maximum value, and the mean (average) of the numbers in the list using the built-in functions min(), max(), and np.mean() respectively. Finally, it prints a formatted string that displays the minimum, maximum, and mean values."
"def compare_clades(pw):
  names = sorted(set([i for i in pw]))
 
  for i in range(0, 4):
 
  wi, bt = {}, {}
 
  for a in names:
 
  for b in pw[a]:
 
  if ';' not in a or ';' not in b:
 
  continue
 
  pident = pw[a][b]
 
  cA, cB = a.split(';')[i], b.split(';')[i]
 
  if i == 0 and '_' in cA and '_' in cB:
 
  cA = cA.rsplit('_', 1)[1]
 
  cB = cB.rsplit('_', 1)[1]
 
  elif '>' in cA or '>' in cB:
 
  cA = cA.split('>')[1]
 
  cB = cB.split('>')[1]
 
  if cA == cB:
 
  if cA not in wi:
 
  wi[cA] = []
 
  wi[cA].append(pident)
 
  else:
 
  if cA not in bt:
 
  bt[cA] = {}
 
  if cB not in bt[cA]:
 
  bt[cA][cB] = []
 
  bt[cA][cB].append(pident)
 
  print('\n# min. within')
 
  for clade, pidents in list(wi.items()):
 
  print('\t'.join(['wi:%s' % str(i), clade, str(min(pidents))]))
 
  # print matrix of maximum between groups
 
  comps = []
 
  print('\n# max. between')
 
  for comp in print_pairwise(bt):
 
  if comp is not None:
 
  print('\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))
 
  if comp[0] != '#':
 
  comps.extend([j for j in comp[1:] if j != '-'])
 
  print_comps(comps)
 
  # print matrix of median between groups
 
  comps = []
 
  print('\n# median between')
 
  for comp in print_pairwise(bt, median = True):
 
  if comp is not None:
 
  print('\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))
 
  if comp[0] != '#':
 
  comps.extend([j for j in comp[1:] if j != '-'])
 
  print_comps(comps)",print min. pident within each clade and then matrix of between-clade max.,"The code is a function called ""compare_clades"" that takes a variable ""pw"" as input. The function performs a comparison between different clades based on sequence data.

First, the code creates a list of unique names by extracting all the keys from the ""pw"" dictionary and sorting them in alphabetical order.

The code then proceeds to iterate through four levels of clades (0 to 3) using a for loop.

Inside the loop, two empty dictionaries are created: ""wi"" and ""bt"". These dictionaries will store the results of pairwise comparisons between clades.

Next, the code iterates over each pair of names in the ""pw"" dictionary. If both names contain a "";"" character, the code continues with the comparison. Otherwise, it skips to the next pair.

The code extracts the sequence identity value (""pident"") associated with the pair of names.

It then splits each name at the "";"" character to extract the clade at the current level of iteration (0 to 3). If it is the first iteration (i=0), it checks if the clades contain underscores (""_""). If they do, it splits the clades at the rightmost underscore and keeps the part after it. If the clades contain "">"", it splits the clades at the "">"" character and keeps the part after it.

If the extracted clades are the same, the code checks which dictionary to store the sequence identity value in. If the clade is not already present in the ""wi"" dictionary, it adds it and stores the sequence identity value. If the clade is already present, it adds the sequence identity value to the existing list.

If the extracted clades are different, the code checks which dictionaries to store the sequence identity value in. If the source clade is not already present in the ""bt"" dictionary, it adds it along with an empty dictionary. If the target clade is not already present in the inner dictionary, it adds it along with an empty list. It then appends the sequence identity value to the list.

After the pairwise comparisons are completed, the code prints the results.

First, it prints the minimum sequence identity value within each clade group.

Then, it prints the maximum sequence identity value between clade groups. It does this by calling the ""print_pairwise"" function with the ""bt"" dictionary as input. The function returns a list of pairwise comparisons as strings, which the code iterates through. If the comparison is not None, the code prints it in a formatted way.

Finally, it prints a summary of the comparisons by calling the ""print_comps"" function with the list of comparisons as input.

The code then proceeds to print a matrix of median sequence identity values between clade groups. It does this in a similar manner as the maximum sequence identity values, but by calling the ""print_pairwise"" function with an additional argument ""median=True"".

Overall, the function performs pairwise comparisons between different clades based on sequence identity values and prints the results in a formatted way."
"def matrix2dictionary(matrix):
  
  pw = {}
 
  for line in matrix:
 
  line = line.strip().split('\t')
 
  if line[0].startswith('#'):
 
  names = line[1:]
 
  continue
 
  a = line[0]
 
  for i, pident in enumerate(line[1:]):
 
  b = names[i]
 
  if a not in pw:
 
  pw[a] = {}
 
  if b not in pw:
 
  pw[b] = {}
 
  if pident != '-':
 
  pident = float(pident)
 
  pw[a][b] = pident
 
  pw[b][a] = pident
 
  return pw",convert matrix to dictionary of comparisons,"This code takes in a matrix as input and converts it into a dictionary. The matrix represents pairwise similarities or relationships between different items or entities. Each row of the matrix contains a unique label followed by a list of similarity scores for that label with other labels. The code starts by initializing an empty dictionary called 'pw' to store the pairwise relationships. 

Next, it iterates through each line of the matrix. It strips any leading or trailing whitespaces and splits the line on tab ('\t') separator to separate the label from the similarity scores. 

If the line starts with a '#' symbol, it indicates that the current line contains the names of the labels. The code extracts these names and then continues to the next line.

For all other lines, it assigns the label to the variable 'a'. It then iterates over the similarity scores, assigning each score to the variable 'pident' and the corresponding label to the variable 'b'.

If the label 'a' is not already a key in the dictionary 'pw', a new empty dictionary is created and assigned to 'a' as the value. This dictionary will store the pairwise relationships for 'a'.

Similarly, if the label 'b' is not already a key in the dictionary 'pw', a new empty dictionary is created and assigned to 'b' as the value. This dictionary will store the pairwise relationships for 'b'.

If the similarity score 'pident' is not '-' (indicating a missing value), it is converted to a float and assigned to 'pident'. 

The similarity score 'pident' is then added to both dictionaries 'pw[a]' and 'pw[b]' with the label 'b' and 'a' as the key, respectively. This ensures that the pairwise relationship is recorded twice for both directions.

Finally, the resulting dictionary 'pw' is returned as the output of the function."
"def foo(parser, metadata=None):
 
  parser.add_argument('-v', action='version',
 
  version=__version__)
 
  subparsers = parser.add_subparsers(help='sub commands help')
 
  create_cmd = subparsers.add_parser('create')
 
  create_cmd.add_argument('name',
 
  help='Specify Python package name.')
 
  create_cmd.add_argument('-d', dest='description', action='store',
 
  help='Short description about your package.')
 
  create_cmd.add_argument('-a', dest='author', action='store',
 
  required=True,
 
  help='Python package author name.')
 
  create_cmd.add_argument('-e', dest='email', action='store',
 
  required=True,
 
  help='Python package author email address.')
 
  create_cmd.add_argument('-l', dest='license',
 
  choices=metadata.licenses().keys(),
 
  default='GPLv3+',
 
  help='Specify license. (default: %(default)s)')
 
  create_cmd.add_argument('-s', dest='status',
 
  choices=metadata.status().keys(),
 
  default='Alpha',
 
  help=('Specify development status. '
 
  '(default: %(default)s)'))
 
  create_cmd.add_argument('--no-check', action='store_true',
 
  help='No checking package name in PyPI.')
 
  create_cmd.add_argument('--with-samples', action='store_true',
 
  help='Generate package with sample code.')
 
  group = create_cmd.add_mutually_exclusive_group(required=True)
 
  group.add_argument('-U', dest='username', action='store',
 
  help='Specify GitHub username.')
 
  group.add_argument('-u', dest='url', action='store', type=valid_url,
 
  help='Python package homepage url.')
 
  create_cmd.add_argument('-o', dest='outdir', action='store',
 
  default=os.path.abspath(os.path.curdir),
 
  help='Specify output directory. (default: $PWD)')
 
  list_cmd = subparsers.add_parser('list')
 
  list_cmd.add_argument('-l', dest='licenses', action='store_true',
 
  help='show license choices.')",Set argument parser option.,"The code defines a function named ""foo"" that takes in a parser object and an optional metadata object as arguments. 

The function uses the parser object to add command line arguments and subcommands. 

The ""-v"" argument is used to display the version of the program. 

The ""create"" subcommand is added with the ""name"" argument, which specifies the name of the Python package. 

The optional ""-d"" argument allows for a short description of the package to be provided. 

The ""-a"" and ""-e"" arguments are used to specify the author name and email address respectively. They are both required. 

The ""-l"" argument is used to specify the license for the package. The available choices for the license are obtained from the metadata object. The default value is set to 'GPLv3+'. 

The ""-s"" argument is used to specify the development status of the package. The available choices for the status are obtained from the metadata object. The default value is set to 'Alpha'. 

The ""--no-check"" argument is a flag that, when provided, indicates that the package name should not be checked in the PyPI. 

The ""--with-samples"" argument is a flag that, when provided, indicates that the package should be generated with sample code. 

The ""-U"" and ""-u"" arguments are mutually exclusive options used to specify the GitHub username or the Python package homepage URL, respectively. One of them is required. 

The ""-o"" argument is used to specify the output directory where the package will be created. The default value is set to the current directory. 

The ""list"" subcommand is added with the ""-l"" argument, which when provided, displays the available license choices."
"def parse_options(metadata):
  parser = argparse.ArgumentParser(description='%(prog)s usage:',
 
  prog=__prog__)
 
  setoption(parser, metadata=metadata)
 
  return parser",Parse argument options.,"This code defines a function called `parse_options` that accepts a `metadata` argument. Inside the function, it creates an argument parser using the `argparse` module, with a description of the program's usage and the program name.

The `setoption` function is called with the parser object and the `metadata` argument. However, the implementation of the `setoption` function is not provided in the code snippet, so it's unclear what it does.

Finally, the function returns the created parser object. It seems that the purpose of this code is to create an argument parser with some options defined by the `metadata` argument, but without the implementation of `setoption`, it's difficult to determine the exact functionality."
"def main():
  
  try:
 
  pkg_version = Update()
 
  if pkg_version.updatable():
 
  pkg_version.show_message()
 
  metadata = control.retreive_metadata()
 
  parser = parse_options(metadata)
 
  argvs = sys.argv
 
  if len(argvs) <= 1:
 
  parser.print_help()
 
  sys.exit(1)
 
  args = parser.parse_args()
 
  control.print_licences(args, metadata)
 
  control.check_repository_existence(args)
 
  control.check_package_existence(args)
 
  control.generate_package(args)
 
  except (RuntimeError, BackendFailure, Conflict) as exc:
 
  sys.stderr.write('{0}\n'.format(exc))
 
  sys.exit(1)",Execute main processes.,"The given code is a Python script that performs a series of operations related to package management and updates. Here is a breakdown of what the code does:

1. First, it defines a function called ""main"".

2. Inside the ""main"" function, there is a try-except block, indicating that any errors or exceptions that occur during the execution of the code will be caught and handled in the except block.

3. It creates an instance of the ""Update"" class and assigns it to the variable ""pkg_version"".

4. It checks if the package version is updatable by calling the ""updatable"" method of the ""pkg_version"" object. If the package is updatable, it proceeds to the next step.

5. It calls the ""show_message"" method of the ""pkg_version"" object to display a message related to the update.

6. It retrieves metadata using the ""retreive_metadata"" function from a ""control"" module.

7. It calls the ""parse_options"" function, passing the retrieved metadata, to create a parser object.

8. It gets the command line arguments using the ""sys.argv"" list.

9. If the number of command line arguments is less than or equal to 1, it prints the help message using the ""print_help"" method of the parser object and exits the program.

10. It parses the command line arguments using the ""parse_args"" method of the parser object and assigns the resulting values to the ""args"" variable.

11. It calls the ""print_licenses"" function from the ""control"" module, passing the ""args"" and ""metadata"" variables, to print the licenses related to the package.

12. It checks the existence of a repository by calling the ""check_repository_existence"" function from the ""control"" module, passing the ""args"" variable.

13. It checks the existence of a package by calling the ""check_package_existence"" function from the ""control"" module, passing the ""args"" variable.

14. It generates a package by calling the ""generate_package"" function from the ""control"" module, passing the ""args"" variable.

15. If any exceptions (RuntimeError, BackendFailure, Conflict) occur during the execution of the code, it writes the exception message to the standard error stream (stderr) and exits the program with a status code of 1.

Overall, this code performs multiple tasks related to package management and updates, including checking for updates, displaying messages, retrieving metadata, parsing command line arguments, printing licenses, checking repository and package existence, and generating a package."
"def _check_or_set_default_params(self):
 
  if not hasattr(self, 'date'):
 
  self._set_param('date', datetime.utcnow().strftime('%Y-%m-%d'))
 
  if not hasattr(self, 'version'):
 
  self._set_param('version', self.default_version)
 
  # pylint: disable=no-member
 
  if not hasattr(self, 'description') or self.description is None:
 
  getattr(self, '_set_param')('description', self.warning_message)",Check key and set default vaule when it does not exists.,"This code checks if certain parameters are set and if not, it sets default values for them. The code first checks if the 'date' parameter is set. If it is not set, it sets the 'date' parameter to the current UTC date in the format 'YYYY-MM-DD'. 

Next, the code checks if the 'version' parameter is set. If it is not set, it sets the 'version' parameter to a default version value.

Then, the code checks if the 'description' parameter is set or if it is set to None. If it is not set or is set to None, it sets the 'description' parameter to a default warning message.

Overall, this code ensures that certain parameters have default values if they are not already set."
"def foo(self):
 
  if not os.path.isdir(self.outdir):
 
  os.makedirs(self.outdir)
 
  shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))",Move directory from working directory to output directory.,"'''
This function is used to move a file from a temporary directory to an output directory. 
First, it checks if the output directory exists. If it doesn't, it creates the directory using os.makedirs(). 
Then, it moves the file from the temporary directory to the output directory using shutil.move(). 
The file is moved with its original name, which is obtained from the 'name' attribute. 
'''"
"def foo(self):
  
  VCS(os.path.join(self.outdir, self.name), self.pkg_data)",Initialize VCS repository.,"This code defines a function named 'foo' which takes self as a parameter. It then calls another function 'VCS' with two arguments. The first argument is the join of self.outdir and self.name, which creates a path for a directory. The second argument is self.pkg_data, which is some data related to the package. This code seems to be setting up a version control system (VCS) for a specific package by providing the path and package data as arguments."
"def find_steam_location(): 
  if registry is None:
 
  return None
 
 
 
  key = registry.CreateKey(registry.HKEY_CURRENT_USER,""Software\Valve\Steam"")
 
  return registry.QueryValueEx(key,""SteamPath"")[0]","Finds the location of the current Steam installation on Windows machines.
 
  Returns None for any non-Windows machines, or for Windows machines where
 
  Steam is not installed.","""""""
This function finds the location of the Steam application on the user's computer. It uses the Windows registry to search for the path to the Steam application. The function first checks if the registry is available. If it is not, it returns None. Next, it creates a key in the registry for the Steam application using the HKEY_CURRENT_USER registry hive. Then, it retrieves the value associated with the ""SteamPath"" key. Finally, it returns the Steam application path as a string. If the Steam application is not found in the registry, it also returns None.
"""""""
"def plot_PCoA(cat_data, otu_name, unifrac, names, colors, xr, yr, outDir,
 
  save_as, plot_style):
 
 
  fig = plt.figure(figsize=(14, 8))
 
  ax = fig.add_subplot(111)
 
 
 
  for i, cat in enumerate(cat_data):
 
  plt.scatter(cat_data[cat][""pc1""], cat_data[cat][""pc2""], cat_data[cat][""size""],
 
  color=colors[cat], alpha=0.85, marker=""o"", edgecolor=""black"",
 
  label=cat)
 
  lgnd = plt.legend(loc=""best"", scatterpoints=3, fontsize=13)
 
  for i in range(len(colors.keys())):
 
  lgnd.legendHandles[i]._sizes = [80] # Change the legend marker size manually
 
  plt.title("" "".join(otu_name.split(""_"")), style=""italic"")
 
  plt.ylabel(""PC2 (Percent Explained Variance {:.3f}%)"".format(float(unifrac[""varexp""][1])))
 
  plt.xlabel(""PC1 (Percent Explained Variance {:.3f}%)"".format(float(unifrac[""varexp""][0])))
 
  plt.xlim(round(xr[0]*1.5, 1), round(xr[1]*1.5, 1))
 
  plt.ylim(round(yr[0]*1.5, 1), round(yr[1]*1.5, 1))
 
  if plot_style:
 
  gu.ggplot2_style(ax)
 
  fc = ""0.8""
 
  else:
 
  fc = ""none""
 
  fig.savefig(os.path.join(outDir, ""_"".join(otu_name.split())) + ""."" + save_as,
 
  facecolor=fc, edgecolor=""none"", format=save_as,
 
  bbox_inches=""tight"", pad_inches=0.2)
 
  plt.close(fig)","Plot PCoA principal coordinates scaled by the relative abundances of
 
  otu_name.","This code is a function called ""plot_PCoA"" that is used to plot Principal Coordinates Analysis (PCoA) results. PCoA is a technique used to visualize the similarity and dissimilarity of samples based on a distance matrix. 

The function takes several parameters: ""cat_data"" is a dictionary containing the PCoA data for each category, ""otu_name"" is the name of the OTU (Operational Taxonomic Unit) being plotted, ""unifrac"" is a dictionary containing the explained variance of the PCoA, ""names"" is a list of names for the different categories, ""colors"" is a dictionary mapping each category to a color, ""xr"" and ""yr"" are tuples specifying the x and y axis ranges, ""outDir"" is the output directory where the plot will be saved, ""save_as"" is the file format to save the plot, and ""plot_style"" is a boolean flag specifying whether to use a custom plot style.

The function first creates a figure and subplot for the plot. It then iterates over each category in the ""cat_data"" dictionary and plots the PCoA data points for that category. The size, color, and label of the data points are specified based on the category. The plot also includes a legend.

The function sets the title of the plot to the name of the OTU. It sets the x-axis label to the explained variance of PC1 and the y-axis label to the explained variance of PC2. The x and y-axis limits are set based on the specified ranges.

If the ""plot_style"" flag is true, a custom plot style is applied to the plot using the ""gu"" module. Otherwise, the plot style remains unchanged.

Finally, the plot is saved in the specified output directory with the OTU name as the file name. The file format is determined by the ""save_as"" parameter. The plot is saved with a white background and no border, and the figure is closed.

In summary, this function takes PCoA data for different categories, plots the data points on a 2D plot with custom colors and legends, and saves the plot with the specified parameters."
"def split_by_category(biom_cols, mapping, category_id):
  
  columns = defaultdict(list)
 
  for i, col in enumerate(biom_cols):
 
  columns[mapping[col['id']][category_id]].append((i, col))
 
 
 
  return columns",Split up the column data in a biom table by mapping category value.,"This function takes in three parameters: `biom_cols`, `mapping`, and `category_id`. 

It creates an empty dictionary called `columns` to store the columns of the `biom_cols` list based on their category. 

It then iterates over each column in the `biom_cols` list and retrieves the category_id from the `mapping` dictionary using the column's id. It appends the column and its corresponding index to the list of columns associated with that category in the `columns` dictionary.

Finally, it returns the `columns` dictionary, which contains the columns of `biom_cols` grouped by their category."
"def print_line(l):
 
  print_lines = ['# STOCKHOLM', '#=GF', '#=GS', ' ']
 
  if len(l.split()) == 0:
 
  return True
 
  for start in print_lines:
 
  if l.startswith(start):
 
  return True
 
  return False",print line if starts with ...,"This code defines a function called `print_line` that takes a string `l` as input. The purpose of the function is to determine whether a given line should be printed or not based on certain conditions.

The function starts by defining a list called `print_lines` which contains four strings: ""# STOCKHOLM"", ""#=GF"", ""#=GS"", and a space character. These strings represent the starting patterns of lines that should be printed.

The code then checks if the given line `l` is empty by checking if the result of splitting `l` into words is zero. If `l` is empty, the function returns `True`, indicating that the line should be printed.

Next, the code iterates through the `print_lines` list and checks if the given line `l` starts with any of the strings in the list. If `l` starts with any of these patterns, the function returns `True`, indicating that the line should be printed.

If the code reaches this point, it means that the given line `l` does not meet any of the conditions for printing, so the function returns `False`, indicating that the line should not be printed."
"def stock2one(stock):
  
  lines = {}
 
  for line in stock:
 
  line = line.strip()
 
  if print_line(line) is True:
 
  yield line
 
  continue
 
  if line.startswith('//'):
 
  continue
 
  ID, seq = line.rsplit(' ', 1)
 
  if ID not in lines:
 
  lines[ID] = ''
 
  else:
 
  # remove preceding white space
 
  seq = seq.strip()
 
  lines[ID] += seq
 
  for ID, line in lines.items():
 
  yield '\t'.join([ID, line])
 
  yield '\n//'",convert stockholm to single line format,"This code implements a function named `stock2one` that takes in a `stock` parameter. The function is designed to process and manipulate a stock of items.

The code starts by declaring an empty dictionary called `lines` which will be used to store the lines of the stock. 

Next, there is a loop that iterates over each line in the `stock` parameter. Each line is stripped of any leading or trailing whitespace.

There is a condition that checks if a function call `print_line(line)` returns `True`. If it does, the line is yielded and the loop continues to the next iteration.

There is another condition that checks if the line starts with the characters `//`. If it does, it skips the current iteration of the loop.

The code then uses the `rsplit()` method to split the line into two parts - the `ID` and the `seq`. The split is done at the last occurrence of a space character. 

If the `ID` is not already in the `lines` dictionary, it adds an empty string as its value. Otherwise, it proceeds to remove any leading or trailing whitespace from the `seq` variable and appends it to the existing value of the `ID` key in the `lines` dictionary.

After processing all the lines, the code enters another loop that iterates over each `ID` and `line` pair in the `lines` dictionary. For each pair, it yields a string that is the concatenation of the `ID`, a tab character, and the `line`.

Finally, the code yields a string consisting of a newline character followed by two slashes ('//'). This signifies the end of the output.

In summary, the `stock2one` function takes in a stock parameter, processes and manipulates the data, and yields the results in a specific format."
"def math_func(f):
 
  """"""
 
  Statics the methods. wut.
 
  """"""
 
  @wraps(f)
 
  def wrapper(*args, **kwargs):
 
  if len(args) > 0:
 
  return_type = type(args[0])
 
  if kwargs.has_key('return_type'):
 
  return_type = kwargs['return_type']
 
  kwargs.pop('return_type')
 
  return return_type(f(*args, **kwargs))
 
  args = list((setify(x) for x in args))
 
  return return_type(f(*args, **kwargs))
 
  return wrapper",Statics the methods. wut.,"The `math_func` function is a decorator that can be applied to other functions. It takes a function `f` as an argument and returns a wrapped version of that function. 

The purpose of the `math_func` decorator is to provide some additional functionality to the decorated function. It adds the ability to specify a return type for the function and converts the output of the function into the specified return type.

The wrapped function takes any number of positional and keyword arguments (`*args` and `**kwargs`). It first checks if any positional arguments were passed. If there are any, it determines the return type based on the type of the first argument. If the `return_type` keyword argument is provided, it overrides the determined return type. The `return_type` keyword argument is then removed from the kwargs.

The wrapped function then calls the original function (`f`) with the modified arguments and kwargs. The result of the original function call is converted to the determined return type using the `return_type` variable. If no positional arguments were passed, the return type is not determined and an error will occur.

Finally, the wrapped function returns the converted result.

Overall, the `math_func` decorator adds a return type conversion functionality to the decorated function while preserving its original behavior."
