[
    {
        "docstring": "str->list\n    Convert XML to URL List.\n    From Biligrab.",
        "code": "def foo(xml_data):\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl"
    },
    {
        "docstring": "Downloads Dailymotion videos by URL.",
        "code": "def foo(url, output_dir='.', merge=True, info_only=False, **kwargs):\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)"
    },
    {
        "docstring": "Downloads Sina videos by URL.",
        "code": "def foo(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        \n        foo_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'\n            foo_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        foo_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)"
    },
    {
        "docstring": "Format text with color or other effects into ANSI escaped string.",
        "code": "def foo(text, *colors):\n    \n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text"
    },
    {
        "docstring": "Print a log message to standard error.",
        "code": "def foo(text, *colors):\n    \n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")"
    },
    {
        "docstring": "Detect operating system.",
        "code": "def foo():\n    \n\n    \n    \n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        \n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os"
    },
    {
        "docstring": "Splicing URLs according to video ID to get video details",
        "code": "def foo(video_id):\n    \n    \n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  \n            return url"
    },
    {
        "docstring": "Converts a string to a valid filename.",
        "code": "def foo(text, os=detect_os()):\n    \n\n    \n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    \n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        \n        text = text.translate({\n            \n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            \n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        \n        if os == 'mac':\n            \n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        \n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] \n    return text"
    },
    {
        "docstring": "Downloads CBS videos by URL.",
        "code": "def foo(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)"
    },
    {
        "docstring": "Override the original one\n        Ugly ugly dirty hack",
        "code": "def foo(self, **kwargs):\n        \n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                \n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                \n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                \n                stream_id = kwargs['stream_id']\n            else:\n                \n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            \n            \n            \n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')"
    },
    {
        "docstring": "Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.",
        "code": "def foo(text, patterns):\n    \n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret"
    },
    {
        "docstring": "Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.",
        "code": "def foo(url, param):\n    \n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None"
    },
    {
        "docstring": "Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
        "code": "def foo(url, headers={}, decoded=True):\n    \n\n    logging.debug('foo: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    \n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    \n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data"
    },
    {
        "docstring": "Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
        "code": "def foo(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \n    if kwargs.get('post_data_raw'):\n        logging.debug('foo: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('foo: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    \n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    \n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data"
    },
    {
        "docstring": "Parses host name and port number from a string.",
        "code": "def foo(host):\n    \n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)"
    },
    {
        "docstring": "Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor",
        "code": "def foo(self, task_instance):\n        \n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True"
    },
    {
        "docstring": "Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events",
        "code": "def foo(self, dag_ids=None):\n        \n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events"
    },
    {
        "docstring": "Returns a snowflake.connection object",
        "code": "def foo(self):\n        \n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn"
    },
    {
        "docstring": "returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements",
        "code": "def foo(self):\n        \n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key"
    },
    {
        "docstring": "Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.",
        "code": "def foo(self, field_name, default=None):\n        \n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default"
    },
    {
        "docstring": "Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.",
        "code": "def foo(self, sql, filename, open=open):\n        \n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()"
    },
    {
        "docstring": "Dumps a database table into a tab-delimited file",
        "code": "def foo(self, table, tmp_file):\n        \n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)"
    },
    {
        "docstring": "Uploads the file to Google cloud storage",
        "code": "def foo(self, context):\n        \n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )"
    },
    {
        "docstring": "Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'",
        "code": "def foo(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.foo(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)"
    },
    {
        "docstring": "Returns a mysql connection object",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            \n            \n            \n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn"
    },
    {
        "docstring": "Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success",
        "code": "def foo(args):\n    \n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())"
    },
    {
        "docstring": "Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client",
        "code": "def foo(self):\n        \n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client"
    },
    {
        "docstring": "Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.",
        "code": "def foo(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )"
    },
    {
        "docstring": "Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict",
        "code": "def foo(self, instance, project_id=None):\n        \n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, body, project_id=None):\n        \n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, body, instance, project_id=None):\n        \n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None",
        "code": "def foo(self, instance, project_id=None):\n        \n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict",
        "code": "def foo(self, instance, database, project_id=None):\n        \n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance, body, project_id=None):\n        \n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance, database, body, project_id=None):\n        \n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance, database, project_id=None):\n        \n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance, body, project_id=None):\n        \n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )"
    },
    {
        "docstring": "Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!",
        "code": "def foo(self):\n        \n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                \n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return"
    },
    {
        "docstring": "Stops running proxy.\n\n        You should stop the proxy after you stop using it.",
        "code": "def foo(self):\n        \n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        \n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            \n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            \n            os.remove(self.credentials_path)"
    },
    {
        "docstring": "Returns version of the Cloud SQL Proxy.",
        "code": "def foo(self):\n        \n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None"
    },
    {
        "docstring": "Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
        "code": "def foo(self, session=None):\n        \n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()"
    },
    {
        "docstring": "Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
        "code": "def foo(self, session=None):\n        \n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None"
    },
    {
        "docstring": "Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
        "code": "def foo(self, session=None):\n        \n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")"
    },
    {
        "docstring": "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner",
        "code": "def foo(self):\n        \n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )"
    },
    {
        "docstring": "Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.",
        "code": "def foo(self):\n        \n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook"
    },
    {
        "docstring": "Clean up database hook after it was used.",
        "code": "def foo(self):\n        \n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)"
    },
    {
        "docstring": "Reserve free TCP port to be used by Cloud SQL Proxy",
        "code": "def foo(self):\n        \n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]"
    },
    {
        "docstring": "Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.",
        "code": "def foo(job_id):\n    \n\n    \n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    \n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    \n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id"
    },
    {
        "docstring": "Extract error code from ftp exception",
        "code": "def foo(self, e):\n        \n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e"
    },
    {
        "docstring": "Remove any existing DAG runs for the perf test DAGs.",
        "code": "def foo():\n    \n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)"
    },
    {
        "docstring": "Remove any existing task instances for the perf test DAGs.",
        "code": "def foo():\n    \n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()"
    },
    {
        "docstring": "Toggle the pause state of the DAGs in the test.",
        "code": "def foo(is_paused):\n    \n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()"
    },
    {
        "docstring": "Print operational metrics for the scheduler test.",
        "code": "def foo(self):\n        \n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()"
    },
    {
        "docstring": "Override the scheduler heartbeat to determine when the test is complete",
        "code": "def foo(self):\n        \n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        \n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        \n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()"
    },
    {
        "docstring": "Invoke Lambda Function",
        "code": "def foo(self, payload):\n        \n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response"
    },
    {
        "docstring": "Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)",
        "code": "def foo(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \n\n    \n    \n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation"
    },
    {
        "docstring": "Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int",
        "code": "def foo(path, mode):\n    \n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)"
    },
    {
        "docstring": "A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str",
        "code": "def foo(s):\n    \n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret"
    },
    {
        "docstring": "Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone",
        "code": "def foo(value, timezone=None):\n    \n    if timezone is None:\n        timezone = TIMEZONE\n\n    \n    if is_localized(value):\n        raise ValueError(\n            \"foo expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        \n        \n        \n        \n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        \n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        \n        return timezone.convert(value)\n    else:\n        \n        return value.replace(tzinfo=timezone)"
    },
    {
        "docstring": "Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime",
        "code": "def foo(value, timezone=None):\n    \n    if timezone is None:\n        timezone = TIMEZONE\n\n    \n    if is_naive(value):\n        raise ValueError(\"foo() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    \n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive"
    },
    {
        "docstring": "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime",
        "code": "def foo(*args, **kwargs):\n    \n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.foo(*args, **kwargs)"
    },
    {
        "docstring": "Establish a connection to druid broker.",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn"
    },
    {
        "docstring": "Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict",
        "code": "def foo(self, headers=None):\n        \n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                \n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session"
    },
    {
        "docstring": "Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict",
        "code": "def foo(self, endpoint, data=None, headers=None, extra_options=None):\n        \n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            \n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            \n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            \n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)"
    },
    {
        "docstring": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response",
        "code": "def foo(self, response):\n        \n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)"
    },
    {
        "docstring": "Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict",
        "code": "def foo(self, session, prepped_request, extra_options):\n        \n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex"
    },
    {
        "docstring": "Contextmanager that will create and teardown a session.",
        "code": "def foo():\n    \n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()"
    },
    {
        "docstring": "Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.",
        "code": "def foo(func):\n    \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper"
    },
    {
        "docstring": "Clear out the database",
        "code": "def foo():\n    \n    from airflow import models\n\n    \n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()"
    },
    {
        "docstring": "Parses some DatabaseError to provide a better error message",
        "code": "def foo(e):\n        \n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)"
    },
    {
        "docstring": "Get a set of records from Presto",
        "code": "def foo(self, hql, parameters=None):\n        \n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))"
    },
    {
        "docstring": "Get a pandas dataframe from a sql query.",
        "code": "def foo(self, hql, parameters=None):\n        \n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df"
    },
    {
        "docstring": "Execute the statement against Presto. Can be used to create views.",
        "code": "def foo(self, hql, parameters=None):\n        \n        return super().run(self._strip_sql(hql), parameters)"
    },
    {
        "docstring": "A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings",
        "code": "def foo(self, table, rows, target_fields=None):\n        \n        super().insert_rows(table, rows, target_fields, 0)"
    },
    {
        "docstring": "Return a cosmos db client.",
        "code": "def foo(self):\n        \n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        \n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client"
    },
    {
        "docstring": "Checks if a collection exists in CosmosDB.",
        "code": "def foo(self, collection_name, database_name=None):\n        \n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True"
    },
    {
        "docstring": "Creates a new collection in the CosmosDB database.",
        "code": "def foo(self, collection_name, database_name=None):\n        \n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        \n        \n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        \n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})"
    },
    {
        "docstring": "Checks if a database exists in CosmosDB.",
        "code": "def foo(self, database_name):\n        \n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True"
    },
    {
        "docstring": "Creates a new database in CosmosDB.",
        "code": "def foo(self, database_name):\n        \n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        \n        \n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        \n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})"
    },
    {
        "docstring": "Deletes an existing database in CosmosDB.",
        "code": "def foo(self, database_name):\n        \n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))"
    },
    {
        "docstring": "Deletes an existing collection in the CosmosDB database.",
        "code": "def foo(self, collection_name, database_name=None):\n        \n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))"
    },
    {
        "docstring": "Insert a list of new documents into an existing collection in the CosmosDB database.",
        "code": "def foo(self, documents, database_name=None, collection_name=None):\n        \n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents"
    },
    {
        "docstring": "Delete an existing document out of a collection in the CosmosDB database.",
        "code": "def foo(self, document_id, database_name=None, collection_name=None):\n        \n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))"
    },
    {
        "docstring": "Get a document from an existing collection in the CosmosDB database.",
        "code": "def foo(self, document_id, database_name=None, collection_name=None):\n        \n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None"
    },
    {
        "docstring": "Get a list of documents from an existing collection in the CosmosDB database via SQL query.",
        "code": "def foo(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        \n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None"
    },
    {
        "docstring": "Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict",
        "code": "def foo(self, name):\n        \n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, location, body, project_id=None):\n        \n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)"
    },
    {
        "docstring": "Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None",
        "code": "def foo(self, name, body, update_mask):\n        \n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)"
    },
    {
        "docstring": "Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.",
        "code": "def foo(self, location, zip_path, project_id=None):\n        \n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                \n                \n                \n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url"
    },
    {
        "docstring": "Deletes the specified Cloud Function.\n\n        :param name: The name of the function.\n        :type name: str\n        :return: None",
        "code": "def foo(self, name):\n        \n        response = self.get_conn().projects().locations().functions().delete(\n            name=name).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)"
    },
    {
        "docstring": "Wrapper around the private _get_dep_statuses method that contains some global\n        checks for all dependencies.\n\n        :param ti: the task instance to get the dependency status for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: the context for which this dependency should be evaluated for\n        :type dep_context: DepContext",
        "code": "def foo(self, ti, session, dep_context=None):\n        \n        \n        from airflow.ti_deps.dep_context import DepContext\n\n        if dep_context is None:\n            dep_context = DepContext()\n\n        if self.IGNOREABLE and dep_context.ignore_all_deps:\n            yield self._passing_status(\n                reason=\"Context specified all dependencies should be ignored.\")\n            return\n\n        if self.IS_TASK_DEP and dep_context.ignore_task_deps:\n            yield self._passing_status(\n                reason=\"Context specified all task dependencies should be ignored.\")\n            return\n\n        for dep_status in self._get_dep_statuses(ti, session, dep_context):\n            yield dep_status"
    },
    {
        "docstring": "Returns whether or not this dependency is met for a given task instance. A\n        dependency is considered met if all of the dependency statuses it reports are\n        passing.\n\n        :param ti: the task instance to see if this dependency is met for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: The context this dependency is being checked under that stores\n            state that can be used by this dependency.\n        :type dep_context: BaseDepContext",
        "code": "def foo(self, ti, session, dep_context=None):\n        \n        return all(status.passed for status in\n                   self.get_dep_statuses(ti, session, dep_context))"
    },
    {
        "docstring": "Returns an iterable of strings that explain why this dependency wasn't met.\n\n        :param ti: the task instance to see if this dependency is met for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: The context this dependency is being checked under that stores\n            state that can be used by this dependency.\n        :type dep_context: BaseDepContext",
        "code": "def foo(self, ti, session, dep_context=None):\n        \n        for dep_status in self.get_dep_statuses(ti, session, dep_context):\n            if not dep_status.passed:\n                yield dep_status.reason"
    },
    {
        "docstring": "Parses a config file for s3 credentials. Can currently\n    parse boto, s3cmd.conf and AWS SDK config formats\n\n    :param config_file_name: path to the config file\n    :type config_file_name: str\n    :param config_format: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n        Defaults to \"boto\"\n    :type config_format: str\n    :param profile: profile name in AWS type config file\n    :type profile: str",
        "code": "def foo(config_file_name, config_format='boto', profile=None):\n    \n    config = configparser.ConfigParser()\n    if config.read(config_file_name):  \n        sections = config.sections()\n    else:\n        raise AirflowException(\"Couldn't read {0}\".format(config_file_name))\n    \n    if config_format is None:\n        config_format = 'boto'\n    conf_format = config_format.lower()\n    if conf_format == 'boto':  \n        if profile is not None and 'profile ' + profile in sections:\n            cred_section = 'profile ' + profile\n        else:\n            cred_section = 'Credentials'\n    elif conf_format == 'aws' and profile is not None:\n        cred_section = profile\n    else:\n        cred_section = 'default'\n    \n    if conf_format in ('boto', 'aws'):  \n        key_id_option = 'aws_access_key_id'\n        secret_key_option = 'aws_secret_access_key'\n        \n    else:\n        key_id_option = 'access_key'\n        secret_key_option = 'secret_key'\n    \n    if cred_section not in sections:\n        raise AirflowException(\"This config file format is not recognized\")\n    else:\n        try:\n            access_key = config.get(cred_section, key_id_option)\n            secret_key = config.get(cred_section, secret_key_option)\n        except Exception:\n            logging.warning(\"Option Error in parsing s3 config file\")\n            raise\n        return access_key, secret_key"
    },
    {
        "docstring": "Get the underlying `botocore.Credentials` object.\n\n        This contains the following authentication attributes: access_key, secret_key and token.",
        "code": "def foo(self, region_name=None):\n        \n        session, _ = self._get_credentials(region_name)\n        \n        \n        \n        return session.get_credentials().get_frozen_credentials()"
    },
    {
        "docstring": "Returns verticaql connection object",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.vertica_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or '',\n            \"database\": conn.schema,\n            \"host\": conn.host or 'localhost'\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 5433\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        conn = connect(**conn_config)\n        return conn"
    },
    {
        "docstring": "Ensure all logging output has been flushed",
        "code": "def foo(self):\n        \n        if len(self._buffer) > 0:\n            self.logger.log(self.level, self._buffer)\n            self._buffer = str()"
    },
    {
        "docstring": "If the path contains a folder with a .zip suffix, then\n    the folder is treated as a zip archive and path to zip is returned.",
        "code": "def foo(fileloc):\n    \n\n    _, archive, filename = re.search(\n        r'((.*\\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return archive\n    else:\n        return fileloc"
    },
    {
        "docstring": "Traverse a directory and look for Python files.\n\n    :param directory: the directory to traverse\n    :type directory: unicode\n    :param safe_mode: whether to use a heuristic to determine whether a file\n        contains Airflow DAG definitions\n    :return: a list of paths to Python files in the specified directory\n    :rtype: list[unicode]",
        "code": "def foo(directory, safe_mode=True,\n                       include_examples=None):\n    \n    if include_examples is None:\n        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')\n    file_paths = []\n    if directory is None:\n        return []\n    elif os.path.isfile(directory):\n        return [directory]\n    elif os.path.isdir(directory):\n        patterns_by_dir = {}\n        for root, dirs, files in os.walk(directory, followlinks=True):\n            patterns = patterns_by_dir.get(root, [])\n            ignore_file = os.path.join(root, '.airflowignore')\n            if os.path.isfile(ignore_file):\n                with open(ignore_file, 'r') as f:\n                    \n                    \n                    patterns += [re.compile(p) for p in f.read().split('\\n') if p]\n\n            \n            \n            \n            dirs[:] = [\n                d\n                for d in dirs\n                if not any(p.search(os.path.join(root, d)) for p in patterns)\n            ]\n\n            \n            \n            for d in dirs:\n                patterns_by_dir[os.path.join(root, d)] = patterns\n\n            for f in files:\n                try:\n                    file_path = os.path.join(root, f)\n                    if not os.path.isfile(file_path):\n                        continue\n                    mod_name, file_ext = os.path.splitext(\n                        os.path.split(file_path)[-1])\n                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):\n                        continue\n                    if any([re.findall(p, file_path) for p in patterns]):\n                        continue\n\n                    \n                    \n                    might_contain_dag = True\n                    if safe_mode and not zipfile.is_zipfile(file_path):\n                        with open(file_path, 'rb') as fp:\n                            content = fp.read()\n                            might_contain_dag = all(\n                                [s in content for s in (b'DAG', b'airflow')])\n\n                    if not might_contain_dag:\n                        continue\n\n                    file_paths.append(file_path)\n                except Exception:\n                    log = LoggingMixin().log\n                    log.exception(\"Error while examining %s\", f)\n    if include_examples:\n        import airflow.example_dags\n        example_dag_folder = airflow.example_dags.__path__[0]\n        file_paths.extend(foo(example_dag_folder, safe_mode, False))\n    return file_paths"
    },
    {
        "docstring": "Construct a TaskInstance from the database based on the primary key\n\n        :param session: DB session.\n        :param lock_for_update: if True, indicates that the database should\n            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n            session is committed.",
        "code": "def foo(self, session=None, lock_for_update=False):\n        \n        TI = airflow.models.TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self._dag_id,\n            TI.task_id == self._task_id,\n            TI.execution_date == self._execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        return ti"
    },
    {
        "docstring": "Launch DagFileProcessorManager processor and start DAG parsing loop in manager.",
        "code": "def foo(self):\n        \n        self._process = self._launch_process(self._dag_directory,\n                                             self._file_paths,\n                                             self._max_runs,\n                                             self._processor_factory,\n                                             self._child_signal_conn,\n                                             self._stat_queue,\n                                             self._result_queue,\n                                             self._async_mode)\n        self.log.info(\"Launched DagFileProcessorManager with pid: %s\", self._process.pid)"
    },
    {
        "docstring": "Send termination signal to DAG parsing processor manager\n        and expect it to terminate all DAG file processors.",
        "code": "def foo(self):\n        \n        self.log.info(\"Sending termination message to manager.\")\n        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)"
    },
    {
        "docstring": "Helper method to clean up DAG file processors to avoid leaving orphan processes.",
        "code": "def foo(self, signum, frame):\n        \n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        self.terminate()\n        self.end()\n        self.log.debug(\"Finished terminating DAG processors.\")\n        sys.exit(os.EX_OK)"
    },
    {
        "docstring": "Use multiple processes to parse and generate tasks for the\n        DAGs in parallel. By processing them in separate processes,\n        we can get parallelism and isolation from potentially harmful\n        user code.",
        "code": "def foo(self):\n        \n\n        self.log.info(\"Processing files using up to %s processes at a time \", self._parallelism)\n        self.log.info(\"Process each file at most once every %s seconds\", self._file_process_interval)\n        self.log.info(\n            \"Checking for new files in %s every %s seconds\", self._dag_directory, self.dag_dir_list_interval\n        )\n\n        if self._async_mode:\n            self.log.debug(\"Starting DagFileProcessorManager in async mode\")\n            self.start_in_async()\n        else:\n            self.log.debug(\"Starting DagFileProcessorManager in sync mode\")\n            self.start_in_sync()"
    },
    {
        "docstring": "Parse DAG files repeatedly in a standalone loop.",
        "code": "def foo(self):\n        \n        while True:\n            loop_start_time = time.time()\n\n            if self._signal_conn.poll():\n                agent_signal = self._signal_conn.recv()\n                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                    self.terminate()\n                    break\n                elif agent_signal == DagParsingSignal.END_MANAGER:\n                    self.end()\n                    sys.exit(os.EX_OK)\n\n            self._refresh_dag_dir()\n\n            simple_dags = self.heartbeat()\n            for simple_dag in simple_dags:\n                self._result_queue.put(simple_dag)\n\n            self._print_stat()\n\n            all_files_processed = all(self.get_last_finish_time(x) is not None\n                                      for x in self.file_paths)\n            max_runs_reached = self.max_runs_reached()\n\n            dag_parsing_stat = DagParsingStat(self._file_paths,\n                                              self.get_all_pids(),\n                                              max_runs_reached,\n                                              all_files_processed,\n                                              len(simple_dags))\n            self._stat_queue.put(dag_parsing_stat)\n\n            if max_runs_reached:\n                self.log.info(\"Exiting dag parsing loop as all files \"\n                              \"have been processed %s times\", self._max_runs)\n                break\n\n            loop_duration = time.time() - loop_start_time\n            if loop_duration < 1:\n                sleep_length = 1 - loop_duration\n                self.log.debug(\"Sleeping for %.2f seconds to prevent excessive logging\", sleep_length)\n                time.sleep(sleep_length)"
    },
    {
        "docstring": "Parse DAG files in a loop controlled by DagParsingSignal.\n        Actual DAG parsing loop will run once upon receiving one\n        agent heartbeat message and will report done when finished the loop.",
        "code": "def foo(self):\n        \n        while True:\n            agent_signal = self._signal_conn.recv()\n            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                self.terminate()\n                break\n            elif agent_signal == DagParsingSignal.END_MANAGER:\n                self.end()\n                sys.exit(os.EX_OK)\n            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:\n\n                self._refresh_dag_dir()\n\n                simple_dags = self.heartbeat()\n                for simple_dag in simple_dags:\n                    self._result_queue.put(simple_dag)\n\n                self._print_stat()\n\n                all_files_processed = all(self.get_last_finish_time(x) is not None\n                                          for x in self.file_paths)\n                max_runs_reached = self.max_runs_reached()\n\n                dag_parsing_stat = DagParsingStat(self._file_paths,\n                                                  self.get_all_pids(),\n                                                  self.max_runs_reached(),\n                                                  all_files_processed,\n                                                  len(simple_dags))\n                self._stat_queue.put(dag_parsing_stat)\n\n                self.wait_until_finished()\n                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n\n                if max_runs_reached:\n                    self.log.info(\"Exiting dag parsing loop as all files \"\n                                  \"have been processed %s times\", self._max_runs)\n                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n                    break"
    },
    {
        "docstring": "Refresh file paths from dag dir if we haven't done it for too long.",
        "code": "def foo(self):\n        \n        elapsed_time_since_refresh = (timezone.utcnow() -\n                                      self.last_dag_dir_refresh_time).total_seconds()\n        if elapsed_time_since_refresh > self.dag_dir_list_interval:\n            \n            self.log.info(\"Searching for files in %s\", self._dag_directory)\n            self._file_paths = list_py_file_paths(self._dag_directory)\n            self.last_dag_dir_refresh_time = timezone.utcnow()\n            self.log.info(\"There are %s files in %s\", len(self._file_paths), self._dag_directory)\n            self.set_file_paths(self._file_paths)\n\n            try:\n                self.log.debug(\"Removing old import errors\")\n                self.clear_nonexistent_import_errors()\n            except Exception:\n                self.log.exception(\"Error removing old import errors\")"
    },
    {
        "docstring": "Occasionally print out stats about how fast the files are getting processed",
        "code": "def foo(self):\n        \n        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >\n                self.print_stats_interval):\n            if len(self._file_paths) > 0:\n                self._log_file_processing_stats(self._file_paths)\n            self.last_stat_print_time = timezone.utcnow()"
    },
    {
        "docstring": "Clears import errors for files that no longer exist.\n\n        :param session: session for ORM operations\n        :type session: sqlalchemy.orm.session.Session",
        "code": "def foo(self, session):\n        \n        query = session.query(errors.ImportError)\n        if self._file_paths:\n            query = query.filter(\n                ~errors.ImportError.filename.in_(self._file_paths)\n            )\n        query.delete(synchronize_session='fetch')\n        session.commit()"
    },
    {
        "docstring": "Print out stats about how files are getting processed.\n\n        :param known_file_paths: a list of file paths that may contain Airflow\n            DAG definitions\n        :type known_file_paths: list[unicode]\n        :return: None",
        "code": "def foo(self, known_file_paths):\n        \n\n        \n        \n        \n        \n        \n        \n        \n        \n        headers = [\"File Path\",\n                   \"PID\",\n                   \"Runtime\",\n                   \"Last Runtime\",\n                   \"Last Run\"]\n\n        rows = []\n        for file_path in known_file_paths:\n            last_runtime = self.get_last_runtime(file_path)\n            file_name = os.path.basename(file_path)\n            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')\n            if last_runtime:\n                Stats.gauge(\n                    'dag_processing.last_runtime.{}'.format(file_name),\n                    last_runtime\n                )\n\n            processor_pid = self.get_pid(file_path)\n            processor_start_time = self.get_start_time(file_path)\n            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()\n                       if processor_start_time else None)\n            last_run = self.get_last_finish_time(file_path)\n            if last_run:\n                seconds_ago = (timezone.utcnow() - last_run).total_seconds()\n                Stats.gauge(\n                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),\n                    seconds_ago\n                )\n\n            rows.append((file_path,\n                         processor_pid,\n                         runtime,\n                         last_runtime,\n                         last_run))\n\n        \n        rows = sorted(rows, key=lambda x: x[3] or 0.0)\n\n        formatted_rows = []\n        for file_path, pid, runtime, last_runtime, last_run in rows:\n            formatted_rows.append((file_path,\n                                   pid,\n                                   \"{:.2f}s\".format(runtime)\n                                   if runtime else None,\n                                   \"{:.2f}s\".format(last_runtime)\n                                   if last_runtime else None,\n                                   last_run.strftime(\"%Y-%m-%dT%H:%M:%S\")\n                                   if last_run else None))\n        log_str = (\"\\n\" +\n                   \"=\" * 80 +\n                   \"\\n\" +\n                   \"DAG File Processing Stats\\n\\n\" +\n                   tabulate(formatted_rows, headers=headers) +\n                   \"\\n\" +\n                   \"=\" * 80)\n\n        self.log.info(log_str)"
    },
    {
        "docstring": "Update this with a new set of paths to DAG definition files.\n\n        :param new_file_paths: list of paths to DAG definition files\n        :type new_file_paths: list[unicode]\n        :return: None",
        "code": "def foo(self, new_file_paths):\n        \n        self._file_paths = new_file_paths\n        self._file_path_queue = [x for x in self._file_path_queue\n                                 if x in new_file_paths]\n        \n        filtered_processors = {}\n        for file_path, processor in self._processors.items():\n            if file_path in new_file_paths:\n                filtered_processors[file_path] = processor\n            else:\n                self.log.warning(\"Stopping processor for %s\", file_path)\n                processor.terminate()\n        self._processors = filtered_processors"
    },
    {
        "docstring": "Sleeps until all the processors are done.",
        "code": "def foo(self):\n        \n        for file_path, processor in self._processors.items():\n            while not processor.done:\n                time.sleep(0.1)"
    },
    {
        "docstring": "This should be periodically called by the manager loop. This method will\n        kick off new processes to process DAG definition files and read the\n        results from the finished processors.\n\n        :return: a list of SimpleDags that were produced by processors that\n            have finished since the last time this was called\n        :rtype: list[airflow.utils.dag_processing.SimpleDag]",
        "code": "def foo(self):\n        \n        finished_processors = {}\n        \n        running_processors = {}\n        \n\n        for file_path, processor in self._processors.items():\n            if processor.done:\n                self.log.debug(\"Processor for %s finished\", file_path)\n                now = timezone.utcnow()\n                finished_processors[file_path] = processor\n                self._last_runtime[file_path] = (now -\n                                                 processor.start_time).total_seconds()\n                self._last_finish_time[file_path] = now\n                self._run_count[file_path] += 1\n            else:\n                running_processors[file_path] = processor\n        self._processors = running_processors\n\n        self.log.debug(\"%s/%s DAG parsing processes running\",\n                       len(self._processors), self._parallelism)\n\n        self.log.debug(\"%s file paths queued for processing\",\n                       len(self._file_path_queue))\n\n        \n        simple_dags = []\n        for file_path, processor in finished_processors.items():\n            if processor.result is None:\n                self.log.warning(\n                    \"Processor for %s exited with return code %s.\",\n                    processor.file_path, processor.exit_code\n                )\n            else:\n                for simple_dag in processor.result:\n                    simple_dags.append(simple_dag)\n\n        \n        \n        if len(self._file_path_queue) == 0:\n            \n            \n            file_paths_in_progress = self._processors.keys()\n            now = timezone.utcnow()\n            file_paths_recently_processed = []\n            for file_path in self._file_paths:\n                last_finish_time = self.get_last_finish_time(file_path)\n                if (last_finish_time is not None and\n                    (now - last_finish_time).total_seconds() <\n                        self._file_process_interval):\n                    file_paths_recently_processed.append(file_path)\n\n            files_paths_at_run_limit = [file_path\n                                        for file_path, num_runs in self._run_count.items()\n                                        if num_runs == self._max_runs]\n\n            files_paths_to_queue = list(set(self._file_paths) -\n                                        set(file_paths_in_progress) -\n                                        set(file_paths_recently_processed) -\n                                        set(files_paths_at_run_limit))\n\n            for file_path, processor in self._processors.items():\n                self.log.debug(\n                    \"File path %s is still being processed (started: %s)\",\n                    processor.file_path, processor.start_time.isoformat()\n                )\n\n            self.log.debug(\n                \"Queuing the following files for processing:\\n\\t%s\",\n                \"\\n\\t\".join(files_paths_to_queue)\n            )\n\n            self._file_path_queue.extend(files_paths_to_queue)\n\n        zombies = self._find_zombies()\n\n        \n        while (self._parallelism - len(self._processors) > 0 and\n               len(self._file_path_queue) > 0):\n            file_path = self._file_path_queue.pop(0)\n            processor = self._processor_factory(file_path, zombies)\n\n            processor.start()\n            self.log.debug(\n                \"Started a process (PID: %s) to generate tasks for %s\",\n                processor.pid, file_path\n            )\n            self._processors[file_path] = processor\n\n        \n        self._run_count[self._heart_beat_key] += 1\n\n        return simple_dags"
    },
    {
        "docstring": "Kill all child processes on exit since we don't want to leave\n        them as orphaned.",
        "code": "def foo(self):\n        \n        pids_to_kill = self.get_all_pids()\n        if len(pids_to_kill) > 0:\n            \n            this_process = psutil.Process(os.getpid())\n            \n            \n            \n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            for child in child_processes:\n                self.log.info(\"Terminating child PID: %s\", child.pid)\n                child.terminate()\n            \n            timeout = 5\n            self.log.info(\"Waiting up to %s seconds for processes to exit...\", timeout)\n            try:\n                psutil.wait_procs(\n                    child_processes, timeout=timeout,\n                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for processes to exit\")\n\n            \n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            if len(child_processes) > 0:\n                self.log.info(\"SIGKILL processes that did not terminate gracefully\")\n                for child in child_processes:\n                    self.log.info(\"Killing child PID: %s\", child.pid)\n                    child.kill()\n                    child.wait()"
    },
    {
        "docstring": "Opens a ssh connection to the remote host.\n\n        :rtype: paramiko.client.SSHClient",
        "code": "def foo(self):\n        \n\n        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)\n        client = paramiko.SSHClient()\n        if not self.allow_host_key_change:\n            self.log.warning('Remote Identification Change is not verified. '\n                             'This wont protect against Man-In-The-Middle attacks')\n            client.load_system_host_keys()\n        if self.no_host_key_check:\n            self.log.warning('No Host Key Verification. This wont protect '\n                             'against Man-In-The-Middle attacks')\n            \n            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        if self.password and self.password.strip():\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           password=self.password,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n        else:\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n\n        if self.keepalive_interval:\n            client.get_transport().set_keepalive(self.keepalive_interval)\n\n        self.client = client\n        return client"
    },
    {
        "docstring": "Creates a transfer job that runs periodically.\n\n        :param body: (Required) A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n        :type body: dict\n        :return: transfer job.\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n        :rtype: dict",
        "code": "def foo(self, body):\n        \n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Gets the latest state of a long-running operation in Google Storage\n        Transfer Service.\n\n        :param job_name: (Required) Name of the job to be fetched\n        :type job_name: str\n        :param project_id: (Optional) the ID of the project that owns the Transfer\n            Job. If set to None or missing, the default project_id from the GCP\n            connection is used.\n        :type project_id: str\n        :return: Transfer Job\n        :rtype: dict",
        "code": "def foo(self, job_name, project_id=None):\n        \n        return (\n            self.get_conn()\n            .transferJobs()\n            .get(jobName=job_name, projectId=project_id)\n            .execute(num_retries=self.num_retries)\n        )"
    },
    {
        "docstring": "Lists long-running operations in Google Storage Transfer\n        Service that match the specified filter.\n\n        :param filter: (Required) A request filter, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n        :type filter: dict\n        :return: List of Transfer Jobs\n        :rtype: list[dict]",
        "code": "def foo(self, filter):\n        \n        conn = self.get_conn()\n        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)\n        request = conn.transferJobs().list(filter=json.dumps(filter))\n        jobs = []\n\n        while request is not None:\n            response = request.execute(num_retries=self.num_retries)\n            jobs.extend(response[TRANSFER_JOBS])\n\n            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)\n\n        return jobs"
    },
    {
        "docstring": "Updates a transfer job that runs periodically.\n\n        :param job_name: (Required) Name of the job to be updated\n        :type job_name: str\n        :param body: A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n        :type body: dict\n        :return: If successful, TransferJob.\n        :rtype: dict",
        "code": "def foo(self, job_name, body):\n        \n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return (\n            self.get_conn()\n            .transferJobs()\n            .patch(jobName=job_name, body=body)\n            .execute(num_retries=self.num_retries)\n        )"
    },
    {
        "docstring": "Deletes a transfer job. This is a soft delete. After a transfer job is\n        deleted, the job and all the transfer executions are subject to garbage\n        collection. Transfer jobs become eligible for garbage collection\n        30 days after soft delete.\n\n        :param job_name: (Required) Name of the job to be deleted\n        :type job_name: str\n        :param project_id: (Optional) the ID of the project that owns the Transfer\n            Job. If set to None or missing, the default project_id from the GCP\n            connection is used.\n        :type project_id: str\n        :rtype: None",
        "code": "def foo(self, job_name, project_id):\n        \n\n        return (\n            self.get_conn()\n            .transferJobs()\n            .patch(\n                jobName=job_name,\n                body={\n                    PROJECT_ID: project_id,\n                    TRANSFER_JOB: {STATUS1: GcpTransferJobsStatus.DELETED},\n                    TRANSFER_JOB_FIELD_MASK: STATUS1,\n                },\n            )\n            .execute(num_retries=self.num_retries)\n        )"
    },
    {
        "docstring": "Cancels an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None",
        "code": "def foo(self, operation_name):\n        \n        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Pauses an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: (Required) Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None",
        "code": "def foo(self, operation_name):\n        \n        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Resumes an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: (Required) Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None",
        "code": "def foo(self, operation_name):\n        \n        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Waits until the job reaches the expected state.\n\n        :param job: Transfer job\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n        :type job: dict\n        :param expected_statuses: State that is expected\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n        :type expected_statuses: set[str]\n        :param timeout:\n        :type timeout: time in which the operation must end in seconds\n        :rtype: None",
        "code": "def foo(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):\n        \n        while timeout > 0:\n            operations = self.list_transfer_operations(\n                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}\n            )\n\n            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n            timeout -= TIME_TO_SLEEP_IN_SECONDS\n        raise AirflowException(\"Timeout. The operation could not be completed within the allotted time.\")"
    },
    {
        "docstring": "Returns all task reschedules for the task instance and try number,\n        in ascending order.\n\n        :param task_instance: the task instance to find task reschedules for\n        :type task_instance: airflow.models.TaskInstance",
        "code": "def foo(task_instance, session):\n        \n        TR = TaskReschedule\n        return (\n            session\n            .query(TR)\n            .filter(TR.dag_id == task_instance.dag_id,\n                    TR.task_id == task_instance.task_id,\n                    TR.execution_date == task_instance.execution_date,\n                    TR.try_number == task_instance.try_number)\n            .order_by(asc(TR.id))\n            .all()\n        )"
    },
    {
        "docstring": "Returns the number of slots open at the moment",
        "code": "def foo(self, session):\n        \n        from airflow.models.taskinstance import \\\n            TaskInstance as TI  \n\n        used_slots = session.query(func.count()).filter(TI.pool == self.pool).filter(\n            TI.state.in_([State.RUNNING, State.QUEUED])).scalar()\n        return self.slots - used_slots"
    },
    {
        "docstring": "Runs command and returns stdout",
        "code": "def foo(command):\n    \n    process = subprocess.Popen(\n        shlex.split(command),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        close_fds=True)\n    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')\n                      for stream in process.communicate()]\n\n    if process.returncode != 0:\n        raise AirflowConfigException(\n            \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\"\n            .format(command, process.returncode, output, stderr)\n        )\n\n    return output"
    },
    {
        "docstring": "Remove an option if it exists in config from a file or\n        default config. If both of config have the same option, this removes\n        the option in both configs unless remove_default=False.",
        "code": "def foo(self, section, option, remove_default=True):\n        \n        if super().has_option(section, option):\n            super().remove_option(section, option)\n\n        if self.airflow_defaults.has_option(section, option) and remove_default:\n            self.airflow_defaults.remove_option(section, option)"
    },
    {
        "docstring": "Returns the section as a dict. Values are converted to int, float, bool\n        as required.\n\n        :param section: section from the config\n        :rtype: dict",
        "code": "def foo(self, section):\n        \n        if (section not in self._sections and\n                section not in self.airflow_defaults._sections):\n            return None\n\n        _section = copy.deepcopy(self.airflow_defaults._sections[section])\n\n        if section in self._sections:\n            _section.update(copy.deepcopy(self._sections[section]))\n\n        section_prefix = 'AIRFLOW__{S}__'.format(S=section.upper())\n        for env_var in sorted(os.environ.keys()):\n            if env_var.startswith(section_prefix):\n                key = env_var.replace(section_prefix, '').lower()\n                _section[key] = self._get_env_var_option(section, key)\n\n        for key, val in iteritems(_section):\n            try:\n                val = int(val)\n            except ValueError:\n                try:\n                    val = float(val)\n                except ValueError:\n                    if val.lower() in ('t', 'true'):\n                        val = True\n                    elif val.lower() in ('f', 'false'):\n                        val = False\n            _section[key] = val\n        return _section"
    },
    {
        "docstring": "Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list",
        "code": "def foo(self, partial_keys):\n        \n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']"
    },
    {
        "docstring": "Begins a new transaction.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n\n        :return: a transaction handle.\n        :rtype: str",
        "code": "def foo(self):\n        \n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .beginTransaction(projectId=self.project_id, body={})\n                .execute(num_retries=self.num_retries))\n\n        return resp['transaction']"
    },
    {
        "docstring": "Commit a transaction, optionally creating, deleting or modifying some entities.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n\n        :param body: the body of the commit request.\n        :type body: dict\n        :return: the response body of the commit request.\n        :rtype: dict",
        "code": "def foo(self, body):\n        \n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .commit(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp"
    },
    {
        "docstring": "Lookup some entities by key.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n\n        :param keys: the keys to lookup.\n        :type keys: list\n        :param read_consistency: the read consistency to use. default, strong or eventual.\n                                 Cannot be used with a transaction.\n        :type read_consistency: str\n        :param transaction: the transaction to use, if any.\n        :type transaction: str\n        :return: the response body of the lookup request.\n        :rtype: dict",
        "code": "def foo(self, keys, read_consistency=None, transaction=None):\n        \n        conn = self.get_conn()\n\n        body = {'keys': keys}\n        if read_consistency:\n            body['readConsistency'] = read_consistency\n        if transaction:\n            body['transaction'] = transaction\n        resp = (conn\n                .projects()\n                .lookup(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp"
    },
    {
        "docstring": "Roll back a transaction.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n\n        :param transaction: the transaction to roll back.\n        :type transaction: str",
        "code": "def foo(self, transaction):\n        \n        conn = self.get_conn()\n\n        conn.projects().rollback(\n            projectId=self.project_id, body={'transaction': transaction}\n        ).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Run a query for entities.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n\n        :param body: the body of the query request.\n        :type body: dict\n        :return: the batch of query results.\n        :rtype: dict",
        "code": "def foo(self, body):\n        \n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .runQuery(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp['batch']"
    },
    {
        "docstring": "Gets the latest state of a long-running operation.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n\n        :param name: the name of the operation resource.\n        :type name: str\n        :return: a resource operation instance.\n        :rtype: dict",
        "code": "def foo(self, name):\n        \n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .get(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp"
    },
    {
        "docstring": "Deletes the long-running operation.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n\n        :param name: the name of the operation resource.\n        :type name: str\n        :return: none if successful.\n        :rtype: dict",
        "code": "def foo(self, name):\n        \n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .delete(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp"
    },
    {
        "docstring": "Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.\n        :rtype: dict",
        "code": "def foo(self, name, polling_interval_in_seconds):\n        \n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result"
    },
    {
        "docstring": "Export entities from Cloud Datastore to Cloud Storage for backup.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: Description of what data from the project is included in the export.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict",
        "code": "def foo(self, bucket, namespace=None, entity_filter=None, labels=None):\n        \n        admin_conn = self.get_conn()\n\n        output_uri_prefix = 'gs://' + '/'.join(filter(None, [bucket, namespace]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'outputUrlPrefix': output_uri_prefix,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .export(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp"
    },
    {
        "docstring": "Import a backup from Cloud Storage to Cloud Datastore.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param file: the metadata file written by the projects.export operation.\n        :type file: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: specify which kinds/namespaces are to be imported.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict",
        "code": "def foo(self, bucket, file, namespace=None, entity_filter=None, labels=None):\n        \n        admin_conn = self.get_conn()\n\n        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'inputUrl': input_url,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .import_(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp"
    },
    {
        "docstring": "Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str",
        "code": "def foo(self, target_arn, message):\n        \n\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )"
    },
    {
        "docstring": "Fetch the hostname using the callable from the config or using\n    `socket.getfqdn` as a fallback.",
        "code": "def foo():\n    \n    \n    try:\n        callable_path = conf.get('core', 'hostname_callable')\n    except AirflowConfigException:\n        callable_path = None\n\n    \n    \n    if not callable_path:\n        return socket.getfqdn()\n\n    \n    module_path, attr_name = callable_path.split(':')\n    module = importlib.import_module(module_path)\n    callable = getattr(module, attr_name)\n    return callable()"
    },
    {
        "docstring": "Retrieves connection to Cloud Natural Language service.\n\n        :return: Cloud Natural Language service object\n        :rtype: google.cloud.language_v1.LanguageServiceClient",
        "code": "def foo(self):\n        \n        if not self._conn:\n            self._conn = LanguageServiceClient(credentials=self._get_credentials())\n        return self._conn"
    },
    {
        "docstring": "Finds named entities in the text along with entity types,\n        salience, mentions for each entity, and other properties.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or class google.cloud.language_v1.types.Document\n        :param encoding_type: The encoding type used by the API to calculate offsets.\n        :type encoding_type: google.cloud.language_v1.types.EncodingType\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse",
        "code": "def foo(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):\n        \n        client = self.get_conn()\n\n        return client.analyze_entities(\n            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata\n        )"
    },
    {
        "docstring": "A convenience method that provides all the features that analyzeSentiment,\n        analyzeEntities, and analyzeSyntax provide in one call.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or google.cloud.language_v1.types.Document\n        :param features: The enabled features.\n            If a dict is provided, it must be of the same form as the protobuf message Features\n        :type features: dict or google.cloud.language_v1.enums.Features\n        :param encoding_type: The encoding type used by the API to calculate offsets.\n        :type encoding_type: google.cloud.language_v1.types.EncodingType\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnnotateTextResponse",
        "code": "def foo(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):\n        \n        client = self.get_conn()\n\n        return client.annotate_text(\n            document=document,\n            features=features,\n            encoding_type=encoding_type,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )"
    },
    {
        "docstring": "Classifies a document into categories.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or class google.cloud.language_v1.types.Document\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse",
        "code": "def foo(self, document, retry=None, timeout=None, metadata=None):\n        \n        client = self.get_conn()\n\n        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)"
    },
    {
        "docstring": "Gets template fields for specific operator class.\n\n    :param fullname: Full path to operator class.\n        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n    :return: List of template field\n    :rtype: list[str]",
        "code": "def foo(env, fullname):\n    \n    modname, classname = fullname.rsplit(\".\", 1)\n\n    try:\n        with mock(env.config.autodoc_mock_imports):\n            mod = import_module(modname)\n    except ImportError:\n        raise RoleException(\"Error loading %s module.\" % (modname, ))\n\n    clazz = getattr(mod, classname)\n    if not clazz:\n        raise RoleException(\"Error finding %s class in %s module.\" % (classname, modname))\n\n    template_fields = getattr(clazz, \"template_fields\")\n\n    if not template_fields:\n        raise RoleException(\n            \"Could not find the template fields for %s class in %s module.\" % (classname, modname)\n        )\n\n    return list(template_fields)"
    },
    {
        "docstring": "A role that allows you to include a list of template fields in the middle of the text. This is especially\n    useful when writing guides describing how to use the operator.\n    The result is a list of fields where each field is shorted in the literal block.\n\n    Sample usage::\n\n    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`\n\n    For further information look at:\n\n    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted\n      Text Roles)",
        "code": "def foo(app, typ, rawtext, text, lineno, inliner, options={}, content=[]):\n    \n    text = utils.unescape(text)\n\n    try:\n        template_fields = get_template_field(app.env, text)\n    except RoleException as e:\n        msg = inliner.reporter.error(\"invalid class name %s \\n%s\" % (text, e, ), line=lineno)\n        prb = inliner.problematic(rawtext, rawtext, msg)\n        return [prb], [msg]\n\n    node = nodes.inline(rawtext=rawtext)\n    for i, field in enumerate(template_fields):\n        if i != 0:\n            node += nodes.Text(\", \")\n        node += nodes.literal(field, \"\", nodes.Text(field))\n\n    return [node], []"
    },
    {
        "docstring": "Properly close pooled database connections",
        "code": "def foo():\n    \n    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())\n    global engine\n    global Session\n\n    if Session:\n        Session.remove()\n        Session = None\n    if engine:\n        engine.dispose()\n        engine = None"
    },
    {
        "docstring": "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath",
        "code": "def foo():\n    \n\n    if DAGS_FOLDER not in sys.path:\n        sys.path.append(DAGS_FOLDER)\n\n    \n    \n    config_path = os.path.join(AIRFLOW_HOME, 'config')\n    if config_path not in sys.path:\n        sys.path.append(config_path)\n\n    if PLUGINS_FOLDER not in sys.path:\n        sys.path.append(PLUGINS_FOLDER)"
    },
    {
        "docstring": "Gets the returned Celery result from the Airflow task\n        ID provided to the sensor, and returns True if the\n        celery result has been finished execution.\n\n        :param context: Airflow's execution context\n        :type context: dict\n        :return: True if task has been executed, otherwise False\n        :rtype: bool",
        "code": "def foo(self, context):\n        \n        ti = context['ti']\n        celery_result = ti.xcom_pull(task_ids=self.target_task_id)\n        return celery_result.ready()"
    },
    {
        "docstring": "Return true if the ticket cache contains \"conf\" information as is found\n    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the\n    Sun Java Krb5LoginModule in Java6, so we need to take an action to work\n    around it.",
        "code": "def foo():\n    \n    ticket_cache = configuration.conf.get('kerberos', 'ccache')\n\n    with open(ticket_cache, 'rb') as f:\n        \n        return b'X-CACHECONF:' in f.read()"
    },
    {
        "docstring": "Transforms a SQLAlchemy model instance into a dictionary",
        "code": "def foo(obj):\n    \n    if not obj:\n        return None\n    d = {}\n    for c in obj.__table__.columns:\n        value = getattr(obj, c.name)\n        if type(value) == datetime:\n            value = value.isoformat()\n        d[c.name] = value\n    return d"
    },
    {
        "docstring": "Yield successive chunks of a given size from a list of items",
        "code": "def foo(items, chunk_size):\n    \n    if chunk_size <= 0:\n        raise ValueError('Chunk size must be a positive integer')\n    for i in range(0, len(items), chunk_size):\n        yield items[i:i + chunk_size]"
    },
    {
        "docstring": "Reduce the given list of items by splitting it into chunks\n    of the given size and passing each chunk through the reducer",
        "code": "def foo(fn, iterable, initializer, chunk_size=0):\n    \n    if len(iterable) == 0:\n        return initializer\n    if chunk_size == 0:\n        chunk_size = len(iterable)\n    return reduce(fn, chunks(iterable, chunk_size), initializer)"
    },
    {
        "docstring": "Given a number of tasks, builds a dependency chain.\n\n    chain(task_1, task_2, task_3, task_4)\n\n    is equivalent to\n\n    task_1.set_downstream(task_2)\n    task_2.set_downstream(task_3)\n    task_3.set_downstream(task_4)",
        "code": "def foo(*tasks):\n    \n    for up_task, down_task in zip(tasks[:-1], tasks[1:]):\n        up_task.set_downstream(down_task)"
    },
    {
        "docstring": "Returns a pretty ascii table from tuples\n\n    If namedtuple are used, the table will have headers",
        "code": "def foo(rows):\n    \n    if not rows:\n        return\n    if hasattr(rows[0], '_fields'):  \n        headers = rows[0]._fields\n    else:\n        headers = [\"col{}\".format(i) for i in range(len(rows[0]))]\n    lens = [len(s) for s in headers]\n\n    for row in rows:\n        for i in range(len(rows[0])):\n            slenght = len(\"{}\".format(row[i]))\n            if slenght > lens[i]:\n                lens[i] = slenght\n    formats = []\n    hformats = []\n    for i in range(len(rows[0])):\n        if isinstance(rows[0][i], int):\n            formats.append(\"%%%dd\" % lens[i])\n        else:\n            formats.append(\"%%-%ds\" % lens[i])\n        hformats.append(\"%%-%ds\" % lens[i])\n    pattern = \" | \".join(formats)\n    hpattern = \" | \".join(hformats)\n    separator = \"-+-\".join(['-' * n for n in lens])\n    s = \"\"\n    s += separator + '\\n'\n    s += (hpattern % tuple(headers)) + '\\n'\n    s += separator + '\\n'\n\n    def f(t):\n        return \"{}\".format(t) if isinstance(t, basestring) else t\n\n    for line in rows:\n        s += pattern % tuple(f(t) for t in line) + '\\n'\n    s += separator + '\\n'\n    return s"
    },
    {
        "docstring": "Given task instance, try_number, filename_template, return the rendered log\n    filename\n\n    :param ti: task instance\n    :param try_number: try_number of the task\n    :param filename_template: filename template, which can be jinja template or\n        python string template",
        "code": "def foo(ti, try_number, filename_template):\n    \n    filename_template, filename_jinja_template = parse_template_string(filename_template)\n    if filename_jinja_template:\n        jinja_context = ti.get_template_context()\n        jinja_context['try_number'] = try_number\n        return filename_jinja_template.render(**jinja_context)\n\n    return filename_template.format(dag_id=ti.dag_id,\n                                    task_id=ti.task_id,\n                                    execution_date=ti.execution_date.isoformat(),\n                                    try_number=try_number)"
    },
    {
        "docstring": "Returns a Google Cloud Dataproc service object.",
        "code": "def foo(self):\n        \n        http_authorized = self._authorize()\n        return build(\n            'dataproc', self.api_version, http=http_authorized,\n            cache_discovery=False)"
    },
    {
        "docstring": "Awaits for Google Cloud Dataproc Operation to complete.",
        "code": "def foo(self, operation):\n        \n        submitted = _DataProcOperation(self.get_conn(), operation,\n                                       self.num_retries)\n        submitted.wait_for_done()"
    },
    {
        "docstring": "Coerces content or all values of content if it is a dict to a string. The\n    function will throw if content contains non-string or non-numeric types.\n\n    The reason why we have this function is because the ``self.json`` field must be a\n    dict with only string values. This is because ``render_template`` will fail\n    for numerical values.",
        "code": "def foo(content, json_path='json'):\n    \n    c = foo\n    if isinstance(content, six.string_types):\n        return content\n    elif isinstance(content, six.integer_types + (float,)):\n        \n        return str(content)\n    elif isinstance(content, (list, tuple)):\n        return [c(e, '{0}[{1}]'.format(json_path, i)) for i, e in enumerate(content)]\n    elif isinstance(content, dict):\n        return {k: c(v, '{0}[{1}]'.format(json_path, k))\n                for k, v in list(content.items())}\n    else:\n        param_type = type(content)\n        msg = 'Type {0} used for parameter {1} is not a number or a string' \\\n            .format(param_type, json_path)\n        raise AirflowException(msg)"
    },
    {
        "docstring": "Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n\n    :param operator: Databricks operator being handled\n    :param context: Airflow context",
        "code": "def foo(operator, hook, log, context):\n    \n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)\n    log.info('Run submitted with run_id: %s', operator.run_id)\n    run_page_url = hook.get_run_page_url(operator.run_id)\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)\n\n    log.info('View run status, Spark UI, and logs at %s', run_page_url)\n    while True:\n        run_state = hook.get_run_state(operator.run_id)\n        if run_state.is_terminal:\n            if run_state.is_successful:\n                log.info('%s completed successfully.', operator.task_id)\n                log.info('View run status, Spark UI, and logs at %s', run_page_url)\n                return\n            else:\n                error_message = '{t} failed with terminal state: {s}'.format(\n                    t=operator.task_id,\n                    s=run_state)\n                raise AirflowException(error_message)\n        else:\n            log.info('%s in run state: %s', operator.task_id, run_state)\n            log.info('View run status, Spark UI, and logs at %s', run_page_url)\n            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)\n            time.sleep(operator.polling_period_seconds)"
    },
    {
        "docstring": "Run an pig script using the pig cli\n\n        >>> ph = PigCliHook()\n        >>> result = ph.run_cli(\"ls /;\")\n        >>> (\"hdfs://\" in result)\n        True",
        "code": "def foo(self, pig, verbose=True):\n        \n\n        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir) as f:\n                f.write(pig.encode('utf-8'))\n                f.flush()\n                fname = f.name\n                pig_bin = 'pig'\n                cmd_extra = []\n\n                pig_cmd = [pig_bin, '-f', fname] + cmd_extra\n\n                if self.pig_properties:\n                    pig_properties_list = self.pig_properties.split()\n                    pig_cmd.extend(pig_properties_list)\n                if verbose:\n                    self.log.info(\"%s\", \" \".join(pig_cmd))\n                sp = subprocess.Popen(\n                    pig_cmd,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    cwd=tmp_dir,\n                    close_fds=True)\n                self.sp = sp\n                stdout = ''\n                for line in iter(sp.stdout.readline, b''):\n                    stdout += line.decode('utf-8')\n                    if verbose:\n                        self.log.info(line.strip())\n                sp.wait()\n\n                if sp.returncode:\n                    raise AirflowException(stdout)\n\n                return stdout"
    },
    {
        "docstring": "Fetch and return the state of the given celery task. The scope of this function is\n    global so that it can be called by subprocesses in the pool.\n\n    :param celery_task: a tuple of the Celery task key and the async Celery object used\n        to fetch the task's state\n    :type celery_task: tuple(str, celery.result.AsyncResult)\n    :return: a tuple of the Celery task key and the Celery state of the task\n    :rtype: tuple[str, str]",
        "code": "def foo(celery_task):\n    \n\n    try:\n        with timeout(seconds=2):\n            \n            \n            res = (celery_task[0], celery_task[1].state)\n    except Exception as e:\n        exception_traceback = \"Celery Task ID: {}\\n{}\".format(celery_task[0],\n                                                              traceback.format_exc())\n        res = ExceptionWithTraceback(e, exception_traceback)\n    return res"
    },
    {
        "docstring": "How many Celery tasks should each worker process send.\n\n        :return: Number of tasks that should be sent per process\n        :rtype: int",
        "code": "def foo(self, to_send_count):\n        \n        return max(1,\n                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))"
    },
    {
        "docstring": "How many Celery tasks should be sent to each worker process.\n\n        :return: Number of tasks that should be used per process\n        :rtype: int",
        "code": "def foo(self):\n        \n        return max(1,\n                   int(math.ceil(1.0 * len(self.tasks) / self._sync_parallelism)))"
    },
    {
        "docstring": "Like a Python builtin dict object, setdefault returns the current value\n        for a key, and if it isn't there, stores the default value and returns it.\n\n        :param key: Dict key for this Variable\n        :type key: str\n        :param default: Default value to set and return if the variable\n            isn't already in the DB\n        :type default: Mixed\n        :param deserialize_json: Store this as a JSON encoded value in the DB\n            and un-encode it when retrieving a value\n        :return: Mixed",
        "code": "def foo(cls, key, default, deserialize_json=False):\n        \n        obj = Variable.get(key, default_var=None,\n                           deserialize_json=deserialize_json)\n        if obj is None:\n            if default is not None:\n                Variable.set(key, default, serialize_json=deserialize_json)\n                return default\n            else:\n                raise ValueError('Default Value must be set')\n        else:\n            return obj"
    },
    {
        "docstring": "Returns a Google MLEngine service object.",
        "code": "def foo(self):\n        \n        authed_http = self._authorize()\n        return build('ml', 'v1', http=authed_http, cache_discovery=False)"
    },
    {
        "docstring": "Launches a MLEngine job and wait for it to reach a terminal state.\n\n        :param project_id: The Google Cloud project id within which MLEngine\n            job will be launched.\n        :type project_id: str\n\n        :param job: MLEngine Job object that should be provided to the MLEngine\n            API, such as: ::\n\n                {\n                  'jobId': 'my_job_id',\n                  'trainingInput': {\n                    'scaleTier': 'STANDARD_1',\n                    ...\n                  }\n                }\n\n        :type job: dict\n\n        :param use_existing_job_fn: In case that a MLEngine job with the same\n            job_id already exist, this method (if provided) will decide whether\n            we should use this existing job, continue waiting for it to finish\n            and returning the job object. It should accepts a MLEngine job\n            object, and returns a boolean value indicating whether it is OK to\n            reuse the existing job. If 'use_existing_job_fn' is not provided,\n            we by default reuse the existing MLEngine job.\n        :type use_existing_job_fn: function\n\n        :return: The MLEngine job object if the job successfully reach a\n            terminal state (which might be FAILED or CANCELLED state).\n        :rtype: dict",
        "code": "def foo(self, project_id, job, use_existing_job_fn=None):\n        \n        request = self._mlengine.projects().jobs().create(\n            parent='projects/{}'.format(project_id),\n            body=job)\n        job_id = job['jobId']\n\n        try:\n            request.execute()\n        except HttpError as e:\n            \n            if e.resp.status == 409:\n                if use_existing_job_fn is not None:\n                    existing_job = self._get_job(project_id, job_id)\n                    if not use_existing_job_fn(existing_job):\n                        self.log.error(\n                            'Job with job_id %s already exist, but it does '\n                            'not match our expectation: %s',\n                            job_id, existing_job\n                        )\n                        raise\n                self.log.info(\n                    'Job with job_id %s already exist. Will waiting for it to finish',\n                    job_id\n                )\n            else:\n                self.log.error('Failed to create MLEngine job: {}'.format(e))\n                raise\n\n        return self._wait_for_job_done(project_id, job_id)"
    },
    {
        "docstring": "Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server",
        "code": "def foo(self, project_id, job_id):\n        \n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    \n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise"
    },
    {
        "docstring": "Waits for the Job to reach a terminal state.\n\n        This method will periodically check the job state until the job reach\n        a terminal state.\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned when getting\n            the job",
        "code": "def foo(self, project_id, job_id, interval=30):\n        \n        if interval <= 0:\n            raise ValueError(\"Interval must be > 0\")\n        while True:\n            job = self._get_job(project_id, job_id)\n            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                return job\n            time.sleep(interval)"
    },
    {
        "docstring": "Creates the Version on Google Cloud ML Engine.\n\n        Returns the operation if the version was created successfully and\n        raises an error otherwise.",
        "code": "def foo(self, project_id, model_name, version_spec):\n        \n        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)\n        create_request = self._mlengine.projects().models().versions().create(\n            parent=parent_name, body=version_spec)\n        response = create_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)"
    },
    {
        "docstring": "Sets a version to be the default. Blocks until finished.",
        "code": "def foo(self, project_id, model_name, version_name):\n        \n        full_version_name = 'projects/{}/models/{}/versions/{}'.format(\n            project_id, model_name, version_name)\n        request = self._mlengine.projects().models().versions().setDefault(\n            name=full_version_name, body={})\n\n        try:\n            response = request.execute()\n            self.log.info('Successfully set version: %s to default', response)\n            return response\n        except HttpError as e:\n            self.log.error('Something went wrong: %s', e)\n            raise"
    },
    {
        "docstring": "Lists all available versions of a model. Blocks until finished.",
        "code": "def foo(self, project_id, model_name):\n        \n        result = []\n        full_parent_name = 'projects/{}/models/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().versions().list(\n            parent=full_parent_name, pageSize=100)\n\n        response = request.execute()\n        next_page_token = response.get('nextPageToken', None)\n        result.extend(response.get('versions', []))\n        while next_page_token is not None:\n            next_request = self._mlengine.projects().models().versions().list(\n                parent=full_parent_name,\n                pageToken=next_page_token,\n                pageSize=100)\n            response = next_request.execute()\n            next_page_token = response.get('nextPageToken', None)\n            result.extend(response.get('versions', []))\n            time.sleep(5)\n        return result"
    },
    {
        "docstring": "Deletes the given version of a model. Blocks until finished.",
        "code": "def foo(self, project_id, model_name, version_name):\n        \n        full_name = 'projects/{}/models/{}/versions/{}'.format(\n            project_id, model_name, version_name)\n        delete_request = self._mlengine.projects().models().versions().delete(\n            name=full_name)\n        response = delete_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)"
    },
    {
        "docstring": "Create a Model. Blocks until finished.",
        "code": "def foo(self, project_id, model):\n        \n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()"
    },
    {
        "docstring": "Gets a Model. Blocks until finished.",
        "code": "def foo(self, project_id, model_name):\n        \n        if not model_name:\n            raise ValueError(\"Model name must be provided and \"\n                             \"it could not be an empty string\")\n        full_model_name = 'projects/{}/models/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().get(name=full_model_name)\n        try:\n            return request.execute()\n        except HttpError as e:\n            if e.resp.status == 404:\n                self.log.error('Model was not found: %s', e)\n                return None\n            raise"
    },
    {
        "docstring": "Write batch items to dynamodb table with provisioned throughout capacity.",
        "code": "def foo(self, items):\n        \n\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n                'Failed to insert items in dynamodb, error: {error}'.format(\n                    error=str(general_error)\n                )\n            )"
    },
    {
        "docstring": "Integrate plugins to the context.",
        "code": "def foo():\n    \n    from airflow.plugins_manager import executors_modules\n    for executors_module in executors_modules:\n        sys.modules[executors_module.__name__] = executors_module\n        globals()[executors_module._name] = executors_module"
    },
    {
        "docstring": "Creates a new instance of the configured executor if none exists and returns it",
        "code": "def foo():\n    \n    global DEFAULT_EXECUTOR\n\n    if DEFAULT_EXECUTOR is not None:\n        return DEFAULT_EXECUTOR\n\n    executor_name = configuration.conf.get('core', 'EXECUTOR')\n\n    DEFAULT_EXECUTOR = _get_executor(executor_name)\n\n    log = LoggingMixin().log\n    log.info(\"Using executor %s\", executor_name)\n\n    return DEFAULT_EXECUTOR"
    },
    {
        "docstring": "Creates a new instance of the named executor.\n    In case the executor name is not know in airflow,\n    look for it in the plugins",
        "code": "def foo(executor_name):\n    \n    if executor_name == Executors.LocalExecutor:\n        return LocalExecutor()\n    elif executor_name == Executors.SequentialExecutor:\n        return SequentialExecutor()\n    elif executor_name == Executors.CeleryExecutor:\n        from airflow.executors.celery_executor import CeleryExecutor\n        return CeleryExecutor()\n    elif executor_name == Executors.DaskExecutor:\n        from airflow.executors.dask_executor import DaskExecutor\n        return DaskExecutor()\n    elif executor_name == Executors.KubernetesExecutor:\n        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor\n        return KubernetesExecutor()\n    else:\n        \n        _integrate_plugins()\n        executor_path = executor_name.split('.')\n        if len(executor_path) != 2:\n            raise AirflowException(\n                \"Executor {0} not supported: \"\n                \"please specify in format plugin_module.executor\".format(executor_name))\n\n        if executor_path[0] in globals():\n            return globals()[executor_path[0]].__dict__[executor_path[1]]()\n        else:\n            raise AirflowException(\"Executor {0} not supported.\".format(executor_name))"
    },
    {
        "docstring": "Handles error callbacks when using Segment with segment_debug_mode set to True",
        "code": "def foo(self, error, items):\n        \n        self.log.error('Encountered Segment error: {segment_error} with '\n                       'items: {with_items}'.format(segment_error=error,\n                                                    with_items=items))\n        raise AirflowException('Segment error: {}'.format(error))"
    },
    {
        "docstring": "Returns a mssql connection object",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.mssql_conn_id)\n        conn = pymssql.connect(\n            server=conn.host,\n            user=conn.login,\n            password=conn.password,\n            database=self.schema or conn.schema,\n            port=conn.port)\n        return conn"
    },
    {
        "docstring": "Trigger a new dag run for a Dag with an execution date of now unless\n    specified in the data.",
        "code": "def foo(dag_id):\n    \n    data = request.get_json(force=True)\n\n    run_id = None\n    if 'run_id' in data:\n        run_id = data['run_id']\n\n    conf = None\n    if 'conf' in data:\n        conf = data['conf']\n\n    execution_date = None\n    if 'execution_date' in data and data['execution_date'] is not None:\n        execution_date = data['execution_date']\n\n        \n        try:\n            execution_date = timezone.parse(execution_date)\n        except ValueError:\n            error_message = (\n                'Given execution date, {}, could not be identified '\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\n                .format(execution_date))\n            _log.info(error_message)\n            response = jsonify({'error': error_message})\n            response.status_code = 400\n\n            return response\n\n    try:\n        dr = trigger.foo(dag_id, run_id, conf, execution_date)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    if getattr(g, 'user', None):\n        _log.info(\"User %s created %s\", g.user, dr)\n\n    response = jsonify(message=\"Created {}\".format(dr))\n    return response"
    },
    {
        "docstring": "Delete all DB records related to the specified Dag.",
        "code": "def foo(dag_id):\n    \n    try:\n        count = delete.foo(dag_id)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    return jsonify(message=\"Removed {} record(s)\".format(count), count=count)"
    },
    {
        "docstring": "Returns a JSON with a task's public instance variables.",
        "code": "def foo(dag_id, task_id):\n    \n    try:\n        info = get_task(dag_id, task_id)\n    except AirflowException as err:\n        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    \n    fields = {k: str(v)\n              for k, v in vars(info).items()\n              if not k.startswith('_')}\n    return jsonify(fields)"
    },
    {
        "docstring": "Get all pools.",
        "code": "def foo():\n    \n    try:\n        pools = pool_api.foo()\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify([p.to_json() for p in pools])"
    },
    {
        "docstring": "Create a pool.",
        "code": "def foo():\n    \n    params = request.get_json(force=True)\n    try:\n        pool = pool_api.foo(**params)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())"
    },
    {
        "docstring": "Delete pool.",
        "code": "def foo(name):\n    \n    try:\n        pool = pool_api.foo(name=name)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())"
    },
    {
        "docstring": "Create a new container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :param container_group: the properties of the container group\n        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup",
        "code": "def foo(self, resource_group, name, container_group):\n        \n        self.connection.container_groups.create_or_update(resource_group,\n                                                          name,\n                                                          container_group)"
    },
    {
        "docstring": "Get the state and exitcode of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :return: A tuple with the state, exitcode, and details.\n            If the exitcode is unknown 0 is returned.\n        :rtype: tuple(state,exitcode,details)",
        "code": "def foo(self, resource_group, name):\n        \n        current_state = self._get_instance_view(resource_group, name).current_state\n        return (current_state.state,\n                current_state.exit_code,\n                current_state.detail_status)"
    },
    {
        "docstring": "Get the messages of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :return: A list of the event messages\n        :rtype: list[str]",
        "code": "def foo(self, resource_group, name):\n        \n        instance_view = self._get_instance_view(resource_group, name)\n\n        return [event.message for event in instance_view.events]"
    },
    {
        "docstring": "Get the tail from logs of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :param tail: the size of the tail\n        :type tail: int\n        :return: A list of log messages\n        :rtype: list[str]",
        "code": "def foo(self, resource_group, name, tail=1000):\n        \n        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)\n        return logs.content.splitlines(True)"
    },
    {
        "docstring": "Delete a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str",
        "code": "def foo(self, resource_group, name):\n        \n        self.connection.container_groups.delete(resource_group, name)"
    },
    {
        "docstring": "Test if a container group exists\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str",
        "code": "def foo(self, resource_group, name):\n        \n        for container in self.connection.container_groups.list_by_resource_group(resource_group):\n            if container.name == name:\n                return True\n        return False"
    },
    {
        "docstring": "Function decorator that Looks for an argument named \"default_args\", and\n    fills the unspecified arguments from it.\n\n    Since python2.* isn't clear about which arguments are missing when\n    calling a function, and that this can be quite confusing with multi-level\n    inheritance and argument defaults, this decorator also alerts with\n    specific information about the missing arguments.",
        "code": "def foo(func):\n    \n\n    \n    \n    \n    \n    sig_cache = signature(func)\n    non_optional_args = {\n        name for (name, param) in sig_cache.parameters.items()\n        if param.default == param.empty and\n        param.name != 'self' and\n        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if len(args) > 1:\n            raise AirflowException(\n                \"Use keyword arguments when initializing operators\")\n        dag_args = {}\n        dag_params = {}\n\n        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG\n        if dag:\n            dag_args = copy(dag.default_args) or {}\n            dag_params = copy(dag.params) or {}\n\n        params = {}\n        if 'params' in kwargs:\n            params = kwargs['params']\n        dag_params.update(params)\n\n        default_args = {}\n        if 'default_args' in kwargs:\n            default_args = kwargs['default_args']\n            if 'params' in default_args:\n                dag_params.update(default_args['params'])\n                del default_args['params']\n\n        dag_args.update(default_args)\n        default_args = dag_args\n\n        for arg in sig_cache.parameters:\n            if arg not in kwargs and arg in default_args:\n                kwargs[arg] = default_args[arg]\n        missing_args = list(non_optional_args - set(kwargs))\n        if missing_args:\n            msg = \"Argument {0} is required\".format(missing_args)\n            raise AirflowException(msg)\n\n        kwargs['params'] = dag_params\n\n        result = func(*args, **kwargs)\n        return result\n    return wrapper"
    },
    {
        "docstring": "Builds an ingest query for an HDFS TSV load.\n\n        :param static_path: The path on hdfs where the data is\n        :type static_path: str\n        :param columns: List of all the columns that are available\n        :type columns: list",
        "code": "def foo(self, static_path, columns):\n        \n\n        \n        \n        \n        num_shards = self.num_shards\n        target_partition_size = self.target_partition_size\n        if self.target_partition_size == -1:\n            if self.num_shards == -1:\n                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n        else:\n            num_shards = -1\n\n        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n\n        \n        \n        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n\n        ingest_query_dict = {\n            \"type\": \"index_hadoop\",\n            \"spec\": {\n                \"dataSchema\": {\n                    \"metricsSpec\": self.metric_spec,\n                    \"granularitySpec\": {\n                        \"queryGranularity\": self.query_granularity,\n                        \"intervals\": self.intervals,\n                        \"type\": \"uniform\",\n                        \"segmentGranularity\": self.segment_granularity,\n                    },\n                    \"parser\": {\n                        \"type\": \"string\",\n                        \"parseSpec\": {\n                            \"columns\": columns,\n                            \"dimensionsSpec\": {\n                                \"dimensionExclusions\": [],\n                                \"dimensions\": dimensions,  \n                                \"spatialDimensions\": []\n                            },\n                            \"timestampSpec\": {\n                                \"column\": self.ts_dim,\n                                \"format\": \"auto\"\n                            },\n                            \"format\": \"tsv\"\n                        }\n                    },\n                    \"dataSource\": self.druid_datasource\n                },\n                \"tuningConfig\": {\n                    \"type\": \"hadoop\",\n                    \"jobProperties\": {\n                        \"mapreduce.job.user.classpath.first\": \"false\",\n                        \"mapreduce.map.output.compress\": \"false\",\n                        \"mapreduce.output.fileoutputformat.compress\": \"false\",\n                    },\n                    \"partitionsSpec\": {\n                        \"type\": \"hashed\",\n                        \"targetPartitionSize\": target_partition_size,\n                        \"numShards\": num_shards,\n                    },\n                },\n                \"ioConfig\": {\n                    \"inputSpec\": {\n                        \"paths\": static_path,\n                        \"type\": \"static\"\n                    },\n                    \"type\": \"hadoop\"\n                }\n            }\n        }\n\n        if self.job_properties:\n            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \\\n                .update(self.job_properties)\n\n        if self.hadoop_dependency_coordinates:\n            ingest_query_dict['hadoopDependencyCoordinates'] \\\n                = self.hadoop_dependency_coordinates\n\n        return ingest_query_dict"
    },
    {
        "docstring": "Check for message on subscribed channels and write to xcom the message with key ``message``\n\n        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``\n\n        :param context: the context object\n        :type context: dict\n        :return: ``True`` if message (with type 'message') is available or ``False`` if not",
        "code": "def foo(self, context):\n        \n        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)\n\n        message = self.pubsub.get_message()\n        self.log.info('Message %s from channel %s', message, self.channels)\n\n        \n        if message and message['type'] == 'message':\n\n            context['ti'].xcom_push(key='message', value=message)\n            self.pubsub.unsubscribe(self.channels)\n\n            return True\n\n        return False"
    },
    {
        "docstring": "Returns a set of dag runs for the given search criteria.\n\n        :param dag_id: the dag_id to find dag runs for\n        :type dag_id: int, list\n        :param run_id: defines the the run id for this dag run\n        :type run_id: str\n        :param execution_date: the execution date\n        :type execution_date: datetime.datetime\n        :param state: the state of the dag run\n        :type state: airflow.utils.state.State\n        :param external_trigger: whether this dag run is externally triggered\n        :type external_trigger: bool\n        :param no_backfills: return no backfills (True), return all (False).\n            Defaults to False\n        :type no_backfills: bool\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session",
        "code": "def foo(dag_id=None, run_id=None, execution_date=None,\n             state=None, external_trigger=None, no_backfills=False,\n             session=None):\n        \n        DR = DagRun\n\n        qry = session.query(DR)\n        if dag_id:\n            qry = qry.filter(DR.dag_id == dag_id)\n        if run_id:\n            qry = qry.filter(DR.run_id == run_id)\n        if execution_date:\n            if isinstance(execution_date, list):\n                qry = qry.filter(DR.execution_date.in_(execution_date))\n            else:\n                qry = qry.filter(DR.execution_date == execution_date)\n        if state:\n            qry = qry.filter(DR.state == state)\n        if external_trigger is not None:\n            qry = qry.filter(DR.external_trigger == external_trigger)\n        if no_backfills:\n            \n            from airflow.jobs import BackfillJob\n            qry = qry.filter(DR.run_id.notlike(BackfillJob.ID_PREFIX + '%'))\n\n        dr = qry.order_by(DR.execution_date).all()\n\n        return dr"
    },
    {
        "docstring": "Returns the task instances for this dag run",
        "code": "def foo(self, state=None, session=None):\n        \n        from airflow.models.taskinstance import TaskInstance  \n        tis = session.query(TaskInstance).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.execution_date == self.execution_date,\n        )\n        if state:\n            if isinstance(state, six.string_types):\n                tis = tis.filter(TaskInstance.state == state)\n            else:\n                \n                if None in state:\n                    tis = tis.filter(\n                        or_(TaskInstance.state.in_(state),\n                            TaskInstance.state.is_(None))\n                    )\n                else:\n                    tis = tis.filter(TaskInstance.state.in_(state))\n\n        if self.dag and self.dag.partial:\n            tis = tis.filter(TaskInstance.task_id.in_(self.dag.task_ids))\n\n        return tis.all()"
    },
    {
        "docstring": "Returns the task instance specified by task_id for this dag run\n\n        :param task_id: the task id",
        "code": "def foo(self, task_id, session=None):\n        \n\n        from airflow.models.taskinstance import TaskInstance  \n        TI = TaskInstance\n        ti = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.execution_date == self.execution_date,\n            TI.task_id == task_id\n        ).first()\n\n        return ti"
    },
    {
        "docstring": "The previous DagRun, if there is one",
        "code": "def foo(self, session=None):\n        \n\n        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date < self.execution_date\n        ).order_by(\n            DagRun.execution_date.desc()\n        ).first()"
    },
    {
        "docstring": "The previous, SCHEDULED DagRun, if there is one",
        "code": "def foo(self, session=None):\n        \n        dag = self.get_dag()\n\n        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date == dag.previous_schedule(self.execution_date)\n        ).first()"
    },
    {
        "docstring": "Determines the overall state of the DagRun based on the state\n        of its TaskInstances.\n\n        :return: State",
        "code": "def foo(self, session=None):\n        \n\n        dag = self.get_dag()\n\n        tis = self.get_task_instances(session=session)\n        self.log.debug(\"Updating state for %s considering %s task(s)\", self, len(tis))\n\n        for ti in list(tis):\n            \n            if ti.state == State.REMOVED:\n                tis.remove(ti)\n            else:\n                ti.task = dag.get_task(ti.task_id)\n\n        \n        \n        start_dttm = timezone.utcnow()\n        unfinished_tasks = self.get_task_instances(\n            state=State.unfinished(),\n            session=session\n        )\n        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)\n        none_task_concurrency = all(t.task.task_concurrency is None\n                                    for t in unfinished_tasks)\n        \n        if unfinished_tasks and none_depends_on_past and none_task_concurrency:\n            \n            no_dependencies_met = True\n            for ut in unfinished_tasks:\n                \n                \n                old_state = ut.state\n                deps_met = ut.are_dependencies_met(\n                    dep_context=DepContext(\n                        flag_upstream_failed=True,\n                        ignore_in_retry_period=True,\n                        ignore_in_reschedule_period=True),\n                    session=session)\n                if deps_met or old_state != ut.current_state(session=session):\n                    no_dependencies_met = False\n                    break\n\n        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000\n        Stats.timing(\"dagrun.dependency-check.{}\".format(self.dag_id), duration)\n\n        root_ids = [t.task_id for t in dag.roots]\n        roots = [t for t in tis if t.task_id in root_ids]\n\n        \n        if (not unfinished_tasks and\n                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):\n            self.log.info('Marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='task_failure',\n                                session=session)\n\n        \n        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)\n                                          for r in roots):\n            self.log.info('Marking run %s successful', self)\n            self.set_state(State.SUCCESS)\n            dag.handle_callback(self, success=True, reason='success', session=session)\n\n        \n        elif (unfinished_tasks and none_depends_on_past and\n              none_task_concurrency and no_dependencies_met):\n            self.log.info('Deadlock; marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',\n                                session=session)\n\n        \n        else:\n            self.set_state(State.RUNNING)\n\n        self._emit_duration_stats_for_finished_state()\n\n        \n        session.merge(self)\n        session.commit()\n\n        return self.state"
    },
    {
        "docstring": "Verifies the DagRun by checking for removed tasks or tasks that are not in the\n        database yet. It will set state to removed or add the task if required.",
        "code": "def foo(self, session=None):\n        \n        from airflow.models.taskinstance import TaskInstance  \n\n        dag = self.get_dag()\n        tis = self.get_task_instances(session=session)\n\n        \n        task_ids = []\n        for ti in tis:\n            task_ids.append(ti.task_id)\n            task = None\n            try:\n                task = dag.get_task(ti.task_id)\n            except AirflowException:\n                if ti.state == State.REMOVED:\n                    pass  \n                elif self.state is not State.RUNNING and not dag.partial:\n                    self.log.warning(\"Failed to get task '{}' for dag '{}'. \"\n                                     \"Marking it as removed.\".format(ti, dag))\n                    Stats.incr(\n                        \"task_removed_from_dag.{}\".format(dag.dag_id), 1, 1)\n                    ti.state = State.REMOVED\n\n            is_task_in_dag = task is not None\n            should_restore_task = is_task_in_dag and ti.state == State.REMOVED\n            if should_restore_task:\n                self.log.info(\"Restoring task '{}' which was previously \"\n                              \"removed from DAG '{}'\".format(ti, dag))\n                Stats.incr(\"task_restored_to_dag.{}\".format(dag.dag_id), 1, 1)\n                ti.state = State.NONE\n\n        \n        for task in six.itervalues(dag.task_dict):\n            if task.start_date > self.execution_date and not self.is_backfill:\n                continue\n\n            if task.task_id not in task_ids:\n                Stats.incr(\n                    \"task_instance_created-{}\".format(task.__class__.__name__),\n                    1, 1)\n                ti = TaskInstance(task, self.execution_date)\n                session.add(ti)\n\n        session.commit()"
    },
    {
        "docstring": "We need to get the headers in addition to the body answer\n    to get the location from them\n    This function uses jenkins_request method from python-jenkins library\n    with just the return call changed\n\n    :param jenkins_server: The server to query\n    :param req: The request to execute\n    :return: Dict containing the response body (key body)\n        and the headers coming along (headers)",
        "code": "def foo(jenkins_server, req):\n    \n    try:\n        response = jenkins_server.jenkins_request(req)\n        response_body = response.content\n        response_headers = response.headers\n        if response_body is None:\n            raise jenkins.EmptyResponseException(\n                \"Error communicating with server[%s]: \"\n                \"empty response\" % jenkins_server.server)\n        return {'body': response_body.decode('utf-8'), 'headers': response_headers}\n    except HTTPError as e:\n        \n        \n        if e.code in [401, 403, 500]:\n            \n            \n            \n            \n            raise JenkinsException(\n                'Error in request. ' +\n                'Possibly authentication failed [%s]: %s' % (\n                    e.code, e.msg)\n            )\n        elif e.code == 404:\n            raise jenkins.NotFoundException('Requested item could not be found')\n        else:\n            raise\n    except socket.timeout as e:\n        raise jenkins.TimeoutException('Error in request: %s' % e)\n    except URLError as e:\n        \n        \n        if str(e.reason) == \"timed out\":\n            raise jenkins.TimeoutException('Error in request: %s' % e.reason)\n        raise JenkinsException('Error in request: %s' % e.reason)"
    },
    {
        "docstring": "Given a context, this function provides a dictionary of values that can be used to\n    externally reconstruct relations between dags, dag_runs, tasks and task_instances.\n    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if\n    in_env_var_format is set to True.\n\n    :param context: The context for the task_instance of interest.\n    :type context: dict\n    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.\n    :type in_env_var_format: bool\n    :return: task_instance context as dict.",
        "code": "def foo(context, in_env_var_format=False):\n    \n    params = dict()\n    if in_env_var_format:\n        name_format = 'env_var_format'\n    else:\n        name_format = 'default'\n    task_instance = context.get('task_instance')\n    if task_instance and task_instance.dag_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][\n            name_format]] = task_instance.dag_id\n    if task_instance and task_instance.task_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][\n            name_format]] = task_instance.task_id\n    if task_instance and task_instance.execution_date:\n        params[\n            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][\n                name_format]] = task_instance.execution_date.isoformat()\n    dag_run = context.get('dag_run')\n    if dag_run and dag_run.run_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][\n            name_format]] = dag_run.run_id\n    return params"
    },
    {
        "docstring": "This function decides whether or not to Trigger the remote DAG",
        "code": "def foo(context, dag_run_obj):\n    \n    c_p = context['params']['condition_param']\n    print(\"Controller DAG : foo = {}\".format(c_p))\n    if context['params']['condition_param']:\n        dag_run_obj.payload = {'message': context['params']['message']}\n        pp.pprint(dag_run_obj.payload)\n        return dag_run_obj"
    },
    {
        "docstring": "Sends a single datapoint metric to DataDog\n\n        :param metric_name: The name of the metric\n        :type metric_name: str\n        :param datapoint: A single integer or float related to the metric\n        :type datapoint: int or float\n        :param tags: A list of tags associated with the metric\n        :type tags: list\n        :param type_: Type of your metric: gauge, rate, or count\n        :type type_: str\n        :param interval: If the type of the metric is rate or count, define the corresponding interval\n        :type interval: int",
        "code": "def foo(self, metric_name, datapoint, tags=None, type_=None, interval=None):\n        \n        response = api.Metric.send(\n            metric=metric_name,\n            points=datapoint,\n            host=self.host,\n            tags=tags,\n            type=type_,\n            interval=interval)\n\n        self.validate_response(response)\n        return response"
    },
    {
        "docstring": "Queries datadog for a specific metric, potentially with some\n        function applied to it and returns the results.\n\n        :param query: The datadog query to execute (see datadog docs)\n        :type query: str\n        :param from_seconds_ago: How many seconds ago to start querying for.\n        :type from_seconds_ago: int\n        :param to_seconds_ago: Up to how many seconds ago to query for.\n        :type to_seconds_ago: int",
        "code": "def foo(self,\n                     query,\n                     from_seconds_ago,\n                     to_seconds_ago):\n        \n        now = int(time.time())\n\n        response = api.Metric.query(\n            start=now - from_seconds_ago,\n            end=now - to_seconds_ago,\n            query=query)\n\n        self.validate_response(response)\n        return response"
    },
    {
        "docstring": "Gets the DAG out of the dictionary, and refreshes it if expired",
        "code": "def foo(self, dag_id):\n        \n        from airflow.models.dag import DagModel  \n\n        \n        root_dag_id = dag_id\n        if dag_id in self.dags:\n            dag = self.dags[dag_id]\n            if dag.is_subdag:\n                root_dag_id = dag.parent_dag.dag_id\n\n        \n        orm_dag = DagModel.get_current(root_dag_id)\n        if orm_dag and (\n                root_dag_id not in self.dags or\n                (\n                    orm_dag.last_expired and\n                    dag.last_loaded < orm_dag.last_expired\n                )\n        ):\n            \n            found_dags = self.process_file(\n                filepath=orm_dag.fileloc, only_if_updated=False)\n\n            \n            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:\n                return self.dags[dag_id]\n            elif dag_id in self.dags:\n                del self.dags[dag_id]\n        return self.dags.get(dag_id)"
    },
    {
        "docstring": "Fail given zombie tasks, which are tasks that haven't\n        had a heartbeat for too long, in the current DagBag.\n\n        :param zombies: zombie task instances to kill.\n        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance\n        :param session: DB session.\n        :type session: sqlalchemy.orm.session.Session",
        "code": "def foo(self, zombies, session=None):\n        \n        from airflow.models.taskinstance import TaskInstance  \n\n        for zombie in zombies:\n            if zombie.dag_id in self.dags:\n                dag = self.dags[zombie.dag_id]\n                if zombie.task_id in dag.task_ids:\n                    task = dag.get_task(zombie.task_id)\n                    ti = TaskInstance(task, zombie.execution_date)\n                    \n                    ti.start_date = zombie.start_date\n                    ti.end_date = zombie.end_date\n                    ti.try_number = zombie.try_number\n                    ti.state = zombie.state\n                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')\n                    ti.handle_failure(\"{} detected as zombie\".format(ti),\n                                      ti.test_mode, ti.get_template_context())\n                    self.log.info(\n                        'Marked zombie job %s as %s', ti, ti.state)\n                    Stats.incr('zombies_killed')\n        session.commit()"
    },
    {
        "docstring": "Adds the DAG into the bag, recurses into sub dags.\n        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags",
        "code": "def foo(self, dag, parent_dag, root_dag):\n        \n\n        dag.test_cycle()  \n\n        dag.resolve_template_files()\n        dag.last_loaded = timezone.utcnow()\n\n        for task in dag.tasks:\n            settings.policy(task)\n\n        subdags = dag.subdags\n\n        try:\n            for subdag in subdags:\n                subdag.full_filepath = dag.full_filepath\n                subdag.parent_dag = dag\n                subdag.is_subdag = True\n                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)\n\n            self.dags[dag.dag_id] = dag\n            self.log.debug('Loaded DAG %s', dag)\n        except AirflowDagCycleException as cycle_exception:\n            \n            self.log.exception('Exception bagging dag: %s', dag.dag_id)\n            \n            \n            if dag == root_dag:\n                for subdag in subdags:\n                    if subdag.dag_id in self.dags:\n                        del self.dags[subdag.dag_id]\n            raise cycle_exception"
    },
    {
        "docstring": "Given a file path or a folder, this method looks for python modules,\n        imports them and adds them to the dagbag collection.\n\n        Note that if a ``.airflowignore`` file is found while processing\n        the directory, it will behave much like a ``.gitignore``,\n        ignoring files that match any of the regex patterns specified\n        in the file.\n\n        **Note**: The patterns in .airflowignore are treated as\n        un-anchored regexes, not shell-like glob patterns.",
        "code": "def foo(\n            self,\n            dag_folder=None,\n            only_if_updated=True,\n            include_examples=configuration.conf.getboolean('core', 'LOAD_EXAMPLES'),\n            safe_mode=configuration.conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE')):\n        \n        start_dttm = timezone.utcnow()\n        dag_folder = dag_folder or self.dag_folder\n\n        \n        stats = []\n        FileLoadStat = namedtuple(\n            'FileLoadStat', \"file duration dag_num task_num dags\")\n\n        dag_folder = correct_maybe_zipped(dag_folder)\n\n        for filepath in list_py_file_paths(dag_folder, safe_mode=safe_mode,\n                                           include_examples=include_examples):\n            try:\n                ts = timezone.utcnow()\n                found_dags = self.process_file(\n                    filepath, only_if_updated=only_if_updated,\n                    safe_mode=safe_mode)\n\n                td = timezone.utcnow() - ts\n                td = td.total_seconds() + (\n                    float(td.microseconds) / 1000000)\n                stats.append(FileLoadStat(\n                    filepath.replace(dag_folder, ''),\n                    td,\n                    len(found_dags),\n                    sum([len(dag.tasks) for dag in found_dags]),\n                    str([dag.dag_id for dag in found_dags]),\n                ))\n            except Exception as e:\n                self.log.exception(e)\n        Stats.gauge(\n            'collect_dags', (timezone.utcnow() - start_dttm).total_seconds(), 1)\n        Stats.gauge(\n            'dagbag_size', len(self.dags), 1)\n        Stats.gauge(\n            'dagbag_import_errors', len(self.import_errors), 1)\n        self.dagbag_stats = sorted(\n            stats, key=lambda x: x.duration, reverse=True)"
    },
    {
        "docstring": "Prints a report around DagBag loading stats",
        "code": "def foo(self):\n        \n        report = textwrap.dedent()\n        stats = self.dagbag_stats\n        return report.format(\n            dag_folder=self.dag_folder,\n            duration=sum([o.duration for o in stats]),\n            dag_num=sum([o.dag_num for o in stats]),\n            task_num=sum([o.task_num for o in stats]),\n            table=pprinttable(stats),\n        )"
    },
    {
        "docstring": "Add or subtract days from a YYYY-MM-DD\n\n    :param ds: anchor date in ``YYYY-MM-DD`` format to add to\n    :type ds: str\n    :param days: number of days to add to the ds, you can use negative values\n    :type days: int\n\n    >>> ds_add('2015-01-01', 5)\n    '2015-01-06'\n    >>> ds_add('2015-01-06', -5)\n    '2015-01-01'",
        "code": "def foo(ds, days):\n    \n\n    ds = datetime.strptime(ds, '%Y-%m-%d')\n    if days:\n        ds = ds + timedelta(days)\n    return ds.isoformat()[:10]"
    },
    {
        "docstring": "Takes an input string and outputs another string\n    as specified in the output format\n\n    :param ds: input string which contains a date\n    :type ds: str\n    :param input_format: input string format. E.g. %Y-%m-%d\n    :type input_format: str\n    :param output_format: output string format  E.g. %Y-%m-%d\n    :type output_format: str\n\n    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n    '01-01-15'\n    >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")\n    '2015-01-05'",
        "code": "def foo(ds, input_format, output_format):\n    \n    return datetime.strptime(ds, input_format).strftime(output_format)"
    },
    {
        "docstring": "poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria",
        "code": "def foo(self, context):\n        \n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)"
    },
    {
        "docstring": "poke for a non empty directory\n\n        :return: Bool depending on the search criteria",
        "code": "def foo(self, context):\n        \n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        if self.be_empty:\n            self.log.info('Poking for filepath %s to a empty directory', self.filepath)\n            return len(result) == 1 and result[0]['path'] == self.filepath\n        else:\n            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)\n            result.pop(0)\n            return bool(result) and result[0]['file_type'] == 'f'"
    },
    {
        "docstring": "Clears a set of task instances, but makes sure the running ones\n    get killed.\n\n    :param tis: a list of task instances\n    :param session: current session\n    :param activate_dag_runs: flag to check for active dag run\n    :param dag: DAG object",
        "code": "def foo(tis,\n                         session,\n                         activate_dag_runs=True,\n                         dag=None,\n                         ):\n    \n    job_ids = []\n    for ti in tis:\n        if ti.state == State.RUNNING:\n            if ti.job_id:\n                ti.state = State.SHUTDOWN\n                job_ids.append(ti.job_id)\n        else:\n            task_id = ti.task_id\n            if dag and dag.has_task(task_id):\n                task = dag.get_task(task_id)\n                task_retries = task.retries\n                ti.max_tries = ti.try_number + task_retries - 1\n            else:\n                \n                \n                \n                \n                ti.max_tries = max(ti.max_tries, ti.try_number - 1)\n            ti.state = State.NONE\n            session.merge(ti)\n\n    if job_ids:\n        from airflow.jobs import BaseJob as BJ\n        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():\n            job.state = State.SHUTDOWN\n\n    if activate_dag_runs and tis:\n        from airflow.models.dagrun import DagRun  \n        drs = session.query(DagRun).filter(\n            DagRun.dag_id.in_({ti.dag_id for ti in tis}),\n            DagRun.execution_date.in_({ti.execution_date for ti in tis}),\n        ).all()\n        for dr in drs:\n            dr.state = State.RUNNING\n            dr.start_date = timezone.utcnow()"
    },
    {
        "docstring": "Return the try number that this task number will be when it is actually\n        run.\n\n        If the TI is currently running, this will match the column in the\n        databse, in all othercases this will be incremenetd",
        "code": "def foo(self):\n        \n        \n        if self.state == State.RUNNING:\n            return self._try_number\n        return self._try_number + 1"
    },
    {
        "docstring": "Generates the shell command required to execute this task instance.\n\n        :param dag_id: DAG ID\n        :type dag_id: unicode\n        :param task_id: Task ID\n        :type task_id: unicode\n        :param execution_date: Execution date for the task\n        :type execution_date: datetime\n        :param mark_success: Whether to mark the task as successful\n        :type mark_success: bool\n        :param ignore_all_deps: Ignore all ignorable dependencies.\n            Overrides the other ignore_* parameters.\n        :type ignore_all_deps: bool\n        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs\n            (e.g. for Backfills)\n        :type ignore_depends_on_past: bool\n        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past\n            and trigger rule\n        :type ignore_task_deps: bool\n        :param ignore_ti_state: Ignore the task instance's previous failure/success\n        :type ignore_ti_state: bool\n        :param local: Whether to run the task locally\n        :type local: bool\n        :param pickle_id: If the DAG was serialized to the DB, the ID\n            associated with the pickled DAG\n        :type pickle_id: unicode\n        :param file_path: path to the file containing the DAG definition\n        :param raw: raw mode (needs more details)\n        :param job_id: job ID (needs more details)\n        :param pool: the Airflow pool that the task should run in\n        :type pool: unicode\n        :param cfg_path: the Path to the configuration file\n        :type cfg_path: basestring\n        :return: shell command that can be used to run the task instance",
        "code": "def foo(dag_id,\n                         task_id,\n                         execution_date,\n                         mark_success=False,\n                         ignore_all_deps=False,\n                         ignore_depends_on_past=False,\n                         ignore_task_deps=False,\n                         ignore_ti_state=False,\n                         local=False,\n                         pickle_id=None,\n                         file_path=None,\n                         raw=False,\n                         job_id=None,\n                         pool=None,\n                         cfg_path=None\n                         ):\n        \n        iso = execution_date.isoformat()\n        cmd = [\"airflow\", \"run\", str(dag_id), str(task_id), str(iso)]\n        cmd.extend([\"--mark_success\"]) if mark_success else None\n        cmd.extend([\"--pickle\", str(pickle_id)]) if pickle_id else None\n        cmd.extend([\"--job_id\", str(job_id)]) if job_id else None\n        cmd.extend([\"-A\"]) if ignore_all_deps else None\n        cmd.extend([\"-i\"]) if ignore_task_deps else None\n        cmd.extend([\"-I\"]) if ignore_depends_on_past else None\n        cmd.extend([\"--force\"]) if ignore_ti_state else None\n        cmd.extend([\"--local\"]) if local else None\n        cmd.extend([\"--pool\", pool]) if pool else None\n        cmd.extend([\"--raw\"]) if raw else None\n        cmd.extend([\"-sd\", file_path]) if file_path else None\n        cmd.extend([\"--cfg_path\", cfg_path]) if cfg_path else None\n        return cmd"
    },
    {
        "docstring": "Get the very latest state from the database, if a session is passed,\n        we use and looking up the state becomes part of the session, otherwise\n        a new session is used.",
        "code": "def foo(self, session=None):\n        \n        TI = TaskInstance\n        ti = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date,\n        ).all()\n        if ti:\n            state = ti[0].state\n        else:\n            state = None\n        return state"
    },
    {
        "docstring": "Forces the task instance's state to FAILED in the database.",
        "code": "def foo(self, session=None):\n        \n        self.log.error(\"Recording the task instance as FAILED\")\n        self.state = State.FAILED\n        session.merge(self)\n        session.commit()"
    },
    {
        "docstring": "Refreshes the task instance from the database based on the primary key\n\n        :param lock_for_update: if True, indicates that the database should\n            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n            session is committed.",
        "code": "def foo(self, session=None, lock_for_update=False):\n        \n        TI = TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        if ti:\n            self.state = ti.state\n            self.start_date = ti.start_date\n            self.end_date = ti.end_date\n            \n            \n            self.try_number = ti._try_number\n            self.max_tries = ti.max_tries\n            self.hostname = ti.hostname\n            self.pid = ti.pid\n            self.executor_config = ti.executor_config\n        else:\n            self.state = None"
    },
    {
        "docstring": "Clears all XCom data from the database for the task instance",
        "code": "def foo(self, session=None):\n        \n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()"
    },
    {
        "docstring": "Returns a tuple that identifies the task instance uniquely",
        "code": "def foo(self):\n        \n        return self.dag_id, self.task_id, self.execution_date, self.try_number"
    },
    {
        "docstring": "Checks whether the dependents of this task instance have all succeeded.\n        This is meant to be used by wait_for_downstream.\n\n        This is useful when you do not want to start processing the next\n        schedule of a task until the dependents are done. For instance,\n        if the task DROPs and recreates a table.",
        "code": "def foo(self, session=None):\n        \n        task = self.task\n\n        if not task.downstream_task_ids:\n            return True\n\n        ti = session.query(func.count(TaskInstance.task_id)).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.task_id.in_(task.downstream_task_ids),\n            TaskInstance.execution_date == self.execution_date,\n            TaskInstance.state == State.SUCCESS,\n        )\n        count = ti[0][0]\n        return count == len(task.downstream_task_ids)"
    },
    {
        "docstring": "Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.",
        "code": "def foo(self):\n        \n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            \n            hash = int(hashlib.sha1(\"{}\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            \n            modded_hash = min_backoff + hash % min_backoff\n            \n            \n            \n            \n            \n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay"
    },
    {
        "docstring": "Checks on whether the task instance is in the right state and timeframe\n        to be retried.",
        "code": "def foo(self):\n        \n        return (self.state == State.UP_FOR_RETRY and\n                self.next_retry_datetime() < timezone.utcnow())"
    },
    {
        "docstring": "Returns a boolean as to whether the slot pool has room for this\n        task to run",
        "code": "def foo(self, session):\n        \n        if not self.task.pool:\n            return False\n\n        pool = (\n            session\n            .query(Pool)\n            .filter(Pool.pool == self.task.pool)\n            .first()\n        )\n        if not pool:\n            return False\n        open_slots = pool.open_slots(session=session)\n\n        return open_slots <= 0"
    },
    {
        "docstring": "Returns the DagRun for this TaskInstance\n\n        :param session:\n        :return: DagRun",
        "code": "def foo(self, session):\n        \n        from airflow.models.dagrun import DagRun  \n        dr = session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date == self.execution_date\n        ).first()\n\n        return dr"
    },
    {
        "docstring": "Make an XCom available for tasks to pull.\n\n        :param key: A key for the XCom\n        :type key: str\n        :param value: A value for the XCom. The value is pickled and stored\n            in the database.\n        :type value: any pickleable object\n        :param execution_date: if provided, the XCom will not be visible until\n            this date. This can be used, for example, to send a message to a\n            task on a future date without it being immediately visible.\n        :type execution_date: datetime",
        "code": "def foo(\n            self,\n            key,\n            value,\n            execution_date=None):\n        \n\n        if execution_date and execution_date < self.execution_date:\n            raise ValueError(\n                'execution_date can not be in the past (current '\n                'execution_date is {}; received {})'.format(\n                    self.execution_date, execution_date))\n\n        XCom.set(\n            key=key,\n            value=value,\n            task_id=self.task_id,\n            dag_id=self.dag_id,\n            execution_date=execution_date or self.execution_date)"
    },
    {
        "docstring": "Pull XComs that optionally meet certain criteria.\n\n        The default value for `key` limits the search to XComs\n        that were returned by other tasks (as opposed to those that were pushed\n        manually). To remove this filter, pass key=None (or any desired value).\n\n        If a single task_id string is provided, the result is the value of the\n        most recent matching XCom from that task_id. If multiple task_ids are\n        provided, a tuple of matching values is returned. None is returned\n        whenever no matches are found.\n\n        :param key: A key for the XCom. If provided, only XComs with matching\n            keys will be returned. The default key is 'return_value', also\n            available as a constant XCOM_RETURN_KEY. This key is automatically\n            given to XComs returned by tasks (as opposed to being pushed\n            manually). To remove the filter, pass key=None.\n        :type key: str\n        :param task_ids: Only XComs from tasks with matching ids will be\n            pulled. Can pass None to remove the filter.\n        :type task_ids: str or iterable of strings (representing task_ids)\n        :param dag_id: If provided, only pulls XComs from this DAG.\n            If None (default), the DAG of the calling task is used.\n        :type dag_id: str\n        :param include_prior_dates: If False, only XComs from the current\n            execution_date are returned. If True, XComs from previous dates\n            are returned as well.\n        :type include_prior_dates: bool",
        "code": "def foo(\n            self,\n            task_ids=None,\n            dag_id=None,\n            key=XCOM_RETURN_KEY,\n            include_prior_dates=False):\n        \n\n        if dag_id is None:\n            dag_id = self.dag_id\n\n        pull_fn = functools.partial(\n            XCom.get_one,\n            execution_date=self.execution_date,\n            key=key,\n            dag_id=dag_id,\n            include_prior_dates=include_prior_dates)\n\n        if is_container(task_ids):\n            return tuple(pull_fn(task_id=t) for t in task_ids)\n        else:\n            return pull_fn(task_id=task_ids)"
    },
    {
        "docstring": "Sets the log context.",
        "code": "def foo(self, raw=False):\n        \n        self.raw = raw\n        self._set_context(self)"
    },
    {
        "docstring": "Close and upload local log file to remote storage Wasb.",
        "code": "def foo(self):\n        \n        \n        \n        \n        \n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            \n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.wasb_write(log, remote_loc, append=True)\n\n            if self.delete_local_copy:\n                shutil.rmtree(os.path.dirname(local_loc))\n        \n        self.closed = True"
    },
    {
        "docstring": "Retrieves connection to Google Compute Engine.\n\n        :return: Google Compute Engine services object\n        :rtype: dict",
        "code": "def foo(self):\n        \n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build('compute', self.api_version,\n                               http=http_authorized, cache_discovery=False)\n        return self._conn"
    },
    {
        "docstring": "Starts an existing instance defined by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the instance exists\n        :type zone: str\n        :param resource_id: Name of the Compute Engine instance resource\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, zone, resource_id, project_id=None):\n        \n        response = self.get_conn().instances().start(\n            project=project_id,\n            zone=zone,\n            instance=resource_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)"
    },
    {
        "docstring": "Sets machine type of an instance defined by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the instance exists.\n        :type zone: str\n        :param resource_id: Name of the Compute Engine instance resource\n        :type resource_id: str\n        :param body: Body required by the Compute Engine setMachineType API,\n            as described in\n            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType\n        :type body: dict\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, zone, resource_id, body, project_id=None):\n        \n        response = self._execute_set_machine_type(zone, resource_id, body, project_id)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)"
    },
    {
        "docstring": "Retrieves instance template by project_id and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param resource_id: Name of the instance template\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :rtype: dict",
        "code": "def foo(self, resource_id, project_id=None):\n        \n        response = self.get_conn().instanceTemplates().get(\n            project=project_id,\n            instanceTemplate=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response"
    },
    {
        "docstring": "Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, body, request_id=None, project_id=None):\n        \n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"
    },
    {
        "docstring": "Retrieves Instance Group Manager by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Instance group manager representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers\n        :rtype: dict",
        "code": "def foo(self, zone, resource_id, project_id=None):\n        \n        response = self.get_conn().instanceGroupManagers().get(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response"
    },
    {
        "docstring": "Patches Instance Group Manager with the specified body.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param body: Instance Group Manager representation as json-merge-patch object\n            according to\n            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again).\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, zone, resource_id,\n                                     body, request_id=None, project_id=None):\n        \n        response = self.get_conn().instanceGroupManagers().patch(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)"
    },
    {
        "docstring": "Waits for the named operation to complete - checks status of the async call.\n\n        :param operation_name: name of the operation\n        :type operation_name: str\n        :param zone: optional region of the request (might be None for global operations)\n        :type zone: str\n        :return: None",
        "code": "def foo(self, project_id, operation_name, zone=None):\n        \n        service = self.get_conn()\n        while True:\n            if zone is None:\n                \n                operation_response = self._check_global_operation_status(\n                    service, operation_name, project_id)\n            else:\n                \n                operation_response = self._check_zone_operation_status(\n                    service, operation_name, project_id, zone, self.num_retries)\n            if operation_response.get(\"status\") == GceOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    code = operation_response.get(\"httpErrorStatusCode\")\n                    msg = operation_response.get(\"httpErrorMessage\")\n                    \n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(\"{} {}: \".format(code, msg) + error_msg)\n                \n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)"
    },
    {
        "docstring": "Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str",
        "code": "def foo(self, bucket_name):\n        \n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False"
    },
    {
        "docstring": "Creates an Amazon S3 bucket.\n\n        :param bucket_name: The name of the bucket\n        :type bucket_name: str\n        :param region_name: The name of the aws region in which to create the bucket.\n        :type region_name: str",
        "code": "def foo(self, bucket_name, region_name=None):\n        \n        s3_conn = self.get_conn()\n        if not region_name:\n            region_name = s3_conn.meta.region_name\n        if region_name == 'us-east-1':\n            self.get_conn().create_bucket(Bucket=bucket_name)\n        else:\n            self.get_conn().create_bucket(Bucket=bucket_name,\n                                          CreateBucketConfiguration={\n                                              'LocationConstraint': region_name\n                                          })"
    },
    {
        "docstring": "Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str",
        "code": "def foo(self, bucket_name, prefix, delimiter):\n        \n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist"
    },
    {
        "docstring": "Lists prefixes in a bucket under prefix\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int",
        "code": "def foo(self, bucket_name, prefix='', delimiter='',\n                      page_size=None, max_items=None):\n        \n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        prefixes = []\n        for page in response:\n            if 'CommonPrefixes' in page:\n                has_results = True\n                for p in page['CommonPrefixes']:\n                    prefixes.append(p['Prefix'])\n\n        if has_results:\n            return prefixes"
    },
    {
        "docstring": "Lists keys in a bucket under prefix and not containing delimiter\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int",
        "code": "def foo(self, bucket_name, prefix='', delimiter='',\n                  page_size=None, max_items=None):\n        \n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        keys = []\n        for page in response:\n            if 'Contents' in page:\n                has_results = True\n                for k in page['Contents']:\n                    keys.append(k['Key'])\n\n        if has_results:\n            return keys"
    },
    {
        "docstring": "Checks if a key exists in a bucket\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str",
        "code": "def foo(self, key, bucket_name=None):\n        \n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        try:\n            self.get_conn().head_object(Bucket=bucket_name, Key=key)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False"
    },
    {
        "docstring": "Returns a boto3.s3.Object\n\n        :param key: the path to the key\n        :type key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str",
        "code": "def foo(self, key, bucket_name=None):\n        \n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        obj = self.get_resource_type('s3').Object(bucket_name, key)\n        obj.load()\n        return obj"
    },
    {
        "docstring": "Reads a key from S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str",
        "code": "def foo(self, key, bucket_name=None):\n        \n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')"
    },
    {
        "docstring": "Reads a key with S3 Select.\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        :param expression: S3 Select expression\n        :type expression: str\n        :param expression_type: S3 Select expression type\n        :type expression_type: str\n        :param input_serialization: S3 Select input data serialization format\n        :type input_serialization: dict\n        :param output_serialization: S3 Select output data serialization format\n        :type output_serialization: dict\n        :return: retrieved subset of original data by S3 Select\n        :rtype: str\n\n        .. seealso::\n            For more details about S3 Select parameters:\n            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content",
        "code": "def foo(self, key, bucket_name=None,\n                   expression='SELECT * FROM S3Object',\n                   expression_type='SQL',\n                   input_serialization=None,\n                   output_serialization=None):\n        \n        if input_serialization is None:\n            input_serialization = {'CSV': {}}\n        if output_serialization is None:\n            output_serialization = {'CSV': {}}\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        response = self.get_conn().select_object_content(\n            Bucket=bucket_name,\n            Key=key,\n            Expression=expression,\n            ExpressionType=expression_type,\n            InputSerialization=input_serialization,\n            OutputSerialization=output_serialization)\n\n        return ''.join(event['Records']['Payload'].decode('utf-8')\n                       for event in response['Payload']\n                       if 'Records' in event)"
    },
    {
        "docstring": "Checks that a key matching a wildcard expression exists in a bucket\n\n        :param wildcard_key: the path to the key\n        :type wildcard_key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param delimiter: the delimiter marks key hierarchy\n        :type delimiter: str",
        "code": "def foo(self,\n                               wildcard_key, bucket_name=None, delimiter=''):\n        \n        return self.get_wildcard_key(wildcard_key=wildcard_key,\n                                     bucket_name=bucket_name,\n                                     delimiter=delimiter) is not None"
    },
    {
        "docstring": "Returns a boto3.s3.Object object matching the wildcard expression\n\n        :param wildcard_key: the path to the key\n        :type wildcard_key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param delimiter: the delimiter marks key hierarchy\n        :type delimiter: str",
        "code": "def foo(self, wildcard_key, bucket_name=None, delimiter=''):\n        \n        if not bucket_name:\n            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)\n\n        prefix = re.split(r'[*]', wildcard_key, 1)[0]\n        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)\n        if klist:\n            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]\n            if key_matches:\n                return self.get_key(key_matches[0], bucket_name)"
    },
    {
        "docstring": "Loads a local file to S3\n\n        :param filename: name of the file to load.\n        :type filename: str\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists. If replace is False and the key exists, an\n            error will be raised.\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool",
        "code": "def foo(self,\n                  filename,\n                  key,\n                  bucket_name=None,\n                  replace=False,\n                  encrypt=False):\n        \n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)"
    },
    {
        "docstring": "Loads a string to S3\n\n        This is provided as a convenience to drop a string in S3. It uses the\n        boto infrastructure to ship a file to s3.\n\n        :param string_data: str to set as content for the key.\n        :type string_data: str\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool",
        "code": "def foo(self,\n                    string_data,\n                    key,\n                    bucket_name=None,\n                    replace=False,\n                    encrypt=False,\n                    encoding='utf-8'):\n        \n        self.load_bytes(string_data.encode(encoding),\n                        key=key,\n                        bucket_name=bucket_name,\n                        replace=replace,\n                        encrypt=encrypt)"
    },
    {
        "docstring": "Loads bytes to S3\n\n        This is provided as a convenience to drop a string in S3. It uses the\n        boto infrastructure to ship a file to s3.\n\n        :param bytes_data: bytes to set as content for the key.\n        :type bytes_data: bytes\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool",
        "code": "def foo(self,\n                   bytes_data,\n                   key,\n                   bucket_name=None,\n                   replace=False,\n                   encrypt=False):\n        \n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        filelike_buffer = BytesIO(bytes_data)\n\n        client = self.get_conn()\n        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)"
    },
    {
        "docstring": "Loads a file object to S3\n\n        :param file_obj: The file-like object to set as the content for the S3 key.\n        :type file_obj: file-like object\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag that indicates whether to overwrite the key\n            if it already exists.\n        :type replace: bool\n        :param encrypt: If True, S3 encrypts the file on the server,\n            and the file is stored in encrypted form at rest in S3.\n        :type encrypt: bool",
        "code": "def foo(self,\n                      file_obj,\n                      key,\n                      bucket_name=None,\n                      replace=False,\n                      encrypt=False):\n        \n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)"
    },
    {
        "docstring": "Creates a copy of an object that is already stored in S3.\n\n        Note: the S3 connection used here needs to have access to both\n        source and destination bucket/key.\n\n        :param source_bucket_key: The key of the source object.\n\n            It can be either full s3:// style url or relative path from root level.\n\n            When it's specified as a full s3:// url, please omit source_bucket_name.\n        :type source_bucket_key: str\n        :param dest_bucket_key: The key of the object to copy to.\n\n            The convention to specify `dest_bucket_key` is the same\n            as `source_bucket_key`.\n        :type dest_bucket_key: str\n        :param source_bucket_name: Name of the S3 bucket where the source object is in.\n\n            It should be omitted when `source_bucket_key` is provided as a full s3:// url.\n        :type source_bucket_name: str\n        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.\n\n            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.\n        :type dest_bucket_name: str\n        :param source_version_id: Version ID of the source object (OPTIONAL)\n        :type source_version_id: str",
        "code": "def foo(self,\n                    source_bucket_key,\n                    dest_bucket_key,\n                    source_bucket_name=None,\n                    dest_bucket_name=None,\n                    source_version_id=None):\n        \n\n        if dest_bucket_name is None:\n            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)\n        else:\n            parsed_url = urlparse(dest_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If dest_bucket_name is provided, ' +\n                                       'dest_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:// url')\n\n        if source_bucket_name is None:\n            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)\n        else:\n            parsed_url = urlparse(source_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If source_bucket_name is provided, ' +\n                                       'source_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:// url')\n\n        CopySource = {'Bucket': source_bucket_name,\n                      'Key': source_bucket_key,\n                      'VersionId': source_version_id}\n        response = self.get_conn().copy_object(Bucket=dest_bucket_name,\n                                               Key=dest_bucket_key,\n                                               CopySource=CopySource)\n        return response"
    },
    {
        "docstring": "Queries cassandra and returns a cursor to the results.",
        "code": "def foo(self):\n        \n        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)\n        session = self.hook.get_conn()\n        cursor = session.execute(self.cql)\n        return cursor"
    },
    {
        "docstring": "Converts a user type to RECORD that contains n fields, where n is the\n        number of attributes. Each element in the user type class will be converted to its\n        corresponding data type in BQ.",
        "code": "def foo(cls, name, value):\n        \n        names = value._fields\n        values = [cls.convert_value(name, getattr(value, name)) for name in names]\n        return cls.generate_data_dict(names, values)"
    },
    {
        "docstring": "Send an email with html content using sendgrid.\n\n    To use this plugin:\n    0. include sendgrid subpackage as part of your Airflow installation, e.g.,\n    pip install 'apache-airflow[sendgrid]'\n    1. update [email] backend in airflow.cfg, i.e.,\n    [email]\n    email_backend = airflow.contrib.utils.sendgrid.send_email\n    2. configure Sendgrid specific environment variables at all Airflow instances:\n    SENDGRID_MAIL_FROM={your-mail-from}\n    SENDGRID_API_KEY={your-sendgrid-api-key}.",
        "code": "def foo(to, subject, html_content, files=None, dryrun=False, cc=None,\n               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):\n    \n    if files is None:\n        files = []\n\n    mail = Mail()\n    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')\n    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')\n    mail.from_email = Email(from_email, from_name)\n    mail.subject = subject\n    mail.mail_settings = MailSettings()\n\n    if sandbox_mode:\n        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)\n\n    \n    personalization = Personalization()\n    to = get_email_address_list(to)\n    for to_address in to:\n        personalization.add_to(Email(to_address))\n    if cc:\n        cc = get_email_address_list(cc)\n        for cc_address in cc:\n            personalization.add_cc(Email(cc_address))\n    if bcc:\n        bcc = get_email_address_list(bcc)\n        for bcc_address in bcc:\n            personalization.add_bcc(Email(bcc_address))\n\n    \n    pers_custom_args = kwargs.get('personalization_custom_args', None)\n    if isinstance(pers_custom_args, dict):\n        for key in pers_custom_args.keys():\n            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))\n\n    mail.add_personalization(personalization)\n    mail.add_content(Content('text/html', html_content))\n\n    categories = kwargs.get('categories', [])\n    for cat in categories:\n        mail.add_category(Category(cat))\n\n    \n    for fname in files:\n        basename = os.path.basename(fname)\n\n        attachment = Attachment()\n        attachment.type = mimetypes.guess_type(basename)[0]\n        attachment.filename = basename\n        attachment.disposition = \"attachment\"\n        attachment.content_id = '<{0}>'.format(basename)\n\n        with open(fname, \"rb\") as f:\n            attachment.content = base64.b64encode(f.read()).decode('utf-8')\n\n        mail.add_attachment(attachment)\n    _post_sendgrid_mail(mail.get())"
    },
    {
        "docstring": "Retrieves connection to Cloud Speech.\n\n        :return: Google Cloud Speech client object.\n        :rtype: google.cloud.speech_v1.SpeechClient",
        "code": "def foo(self):\n        \n        if not self._client:\n            self._client = SpeechClient(credentials=self._get_credentials())\n        return self._client"
    },
    {
        "docstring": "Recognizes audio input\n\n        :param config: information to the recognizer that specifies how to process the request.\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n        :param audio: audio data to be recognized\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio\n        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float",
        "code": "def foo(self, config, audio, retry=None, timeout=None):\n        \n        client = self.get_conn()\n        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response"
    },
    {
        "docstring": "Call the SparkSqlHook to run the provided sql query",
        "code": "def foo(self, context):\n        \n        self._hook = SparkSqlHook(sql=self._sql,\n                                  conf=self._conf,\n                                  conn_id=self._conn_id,\n                                  total_executor_cores=self._total_executor_cores,\n                                  executor_cores=self._executor_cores,\n                                  executor_memory=self._executor_memory,\n                                  keytab=self._keytab,\n                                  principal=self._principal,\n                                  name=self._name,\n                                  num_executors=self._num_executors,\n                                  master=self._master,\n                                  yarn_queue=self._yarn_queue\n                                  )\n        self._hook.run_query()"
    },
    {
        "docstring": "Load AirflowPlugin subclasses from the entrypoints\n    provided. The entry_point group should be 'airflow.plugins'.\n\n    :param entry_points: A collection of entrypoints to search for plugins\n    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n    :param airflow_plugins: A collection of existing airflow plugins to\n        ensure we don't load duplicates\n    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]\n    :rtype: list[airflow.plugins_manager.AirflowPlugin]",
        "code": "def foo(entry_points, airflow_plugins):\n    \n    for entry_point in entry_points:\n        log.debug('Importing entry_point plugin %s', entry_point.name)\n        plugin_obj = entry_point.load()\n        if is_valid_plugin(plugin_obj, airflow_plugins):\n            if callable(getattr(plugin_obj, 'on_load', None)):\n                plugin_obj.on_load()\n                airflow_plugins.append(plugin_obj)\n    return airflow_plugins"
    },
    {
        "docstring": "Check whether a potential object is a subclass of\n    the AirflowPlugin class.\n\n    :param plugin_obj: potential subclass of AirflowPlugin\n    :param existing_plugins: Existing list of AirflowPlugin subclasses\n    :return: Whether or not the obj is a valid subclass of\n        AirflowPlugin",
        "code": "def foo(plugin_obj, existing_plugins):\n    \n    if (\n        inspect.isclass(plugin_obj) and\n        issubclass(plugin_obj, AirflowPlugin) and\n        (plugin_obj is not AirflowPlugin)\n    ):\n        plugin_obj.validate()\n        return plugin_obj not in existing_plugins\n    return False"
    },
    {
        "docstring": "Sets tasks instances to skipped from the same dag run.\n\n        :param dag_run: the DagRun for which to set the tasks to skipped\n        :param execution_date: execution_date\n        :param tasks: tasks to skip (not task_ids)\n        :param session: db session to use",
        "code": "def foo(self, dag_run, execution_date, tasks, session=None):\n        \n        if not tasks:\n            return\n\n        task_ids = [d.task_id for d in tasks]\n        now = timezone.utcnow()\n\n        if dag_run:\n            session.query(TaskInstance).filter(\n                TaskInstance.dag_id == dag_run.dag_id,\n                TaskInstance.execution_date == dag_run.execution_date,\n                TaskInstance.task_id.in_(task_ids)\n            ).update({TaskInstance.state: State.SKIPPED,\n                      TaskInstance.start_date: now,\n                      TaskInstance.end_date: now},\n                     synchronize_session=False)\n            session.commit()\n        else:\n            assert execution_date is not None, \"Execution date is None and no dag run\"\n\n            self.log.warning(\"No DAG RUN present this should not happen\")\n            \n            for task in tasks:\n                ti = TaskInstance(task, execution_date=execution_date)\n                ti.state = State.SKIPPED\n                ti.start_date = now\n                ti.end_date = now\n                session.merge(ti)\n\n            session.commit()"
    },
    {
        "docstring": "Return a AzureDLFileSystem object.",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        self.account_name = service_options.get('account_name')\n\n        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),\n                            client_secret=conn.password,\n                            client_id=conn.login)\n        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,\n                                                      store_name=self.account_name)\n        adlsFileSystemClient.connect()\n        return adlsFileSystemClient"
    },
    {
        "docstring": "Check if a file exists on Azure Data Lake.\n\n        :param file_path: Path and name of the file.\n        :type file_path: str\n        :return: True if the file exists, False otherwise.\n        :rtype: bool",
        "code": "def foo(self, file_path):\n        \n        try:\n            files = self.connection.glob(file_path, details=False, invalidate_cache=True)\n            return len(files) == 1\n        except FileNotFoundError:\n            return False"
    },
    {
        "docstring": "Upload a file to Azure Data Lake.\n\n        :param local_path: local path. Can be single file, directory (in which case,\n            upload recursively) or glob pattern. Recursive glob patterns using `**`\n            are not supported.\n        :type local_path: str\n        :param remote_path: Remote path to upload to; if multiple files, this is the\n            directory root to write within.\n        :type remote_path: str\n        :param nthreads: Number of threads to use. If None, uses the number of cores.\n        :type nthreads: int\n        :param overwrite: Whether to forcibly overwrite existing files/directories.\n            If False and remote path is a directory, will quit regardless if any files\n            would be overwritten or not. If True, only matching filenames are actually\n            overwritten.\n        :type overwrite: bool\n        :param buffersize: int [2**22]\n            Number of bytes for internal buffer. This block cannot be bigger than\n            a chunk and cannot be smaller than a block.\n        :type buffersize: int\n        :param blocksize: int [2**22]\n            Number of bytes for a block. Within each chunk, we write a smaller\n            block for each API call. This block cannot be bigger than a chunk.\n        :type blocksize: int",
        "code": "def foo(self, local_path, remote_path, nthreads=64, overwrite=True,\n                    buffersize=4194304, blocksize=4194304):\n        \n        multithread.ADLUploader(self.connection,\n                                lpath=local_path,\n                                rpath=remote_path,\n                                nthreads=nthreads,\n                                overwrite=overwrite,\n                                buffersize=buffersize,\n                                blocksize=blocksize)"
    },
    {
        "docstring": "List files in Azure Data Lake Storage\n\n        :param path: full path/globstring to use to list files in ADLS\n        :type path: str",
        "code": "def foo(self, path):\n        \n        if \"*\" in path:\n            return self.connection.glob(path)\n        else:\n            return self.connection.walk(path)"
    },
    {
        "docstring": "Run Presto Query on Athena",
        "code": "def foo(self, context):\n        \n        self.hook = self.get_hook()\n        self.hook.get_conn()\n\n        self.query_execution_context['Database'] = self.database\n        self.result_configuration['OutputLocation'] = self.output_location\n        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,\n                                                      self.result_configuration, self.client_request_token)\n        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)\n\n        if query_status in AWSAthenaHook.FAILURE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))\n        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}. '\n                'Max tries of poll status exceeded, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))"
    },
    {
        "docstring": "Uncompress gz and bz2 files",
        "code": "def foo(input_file_name, file_extension, dest_dir):\n    \n    if file_extension.lower() not in ('.gz', '.bz2'):\n        raise NotImplementedError(\"Received {} format. Only gz and bz2 \"\n                                  \"files can currently be uncompressed.\"\n                                  .format(file_extension))\n    if file_extension.lower() == '.gz':\n        fmodule = gzip.GzipFile\n    elif file_extension.lower() == '.bz2':\n        fmodule = bz2.BZ2File\n    with fmodule(input_file_name, mode='rb') as f_compressed,\\\n        NamedTemporaryFile(dir=dest_dir,\n                           mode='wb',\n                           delete=False) as f_uncompressed:\n        shutil.copyfileobj(f_compressed, f_uncompressed)\n    return f_uncompressed.name"
    },
    {
        "docstring": "Queries MSSQL and returns a cursor of results.\n\n        :return: mssql cursor",
        "code": "def foo(self):\n        \n        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)\n        conn = mssql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor"
    },
    {
        "docstring": "Decorates function to execute function at the same time submitting action_logging\n    but in CLI context. It will call action logger callbacks twice,\n    one for pre-execution and the other one for post-execution.\n\n    Action logger will be called with below keyword parameters:\n        sub_command : name of sub-command\n        start_datetime : start datetime instance by utc\n        end_datetime : end datetime instance by utc\n        full_command : full command line arguments\n        user : current user\n        log : airflow.models.log.Log ORM instance\n        dag_id : dag id (optional)\n        task_id : task_id (optional)\n        execution_date : execution date (optional)\n        error : exception instance if there's an exception\n\n    :param f: function instance\n    :return: wrapped function",
        "code": "def foo(f):\n    \n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        \n        assert args\n        assert isinstance(args[0], Namespace), \\\n            \"1st positional argument should be argparse.Namespace instance, \" \\\n            \"but {}\".format(args[0])\n        metrics = _build_metrics(f.__name__, args[0])\n        cli_action_loggers.on_pre_execution(**metrics)\n        try:\n            return f(*args, **kwargs)\n        except Exception as e:\n            metrics['error'] = e\n            raise\n        finally:\n            metrics['end_datetime'] = datetime.utcnow()\n            cli_action_loggers.on_post_execution(**metrics)\n\n    return wrapper"
    },
    {
        "docstring": "Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics",
        "code": "def foo(func_name, namespace):\n    \n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n\n    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))\n    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n        dag_id=metrics.get('dag_id'),\n        execution_date=metrics.get('execution_date'))\n    metrics['log'] = log\n    return metrics"
    },
    {
        "docstring": "Create the specified cgroup.\n\n        :param path: The path of the cgroup to create.\n        E.g. cpu/mygroup/mysubgroup\n        :return: the Node associated with the created cgroup.\n        :rtype: cgroupspy.nodes.Node",
        "code": "def foo(self, path):\n        \n        node = trees.Tree().root\n        path_split = path.split(os.sep)\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.debug(\"Creating cgroup %s in %s\", path_element, node.path)\n                node = node.create_cgroup(path_element)\n            else:\n                self.log.debug(\n                    \"Not creating cgroup %s in %s since it already exists\",\n                    path_element, node.path\n                )\n                node = name_to_node[path_element]\n        return node"
    },
    {
        "docstring": "Delete the specified cgroup.\n\n        :param path: The path of the cgroup to delete.\n        E.g. cpu/mygroup/mysubgroup",
        "code": "def foo(self, path):\n        \n        node = trees.Tree().root\n        path_split = path.split(\"/\")\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.warning(\"Cgroup does not exist: %s\", path)\n                return\n            else:\n                node = name_to_node[path_element]\n        \n        parent = node.parent\n        self.log.debug(\"Deleting cgroup %s/%s\", parent, node.name)\n        parent.delete_cgroup(node.name)"
    },
    {
        "docstring": "The purpose of this function is to be robust to improper connections\n        settings provided by users, specifically in the host field.\n\n        For example -- when users supply ``https://xx.cloud.databricks.com`` as the\n        host, we must strip out the protocol to get the host.::\n\n            h = DatabricksHook()\n            assert h._parse_host('https://xx.cloud.databricks.com') == \\\n                'xx.cloud.databricks.com'\n\n        In the case where users supply the correct ``xx.cloud.databricks.com`` as the\n        host, this function is a no-op.::\n\n            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'",
        "code": "def foo(host):\n        \n        urlparse_host = urlparse.urlparse(host).hostname\n        if urlparse_host:\n            \n            return urlparse_host\n        else:\n            \n            return host"
    },
    {
        "docstring": "Utility function to perform an API call with retries\n\n        :param endpoint_info: Tuple of method and endpoint\n        :type endpoint_info: tuple[string, string]\n        :param json: Parameters for this API call.\n        :type json: dict\n        :return: If the api call returns a OK status code,\n            this function returns the response in JSON. Otherwise,\n            we throw an AirflowException.\n        :rtype: dict",
        "code": "def foo(self, endpoint_info, json):\n        \n        method, endpoint = endpoint_info\n        url = 'https://{host}/{endpoint}'.format(\n            host=self._parse_host(self.databricks_conn.host),\n            endpoint=endpoint)\n        if 'token' in self.databricks_conn.extra_dejson:\n            self.log.info('Using token auth.')\n            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])\n        else:\n            self.log.info('Using basic auth.')\n            auth = (self.databricks_conn.login, self.databricks_conn.password)\n        if method == 'GET':\n            request_func = requests.get\n        elif method == 'POST':\n            request_func = requests.post\n        else:\n            raise AirflowException('Unexpected HTTP Method: ' + method)\n\n        attempt_num = 1\n        while True:\n            try:\n                response = request_func(\n                    url,\n                    json=json,\n                    auth=auth,\n                    headers=USER_AGENT_HEADER,\n                    timeout=self.timeout_seconds)\n                response.raise_for_status()\n                return response.json()\n            except requests_exceptions.RequestException as e:\n                if not _retryable_error(e):\n                    \n                    \n                    raise AirflowException('Response: {0}, Status Code: {1}'.format(\n                        e.response.content, e.response.status_code))\n\n                self._log_request_error(attempt_num, e)\n\n            if attempt_num == self.retry_limit:\n                raise AirflowException(('API requests to Databricks failed {} times. ' +\n                                        'Giving up.').format(self.retry_limit))\n\n            attempt_num += 1\n            sleep(self.retry_delay)"
    },
    {
        "docstring": "Sign into Salesforce, only if we are not already signed in.",
        "code": "def foo(self):\n        \n        if not self.conn:\n            connection = self.get_connection(self.conn_id)\n            extras = connection.extra_dejson\n            self.conn = Salesforce(\n                username=connection.login,\n                password=connection.password,\n                security_token=extras['security_token'],\n                instance_url=connection.host,\n                sandbox=extras.get('sandbox', False)\n            )\n        return self.conn"
    },
    {
        "docstring": "Make a query to Salesforce.\n\n        :param query: The query to make to Salesforce.\n        :type query: str\n        :return: The query result.\n        :rtype: dict",
        "code": "def foo(self, query):\n        \n        conn = self.get_conn()\n\n        self.log.info(\"Querying for all objects\")\n        query_results = conn.query_all(query)\n\n        self.log.info(\"Received results: Total size: %s; Done: %s\",\n                      query_results['totalSize'], query_results['done'])\n\n        return query_results"
    },
    {
        "docstring": "Get the description of an object from Salesforce.\n        This description is the object's schema and\n        some extra metadata that Salesforce stores for each object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the description of the Salesforce object.\n        :rtype: dict",
        "code": "def foo(self, obj):\n        \n        conn = self.get_conn()\n\n        return conn.__getattr__(obj).describe()"
    },
    {
        "docstring": "Get a list of all available fields for an object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the names of the fields.\n        :rtype: list of str",
        "code": "def foo(self, obj):\n        \n        self.get_conn()\n\n        obj_description = self.describe_object(obj)\n\n        return [field['name'] for field in obj_description['fields']]"
    },
    {
        "docstring": "Get all instances of the `object` from Salesforce.\n        For each model, only get the fields specified in fields.\n\n        All we really do underneath the hood is run:\n            SELECT <fields> FROM <obj>;\n\n        :param obj: The object name to get from Salesforce.\n        :type obj: str\n        :param fields: The fields to get from the object.\n        :type fields: iterable\n        :return: all instances of the object from Salesforce.\n        :rtype: dict",
        "code": "def foo(self, obj, fields):\n        \n        query = \"SELECT {} FROM {}\".format(\",\".join(fields), obj)\n\n        self.log.info(\"Making query to Salesforce: %s\",\n                      query if len(query) < 30 else \" ... \".join([query[:15], query[-15:]]))\n\n        return self.make_query(query)"
    },
    {
        "docstring": "Convert a column of a dataframe to UNIX timestamps if applicable\n\n        :param column: A Series object representing a column of a dataframe.\n        :type column: pd.Series\n        :return: a new series that maintains the same index as the original\n        :rtype: pd.Series",
        "code": "def foo(cls, column):\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        try:\n            column = pd.to_datetime(column)\n        except ValueError:\n            log = LoggingMixin().log\n            log.warning(\"Could not convert field to timestamps: %s\", column.name)\n            return column\n\n        \n        \n        \n        \n        converted = []\n        for value in column:\n            try:\n                converted.append(value.timestamp())\n            except (ValueError, AttributeError):\n                converted.append(pd.np.NaN)\n\n        return pd.Series(converted, index=column.index)"
    },
    {
        "docstring": "Write query results to file.\n\n        Acceptable formats are:\n            - csv:\n                comma-separated-values file. This is the default format.\n            - json:\n                JSON array. Each element in the array is a different row.\n            - ndjson:\n                JSON array but each element is new-line delimited instead of comma delimited like in `json`\n\n        This requires a significant amount of cleanup.\n        Pandas doesn't handle output to CSV and json in a uniform way.\n        This is especially painful for datetime types.\n        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.\n\n        By default, this function will try and leave all values as they are represented in Salesforce.\n        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).\n        This is can be greatly beneficial as it will make all of your datetime fields look the same,\n        and makes it easier to work with in other database environments\n\n        :param query_results: the results from a SQL query\n        :type query_results: list of dict\n        :param filename: the name of the file where the data should be dumped to\n        :type filename: str\n        :param fmt: the format you want the output in. Default:  'csv'\n        :type fmt: str\n        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.\n            False if you want them to be left in the same format as they were in Salesforce.\n            Leaving the value as False will result in datetimes being strings. Default: False\n        :type coerce_to_timestamp: bool\n        :param record_time_added: True if you want to add a Unix timestamp field\n            to the resulting data that marks when the data was fetched from Salesforce. Default: False\n        :type record_time_added: bool\n        :return: the dataframe that gets written to the file.\n        :rtype: pd.Dataframe",
        "code": "def foo(self,\n                             query_results,\n                             filename,\n                             fmt=\"csv\",\n                             coerce_to_timestamp=False,\n                             record_time_added=False):\n        \n        fmt = fmt.lower()\n        if fmt not in ['csv', 'json', 'ndjson']:\n            raise ValueError(\"Format value is not recognized: {}\".format(fmt))\n\n        \n        \n        \n        \n        df = pd.DataFrame.from_records(query_results, exclude=[\"attributes\"])\n\n        df.columns = [column.lower() for column in df.columns]\n\n        \n        \n        \n        \n        if coerce_to_timestamp and df.shape[0] > 0:\n            \n            \n            \n            object_name = query_results[0]['attributes']['type']\n\n            self.log.info(\"Coercing timestamps for: %s\", object_name)\n\n            schema = self.describe_object(object_name)\n\n            \n            \n            \n            possible_timestamp_cols = [\n                field['name'].lower()\n                for field in schema['fields']\n                if field['type'] in [\"date\", \"datetime\"] and field['name'].lower() in df.columns\n            ]\n            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)\n\n        if record_time_added:\n            fetched_time = time.time()\n            df[\"time_fetched_from_salesforce\"] = fetched_time\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        if fmt == \"csv\":\n            \n            \n            self.log.info(\"Cleaning data and writing to CSV\")\n            possible_strings = df.columns[df.dtypes == \"object\"]\n            df[possible_strings] = df[possible_strings].apply(\n                lambda x: x.str.replace(\"\\r\\n\", \"\").str.replace(\"\\n\", \"\")\n            )\n            \n            df.to_csv(filename, index=False)\n        elif fmt == \"json\":\n            df.to_json(filename, \"records\", date_unit=\"s\")\n        elif fmt == \"ndjson\":\n            df.to_json(filename, \"records\", lines=True, date_unit=\"s\")\n\n        return df"
    },
    {
        "docstring": "Fetches PyMongo Client",
        "code": "def foo(self):\n        \n        if self.client is not None:\n            return self.client\n\n        \n        options = self.extras\n\n        \n        if options.get('ssl', False):\n            options.update({'ssl_cert_reqs': CERT_NONE})\n\n        self.client = MongoClient(self.uri, **options)\n\n        return self.client"
    },
    {
        "docstring": "Fetches a mongo collection object for querying.\n\n        Uses connection schema as DB unless specified.",
        "code": "def foo(self, mongo_collection, mongo_db=None):\n        \n        mongo_db = mongo_db if mongo_db is not None else self.connection.schema\n        mongo_conn = self.get_conn()\n\n        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)"
    },
    {
        "docstring": "Replaces many documents in a mongo collection.\n\n        Uses bulk_write with multiple ReplaceOne operations\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write\n\n        .. note::\n            If no ``filter_docs``are given, it is assumed that all\n            replacement documents contain the ``_id`` field which are then\n            used as filters.\n\n        :param mongo_collection: The name of the collection to update.\n        :type mongo_collection: str\n        :param docs: The new documents.\n        :type docs: list[dict]\n        :param filter_docs: A list of queries that match the documents to replace.\n            Can be omitted; then the _id fields from docs will be used.\n        :type filter_docs: list[dict]\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n        :param upsert: If ``True``, perform an insert if no documents\n            match the filters for the replace operation.\n        :type upsert: bool\n        :param collation: An instance of\n            :class:`~pymongo.collation.Collation`. This option is only\n            supported on MongoDB 3.4 and above.\n        :type collation: pymongo.collation.Collation",
        "code": "def foo(self, mongo_collection, docs,\n                     filter_docs=None, mongo_db=None, upsert=False, collation=None,\n                     **kwargs):\n        \n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        if not filter_docs:\n            filter_docs = [{'_id': doc['_id']} for doc in docs]\n\n        requests = [\n            ReplaceOne(\n                filter_docs[i],\n                docs[i],\n                upsert=upsert,\n                collation=collation)\n            for i in range(len(docs))\n        ]\n\n        return collection.bulk_write(requests, **kwargs)"
    },
    {
        "docstring": "Checks the mail folder for mails containing attachments with the given name.\n\n        :param name: The name of the attachment that will be searched for.\n        :type name: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :returns: True if there is an attachment with the given name and False if not.\n        :rtype: bool",
        "code": "def foo(self, name, mail_folder='INBOX', check_regex=False):\n        \n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only=True)\n        return len(mail_attachments) > 0"
    },
    {
        "docstring": "Retrieves mail's attachments in the mail folder by its name.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only retrieve\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        :returns: a list of tuple each containing the attachment filename and its payload.\n        :rtype: a list of tuple",
        "code": "def foo(self,\n                                  name,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        return mail_attachments"
    },
    {
        "docstring": "Downloads mail's attachments in the mail folder by its name to the local directory.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param local_output_directory: The output directory on the local machine\n                                       where the files will be downloaded to.\n        :type local_output_directory: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only download\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str",
        "code": "def foo(self,\n                                  name,\n                                  local_output_directory,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        self._create_files(mail_attachments, local_output_directory)"
    },
    {
        "docstring": "Gets all attachments by name for the mail.\n\n        :param name: The name of the attachment to look for.\n        :type name: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param find_first: If set to True it will only find the first match and then quit.\n        :type find_first: bool\n        :returns: a list of tuples each containing name and payload\n                  where the attachments name matches the given name.\n        :rtype: list of tuple",
        "code": "def foo(self, name, check_regex, find_first=False):\n        \n        attachments = []\n\n        for part in self.mail.walk():\n            mail_part = MailPart(part)\n            if mail_part.is_attachment():\n                found_attachment = mail_part.has_matching_name(name) if check_regex \\\n                    else mail_part.has_equal_name(name)\n                if found_attachment:\n                    file_name, file_payload = mail_part.get_file()\n                    self.log.info('Found attachment: {}'.format(file_name))\n                    attachments.append((file_name, file_payload))\n                    if find_first:\n                        break\n\n        return attachments"
    },
    {
        "docstring": "Gets the file including name and payload.\n\n        :returns: the part's name and payload.\n        :rtype: tuple",
        "code": "def foo(self):\n        \n        return self.part.get_filename(), self.part.get_payload(decode=True)"
    },
    {
        "docstring": "Write batch records to Kinesis Firehose",
        "code": "def foo(self, records):\n        \n\n        firehose_conn = self.get_conn()\n\n        response = firehose_conn.put_record_batch(\n            DeliveryStreamName=self.delivery_stream,\n            Records=records\n        )\n\n        return response"
    },
    {
        "docstring": "Determines whether a task is ready to be rescheduled. Only tasks in\n        NONE state with at least one row in task_reschedule table are\n        handled by this dependency class, otherwise this dependency is\n        considered as passed. This dependency fails if the latest reschedule\n        request's reschedule date is still in future.",
        "code": "def foo(self, ti, session, dep_context):\n        \n        if dep_context.ignore_in_reschedule_period:\n            yield self._passing_status(\n                reason=\"The context specified that being in a reschedule period was \"\n                       \"permitted.\")\n            return\n\n        if ti.state not in self.RESCHEDULEABLE_STATES:\n            yield self._passing_status(\n                reason=\"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\")\n            return\n\n        task_reschedules = TaskReschedule.find_for_task_instance(task_instance=ti)\n        if not task_reschedules:\n            yield self._passing_status(\n                reason=\"There is no reschedule request for this task instance.\")\n            return\n\n        now = timezone.utcnow()\n        next_reschedule_date = task_reschedules[-1].reschedule_date\n        if now >= next_reschedule_date:\n            yield self._passing_status(\n                reason=\"Task instance id ready for reschedule.\")\n            return\n\n        yield self._failing_status(\n            reason=\"Task is not ready for reschedule yet but will be rescheduled \"\n                   \"automatically. Current date is {0} and task will be rescheduled \"\n                   \"at {1}.\".format(now.isoformat(), next_reschedule_date.isoformat()))"
    },
    {
        "docstring": "Send email using backend specified in EMAIL_BACKEND.",
        "code": "def foo(to, subject, html_content,\n               files=None, dryrun=False, cc=None, bcc=None,\n               mime_subtype='mixed', mime_charset='utf-8', **kwargs):\n    \n    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)\n    module = importlib.import_module(path)\n    backend = getattr(module, attr)\n    to = get_email_address_list(to)\n    to = \", \".join(to)\n\n    return backend(to, subject, html_content, files=files,\n                   dryrun=dryrun, cc=cc, bcc=bcc,\n                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)"
    },
    {
        "docstring": "Send an email with html content\n\n    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)",
        "code": "def foo(to, subject, html_content, files=None,\n                    dryrun=False, cc=None, bcc=None,\n                    mime_subtype='mixed', mime_charset='utf-8',\n                    **kwargs):\n    \n    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')\n\n    to = get_email_address_list(to)\n\n    msg = MIMEMultipart(mime_subtype)\n    msg['Subject'] = subject\n    msg['From'] = smtp_mail_from\n    msg['To'] = \", \".join(to)\n    recipients = to\n    if cc:\n        cc = get_email_address_list(cc)\n        msg['CC'] = \", \".join(cc)\n        recipients = recipients + cc\n\n    if bcc:\n        \n        bcc = get_email_address_list(bcc)\n        recipients = recipients + bcc\n\n    msg['Date'] = formatdate(localtime=True)\n    mime_text = MIMEText(html_content, 'html', mime_charset)\n    msg.attach(mime_text)\n\n    for fname in files or []:\n        basename = os.path.basename(fname)\n        with open(fname, \"rb\") as f:\n            part = MIMEApplication(\n                f.read(),\n                Name=basename\n            )\n            part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename\n            part['Content-ID'] = '<%s>' % basename\n            msg.attach(part)\n\n    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)"
    },
    {
        "docstring": "Processes DateTimes from the DB making sure it is always\n        returning UTC. Not using timezone.convert_to_utc as that\n        converts to configured TIMEZONE while the DB might be\n        running with some other setting. We assume UTC datetimes\n        in the database.",
        "code": "def foo(self, value, dialect):\n        \n        if value is not None:\n            if value.tzinfo is None:\n                value = value.replace(tzinfo=utc)\n            else:\n                value = value.astimezone(utc)\n\n        return value"
    },
    {
        "docstring": "Check if a blob exists on Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.exists()` takes.\n        :type kwargs: object\n        :return: True if the blob exists, False otherwise.\n        :rtype: bool",
        "code": "def foo(self, container_name, blob_name, **kwargs):\n        \n        return self.connection.exists(container_name, blob_name, **kwargs)"
    },
    {
        "docstring": "Check if a prefix exists on Azure Blob storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param prefix: Prefix of the blob.\n        :type prefix: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.list_blobs()` takes.\n        :type kwargs: object\n        :return: True if blobs matching the prefix exist, False otherwise.\n        :rtype: bool",
        "code": "def foo(self, container_name, prefix, **kwargs):\n        \n        matches = self.connection.list_blobs(container_name, prefix,\n                                             num_results=1, **kwargs)\n        return len(list(matches)) > 0"
    },
    {
        "docstring": "Upload a string to Azure Blob Storage.\n\n        :param string_data: String to load.\n        :type string_data: str\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_text()` takes.\n        :type kwargs: object",
        "code": "def foo(self, string_data, container_name, blob_name, **kwargs):\n        \n        \n        self.connection.create_blob_from_text(container_name, blob_name,\n                                              string_data, **kwargs)"
    },
    {
        "docstring": "Read a file from Azure Blob Storage and return as a string.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object",
        "code": "def foo(self, container_name, blob_name, **kwargs):\n        \n        return self.connection.get_blob_to_text(container_name,\n                                                blob_name,\n                                                **kwargs).content"
    },
    {
        "docstring": "Delete a file from Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param is_prefix: If blob_name is a prefix, delete all matching files\n        :type is_prefix: bool\n        :param ignore_if_missing: if True, then return success even if the\n            blob does not exist.\n        :type ignore_if_missing: bool\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object",
        "code": "def foo(self, container_name, blob_name, is_prefix=False,\n                    ignore_if_missing=False, **kwargs):\n        \n\n        if is_prefix:\n            blobs_to_delete = [\n                blob.name for blob in self.connection.list_blobs(\n                    container_name, prefix=blob_name, **kwargs\n                )\n            ]\n        elif self.check_for_blob(container_name, blob_name):\n            blobs_to_delete = [blob_name]\n        else:\n            blobs_to_delete = []\n\n        if not ignore_if_missing and len(blobs_to_delete) == 0:\n            raise AirflowException('Blob(s) not found: {}'.format(blob_name))\n\n        for blob_uri in blobs_to_delete:\n            self.log.info(\"Deleting blob: \" + blob_uri)\n            self.connection.delete_blob(container_name,\n                                        blob_uri,\n                                        delete_snapshots='include',\n                                        **kwargs)"
    },
    {
        "docstring": "BACKPORT FROM PYTHON3 FTPLIB.\n\n    List a directory in a standardized format by using MLSD\n    command (RFC-3659). If path is omitted the current directory\n    is assumed. \"facts\" is a list of strings representing the type\n    of information desired (e.g. [\"type\", \"size\", \"perm\"]).\n\n    Return a generator object yielding a tuple of two elements\n    for every file found in path.\n    First element is the file name, the second one is a dictionary\n    including a variable number of \"facts\" depending on the server\n    and whether \"facts\" argument has been provided.",
        "code": "def foo(conn, path=\"\", facts=None):\n    \n    facts = facts or []\n    if facts:\n        conn.sendcmd(\"OPTS MLST \" + \";\".join(facts) + \";\")\n    if path:\n        cmd = \"MLSD %s\" % path\n    else:\n        cmd = \"MLSD\"\n    lines = []\n    conn.retrlines(cmd, lines.append)\n    for line in lines:\n        facts_found, _, name = line.rstrip(ftplib.CRLF).partition(' ')\n        entry = {}\n        for fact in facts_found[:-1].split(\";\"):\n            key, _, value = fact.partition(\"=\")\n            entry[key.lower()] = value\n        yield (name, entry)"
    },
    {
        "docstring": "Returns a FTP connection object",
        "code": "def foo(self):\n        \n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn"
    },
    {
        "docstring": "Returns a list of files on the remote system.\n\n        :param path: full path to the remote directory to list\n        :type path: str",
        "code": "def foo(self, path, nlst=False):\n        \n        conn = self.get_conn()\n        conn.cwd(path)\n\n        files = conn.nlst()\n        return files"
    },
    {
        "docstring": "Transfers the remote file to a local location.\n\n        If local_full_path_or_buffer is a string path, the file will be put\n        at that location; if it is a file-like buffer, the file will\n        be written to the buffer but not closed.\n\n        :param remote_full_path: full path to the remote file\n        :type remote_full_path: str\n        :param local_full_path_or_buffer: full path to the local file or a\n            file-like buffer\n        :type local_full_path_or_buffer: str or file-like buffer\n        :param callback: callback which is called each time a block of data\n            is read. if you do not use a callback, these blocks will be written\n            to the file or buffer passed in. if you do pass in a callback, note\n            that writing to a file or buffer will need to be handled inside the\n            callback.\n            [default: output_handle.write()]\n        :type callback: callable\n\n        :Example::\n\n            hook = FTPHook(ftp_conn_id='my_conn')\n\n            remote_path = '/path/to/remote/file'\n            local_path = '/path/to/local/file'\n\n            # with a custom callback (in this case displaying progress on each read)\n            def print_progress(percent_progress):\n                self.log.info('Percent Downloaded: %s%%' % percent_progress)\n\n            total_downloaded = 0\n            total_file_size = hook.get_size(remote_path)\n            output_handle = open(local_path, 'wb')\n            def write_to_file_with_progress(data):\n                total_downloaded += len(data)\n                output_handle.write(data)\n                percent_progress = (total_downloaded / total_file_size) * 100\n                print_progress(percent_progress)\n            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)\n\n            # without a custom callback data is written to the local_path\n            hook.retrieve_file(remote_path, local_path)",
        "code": "def foo(\n            self,\n            remote_full_path,\n            local_full_path_or_buffer,\n            callback=None):\n        \n        conn = self.get_conn()\n\n        is_path = isinstance(local_full_path_or_buffer, basestring)\n\n        \n        \n        if not callback:\n            if is_path:\n                output_handle = open(local_full_path_or_buffer, 'wb')\n            else:\n                output_handle = local_full_path_or_buffer\n            callback = output_handle.write\n        else:\n            output_handle = None\n\n        remote_path, remote_file_name = os.path.split(remote_full_path)\n        conn.cwd(remote_path)\n        self.log.info('Retrieving file from FTP: %s', remote_full_path)\n        conn.retrbinary('RETR %s' % remote_file_name, callback)\n        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)\n\n        if is_path and output_handle:\n            output_handle.close()"
    },
    {
        "docstring": "Transfers a local file to the remote location.\n\n        If local_full_path_or_buffer is a string path, the file will be read\n        from that location; if it is a file-like buffer, the file will\n        be read from the buffer but not closed.\n\n        :param remote_full_path: full path to the remote file\n        :type remote_full_path: str\n        :param local_full_path_or_buffer: full path to the local file or a\n            file-like buffer\n        :type local_full_path_or_buffer: str or file-like buffer",
        "code": "def foo(self, remote_full_path, local_full_path_or_buffer):\n        \n        conn = self.get_conn()\n\n        is_path = isinstance(local_full_path_or_buffer, basestring)\n\n        if is_path:\n            input_handle = open(local_full_path_or_buffer, 'rb')\n        else:\n            input_handle = local_full_path_or_buffer\n        remote_path, remote_file_name = os.path.split(remote_full_path)\n        conn.cwd(remote_path)\n        conn.storbinary('STOR %s' % remote_file_name, input_handle)\n\n        if is_path:\n            input_handle.close()"
    },
    {
        "docstring": "Returns a datetime object representing the last time the file was modified\n\n        :param path: remote file path\n        :type path: string",
        "code": "def foo(self, path):\n        \n        conn = self.get_conn()\n        ftp_mdtm = conn.sendcmd('MDTM ' + path)\n        time_val = ftp_mdtm[4:]\n        \n        try:\n            return datetime.datetime.strptime(time_val, \"%Y%m%d%H%M%S.%f\")\n        except ValueError:\n            return datetime.datetime.strptime(time_val, '%Y%m%d%H%M%S')"
    },
    {
        "docstring": "Call the DiscordWebhookHook to post message",
        "code": "def foo(self, context):\n        \n        self.hook = DiscordWebhookHook(\n            self.http_conn_id,\n            self.webhook_endpoint,\n            self.message,\n            self.username,\n            self.avatar_url,\n            self.tts,\n            self.proxy\n        )\n        self.hook.execute()"
    },
    {
        "docstring": "Return the FileService object.",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)"
    },
    {
        "docstring": "Check if a directory exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool",
        "code": "def foo(self, share_name, directory_name, **kwargs):\n        \n        return self.connection.exists(share_name, directory_name,\n                                      **kwargs)"
    },
    {
        "docstring": "Check if a file exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool",
        "code": "def foo(self, share_name, directory_name, file_name, **kwargs):\n        \n        return self.connection.exists(share_name, directory_name,\n                                      file_name, **kwargs)"
    },
    {
        "docstring": "Return the list of directories and files stored on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.list_directories_and_files()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list",
        "code": "def foo(self, share_name, directory_name=None, **kwargs):\n        \n        return self.connection.list_directories_and_files(share_name,\n                                                          directory_name,\n                                                          **kwargs)"
    },
    {
        "docstring": "Create a new directory on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_directory()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list",
        "code": "def foo(self, share_name, directory_name, **kwargs):\n        \n        return self.connection.create_directory(share_name, directory_name, **kwargs)"
    },
    {
        "docstring": "Upload a file to Azure File Share.\n\n        :param file_path: Path to the file to load.\n        :type file_path: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_path()` takes.\n        :type kwargs: object",
        "code": "def foo(self, file_path, share_name, directory_name, file_name, **kwargs):\n        \n        self.connection.create_file_from_path(share_name, directory_name,\n                                              file_name, file_path, **kwargs)"
    },
    {
        "docstring": "Upload a string to Azure File Share.\n\n        :param string_data: String to load.\n        :type string_data: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_text()` takes.\n        :type kwargs: object",
        "code": "def foo(self, string_data, share_name, directory_name, file_name, **kwargs):\n        \n        self.connection.create_file_from_text(share_name, directory_name,\n                                              file_name, string_data, **kwargs)"
    },
    {
        "docstring": "Upload a stream to Azure File Share.\n\n        :param stream: Opened file/stream to upload as the file content.\n        :type stream: file-like\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param count: Size of the stream in bytes\n        :type count: int\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_stream()` takes.\n        :type kwargs: object",
        "code": "def foo(self, stream, share_name, directory_name, file_name, count, **kwargs):\n        \n        self.connection.create_file_from_stream(share_name, directory_name,\n                                                file_name, stream, count, **kwargs)"
    },
    {
        "docstring": "Returns a Google Cloud Storage service object.",
        "code": "def foo(self):\n        \n        if not self._conn:\n            self._conn = storage.Client(credentials=self._get_credentials())\n\n        return self._conn"
    },
    {
        "docstring": "Copies an object from a bucket to another, with renaming if requested.\n\n        destination_bucket or destination_object can be omitted, in which case\n        source bucket/object is used, but not both.\n\n        :param source_bucket: The bucket of the object to copy from.\n        :type source_bucket: str\n        :param source_object: The object to copy.\n        :type source_object: str\n        :param destination_bucket: The destination of the object to copied to.\n            Can be omitted; then the same bucket is used.\n        :type destination_bucket: str\n        :param destination_object: The (renamed) path of the object if given.\n            Can be omitted; then the same name is used.\n        :type destination_object: str",
        "code": "def foo(self, source_bucket, source_object, destination_bucket=None,\n             destination_object=None):\n        \n        destination_bucket = destination_bucket or source_bucket\n        destination_object = destination_object or source_object\n        if source_bucket == destination_bucket and \\\n                source_object == destination_object:\n\n            raise ValueError(\n                'Either source/destination bucket or source/destination object '\n                'must be different, not both the same: bucket=%s, object=%s' %\n                (source_bucket, source_object))\n        if not source_bucket or not source_object:\n            raise ValueError('source_bucket and source_object cannot be empty.')\n\n        client = self.get_conn()\n        source_bucket = client.get_bucket(source_bucket)\n        source_object = source_bucket.blob(source_object)\n        destination_bucket = client.get_bucket(destination_bucket)\n        destination_object = source_bucket.copy_blob(\n            blob=source_object,\n            destination_bucket=destination_bucket,\n            new_name=destination_object)\n\n        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',\n                      source_object.name, source_bucket.name,\n                      destination_object.name, destination_bucket.name)"
    },
    {
        "docstring": "Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str\n        :param filename: If set, a local file path where the file should be written to.\n        :type filename: str",
        "code": "def foo(self, bucket_name, object_name, filename=None):\n        \n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()"
    },
    {
        "docstring": "Uploads a local file to Google Cloud Storage.\n\n        :param bucket_name: The bucket to upload to.\n        :type bucket_name: str\n        :param object_name: The object name to set when uploading the local file.\n        :type object_name: str\n        :param filename: The local file path to the file to be uploaded.\n        :type filename: str\n        :param mime_type: The MIME type to set when uploading the file.\n        :type mime_type: str\n        :param gzip: Option to compress file for upload\n        :type gzip: bool",
        "code": "def foo(self, bucket_name, object_name, filename,\n               mime_type='application/octet-stream', gzip=False):\n        \n\n        if gzip:\n            filename_gz = filename + '.gz'\n\n            with open(filename, 'rb') as f_in:\n                with gz.open(filename_gz, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n                    filename = filename_gz\n\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.upload_from_filename(filename=filename,\n                                  content_type=mime_type)\n\n        if gzip:\n            os.remove(filename)\n        self.log.info('File %s uploaded to %s in %s bucket', filename, object_name, bucket_name)"
    },
    {
        "docstring": "Checks for the existence of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the object is.\n        :type bucket_name: str\n        :param object_name: The name of the blob_name to check in the Google cloud\n            storage bucket.\n        :type object_name: str",
        "code": "def foo(self, bucket_name, object_name):\n        \n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        return blob.exists()"
    },
    {
        "docstring": "Checks if an blob_name is updated in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the object is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket.\n        :type object_name: str\n        :param ts: The timestamp to check against.\n        :type ts: datetime.datetime",
        "code": "def foo(self, bucket_name, object_name, ts):\n        \n        client = self.get_conn()\n        bucket = storage.Bucket(client=client, name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n\n        blob_update_time = blob.updated\n\n        if blob_update_time is not None:\n            import dateutil.tz\n\n            if not ts.tzinfo:\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\n\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\n\n            if blob_update_time > ts:\n                return True\n\n        return False"
    },
    {
        "docstring": "Deletes an object from the bucket.\n\n        :param bucket_name: name of the bucket, where the object resides\n        :type bucket_name: str\n        :param object_name: name of the object to delete\n        :type object_name: str",
        "code": "def foo(self, bucket_name, object_name):\n        \n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.delete()\n\n        self.log.info('Blob %s deleted.', object_name)"
    },
    {
        "docstring": "List all objects from the bucket with the give string prefix in name\n\n        :param bucket_name: bucket name\n        :type bucket_name: str\n        :param versions: if true, list all versions of the objects\n        :type versions: bool\n        :param max_results: max count of items to return in a single page of responses\n        :type max_results: int\n        :param prefix: prefix string which filters objects whose name begin with\n            this prefix\n        :type prefix: str\n        :param delimiter: filters objects based on the delimiter (for e.g '.csv')\n        :type delimiter: str\n        :return: a stream of object names matching the filtering criteria",
        "code": "def foo(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None):\n        \n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n\n        ids = []\n        pageToken = None\n        while True:\n            blobs = bucket.list_blobs(\n                max_results=max_results,\n                page_token=pageToken,\n                prefix=prefix,\n                delimiter=delimiter,\n                versions=versions\n            )\n\n            blob_names = []\n            for blob in blobs:\n                blob_names.append(blob.name)\n\n            prefixes = blobs.prefixes\n            if prefixes:\n                ids += list(prefixes)\n            else:\n                ids += blob_names\n\n            pageToken = blobs.next_page_token\n            if pageToken is None:\n                \n                break\n        return ids"
    },
    {
        "docstring": "Gets the size of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google\n            cloud storage bucket_name.\n        :type object_name: str",
        "code": "def foo(self, bucket_name, object_name):\n        \n        self.log.info('Checking the file size of object: %s in bucket_name: %s',\n                      object_name,\n                      bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_size = blob.size\n        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)\n        return blob_size"
    },
    {
        "docstring": "Gets the CRC32c checksum of an object in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket_name.\n        :type object_name: str",
        "code": "def foo(self, bucket_name, object_name):\n        \n        self.log.info('Retrieving the crc32c checksum of '\n                      'object_name: %s in bucket_name: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_crc32c = blob.crc32c\n        self.log.info('The crc32c checksum of %s is %s', object_name, blob_crc32c)\n        return blob_crc32c"
    },
    {
        "docstring": "Gets the MD5 hash of an object in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket_name.\n        :type object_name: str",
        "code": "def foo(self, bucket_name, object_name):\n        \n        self.log.info('Retrieving the MD5 hash of '\n                      'object: %s in bucket: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_md5hash = blob.md5_hash\n        self.log.info('The md5Hash of %s is %s', object_name, blob_md5hash)\n        return blob_md5hash"
    },
    {
        "docstring": "Creates a new bucket. Google Cloud Storage uses a flat namespace, so\n        you can't create a bucket with a name that is already in use.\n\n        .. seealso::\n            For more information, see Bucket Naming Guidelines:\n            https://cloud.google.com/storage/docs/bucketnaming.html#requirements\n\n        :param bucket_name: The name of the bucket.\n        :type bucket_name: str\n        :param resource: An optional dict with parameters for creating the bucket.\n            For information on available parameters, see Cloud Storage API doc:\n            https://cloud.google.com/storage/docs/json_api/v1/buckets/insert\n        :type resource: dict\n        :param storage_class: This defines how objects in the bucket are stored\n            and determines the SLA and the cost of storage. Values include\n\n            - ``MULTI_REGIONAL``\n            - ``REGIONAL``\n            - ``STANDARD``\n            - ``NEARLINE``\n            - ``COLDLINE``.\n\n            If this value is not specified when the bucket is\n            created, it will default to STANDARD.\n        :type storage_class: str\n        :param location: The location of the bucket.\n            Object data for objects in the bucket resides in physical storage\n            within this region. Defaults to US.\n\n            .. seealso::\n                https://developers.google.com/storage/docs/bucket-locations\n\n        :type location: str\n        :param project_id: The ID of the GCP Project.\n        :type project_id: str\n        :param labels: User-provided labels, in key/value pairs.\n        :type labels: dict\n        :return: If successful, it returns the ``id`` of the bucket.",
        "code": "def foo(self,\n                      bucket_name,\n                      resource=None,\n                      storage_class='MULTI_REGIONAL',\n                      location='US',\n                      project_id=None,\n                      labels=None\n                      ):\n        \n\n        self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',\n                      bucket_name, location, storage_class)\n\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        bucket_resource = resource or {}\n\n        for item in bucket_resource:\n            if item != \"name\":\n                bucket._patch_property(name=item, value=resource[item])\n\n        bucket.storage_class = storage_class\n        bucket.labels = labels or {}\n        bucket.create(project=project_id, location=location)\n        return bucket.id"
    },
    {
        "docstring": "Composes a list of existing object into a new object in the same storage bucket_name\n\n        Currently it only supports up to 32 objects that can be concatenated\n        in a single operation\n\n        https://cloud.google.com/storage/docs/json_api/v1/objects/compose\n\n        :param bucket_name: The name of the bucket containing the source objects.\n            This is also the same bucket to store the composed destination object.\n        :type bucket_name: str\n        :param source_objects: The list of source objects that will be composed\n            into a single object.\n        :type source_objects: list\n        :param destination_object: The path of the object if given.\n        :type destination_object: str",
        "code": "def foo(self, bucket_name, source_objects, destination_object):\n        \n\n        if not source_objects or not len(source_objects):\n            raise ValueError('source_objects cannot be empty.')\n\n        if not bucket_name or not destination_object:\n            raise ValueError('bucket_name and destination_object cannot be empty.')\n\n        self.log.info(\"Composing %s to %s in the bucket %s\",\n                      source_objects, destination_object, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        destination_blob = bucket.blob(destination_object)\n        destination_blob.compose(\n            sources=[\n                bucket.blob(blob_name=source_object) for source_object in source_objects\n            ])\n\n        self.log.info(\"Completed successfully.\")"
    },
    {
        "docstring": "Returns true if training job's secondary status message has changed.\n\n    :param current_job_description: Current job description, returned from DescribeTrainingJob call.\n    :type current_job_description: dict\n    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.\n    :type prev_job_description: dict\n\n    :return: Whether the secondary status message of a training job changed or not.",
        "code": "def foo(current_job_description, prev_job_description):\n    \n    current_secondary_status_transitions = current_job_description.get('SecondaryStatusTransitions')\n    if current_secondary_status_transitions is None or len(current_secondary_status_transitions) == 0:\n        return False\n\n    prev_job_secondary_status_transitions = prev_job_description.get('SecondaryStatusTransitions') \\\n        if prev_job_description is not None else None\n\n    last_message = prev_job_secondary_status_transitions[-1]['StatusMessage'] \\\n        if prev_job_secondary_status_transitions is not None \\\n        and len(prev_job_secondary_status_transitions) > 0 else ''\n\n    message = current_job_description['SecondaryStatusTransitions'][-1]['StatusMessage']\n\n    return message != last_message"
    },
    {
        "docstring": "Returns a string contains start time and the secondary training job status message.\n\n    :param job_description: Returned response from DescribeTrainingJob call\n    :type job_description: dict\n    :param prev_description: Previous job description from DescribeTrainingJob call\n    :type prev_description: dict\n\n    :return: Job status string to be printed.",
        "code": "def foo(job_description, prev_description):\n    \n\n    if job_description is None or job_description.get('SecondaryStatusTransitions') is None\\\n            or len(job_description.get('SecondaryStatusTransitions')) == 0:\n        return ''\n\n    prev_description_secondary_transitions = prev_description.get('SecondaryStatusTransitions')\\\n        if prev_description is not None else None\n    prev_transitions_num = len(prev_description['SecondaryStatusTransitions'])\\\n        if prev_description_secondary_transitions is not None else 0\n    current_transitions = job_description['SecondaryStatusTransitions']\n\n    transitions_to_print = current_transitions[-1:] if len(current_transitions) == prev_transitions_num else \\\n        current_transitions[prev_transitions_num - len(current_transitions):]\n\n    status_strs = []\n    for transition in transitions_to_print:\n        message = transition['StatusMessage']\n        time_str = timezone.convert_to_utc(job_description['LastModifiedTime']).strftime('%Y-%m-%d %H:%M:%S')\n        status_strs.append('{} {} - {}'.format(time_str, transition['Status'], message))\n\n    return '\\n'.join(status_strs)"
    },
    {
        "docstring": "Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None",
        "code": "def foo(self, path, key, bucket):\n        \n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)"
    },
    {
        "docstring": "Extract the S3 operations from the configuration and execute them.\n\n        :param config: config of SageMaker operation\n        :type config: dict\n        :rtype: dict",
        "code": "def foo(self, config):\n        \n        s3_operations = config.pop('S3Operations', None)\n\n        if s3_operations is not None:\n            create_bucket_ops = s3_operations.get('S3CreateBucket', [])\n            upload_ops = s3_operations.get('S3Upload', [])\n            for op in create_bucket_ops:\n                self.s3_hook.create_bucket(bucket_name=op['Bucket'])\n            for op in upload_ops:\n                if op['Tar']:\n                    self.tar_and_s3_upload(op['Path'], op['Key'],\n                                           op['Bucket'])\n                else:\n                    self.s3_hook.load_file(op['Path'], op['Key'],\n                                           op['Bucket'])"
    },
    {
        "docstring": "Check if an S3 URL exists\n\n        :param s3url: S3 url\n        :type s3url: str\n        :rtype: bool",
        "code": "def foo(self, s3url):\n        \n        bucket, key = S3Hook.parse_s3_url(s3url)\n        if not self.s3_hook.check_for_bucket(bucket_name=bucket):\n            raise AirflowException(\n                \"The input S3 Bucket {} does not exist \".format(bucket))\n        if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket)\\\n           and not self.s3_hook.check_for_prefix(\n                prefix=key, bucket_name=bucket, delimiter='/'):\n            \n            \n            \n            raise AirflowException(\"The input S3 Key \"\n                                   \"or Prefix {} does not exist in the Bucket {}\"\n                                   .format(s3url, bucket))\n        return True"
    },
    {
        "docstring": "Establish an AWS connection for retrieving logs during training\n\n        :rtype: CloudWatchLogs.Client",
        "code": "def foo(self):\n        \n        config = botocore.config.Config(retries={'max_attempts': 15})\n        return self.get_client_type('logs', config=config)"
    },
    {
        "docstring": "Create a training job\n\n        :param config: the config for training\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to training job creation",
        "code": "def foo(self, config, wait_for_completion=True, print_log=True,\n                            check_interval=30, max_ingestion_time=None):\n        \n\n        self.check_training_config(config)\n\n        response = self.get_conn().create_training_job(**config)\n        if print_log:\n            self.check_training_status_with_log(config['TrainingJobName'],\n                                                self.non_terminal_states,\n                                                self.failed_states,\n                                                wait_for_completion,\n                                                check_interval, max_ingestion_time\n                                                )\n        elif wait_for_completion:\n            describe_response = self.check_status(config['TrainingJobName'],\n                                                  'TrainingJobStatus',\n                                                  self.describe_training_job,\n                                                  check_interval, max_ingestion_time\n                                                  )\n\n            billable_time = \\\n                (describe_response['TrainingEndTime'] - describe_response['TrainingStartTime']) * \\\n                describe_response['ResourceConfig']['InstanceCount']\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))\n\n        return response"
    },
    {
        "docstring": "Create a tuning job\n\n        :param config: the config for tuning\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to tuning job creation",
        "code": "def foo(self, config, wait_for_completion=True,\n                          check_interval=30, max_ingestion_time=None):\n        \n\n        self.check_tuning_config(config)\n\n        response = self.get_conn().create_hyper_parameter_tuning_job(**config)\n        if wait_for_completion:\n            self.check_status(config['HyperParameterTuningJobName'],\n                              'HyperParameterTuningJobStatus',\n                              self.describe_tuning_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response"
    },
    {
        "docstring": "Create a transform job\n\n        :param config: the config for transform job\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to transform job creation",
        "code": "def foo(self, config, wait_for_completion=True,\n                             check_interval=30, max_ingestion_time=None):\n        \n\n        self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri'])\n\n        response = self.get_conn().create_transform_job(**config)\n        if wait_for_completion:\n            self.check_status(config['TransformJobName'],\n                              'TransformJobStatus',\n                              self.describe_transform_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response"
    },
    {
        "docstring": "Create an endpoint\n\n        :param config: the config for endpoint\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to endpoint creation",
        "code": "def foo(self, config, wait_for_completion=True,\n                        check_interval=30, max_ingestion_time=None):\n        \n\n        response = self.get_conn().create_endpoint(**config)\n        if wait_for_completion:\n            self.check_status(config['EndpointName'],\n                              'EndpointStatus',\n                              self.describe_endpoint,\n                              check_interval, max_ingestion_time,\n                              non_terminal_states=self.endpoint_non_terminal_states\n                              )\n        return response"
    },
    {
        "docstring": "Return the training job info associated with job_name and print CloudWatch logs",
        "code": "def foo(self, job_name, positions, stream_names,\n                                       instance_count, state, last_description,\n                                       last_describe_job_call):\n        \n        log_group = '/aws/sagemaker/TrainingJobs'\n\n        if len(stream_names) < instance_count:\n            \n            \n            logs_conn = self.get_log_conn()\n            try:\n                streams = logs_conn.describe_log_streams(\n                    logGroupName=log_group,\n                    logStreamNamePrefix=job_name + '/',\n                    orderBy='LogStreamName',\n                    limit=instance_count\n                )\n                stream_names = [s['logStreamName'] for s in streams['logStreams']]\n                positions.update([(s, Position(timestamp=0, skip=0))\n                                  for s in stream_names if s not in positions])\n            except logs_conn.exceptions.ResourceNotFoundException:\n                \n                \n                pass\n\n        if len(stream_names) > 0:\n            for idx, event in self.multi_stream_iter(log_group, stream_names, positions):\n                self.log.info(event['message'])\n                ts, count = positions[stream_names[idx]]\n                if event['timestamp'] == ts:\n                    positions[stream_names[idx]] = Position(timestamp=ts, skip=count + 1)\n                else:\n                    positions[stream_names[idx]] = Position(timestamp=event['timestamp'], skip=1)\n\n        if state == LogState.COMPLETE:\n            return state, last_description, last_describe_job_call\n\n        if state == LogState.JOB_COMPLETE:\n            state = LogState.COMPLETE\n        elif time.time() - last_describe_job_call >= 30:\n            description = self.describe_training_job(job_name)\n            last_describe_job_call = time.time()\n\n            if secondary_training_status_changed(description, last_description):\n                self.log.info(secondary_training_status_message(description, last_description))\n                last_description = description\n\n            status = description['TrainingJobStatus']\n\n            if status not in self.non_terminal_states:\n                state = LogState.JOB_COMPLETE\n        return state, last_description, last_describe_job_call"
    },
    {
        "docstring": "Check status of a SageMaker job\n\n        :param job_name: name of the job to check status\n        :type job_name: str\n        :param key: the key of the response dict\n            that points to the state\n        :type key: str\n        :param describe_function: the function used to retrieve the status\n        :type describe_function: python callable\n        :param args: the arguments for the function\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :param non_terminal_states: the set of nonterminal states\n        :type non_terminal_states: set\n        :return: response of describe call after job is done",
        "code": "def foo(self, job_name, key,\n                     describe_function, check_interval,\n                     max_ingestion_time,\n                     non_terminal_states=None):\n        \n        if not non_terminal_states:\n            non_terminal_states = self.non_terminal_states\n\n        sec = 0\n        running = True\n\n        while running:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            try:\n                response = describe_function(job_name)\n                status = response[key]\n                self.log.info('Job still running for %s seconds... '\n                              'current status is %s' % (sec, status))\n            except KeyError:\n                raise AirflowException('Could not get status of the SageMaker job')\n            except ClientError:\n                raise AirflowException('AWS request failed, check logs for more info')\n\n            if status in non_terminal_states:\n                running = True\n            elif status in self.failed_states:\n                raise AirflowException('SageMaker job failed because %s' % response['FailureReason'])\n            else:\n                running = False\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                \n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        self.log.info('SageMaker Job Compeleted')\n        response = describe_function(job_name)\n        return response"
    },
    {
        "docstring": "Display the logs for a given training job, optionally tailing them until the\n        job is complete.\n\n        :param job_name: name of the training job to check status and display logs for\n        :type job_name: str\n        :param non_terminal_states: the set of non_terminal states\n        :type non_terminal_states: set\n        :param failed_states: the set of failed states\n        :type failed_states: set\n        :param wait_for_completion: Whether to keep looking for new log entries\n            until the job completes\n        :type wait_for_completion: bool\n        :param check_interval: The interval in seconds between polling for new log entries and job completion\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: None",
        "code": "def foo(self, job_name, non_terminal_states, failed_states,\n                                       wait_for_completion, check_interval, max_ingestion_time):\n        \n\n        sec = 0\n        description = self.describe_training_job(job_name)\n        self.log.info(secondary_training_status_message(description, None))\n        instance_count = description['ResourceConfig']['InstanceCount']\n        status = description['TrainingJobStatus']\n\n        stream_names = []  \n        positions = {}     \n\n        job_already_completed = status not in non_terminal_states\n\n        state = LogState.TAILING if wait_for_completion and not job_already_completed else LogState.COMPLETE\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        last_describe_job_call = time.time()\n        last_description = description\n\n        while True:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            state, last_description, last_describe_job_call = \\\n                self.describe_training_job_with_log(job_name, positions, stream_names,\n                                                    instance_count, state, last_description,\n                                                    last_describe_job_call)\n            if state == LogState.COMPLETE:\n                break\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                \n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        if wait_for_completion:\n            status = last_description['TrainingJobStatus']\n            if status in failed_states:\n                reason = last_description.get('FailureReason', '(No reason provided)')\n                raise AirflowException('Error training {}: {} Reason: {}'.format(job_name, status, reason))\n            billable_time = (last_description['TrainingEndTime'] - last_description['TrainingStartTime']) \\\n                * instance_count\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))"
    },
    {
        "docstring": "Execute the python dataflow job.",
        "code": "def foo(self, context):\n        \n        bucket_helper = GoogleCloudBucketHelper(\n            self.gcp_conn_id, self.delegate_to)\n        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)\n        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,\n                            delegate_to=self.delegate_to,\n                            poll_sleep=self.poll_sleep)\n        dataflow_options = self.dataflow_default_options.copy()\n        dataflow_options.update(self.options)\n        \n        camel_to_snake = lambda name: re.sub(\n            r'[A-Z]', lambda x: '_' + x.group(0).lower(), name)\n        formatted_options = {camel_to_snake(key): dataflow_options[key]\n                             for key in dataflow_options}\n        hook.start_python_dataflow(\n            self.job_name, formatted_options,\n            self.py_file, self.py_options)"
    },
    {
        "docstring": "Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.",
        "code": "def foo():\n    \n    context.configure(\n        url=settings.SQL_ALCHEMY_CONN, target_metadata=target_metadata,\n        literal_binds=True, compare_type=COMPARE_TYPE)\n\n    with context.begin_transaction():\n        context.run_migrations()"
    },
    {
        "docstring": "Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.",
        "code": "def foo():\n    \n    connectable = settings.engine\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            transaction_per_migration=True,\n            target_metadata=target_metadata,\n            compare_type=COMPARE_TYPE,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()"
    },
    {
        "docstring": "Deletes the specified Cloud Bigtable instance.\n        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does\n        not exist.\n\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Bigtable instance.\n        :type instance_id: str",
        "code": "def foo(self, instance_id, project_id=None):\n        \n        instance = self.get_instance(instance_id=instance_id, project_id=project_id)\n        if instance:\n            instance.delete()\n        else:\n            self.log.info(\"The instance '%s' does not exist in project '%s'. Exiting\", instance_id,\n                          project_id)"
    },
    {
        "docstring": "Creates new instance.\n\n        :type instance_id: str\n        :param instance_id: The ID for the new instance.\n        :type main_cluster_id: str\n        :param main_cluster_id: The ID for main cluster for the new instance.\n        :type main_cluster_zone: str\n        :param main_cluster_zone: The zone for main cluster.\n            See https://cloud.google.com/bigtable/docs/locations for more details.\n        :type project_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type replica_cluster_id: str\n        :param replica_cluster_id: (optional) The ID for replica cluster for the new\n            instance.\n        :type replica_cluster_zone: str\n        :param replica_cluster_zone: (optional)  The zone for replica cluster.\n        :type instance_type: enums.Instance.Type\n        :param instance_type: (optional) The type of the instance.\n        :type instance_display_name: str\n        :param instance_display_name: (optional) Human-readable name of the instance.\n                Defaults to ``instance_id``.\n        :type instance_labels: dict\n        :param instance_labels: (optional) Dictionary of labels to associate with the\n            instance.\n        :type cluster_nodes: int\n        :param cluster_nodes: (optional) Number of nodes for cluster.\n        :type cluster_storage_type: enums.StorageType\n        :param cluster_storage_type: (optional) The type of storage.\n        :type timeout: int\n        :param timeout: (optional) timeout (in seconds) for instance creation.\n                        If None is not specified, Operator will wait indefinitely.",
        "code": "def foo(self,\n                        instance_id,\n                        main_cluster_id,\n                        main_cluster_zone,\n                        project_id=None,\n                        replica_cluster_id=None,\n                        replica_cluster_zone=None,\n                        instance_display_name=None,\n                        instance_type=enums.Instance.Type.TYPE_UNSPECIFIED,\n                        instance_labels=None,\n                        cluster_nodes=None,\n                        cluster_storage_type=enums.StorageType.STORAGE_TYPE_UNSPECIFIED,\n                        timeout=None):\n        \n        cluster_storage_type = enums.StorageType(cluster_storage_type)\n        instance_type = enums.Instance.Type(instance_type)\n\n        instance = Instance(\n            instance_id,\n            self._get_client(project_id=project_id),\n            instance_display_name,\n            instance_type,\n            instance_labels,\n        )\n\n        clusters = [\n            instance.cluster(\n                main_cluster_id,\n                main_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            )\n        ]\n        if replica_cluster_id and replica_cluster_zone:\n            clusters.append(instance.cluster(\n                replica_cluster_id,\n                replica_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            ))\n        operation = instance.create(\n            clusters=clusters\n        )\n        operation.result(timeout)\n        return instance"
    },
    {
        "docstring": "Creates the specified Cloud Bigtable table.\n        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.\n\n        :type instance: Instance\n        :param instance: The Cloud Bigtable instance that owns the table.\n        :type table_id: str\n        :param table_id: The ID of the table to create in Cloud Bigtable.\n        :type initial_split_keys: list\n        :param initial_split_keys: (Optional) A list of row keys in bytes to use to\n            initially split the table.\n        :type column_families: dict\n        :param column_families: (Optional) A map of columns to create. The key is the\n            column_id str, and the value is a\n            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.",
        "code": "def foo(instance,\n                     table_id,\n                     initial_split_keys=None,\n                     column_families=None):\n        \n        if column_families is None:\n            column_families = {}\n        if initial_split_keys is None:\n            initial_split_keys = []\n        table = Table(table_id, instance)\n        table.create(initial_split_keys, column_families)"
    },
    {
        "docstring": "Deletes the specified table in Cloud Bigtable.\n        Raises google.api_core.exceptions.NotFound if the table does not exist.\n\n        :type instance_id: str\n        :param instance_id: The ID of the Cloud Bigtable instance.\n        :type table_id: str\n        :param table_id: The ID of the table in Cloud Bigtable.\n        :type project_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.",
        "code": "def foo(self, instance_id, table_id, project_id=None):\n        \n        table = self.get_instance(instance_id=instance_id, project_id=project_id).table(table_id=table_id)\n        table.delete()"
    },
    {
        "docstring": "Updates number of nodes in the specified Cloud Bigtable cluster.\n        Raises google.api_core.exceptions.NotFound if the cluster does not exist.\n\n        :type instance: Instance\n        :param instance: The Cloud Bigtable instance that owns the cluster.\n        :type cluster_id: str\n        :param cluster_id: The ID of the cluster.\n        :type nodes: int\n        :param nodes: The desired number of nodes.",
        "code": "def foo(instance, cluster_id, nodes):\n        \n        cluster = Cluster(cluster_id, instance)\n        cluster.serve_nodes = nodes\n        cluster.update()"
    },
    {
        "docstring": "This function creates the command list from available information",
        "code": "def foo(self):\n        \n        conn = self.conn\n        hive_bin = 'hive'\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = 'beeline'\n            jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\".format(\n                host=conn.host, port=conn.port, schema=conn.schema)\n            if configuration.conf.get('core', 'security') == 'kerberos':\n                template = conn.extra_dejson.get(\n                    'principal', \"hive/_HOST@EXAMPLE.COM\")\n                if \"_HOST\" in template:\n                    template = utils.replace_hostname_pattern(\n                        utils.get_components(template))\n\n                proxy_user = \"\"  \n                if conn.extra_dejson.get('proxy_user') == \"login\" and conn.login:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(conn.login)\n                elif conn.extra_dejson.get('proxy_user') == \"owner\" and self.run_as:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(self.run_as)\n\n                jdbc_url += \";principal={template};{proxy_user}\".format(\n                    template=template, proxy_user=proxy_user)\n            elif self.auth:\n                jdbc_url += \";auth=\" + self.auth\n\n            jdbc_url = '\"{}\"'.format(jdbc_url)\n\n            cmd_extra += ['-u', jdbc_url]\n            if conn.login:\n                cmd_extra += ['-n', conn.login]\n            if conn.password:\n                cmd_extra += ['-p', conn.password]\n\n        hive_params_list = self.hive_cli_params.split()\n\n        return [hive_bin] + cmd_extra + hive_params_list"
    },
    {
        "docstring": "This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)\n        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]",
        "code": "def foo(d):\n        \n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )"
    },
    {
        "docstring": "Loads a pandas DataFrame into hive.\n\n        Hive data types will be inferred if not passed but column names will\n        not be sanitized.\n\n        :param df: DataFrame to load into a Hive table\n        :type df: pandas.DataFrame\n        :param table: target Hive table, use dot notation to target a\n            specific database\n        :type table: str\n        :param field_dict: mapping from column name to hive data type.\n            Note that it must be OrderedDict so as to keep columns' order.\n        :type field_dict: collections.OrderedDict\n        :param delimiter: field delimiter in the file\n        :type delimiter: str\n        :param encoding: str encoding to use when writing DataFrame to file\n        :type encoding: str\n        :param pandas_kwargs: passed to DataFrame.to_csv\n        :type pandas_kwargs: dict\n        :param kwargs: passed to self.load_file",
        "code": "def foo(\n            self,\n            df,\n            table,\n            field_dict=None,\n            delimiter=',',\n            encoding='utf8',\n            pandas_kwargs=None, **kwargs):\n        \n\n        def _infer_field_types_from_df(df):\n            DTYPE_KIND_HIVE_TYPE = {\n                'b': 'BOOLEAN',    \n                'i': 'BIGINT',     \n                'u': 'BIGINT',     \n                'f': 'DOUBLE',     \n                'c': 'STRING',     \n                'M': 'TIMESTAMP',  \n                'O': 'STRING',     \n                'S': 'STRING',     \n                'U': 'STRING',     \n                'V': 'STRING'      \n            }\n\n            d = OrderedDict()\n            for col, dtype in df.dtypes.iteritems():\n                d[col] = DTYPE_KIND_HIVE_TYPE[dtype.kind]\n            return d\n\n        if pandas_kwargs is None:\n            pandas_kwargs = {}\n\n        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, mode=\"w\") as f:\n\n                if field_dict is None:\n                    field_dict = _infer_field_types_from_df(df)\n\n                df.to_csv(path_or_buf=f,\n                          sep=delimiter,\n                          header=False,\n                          index=False,\n                          encoding=encoding,\n                          date_format=\"%Y-%m-%d %H:%M:%S\",\n                          **pandas_kwargs)\n                f.flush()\n\n                return self.load_file(filepath=f.name,\n                                      table=table,\n                                      delimiter=delimiter,\n                                      field_dict=field_dict,\n                                      **kwargs)"
    },
    {
        "docstring": "Loads a local file into Hive\n\n        Note that the table generated in Hive uses ``STORED AS textfile``\n        which isn't the most efficient serialization format. If a\n        large amount of data is loaded and/or if the tables gets\n        queried considerably, you may want to use this operator only to\n        stage the data into a temporary table before loading it into its\n        final destination using a ``HiveOperator``.\n\n        :param filepath: local filepath of the file to load\n        :type filepath: str\n        :param table: target Hive table, use dot notation to target a\n            specific database\n        :type table: str\n        :param delimiter: field delimiter in the file\n        :type delimiter: str\n        :param field_dict: A dictionary of the fields name in the file\n            as keys and their Hive types as values.\n            Note that it must be OrderedDict so as to keep columns' order.\n        :type field_dict: collections.OrderedDict\n        :param create: whether to create the table if it doesn't exist\n        :type create: bool\n        :param overwrite: whether to overwrite the data in table or partition\n        :type overwrite: bool\n        :param partition: target partition as a dict of partition columns\n            and values\n        :type partition: dict\n        :param recreate: whether to drop and recreate the table at every\n            execution\n        :type recreate: bool\n        :param tblproperties: TBLPROPERTIES of the hive table being created\n        :type tblproperties: dict",
        "code": "def foo(\n            self,\n            filepath,\n            table,\n            delimiter=\",\",\n            field_dict=None,\n            create=True,\n            overwrite=True,\n            partition=None,\n            recreate=False,\n            tblproperties=None):\n        \n        hql = ''\n        if recreate:\n            hql += \"DROP TABLE IF EXISTS {table};\\n\".format(table=table)\n        if create or recreate:\n            if field_dict is None:\n                raise ValueError(\"Must provide a field dict when creating a table\")\n            fields = \",\\n    \".join(\n                [k + ' ' + v for k, v in field_dict.items()])\n            hql += \"CREATE TABLE IF NOT EXISTS {table} (\\n{fields})\\n\".format(\n                table=table, fields=fields)\n            if partition:\n                pfields = \",\\n    \".join(\n                    [p + \" STRING\" for p in partition])\n                hql += \"PARTITIONED BY ({pfields})\\n\".format(pfields=pfields)\n            hql += \"ROW FORMAT DELIMITED\\n\"\n            hql += \"FIELDS TERMINATED BY '{delimiter}'\\n\".format(delimiter=delimiter)\n            hql += \"STORED AS textfile\\n\"\n            if tblproperties is not None:\n                tprops = \", \".join(\n                    [\"'{0}'='{1}'\".format(k, v) for k, v in tblproperties.items()])\n                hql += \"TBLPROPERTIES({tprops})\\n\".format(tprops=tprops)\n        hql += \";\"\n        self.log.info(hql)\n        self.run_cli(hql)\n        hql = \"LOAD DATA LOCAL INPATH '{filepath}' \".format(filepath=filepath)\n        if overwrite:\n            hql += \"OVERWRITE \"\n        hql += \"INTO TABLE {table} \".format(table=table)\n        if partition:\n            pvals = \", \".join(\n                [\"{0}='{1}'\".format(k, v) for k, v in partition.items()])\n            hql += \"PARTITION ({pvals})\".format(pvals=pvals)\n\n        \n        \n        hql += ';\\n'\n\n        self.log.info(hql)\n        self.run_cli(hql)"
    },
    {
        "docstring": "Returns a Hive thrift client.",
        "code": "def foo(self):\n        \n        import hmsclient\n        from thrift.transport import TSocket, TTransport\n        from thrift.protocol import TBinaryProtocol\n        ms = self.metastore_conn\n        auth_mechanism = ms.extra_dejson.get('authMechanism', 'NOSASL')\n        if configuration.conf.get('core', 'security') == 'kerberos':\n            auth_mechanism = ms.extra_dejson.get('authMechanism', 'GSSAPI')\n            kerberos_service_name = ms.extra_dejson.get('kerberos_service_name', 'hive')\n\n        socket = TSocket.TSocket(ms.host, ms.port)\n        if configuration.conf.get('core', 'security') == 'kerberos' \\\n                and auth_mechanism == 'GSSAPI':\n            try:\n                import saslwrapper as sasl\n            except ImportError:\n                import sasl\n\n            def sasl_factory():\n                sasl_client = sasl.Client()\n                sasl_client.setAttr(\"host\", ms.host)\n                sasl_client.setAttr(\"service\", kerberos_service_name)\n                sasl_client.init()\n                return sasl_client\n\n            from thrift_sasl import TSaslClientTransport\n            transport = TSaslClientTransport(sasl_factory, \"GSSAPI\", socket)\n        else:\n            transport = TTransport.TBufferedTransport(socket)\n\n        protocol = TBinaryProtocol.TBinaryProtocol(transport)\n\n        return hmsclient.HMSClient(iprot=protocol)"
    },
    {
        "docstring": "Checks whether a partition with a given name exists\n\n        :param schema: Name of hive schema (database) @table belongs to\n        :type schema: str\n        :param table: Name of hive table @partition belongs to\n        :type schema: str\n        :partition: Name of the partitions to check for (eg `a=b/c=d`)\n        :type schema: str\n        :rtype: bool\n\n        >>> hh = HiveMetastoreHook()\n        >>> t = 'static_babynames_partitioned'\n        >>> hh.check_for_named_partition('airflow', t, \"ds=2015-01-01\")\n        True\n        >>> hh.check_for_named_partition('airflow', t, \"ds=xxx\")\n        False",
        "code": "def foo(self, schema, table, partition_name):\n        \n        with self.metastore as client:\n            return client.check_for_named_partition(schema, table, partition_name)"
    },
    {
        "docstring": "Check if table exists\n\n        >>> hh = HiveMetastoreHook()\n        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n        True\n        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n        False",
        "code": "def foo(self, table_name, db='default'):\n        \n        try:\n            self.get_table(table_name, db)\n            return True\n        except Exception:\n            return False"
    },
    {
        "docstring": "Returns a Hive connection object.",
        "code": "def foo(self, schema=None):\n        \n        db = self.get_connection(self.hiveserver2_conn_id)\n        auth_mechanism = db.extra_dejson.get('authMechanism', 'NONE')\n        if auth_mechanism == 'NONE' and db.login is None:\n            \n            username = 'airflow'\n        kerberos_service_name = None\n        if configuration.conf.get('core', 'security') == 'kerberos':\n            auth_mechanism = db.extra_dejson.get('authMechanism', 'KERBEROS')\n            kerberos_service_name = db.extra_dejson.get('kerberos_service_name', 'hive')\n\n        \n        if auth_mechanism == 'GSSAPI':\n            self.log.warning(\n                \"Detected deprecated 'GSSAPI' for authMechanism \"\n                \"for %s. Please use 'KERBEROS' instead\",\n                self.hiveserver2_conn_id\n            )\n            auth_mechanism = 'KERBEROS'\n\n        from pyhive.hive import connect\n        return connect(\n            host=db.host,\n            port=db.port,\n            auth=auth_mechanism,\n            kerberos_service_name=kerberos_service_name,\n            username=db.login or username,\n            password=db.password,\n            database=schema or db.schema or 'default')"
    },
    {
        "docstring": "Get results of the provided hql in target schema.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param fetch_size: max size of result to fetch.\n        :type fetch_size: int\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n        :return: results of hql execution, dict with data (list of results) and header\n        :rtype: dict",
        "code": "def foo(self, hql, schema='default', fetch_size=None, hive_conf=None):\n        \n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        results = {\n            'data': list(results_iter),\n            'header': header\n        }\n        return results"
    },
    {
        "docstring": "Execute hql in target schema and write results to a csv file.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param csv_filepath: filepath of csv to write results into.\n        :type csv_filepath: str\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param delimiter: delimiter of the csv file, default to ','.\n        :type delimiter: str\n        :param lineterminator: lineterminator of the csv file.\n        :type lineterminator: str\n        :param output_header: header of the csv file, default to True.\n        :type output_header: bool\n        :param fetch_size: number of result rows to write into the csv file, default to 1000.\n        :type fetch_size: int\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict",
        "code": "def foo(\n            self,\n            hql,\n            csv_filepath,\n            schema='default',\n            delimiter=',',\n            lineterminator='\\r\\n',\n            output_header=True,\n            fetch_size=1000,\n            hive_conf=None):\n        \n\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        message = None\n\n        i = 0\n        with open(csv_filepath, 'wb') as f:\n            writer = csv.writer(f,\n                                delimiter=delimiter,\n                                lineterminator=lineterminator,\n                                encoding='utf-8')\n            try:\n                if output_header:\n                    self.log.debug('Cursor description is %s', header)\n                    writer.writerow([c[0] for c in header])\n\n                for i, row in enumerate(results_iter, 1):\n                    writer.writerow(row)\n                    if i % fetch_size == 0:\n                        self.log.info(\"Written %s rows so far.\", i)\n            except ValueError as exception:\n                message = str(exception)\n\n        if message:\n            \n            os.remove(csv_filepath)\n            raise ValueError(message)\n\n        self.log.info(\"Done. Loaded a total of %s rows.\", i)"
    },
    {
        "docstring": "Get a set of records from a Hive query.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n        :return: result of hive execution\n        :rtype: list\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> len(hh.get_records(sql))\n        100",
        "code": "def foo(self, hql, schema='default', hive_conf=None):\n        \n        return self.get_results(hql, schema=schema, hive_conf=hive_conf)['data']"
    },
    {
        "docstring": "Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame",
        "code": "def foo(self, hql, schema='default'):\n        \n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df"
    },
    {
        "docstring": "Retrieves connection to Cloud Vision.\n\n        :return: Google Cloud Vision client object.\n        :rtype: google.cloud.vision_v1.ProductSearchClient",
        "code": "def foo(self):\n        \n        if not self._client:\n            self._client = ProductSearchClient(credentials=self._get_credentials())\n        return self._client"
    },
    {
        "docstring": "Get Dingding endpoint for sending message.",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.http_conn_id)\n        token = conn.password\n        if not token:\n            raise AirflowException('Dingding token is requests but get nothing, '\n                                   'check you conn_id configuration.')\n        return 'robot/send?access_token={}'.format(token)"
    },
    {
        "docstring": "Send Dingding message",
        "code": "def foo(self):\n        \n        support_type = ['text', 'link', 'markdown', 'actionCard', 'feedCard']\n        if self.message_type not in support_type:\n            raise ValueError('DingdingWebhookHook only support {} '\n                             'so far, but receive {}'.format(support_type, self.message_type))\n\n        data = self._build_message()\n        self.log.info('Sending Dingding type %s message %s', self.message_type, data)\n        resp = self.run(endpoint=self._get_endpoint(),\n                        data=data,\n                        headers={'Content-Type': 'application/json'})\n\n        \n        if int(resp.json().get('errcode')) != 0:\n            raise AirflowException('Send Dingding message failed, receive error '\n                                   'message %s', resp.text)\n        self.log.info('Success Send Dingding message')"
    },
    {
        "docstring": "Helper method that binds parameters to a SQL query.",
        "code": "def foo(operation, parameters):\n    \n    \n    string_parameters = {}\n    for (name, value) in iteritems(parameters):\n        if value is None:\n            string_parameters[name] = 'NULL'\n        elif isinstance(value, basestring):\n            string_parameters[name] = \"'\" + _escape(value) + \"'\"\n        else:\n            string_parameters[name] = str(value)\n    return operation % string_parameters"
    },
    {
        "docstring": "Helper method that escapes parameters to a SQL query.",
        "code": "def foo(s):\n    \n    e = s\n    e = e.replace('\\\\', '\\\\\\\\')\n    e = e.replace('\\n', '\\\\n')\n    e = e.replace('\\r', '\\\\r')\n    e = e.replace(\"'\", \"\\\\'\")\n    e = e.replace('\"', '\\\\\"')\n    return e"
    },
    {
        "docstring": "Helper method that casts a BigQuery row to the appropriate data types.\n    This is useful because BigQuery returns all fields as strings.",
        "code": "def foo(string_field, bq_type):\n    \n    if string_field is None:\n        return None\n    elif bq_type == 'INTEGER':\n        return int(string_field)\n    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':\n        return float(string_field)\n    elif bq_type == 'BOOLEAN':\n        if string_field not in ['true', 'false']:\n            raise ValueError(\"{} must have value 'true' or 'false'\".format(\n                string_field))\n        return string_field == 'true'\n    else:\n        return string_field"
    },
    {
        "docstring": "function to check expected type and raise\n    error if type is not correct",
        "code": "def foo(key, value, expected_type):\n    \n    if not isinstance(value, expected_type):\n        raise TypeError(\"{} argument must have a type {} not {}\".format(\n            key, expected_type, type(value)))"
    },
    {
        "docstring": "Returns a BigQuery PEP 249 connection object.",
        "code": "def foo(self):\n        \n        service = self.get_service()\n        project = self._get_field('project')\n        return BigQueryConnection(\n            service=service,\n            project_id=project,\n            use_legacy_sql=self.use_legacy_sql,\n            location=self.location,\n            num_retries=self.num_retries\n        )"
    },
    {
        "docstring": "Returns a BigQuery service object.",
        "code": "def foo(self):\n        \n        http_authorized = self._authorize()\n        return build(\n            'bigquery', 'v2', http=http_authorized, cache_discovery=False)"
    },
    {
        "docstring": "Checks for the existence of a table in Google BigQuery.\n\n        :param project_id: The Google cloud project in which to look for the\n            table. The connection supplied to the hook must provide access to\n            the specified project.\n        :type project_id: str\n        :param dataset_id: The name of the dataset in which to look for the\n            table.\n        :type dataset_id: str\n        :param table_id: The name of the table to check the existence of.\n        :type table_id: str",
        "code": "def foo(self, project_id, dataset_id, table_id):\n        \n        service = self.get_service()\n        try:\n            service.tables().get(\n                projectId=project_id, datasetId=dataset_id,\n                tableId=table_id).execute(num_retries=self.num_retries)\n            return True\n        except HttpError as e:\n            if e.resp['status'] == '404':\n                return False\n            raise"
    },
    {
        "docstring": "Creates a new, empty table in the dataset.\n        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg\n\n        :param project_id: The project to create the table into.\n        :type project_id: str\n        :param dataset_id: The dataset to create the table into.\n        :type dataset_id: str\n        :param table_id: The Name of the table to be created.\n        :type table_id: str\n        :param schema_fields: If set, the schema field list as defined here:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n        :type schema_fields: list\n        :param labels: a dictionary containing labels for the table, passed to BigQuery\n        :type labels: dict\n\n        **Example**: ::\n\n            schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                           {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n\n        :param time_partitioning: configure optional time partitioning fields i.e.\n            partition by field, type and expiration as per API specifications.\n\n            .. seealso::\n                https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning\n        :type time_partitioning: dict\n        :param cluster_fields: [Optional] The fields used for clustering.\n            Must be specified with time_partitioning, data in the table will be first\n            partitioned and subsequently clustered.\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields\n        :type cluster_fields: list\n        :param view: [Optional] A dictionary containing definition for the view.\n            If set, it will create a view instead of a table:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view\n        :type view: dict\n\n        **Example**: ::\n\n            view = {\n                \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000\",\n                \"useLegacySql\": False\n            }\n\n        :return: None",
        "code": "def foo(self,\n                           project_id,\n                           dataset_id,\n                           table_id,\n                           schema_fields=None,\n                           time_partitioning=None,\n                           cluster_fields=None,\n                           labels=None,\n                           view=None,\n                           num_retries=None):\n        \n\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {\n            'tableReference': {\n                'tableId': table_id\n            }\n        }\n\n        if schema_fields:\n            table_resource['schema'] = {'fields': schema_fields}\n\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n\n        if cluster_fields:\n            table_resource['clustering'] = {\n                'fields': cluster_fields\n            }\n\n        if labels:\n            table_resource['labels'] = labels\n\n        if view:\n            table_resource['view'] = view\n\n        num_retries = num_retries if num_retries else self.num_retries\n\n        self.log.info('Creating Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().insert(\n                projectId=project_id,\n                datasetId=dataset_id,\n                body=table_resource).execute(num_retries=num_retries)\n\n            self.log.info('Table created successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )"
    },
    {
        "docstring": "Patch information in an existing table.\n        It only updates fileds that are provided in the request object.\n\n        Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch\n\n        :param dataset_id: The dataset containing the table to be patched.\n        :type dataset_id: str\n        :param table_id: The Name of the table to be patched.\n        :type table_id: str\n        :param project_id: The project containing the table to be patched.\n        :type project_id: str\n        :param description: [Optional] A user-friendly description of this table.\n        :type description: str\n        :param expiration_time: [Optional] The time when this table expires,\n            in milliseconds since the epoch.\n        :type expiration_time: int\n        :param external_data_configuration: [Optional] A dictionary containing\n            properties of a table stored outside of BigQuery.\n        :type external_data_configuration: dict\n        :param friendly_name: [Optional] A descriptive name for this table.\n        :type friendly_name: str\n        :param labels: [Optional] A dictionary containing labels associated with this table.\n        :type labels: dict\n        :param schema: [Optional] If set, the schema field list as defined here:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n            The supported schema modifications and unsupported schema modification are listed here:\n            https://cloud.google.com/bigquery/docs/managing-table-schemas\n            **Example**: ::\n\n                schema=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                               {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n\n        :type schema: list\n        :param time_partitioning: [Optional] A dictionary containing time-based partitioning\n             definition for the table.\n        :type time_partitioning: dict\n        :param view: [Optional] A dictionary containing definition for the view.\n            If set, it will patch a view instead of a table:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view\n            **Example**: ::\n\n                view = {\n                    \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500\",\n                    \"useLegacySql\": False\n                }\n\n        :type view: dict\n        :param require_partition_filter: [Optional] If true, queries over the this table require a\n            partition filter. If false, queries over the table\n        :type require_partition_filter: bool",
        "code": "def foo(self,\n                    dataset_id,\n                    table_id,\n                    project_id=None,\n                    description=None,\n                    expiration_time=None,\n                    external_data_configuration=None,\n                    friendly_name=None,\n                    labels=None,\n                    schema=None,\n                    time_partitioning=None,\n                    view=None,\n                    require_partition_filter=None):\n        \n\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {}\n\n        if description is not None:\n            table_resource['description'] = description\n        if expiration_time is not None:\n            table_resource['expirationTime'] = expiration_time\n        if external_data_configuration:\n            table_resource['externalDataConfiguration'] = external_data_configuration\n        if friendly_name is not None:\n            table_resource['friendlyName'] = friendly_name\n        if labels:\n            table_resource['labels'] = labels\n        if schema:\n            table_resource['schema'] = {'fields': schema}\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n        if view:\n            table_resource['view'] = view\n        if require_partition_filter is not None:\n            table_resource['requirePartitionFilter'] = require_partition_filter\n\n        self.log.info('Patching Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().patch(\n                projectId=project_id,\n                datasetId=dataset_id,\n                tableId=table_id,\n                body=table_resource).execute(num_retries=self.num_retries)\n\n            self.log.info('Table patched successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )"
    },
    {
        "docstring": "Cancel all started queries that have not yet completed",
        "code": "def foo(self):\n        \n        jobs = self.service.jobs()\n        if (self.running_job_id and\n                not self.poll_job_complete(self.running_job_id)):\n            self.log.info('Attempting to cancel job : %s, %s', self.project_id,\n                          self.running_job_id)\n            if self.location:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id,\n                    location=self.location).execute(num_retries=self.num_retries)\n            else:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id).execute(num_retries=self.num_retries)\n        else:\n            self.log.info('No running BigQuery jobs to cancel.')\n            return\n\n        \n        max_polling_attempts = 12\n        polling_attempts = 0\n\n        job_complete = False\n        while polling_attempts < max_polling_attempts and not job_complete:\n            polling_attempts = polling_attempts + 1\n            job_complete = self.poll_job_complete(self.running_job_id)\n            if job_complete:\n                self.log.info('Job successfully canceled: %s, %s',\n                              self.project_id, self.running_job_id)\n            elif polling_attempts == max_polling_attempts:\n                self.log.info(\n                    \"Stopping polling due to timeout. Job with id %s \"\n                    \"has not completed cancel and may or may not finish.\",\n                    self.running_job_id)\n            else:\n                self.log.info('Waiting for canceled job with id %s to finish.',\n                              self.running_job_id)\n                time.sleep(5)"
    },
    {
        "docstring": "Delete an existing table from the dataset;\n        If the table does not exist, return an error unless ignore_if_missing\n        is set to True.\n\n        :param deletion_dataset_table: A dotted\n            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table\n            will be deleted.\n        :type deletion_dataset_table: str\n        :param ignore_if_missing: if True, then return success even if the\n            requested table does not exist.\n        :type ignore_if_missing: bool\n        :return:",
        "code": "def foo(self, deletion_dataset_table,\n                         ignore_if_missing=False):\n        \n        deletion_project, deletion_dataset, deletion_table = \\\n            _split_tablename(table_input=deletion_dataset_table,\n                             default_project_id=self.project_id)\n\n        try:\n            self.service.tables() \\\n                .delete(projectId=deletion_project,\n                        datasetId=deletion_dataset,\n                        tableId=deletion_table) \\\n                .execute(num_retries=self.num_retries)\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\n                          deletion_dataset, deletion_table)\n        except HttpError:\n            if not ignore_if_missing:\n                raise Exception('Table deletion failed. Table does not exist.')\n            else:\n                self.log.info('Table does not exist. Skipping.')"
    },
    {
        "docstring": "creates a new, empty table in the dataset;\n        If the table already exists, update the existing table.\n        Since BigQuery does not natively allow table upserts, this is not an\n        atomic operation.\n\n        :param dataset_id: the dataset to upsert the table into.\n        :type dataset_id: str\n        :param table_resource: a table resource. see\n            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n        :type table_resource: dict\n        :param project_id: the project to upsert the table into.  If None,\n            project will be self.project_id.\n        :return:",
        "code": "def foo(self, dataset_id, table_resource, project_id=None):\n        \n        \n        table_id = table_resource['tableReference']['tableId']\n        project_id = project_id if project_id is not None else self.project_id\n        tables_list_resp = self.service.tables().list(\n            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)\n        while True:\n            for table in tables_list_resp.get('tables', []):\n                if table['tableReference']['tableId'] == table_id:\n                    \n                    self.log.info('Table %s:%s.%s exists, updating.',\n                                  project_id, dataset_id, table_id)\n                    return self.service.tables().update(\n                        projectId=project_id,\n                        datasetId=dataset_id,\n                        tableId=table_id,\n                        body=table_resource).execute(num_retries=self.num_retries)\n            \n            if 'nextPageToken' in tables_list_resp:\n                tables_list_resp = self.service.tables()\\\n                    .list(projectId=project_id,\n                          datasetId=dataset_id,\n                          pageToken=tables_list_resp['nextPageToken'])\\\n                    .execute(num_retries=self.num_retries)\n            \n            else:\n                \n                self.log.info('Table %s:%s.%s does not exist. creating.',\n                              project_id, dataset_id, table_id)\n                return self.service.tables().insert(\n                    projectId=project_id,\n                    datasetId=dataset_id,\n                    body=table_resource).execute(num_retries=self.num_retries)"
    },
    {
        "docstring": "Grant authorized view access of a dataset to a view table.\n        If this view has already been granted access to the dataset, do nothing.\n        This method is not atomic.  Running it may clobber a simultaneous update.\n\n        :param source_dataset: the source dataset\n        :type source_dataset: str\n        :param view_dataset: the dataset that the view is in\n        :type view_dataset: str\n        :param view_table: the table of the view\n        :type view_table: str\n        :param source_project: the project of the source dataset. If None,\n            self.project_id will be used.\n        :type source_project: str\n        :param view_project: the project that the view is in. If None,\n            self.project_id will be used.\n        :type view_project: str\n        :return: the datasets resource of the source dataset.",
        "code": "def foo(self,\n                                      source_dataset,\n                                      view_dataset,\n                                      view_table,\n                                      source_project=None,\n                                      view_project=None):\n        \n\n        \n        source_project = source_project if source_project else self.project_id\n        view_project = view_project if view_project else self.project_id\n\n        \n        \n        source_dataset_resource = self.service.datasets().get(\n            projectId=source_project, datasetId=source_dataset).execute(num_retries=self.num_retries)\n        access = source_dataset_resource[\n            'access'] if 'access' in source_dataset_resource else []\n        view_access = {\n            'view': {\n                'projectId': view_project,\n                'datasetId': view_dataset,\n                'tableId': view_table\n            }\n        }\n        \n        if view_access not in access:\n            self.log.info(\n                'Granting table %s:%s.%s authorized view access to %s:%s dataset.',\n                view_project, view_dataset, view_table, source_project,\n                source_dataset)\n            access.append(view_access)\n            return self.service.datasets().patch(\n                projectId=source_project,\n                datasetId=source_dataset,\n                body={\n                    'access': access\n                }).execute(num_retries=self.num_retries)\n        else:\n            \n            self.log.info(\n                'Table %s:%s.%s already has authorized view access to %s:%s dataset.',\n                view_project, view_dataset, view_table, source_project, source_dataset)\n            return source_dataset_resource"
    },
    {
        "docstring": "Method returns dataset_resource if dataset exist\n        and raised 404 error if dataset does not exist\n\n        :param dataset_id: The BigQuery Dataset ID\n        :type dataset_id: str\n        :param project_id: The GCP Project ID\n        :type project_id: str\n        :return: dataset_resource\n\n            .. seealso::\n                For more information, see Dataset Resource content:\n                https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource",
        "code": "def foo(self, dataset_id, project_id=None):\n        \n\n        if not dataset_id or not isinstance(dataset_id, str):\n            raise ValueError(\"dataset_id argument must be provided and has \"\n                             \"a type 'str'. You provided: {}\".format(dataset_id))\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            dataset_resource = self.service.datasets().get(\n                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)\n            self.log.info(\"Dataset Resource: %s\", dataset_resource)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return dataset_resource"
    },
    {
        "docstring": "Method returns full list of BigQuery datasets in the current project\n\n        .. seealso::\n            For more information, see:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list\n\n        :param project_id: Google Cloud Project for which you\n            try to get all datasets\n        :type project_id: str\n        :return: datasets_list\n\n            Example of returned datasets_list: ::\n\n                   {\n                      \"kind\":\"bigquery#dataset\",\n                      \"location\":\"US\",\n                      \"id\":\"your-project:dataset_2_test\",\n                      \"datasetReference\":{\n                         \"projectId\":\"your-project\",\n                         \"datasetId\":\"dataset_2_test\"\n                      }\n                   },\n                   {\n                      \"kind\":\"bigquery#dataset\",\n                      \"location\":\"US\",\n                      \"id\":\"your-project:dataset_1_test\",\n                      \"datasetReference\":{\n                         \"projectId\":\"your-project\",\n                         \"datasetId\":\"dataset_1_test\"\n                      }\n                   }\n                ]",
        "code": "def foo(self, project_id=None):\n        \n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            datasets_list = self.service.datasets().list(\n                projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets']\n            self.log.info(\"Datasets List: %s\", datasets_list)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return datasets_list"
    },
    {
        "docstring": "Method to stream data into BigQuery one record at a time without needing\n        to run a load job\n\n        .. seealso::\n            For more information, see:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\n\n        :param project_id: The name of the project where we have the table\n        :type project_id: str\n        :param dataset_id: The name of the dataset where we have the table\n        :type dataset_id: str\n        :param table_id: The name of the table\n        :type table_id: str\n        :param rows: the rows to insert\n        :type rows: list\n\n        **Example or rows**:\n            rows=[{\"json\": {\"a_key\": \"a_value_0\"}}, {\"json\": {\"a_key\": \"a_value_1\"}}]\n\n        :param ignore_unknown_values: [Optional] Accept rows that contain values\n            that do not match the schema. The unknown values are ignored.\n            The default value  is false, which treats unknown values as errors.\n        :type ignore_unknown_values: bool\n        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,\n            even if invalid rows exist. The default value is false, which causes\n            the entire request to fail if any invalid rows exist.\n        :type skip_invalid_rows: bool\n        :param fail_on_error: [Optional] Force the task to fail if any errors occur.\n            The default value is false, which indicates the task should not fail\n            even if any insertion errors occur.\n        :type fail_on_error: bool",
        "code": "def foo(self, project_id, dataset_id, table_id,\n                   rows, ignore_unknown_values=False,\n                   skip_invalid_rows=False, fail_on_error=False):\n        \n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        body = {\n            \"rows\": rows,\n            \"ignoreUnknownValues\": ignore_unknown_values,\n            \"kind\": \"bigquery\n            \"skipInvalidRows\": skip_invalid_rows,\n        }\n\n        try:\n            self.log.info(\n                'Inserting %s row(s) into Table %s:%s.%s',\n                len(rows), dataset_project_id, dataset_id, table_id\n            )\n\n            resp = self.service.tabledata().insertAll(\n                projectId=dataset_project_id, datasetId=dataset_id,\n                tableId=table_id, body=body\n            ).execute(num_retries=self.num_retries)\n\n            if 'insertErrors' not in resp:\n                self.log.info(\n                    'All row(s) inserted successfully: %s:%s.%s',\n                    dataset_project_id, dataset_id, table_id\n                )\n            else:\n                error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}'.format(\n                    len(resp['insertErrors']),\n                    dataset_project_id, dataset_id, table_id, resp['insertErrors'])\n                if fail_on_error:\n                    raise AirflowException(\n                        'BigQuery job failed. Error was: {}'.format(error_msg)\n                    )\n                self.log.info(error_msg)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )"
    },
    {
        "docstring": "Executes a BigQuery query, and returns the job ID.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param parameters: Parameters to substitute into the query.\n        :type parameters: dict",
        "code": "def foo(self, operation, parameters=None):\n        \n        sql = _bind_parameters(operation,\n                               parameters) if parameters else operation\n        self.job_id = self.run_query(sql)"
    },
    {
        "docstring": "Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list",
        "code": "def foo(self, operation, seq_of_parameters):\n        \n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)"
    },
    {
        "docstring": "Helper method for fetchone, which returns the next row from a buffer.\n        If the buffer is empty, attempts to paginate through the result set for\n        the next page, and load it into the buffer.",
        "code": "def foo(self):\n        \n        if not self.job_id:\n            return None\n\n        if len(self.buffer) == 0:\n            if self.all_pages_loaded:\n                return None\n\n            query_results = (self.service.jobs().getQueryResults(\n                projectId=self.project_id,\n                jobId=self.job_id,\n                pageToken=self.page_token).execute(num_retries=self.num_retries))\n\n            if 'rows' in query_results and query_results['rows']:\n                self.page_token = query_results.get('pageToken')\n                fields = query_results['schema']['fields']\n                col_types = [field['type'] for field in fields]\n                rows = query_results['rows']\n\n                for dict_row in rows:\n                    typed_row = ([\n                        _bq_cast(vs['v'], col_types[idx])\n                        for idx, vs in enumerate(dict_row['f'])\n                    ])\n                    self.buffer.append(typed_row)\n\n                if not self.page_token:\n                    self.all_pages_loaded = True\n\n            else:\n                \n                self.page_token = None\n                self.job_id = None\n                self.page_token = None\n                return None\n\n        return self.buffer.pop(0)"
    },
    {
        "docstring": "Queries Postgres and returns a cursor to the results.",
        "code": "def foo(self):\n        \n        postgres = PostgresHook(postgres_conn_id=self.postgres_conn_id)\n        conn = postgres.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql, self.parameters)\n        return cursor"
    },
    {
        "docstring": "Create all the intermediate directories in a remote host\n\n    :param sftp_client: A Paramiko SFTP client.\n    :param remote_directory: Absolute Path of the directory containing the file\n    :return:",
        "code": "def foo(sftp_client, remote_directory):\n    \n    if remote_directory == '/':\n        sftp_client.chdir('/')\n        return\n    if remote_directory == '':\n        return\n    try:\n        sftp_client.chdir(remote_directory)\n    except IOError:\n        dirname, basename = os.path.split(remote_directory.rstrip('/'))\n        foo(sftp_client, dirname)\n        sftp_client.mkdir(basename)\n        sftp_client.chdir(basename)\n        return"
    },
    {
        "docstring": "Create queue using connection object\n\n        :param queue_name: name of the queue.\n        :type queue_name: str\n        :param attributes: additional attributes for the queue (default: None)\n            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`\n        :type attributes: dict\n\n        :return: dict with the information about the queue\n            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`\n        :rtype: dict",
        "code": "def foo(self, queue_name, attributes=None):\n        \n        return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})"
    },
    {
        "docstring": "Send message to the queue\n\n        :param queue_url: queue url\n        :type queue_url: str\n        :param message_body: the contents of the message\n        :type message_body: str\n        :param delay_seconds: seconds to delay the message\n        :type delay_seconds: int\n        :param message_attributes: additional attributes for the message (default: None)\n            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`\n        :type message_attributes: dict\n\n        :return: dict with the information about the message sent\n            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`\n        :rtype: dict",
        "code": "def foo(self, queue_url, message_body, delay_seconds=0, message_attributes=None):\n        \n        return self.get_conn().send_message(QueueUrl=queue_url,\n                                            MessageBody=message_body,\n                                            DelaySeconds=delay_seconds,\n                                            MessageAttributes=message_attributes or {})"
    },
    {
        "docstring": "Run the task command.\n\n        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n        :type run_with: list\n        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n            ``['airflow run']``\n        :param join_args: bool\n        :return: the process that was run\n        :rtype: subprocess.Popen",
        "code": "def foo(self, run_with=None, join_args=False):\n        \n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid\n        )\n\n        \n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc"
    },
    {
        "docstring": "A callback that should be called when this is done running.",
        "code": "def foo(self):\n        \n        if self._cfg_path and os.path.isfile(self._cfg_path):\n            if self.run_as_user:\n                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)\n            else:\n                os.remove(self._cfg_path)"
    },
    {
        "docstring": "Parse options and process commands",
        "code": "def foo():\n    \n    \n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))\n    parser.add_option(\"-q\", \"--quiet\",\n                      action=\"store_false\", dest=\"verbose\", default=True,\n                      help=\"don't print messages to stdout\")\n\n    (options, args) = parser.parse_args()"
    },
    {
        "docstring": "generate HTML header content",
        "code": "def foo(self):\n        \n        self.htmlheader = ''\n        \n        global _js_initialized\n        if '_js_initialized' not in globals() or not _js_initialized:\n            for css in self.header_css:\n                self.htmlheader += css\n            for js in self.header_js:\n                self.htmlheader += js"
    },
    {
        "docstring": "generate HTML div",
        "code": "def foo(self):\n        \n        if self.container:\n            return\n\n        \n        if self.width:\n            if self.width[-1] != '%':\n                self.style += 'width:%spx;' % self.width\n            else:\n                self.style += 'width:%s;' % self.width\n        if self.height:\n            if self.height[-1] != '%':\n                self.style += 'height:%spx;' % self.height\n            else:\n                self.style += 'height:%s;' % self.height\n        if self.style:\n            self.style = 'style=\"%s\"' % self.style\n\n        self.container = self.containerheader + \\\n            '<div id=\"%s\"><svg %s></svg></div>\\n' % (self.name, self.style)"
    },
    {
        "docstring": "generate javascript code for the chart",
        "code": "def foo(self):\n        \n        self.jschart = ''\n\n        \n        \n        if self.tooltip_condition_string == '':\n            self.tooltip_condition_string = 'var y = String(graph.point.y);\\n'\n\n        \n        self.series_js = json.dumps(self.series)"
    },
    {
        "docstring": "Create X-axis",
        "code": "def foo(self, name, label=None, format=None, date=False, custom_format=False):\n        \n        axis = {}\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            if format == 'AM_PM':\n                axis['tickFormat'] = \"function(d) { return get_am_pm(parseInt(d)); }\"\n            else:\n                axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        \n        if date:\n            self.dateformat = format\n            axis['tickFormat'] = (\"function(d) { return d3.time.format('%s')\"\n                                  \"(new Date(parseInt(d))) }\\n\"\n                                  \"\" % self.dateformat)\n            \n            if name[0] == 'x':\n                self.x_axis_date = True\n\n        \n        self.axislist[name] = axis\n\n        \n        if name == \"xAxis\" and self.focus_enable:\n            self.axislist['x2Axis'] = axis"
    },
    {
        "docstring": "Create Y-axis",
        "code": "def foo(self, name, label=None, format=None, custom_format=False):\n        \n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        \n        self.axislist[name] = axis"
    },
    {
        "docstring": "Returns a sqlite connection object",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.sqlite_conn_id)\n        conn = sqlite3.connect(conn.host)\n        return conn"
    },
    {
        "docstring": "Decorator to log user actions",
        "code": "def foo(f):\n    \n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n\n        with create_session() as session:\n            if g.user.is_anonymous:\n                user = 'anonymous'\n            else:\n                user = g.user.username\n\n            log = Log(\n                event=f.__name__,\n                task_instance=None,\n                owner=user,\n                extra=str(list(request.args.items())),\n                task_id=request.args.get('task_id'),\n                dag_id=request.args.get('dag_id'))\n\n            if 'execution_date' in request.args:\n                log.execution_date = pendulum.parse(\n                    request.args.get('execution_date'))\n\n            session.add(log)\n\n        return f(*args, **kwargs)\n\n    return wrapper"
    },
    {
        "docstring": "Decorator to make a view compressed",
        "code": "def foo(f):\n    \n    @functools.wraps(f)\n    def view_func(*args, **kwargs):\n        @after_this_request\n        def zipper(response):\n            accept_encoding = request.headers.get('Accept-Encoding', '')\n\n            if 'gzip' not in accept_encoding.lower():\n                return response\n\n            response.direct_passthrough = False\n\n            if (response.status_code < 200 or response.status_code >= 300 or\n                    'Content-Encoding' in response.headers):\n                return response\n            gzip_buffer = IO()\n            gzip_file = gzip.GzipFile(mode='wb',\n                                      fileobj=gzip_buffer)\n            gzip_file.write(response.data)\n            gzip_file.close()\n\n            response.data = gzip_buffer.getvalue()\n            response.headers['Content-Encoding'] = 'gzip'\n            response.headers['Vary'] = 'Accept-Encoding'\n            response.headers['Content-Length'] = len(response.data)\n\n            return response\n\n        return f(*args, **kwargs)\n\n    return view_func"
    },
    {
        "docstring": "Returns the last dag run for a dag, None if there was none.\n    Last dag run can be any type of run eg. scheduled or backfilled.\n    Overridden DagRuns are ignored.",
        "code": "def foo(dag_id, session, include_externally_triggered=False):\n    \n    DR = DagRun\n    query = session.query(DR).filter(DR.dag_id == dag_id)\n    if not include_externally_triggered:\n        query = query.filter(DR.external_trigger == False)  \n    query = query.order_by(DR.execution_date.desc())\n    return query.first()"
    },
    {
        "docstring": "Creates a dag run from this dag including the tasks associated with this dag.\n        Returns the dag run.\n\n        :param run_id: defines the the run id for this dag run\n        :type run_id: str\n        :param execution_date: the execution date of this dag run\n        :type execution_date: datetime.datetime\n        :param state: the state of the dag run\n        :type state: airflow.utils.state.State\n        :param start_date: the date this dag run should be evaluated\n        :type start_date: datetime.datetime\n        :param external_trigger: whether this dag run is externally triggered\n        :type external_trigger: bool\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session",
        "code": "def foo(self,\n                      run_id,\n                      state,\n                      execution_date,\n                      start_date=None,\n                      external_trigger=False,\n                      conf=None,\n                      session=None):\n        \n\n        return self.get_dag().create_dagrun(run_id=run_id,\n                                            state=state,\n                                            execution_date=execution_date,\n                                            start_date=start_date,\n                                            external_trigger=external_trigger,\n                                            conf=conf,\n                                            session=session)"
    },
    {
        "docstring": "Publish the message to SQS queue\n\n        :param context: the context object\n        :type context: dict\n        :return: dict with information about the message sent\n            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`\n        :rtype: dict",
        "code": "def foo(self, context):\n        \n\n        hook = SQSHook(aws_conn_id=self.aws_conn_id)\n\n        result = hook.send_message(queue_url=self.sqs_queue,\n                                   message_body=self.message_content,\n                                   delay_seconds=self.delay_seconds,\n                                   message_attributes=self.message_attributes)\n\n        self.log.info('result is send_message is %s', result)\n\n        return result"
    },
    {
        "docstring": "returns a json response from a json serializable python object",
        "code": "def foo(obj):\n    \n    return Response(\n        response=json.dumps(\n            obj, indent=4, cls=AirflowJsonEncoder),\n        status=200,\n        mimetype=\"application/json\")"
    },
    {
        "docstring": "Opens the given file. If the path contains a folder with a .zip suffix, then\n    the folder is treated as a zip archive, opening the file inside the archive.\n\n    :return: a file object, as in `open`, or as in `ZipFile.open`.",
        "code": "def foo(f, mode='r'):\n    \n\n    _, archive, filename = ZIP_REGEX.search(f).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return zipfile.ZipFile(archive, mode=mode).open(filename)\n    else:\n        return io.open(f, mode=mode)"
    },
    {
        "docstring": "Used by cache to get a unique key per URL",
        "code": "def foo(*args, **kwargs):\n    \n    path = request.path\n    args = str(hash(frozenset(request.args.items())))\n    return (path + args).encode('ascii', 'ignore')"
    },
    {
        "docstring": "Returns Gcp Video Intelligence Service client\n\n        :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient",
        "code": "def foo(self):\n        \n        if not self._conn:\n            self._conn = VideoIntelligenceServiceClient(credentials=self._get_credentials())\n        return self._conn"
    },
    {
        "docstring": "Performs video annotation.\n\n        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n            which must be specified in the following format: ``gs://bucket-id/object-id``.\n        :type input_uri: str\n        :param input_content: The video data bytes.\n            If unset, the input video(s) should be specified via ``input_uri``.\n            If set, ``input_uri`` should be unset.\n        :type input_content: bytes\n        :param features: Requested video annotation features.\n        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]\n        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,\n            only Google Cloud Storage URIs are supported, which must be specified in the following format:\n            ``gs://bucket-id/object-id``.\n        :type output_uri: str\n        :param video_context: Optional, Additional video context and/or feature-specific parameters.\n        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext\n        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:\n            us-east1, us-west1, europe-west1, asia-east1.\n            If no region is specified, a region will be determined based on video file location.\n        :type location: str\n        :param retry: Retry object used to determine when/if to retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Optional, Additional metadata that is provided to the method.\n        :type metadata: seq[tuple[str, str]]",
        "code": "def foo(\n        self,\n        input_uri=None,\n        input_content=None,\n        features=None,\n        video_context=None,\n        output_uri=None,\n        location=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \n        client = self.get_conn()\n        return client.annotate_video(\n            input_uri=input_uri,\n            input_content=input_content,\n            features=features,\n            video_context=video_context,\n            output_uri=output_uri,\n            location_id=location,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )"
    },
    {
        "docstring": "Get Opsgenie api_key for creating alert",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.http_conn_id)\n        api_key = conn.password\n        if not api_key:\n            raise AirflowException('Opsgenie API Key is required for this hook, '\n                                   'please check your conn_id configuration.')\n        return api_key"
    },
    {
        "docstring": "Overwrite HttpHook get_conn because this hook just needs base_url\n        and headers, and does not need generic params\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict",
        "code": "def foo(self, headers=None):\n        \n        conn = self.get_connection(self.http_conn_id)\n        self.base_url = conn.host if conn.host else 'https://api.opsgenie.com'\n        session = requests.Session()\n        if headers:\n            session.headers.update(headers)\n        return session"
    },
    {
        "docstring": "Execute the Opsgenie Alert call\n\n        :param payload: Opsgenie API Create Alert payload values\n            See https://docs.opsgenie.com/docs/alert-api#section-create-alert\n        :type payload: dict",
        "code": "def foo(self, payload={}):\n        \n        api_key = self._get_api_key()\n        return self.run(endpoint='v2/alerts',\n                        data=json.dumps(payload),\n                        headers={'Content-Type': 'application/json',\n                                 'Authorization': 'GenieKey %s' % api_key})"
    },
    {
        "docstring": "Construct the Opsgenie JSON payload. All relevant parameters are combined here\n        to a valid Opsgenie JSON payload.\n\n        :return: Opsgenie payload (dict) to send",
        "code": "def foo(self):\n        \n        payload = {}\n\n        for key in [\n            \"message\", \"alias\", \"description\", \"responders\",\n            \"visibleTo\", \"actions\", \"tags\", \"details\", \"entity\",\n            \"source\", \"priority\", \"user\", \"note\"\n        ]:\n            val = getattr(self, key)\n            if val:\n                payload[key] = val\n        return payload"
    },
    {
        "docstring": "Call the OpsgenieAlertHook to post message",
        "code": "def foo(self, context):\n        \n        self.hook = OpsgenieAlertHook(self.opsgenie_conn_id)\n        self.hook.execute(self._build_opsgenie_payload())"
    },
    {
        "docstring": "check if aws conn exists already or create one and return it\n\n        :return: boto3 session",
        "code": "def foo(self):\n        \n        if not self.conn:\n            self.conn = self.get_client_type('athena')\n        return self.conn"
    },
    {
        "docstring": "Run Presto query on athena with provided config and return submitted query_execution_id\n\n        :param query: Presto query to run\n        :type query: str\n        :param query_context: Context in which query need to be run\n        :type query_context: dict\n        :param result_configuration: Dict with path to store results in and config related to encryption\n        :type result_configuration: dict\n        :param client_request_token: Unique token created by user to avoid multiple executions of same query\n        :type client_request_token: str\n        :return: str",
        "code": "def foo(self, query, query_context, result_configuration, client_request_token=None):\n        \n        response = self.conn.start_query_execution(QueryString=query,\n                                                   ClientRequestToken=client_request_token,\n                                                   QueryExecutionContext=query_context,\n                                                   ResultConfiguration=result_configuration)\n        query_execution_id = response['QueryExecutionId']\n        return query_execution_id"
    },
    {
        "docstring": "Fetch the status of submitted athena query. Returns None or one of valid query states.\n\n        :param query_execution_id: Id of submitted athena query\n        :type query_execution_id: str\n        :return: str",
        "code": "def foo(self, query_execution_id):\n        \n        response = self.conn.get_query_execution(QueryExecutionId=query_execution_id)\n        state = None\n        try:\n            state = response['QueryExecution']['Status']['State']\n        except Exception as ex:\n            self.log.error('Exception while getting query state', ex)\n        finally:\n            return state"
    },
    {
        "docstring": "Poll the status of submitted athena query until query state reaches final state.\n        Returns one of the final states\n\n        :param query_execution_id: Id of submitted athena query\n        :type query_execution_id: str\n        :param max_tries: Number of times to poll for query state before function exits\n        :type max_tries: int\n        :return: str",
        "code": "def foo(self, query_execution_id, max_tries=None):\n        \n        try_number = 1\n        final_query_state = None  \n        while True:\n            query_state = self.check_query_status(query_execution_id)\n            if query_state is None:\n                self.log.info('Trial {try_number}: Invalid query state. Retrying again'.format(\n                    try_number=try_number))\n            elif query_state in self.INTERMEDIATE_STATES:\n                self.log.info('Trial {try_number}: Query is still in an intermediate state - {state}'\n                              .format(try_number=try_number, state=query_state))\n            else:\n                self.log.info('Trial {try_number}: Query execution completed. Final state is {state}'\n                              .format(try_number=try_number, state=query_state))\n                final_query_state = query_state\n                break\n            if max_tries and try_number >= max_tries:  \n                final_query_state = query_state\n                break\n            try_number += 1\n            sleep(self.sleep_time)\n        return final_query_state"
    },
    {
        "docstring": "Returns an SFTP connection object",
        "code": "def foo(self):\n        \n        if self.conn is None:\n            cnopts = pysftp.CnOpts()\n            if self.no_host_key_check:\n                cnopts.hostkeys = None\n            cnopts.compression = self.compress\n            conn_params = {\n                'host': self.remote_host,\n                'port': self.port,\n                'username': self.username,\n                'cnopts': cnopts\n            }\n            if self.password and self.password.strip():\n                conn_params['password'] = self.password\n            if self.key_file:\n                conn_params['private_key'] = self.key_file\n            if self.private_key_pass:\n                conn_params['private_key_pass'] = self.private_key_pass\n\n            self.conn = pysftp.Connection(**conn_params)\n        return self.conn"
    },
    {
        "docstring": "Sleep for the time specified in the exception. If not specified, wait\n        for 60 seconds.",
        "code": "def foo(self, rate_limit_exception):\n        \n        retry_after = int(\n            rate_limit_exception.response.headers.get('Retry-After', 60))\n        self.log.info(\n            \"Hit Zendesk API rate limit. Pausing for %s seconds\",\n            retry_after\n        )\n        time.sleep(retry_after)"
    },
    {
        "docstring": "Call Zendesk API and return results\n\n        :param path: The Zendesk API to call\n        :param query: Query parameters\n        :param get_all_pages: Accumulate results over all pages before\n               returning. Due to strict rate limiting, this can often timeout.\n               Waits for recommended period between tries after a timeout.\n        :param side_loading: Retrieve related records as part of a single\n               request. In order to enable side-loading, add an 'include'\n               query parameter containing a comma-separated list of resources\n               to load. For more information on side-loading see\n               https://developer.zendesk.com/rest_api/docs/core/side_loading",
        "code": "def foo(self, path, query=None, get_all_pages=True, side_loading=False):\n        \n        zendesk = self.get_conn()\n        first_request_successful = False\n\n        while not first_request_successful:\n            try:\n                results = zendesk.call(path, query)\n                first_request_successful = True\n            except RateLimitError as rle:\n                self.__handle_rate_limit_exception(rle)\n\n        \n        keys = [path.split(\"/\")[-1].split(\".json\")[0]]\n        next_page = results['next_page']\n        if side_loading:\n            keys += query['include'].split(',')\n        results = {key: results[key] for key in keys}\n\n        if get_all_pages:\n            while next_page is not None:\n                try:\n                    \n                    \n                    \n                    next_url = next_page.split(self.__url)[1]\n                    self.log.info(\"Calling %s\", next_url)\n                    more_res = zendesk.call(next_url)\n                    for key in results:\n                        results[key].extend(more_res[key])\n                    if next_page == more_res['next_page']:\n                        \n                        \n                        \n                        \n                        break\n                    else:\n                        next_page = more_res['next_page']\n                except RateLimitError as rle:\n                    self.__handle_rate_limit_exception(rle)\n                except ZendeskError as ze:\n                    if b\"Use a start_time older than 5 minutes\" in ze.msg:\n                        \n                        break\n                    else:\n                        raise ze\n\n        return results"
    },
    {
        "docstring": "Retrieves the partition values for a table.\n\n        :param database_name: The name of the catalog database where the partitions reside.\n        :type database_name: str\n        :param table_name: The name of the partitions' table.\n        :type table_name: str\n        :param expression: An expression filtering the partitions to be returned.\n            Please see official AWS documentation for further information.\n            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n        :type expression: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        :return: set of partition values where each value is a tuple since\n            a partition may be composed of multiple columns. For example:\n            ``{('2018-01-01','1'), ('2018-01-01','2')}``",
        "code": "def foo(self,\n                       database_name,\n                       table_name,\n                       expression='',\n                       page_size=None,\n                       max_items=None):\n        \n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('get_partitions')\n        response = paginator.paginate(\n            DatabaseName=database_name,\n            TableName=table_name,\n            Expression=expression,\n            PaginationConfig=config\n        )\n\n        partitions = set()\n        for page in response:\n            for p in page['Partitions']:\n                partitions.add(tuple(p['Values']))\n\n        return partitions"
    },
    {
        "docstring": "Get the information of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :rtype: dict\n\n        >>> hook = AwsGlueCatalogHook()\n        >>> r = hook.get_table('db', 'table_foo')\n        >>> r['Name'] = 'table_foo'",
        "code": "def foo(self, database_name, table_name):\n        \n\n        result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)\n\n        return result['Table']"
    },
    {
        "docstring": "Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str",
        "code": "def foo(self, database_name, table_name):\n        \n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']"
    },
    {
        "docstring": "Return status of a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str",
        "code": "def foo(self, cluster_identifier):\n        \n        conn = self.get_conn()\n        try:\n            response = conn.describe_clusters(\n                ClusterIdentifier=cluster_identifier)['Clusters']\n            return response[0]['ClusterStatus'] if response else None\n        except conn.exceptions.ClusterNotFoundFault:\n            return 'cluster_not_found'"
    },
    {
        "docstring": "Delete a cluster and optionally create a snapshot\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        :param skip_final_cluster_snapshot: determines cluster snapshot creation\n        :type skip_final_cluster_snapshot: bool\n        :param final_cluster_snapshot_identifier: name of final cluster snapshot\n        :type final_cluster_snapshot_identifier: str",
        "code": "def foo(\n            self,\n            cluster_identifier,\n            skip_final_cluster_snapshot=True,\n            final_cluster_snapshot_identifier=''):\n        \n        response = self.get_conn().delete_cluster(\n            ClusterIdentifier=cluster_identifier,\n            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,\n            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None"
    },
    {
        "docstring": "Gets a list of snapshots for a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str",
        "code": "def foo(self, cluster_identifier):\n        \n        response = self.get_conn().describe_cluster_snapshots(\n            ClusterIdentifier=cluster_identifier\n        )\n        if 'Snapshots' not in response:\n            return None\n        snapshots = response['Snapshots']\n        snapshots = filter(lambda x: x['Status'], snapshots)\n        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)\n        return snapshots"
    },
    {
        "docstring": "Restores a cluster from its snapshot\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str",
        "code": "def foo(self, cluster_identifier, snapshot_identifier):\n        \n        response = self.get_conn().restore_from_cluster_snapshot(\n            ClusterIdentifier=cluster_identifier,\n            SnapshotIdentifier=snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None"
    },
    {
        "docstring": "Creates a snapshot of a cluster\n\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str",
        "code": "def foo(self, snapshot_identifier, cluster_identifier):\n        \n        response = self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n        )\n        return response['Snapshot'] if response['Snapshot'] else None"
    },
    {
        "docstring": "SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n        It should not prevent a DAG from completing in success",
        "code": "def foo(self, **kwargs):\n        \n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\n        slack.call(self.method, self.api_params)"
    },
    {
        "docstring": "Creates a job flow using the config from the EMR connection.\n        Keys of the json extra hash may have the arguments of the boto3\n        run_job_flow method.\n        Overrides for this config may be passed as the job_flow_overrides.",
        "code": "def foo(self, job_flow_overrides):\n        \n\n        if not self.emr_conn_id:\n            raise AirflowException('emr_conn_id must be present to use create_job_flow')\n\n        emr_conn = self.get_connection(self.emr_conn_id)\n\n        config = emr_conn.extra_dejson.copy()\n        config.update(job_flow_overrides)\n\n        response = self.get_conn().run_job_flow(**config)\n\n        return response"
    },
    {
        "docstring": "Will test the filepath result and test if its size is at least self.filesize\n\n        :param result: a list of dicts returned by Snakebite ls\n        :param size: the file size in MB a file should be at least to trigger True\n        :return: (bool) depending on the matching criteria",
        "code": "def foo(result, size=None):\n        \n        if size:\n            log = LoggingMixin().log\n            log.debug(\n                'Filtering for file size >= %s in files: %s',\n                size, map(lambda x: x['path'], result)\n            )\n            size *= settings.MEGABYTE\n            result = [x for x in result if x['length'] >= size]\n            log.debug('HdfsSensor.poke: after size filter result is %s', result)\n        return result"
    },
    {
        "docstring": "Will filter if instructed to do so the result to remove matching criteria\n\n        :param result: list of dicts returned by Snakebite ls\n        :type result: list[dict]\n        :param ignored_ext: list of ignored extensions\n        :type ignored_ext: list\n        :param ignore_copying: shall we ignore ?\n        :type ignore_copying: bool\n        :return: list of dicts which were not removed\n        :rtype: list[dict]",
        "code": "def foo(result, ignored_ext, ignore_copying):\n        \n        if ignore_copying:\n            log = LoggingMixin().log\n            regex_builder = r\"^.*\\.(%s$)$\" % '$|'.join(ignored_ext)\n            ignored_extensions_regex = re.compile(regex_builder)\n            log.debug(\n                'Filtering result for ignored extensions: %s in files %s',\n                ignored_extensions_regex.pattern, map(lambda x: x['path'], result)\n            )\n            result = [x for x in result if not ignored_extensions_regex.match(x['path'])]\n            log.debug('HdfsSensor.poke: after ext filter result is %s', result)\n        return result"
    },
    {
        "docstring": "Executed by task_instance at runtime",
        "code": "def foo(self, context):\n        \n        s3_conn = S3Hook(self.s3_conn_id)\n\n        \n        if self.is_pipeline:\n            results = MongoHook(self.mongo_conn_id).aggregate(\n                mongo_collection=self.mongo_collection,\n                aggregate_query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        else:\n            results = MongoHook(self.mongo_conn_id).find(\n                mongo_collection=self.mongo_collection,\n                query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        \n        docs_str = self._stringify(self.transform(results))\n\n        \n        s3_conn.load_string(\n            string_data=docs_str,\n            key=self.s3_key,\n            bucket_name=self.s3_bucket,\n            replace=self.replace\n        )\n\n        return True"
    },
    {
        "docstring": "Get pool by a given name.",
        "code": "def foo(name, session=None):\n    \n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    return pool"
    },
    {
        "docstring": "Create a pool with a given parameters.",
        "code": "def foo(name, slots, description, session=None):\n    \n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    try:\n        slots = int(slots)\n    except ValueError:\n        raise AirflowBadRequest(\"Bad value for `slots`: %s\" % slots)\n\n    session.expire_on_commit = False\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        pool = Pool(pool=name, slots=slots, description=description)\n        session.add(pool)\n    else:\n        pool.slots = slots\n        pool.description = description\n\n    session.commit()\n\n    return pool"
    },
    {
        "docstring": "Delete pool by a given name.",
        "code": "def foo(name, session=None):\n    \n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    session.delete(pool)\n    session.commit()\n\n    return pool"
    },
    {
        "docstring": "Converts a python dictionary to the proto supplied\n\n        :param py_dict: The dictionary to convert\n        :type py_dict: dict\n        :param proto: The proto object to merge with dictionary\n        :type proto: protobuf\n        :return: A parsed python dictionary in provided proto format\n        :raises:\n            ParseError: On JSON parsing problems.",
        "code": "def foo(py_dict, proto):\n        \n        dict_json_str = json.dumps(py_dict)\n        return json_format.Parse(dict_json_str, proto)"
    },
    {
        "docstring": "Given an operation, continuously fetches the status from Google Cloud until either\n        completion or an error occurring\n\n        :param operation: The Operation to wait for\n        :type operation: google.cloud.container_V1.gapic.enums.Operation\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :return: A new, updated operation fetched from Google Cloud",
        "code": "def foo(self, operation, project_id=None):\n        \n        self.log.info(\"Waiting for OPERATION_NAME %s\", operation.name)\n        time.sleep(OPERATIONAL_POLL_INTERVAL)\n        while operation.status != Operation.Status.DONE:\n            if operation.status == Operation.Status.RUNNING or operation.status == \\\n                    Operation.Status.PENDING:\n                time.sleep(OPERATIONAL_POLL_INTERVAL)\n            else:\n                raise exceptions.GoogleCloudError(\n                    \"Operation has failed with status: %s\" % operation.status)\n            \n            operation = self.get_operation(operation.name, project_id=project_id or self.project_id)\n        return operation"
    },
    {
        "docstring": "Fetches the operation from Google Cloud\n\n        :param operation_name: Name of operation to fetch\n        :type operation_name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :return: The new, updated operation from Google Cloud",
        "code": "def foo(self, operation_name, project_id=None):\n        \n        return self.get_client().get_operation(project_id=project_id or self.project_id,\n                                               zone=self.location,\n                                               operation_id=operation_name)"
    },
    {
        "docstring": "Append labels to provided Cluster Protobuf\n\n        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current\n         airflow version string follows semantic versioning spec: x.y.z).\n\n        :param cluster_proto: The proto to append resource_label airflow\n            version to\n        :type cluster_proto: google.cloud.container_v1.types.Cluster\n        :param key: The key label\n        :type key: str\n        :param val:\n        :type val: str\n        :return: The cluster proto updated with new label",
        "code": "def foo(cluster_proto, key, val):\n        \n        val = val.replace('.', '-').replace('+', '-')\n        cluster_proto.resource_labels.update({key: val})\n        return cluster_proto"
    },
    {
        "docstring": "Creates a cluster, consisting of the specified number and type of Google Compute\n        Engine instances.\n\n        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n            be of the same form as the protobuf message\n            :class:`google.cloud.container_v1.types.Cluster`\n        :type cluster: dict or google.cloud.container_v1.types.Cluster\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n            retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: The full url to the new, or existing, cluster\n        :raises:\n            ParseError: On JSON parsing problems when trying to convert dict\n            AirflowException: cluster is not dict type nor Cluster proto type",
        "code": "def foo(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \n\n        if isinstance(cluster, dict):\n            cluster_proto = Cluster()\n            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)\n        elif not isinstance(cluster, Cluster):\n            raise AirflowException(\n                \"cluster is not instance of Cluster proto or python dict\")\n\n        self._append_label(cluster, 'airflow-version', 'v' + version.version)\n\n        self.log.info(\n            \"Creating (project_id=%s, zone=%s, cluster_name=%s)\",\n            self.project_id, self.location, cluster.name\n        )\n        try:\n            op = self.get_client().create_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster=cluster,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n\n            return op.target_link\n        except AlreadyExists as error:\n            self.log.info('Assuming Success: %s', error.message)\n            return self.get_cluster(name=cluster.name).self_link"
    },
    {
        "docstring": "Gets details of specified cluster\n\n        :param name: The name of the cluster to retrieve\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: google.cloud.container_v1.types.Cluster",
        "code": "def foo(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \n        self.log.info(\n            \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\",\n            project_id or self.project_id, self.location, name\n        )\n\n        return self.get_client().get_cluster(project_id=project_id or self.project_id,\n                                             zone=self.location,\n                                             cluster_id=name,\n                                             retry=retry,\n                                             timeout=timeout).self_link"
    },
    {
        "docstring": "Given a Discord http_conn_id, return the default webhook endpoint or override if a\n        webhook_endpoint is manually supplied.\n\n        :param http_conn_id: The provided connection ID\n        :param webhook_endpoint: The manually provided webhook endpoint\n        :return: Webhook endpoint (str) to use",
        "code": "def foo(self, http_conn_id, webhook_endpoint):\n        \n        if webhook_endpoint:\n            endpoint = webhook_endpoint\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson\n            endpoint = extra.get('webhook_endpoint', '')\n        else:\n            raise AirflowException('Cannot get webhook endpoint: No valid Discord '\n                                   'webhook endpoint or http_conn_id supplied.')\n\n        \n        if not re.match('^webhooks/[0-9]+/[a-zA-Z0-9_-]+$', endpoint):\n            raise AirflowException('Expected Discord webhook endpoint in the form '\n                                   'of \"webhooks/{webhook.id}/{webhook.token}\".')\n\n        return endpoint"
    },
    {
        "docstring": "Construct the Discord JSON payload. All relevant parameters are combined here\n        to a valid Discord JSON payload.\n\n        :return: Discord payload (str) to send",
        "code": "def foo(self):\n        \n        payload = {}\n\n        if self.username:\n            payload['username'] = self.username\n        if self.avatar_url:\n            payload['avatar_url'] = self.avatar_url\n\n        payload['tts'] = self.tts\n\n        if len(self.message) <= 2000:\n            payload['content'] = self.message\n        else:\n            raise AirflowException('Discord message length must be 2000 or fewer '\n                                   'characters.')\n\n        return json.dumps(payload)"
    },
    {
        "docstring": "Execute the Discord webhook call",
        "code": "def foo(self):\n        \n        proxies = {}\n        if self.proxy:\n            \n            proxies = {'https': self.proxy}\n\n        discord_payload = self._build_discord_payload()\n\n        self.run(endpoint=self.webhook_endpoint,\n                 data=discord_payload,\n                 headers={'Content-type': 'application/json'},\n                 extra_options={'proxies': proxies})"
    },
    {
        "docstring": "Encrypts a plaintext message using Google Cloud KMS.\n\n        :param key_name: The Resource Name for the key (or key version)\n                         to be used for encyption. Of the form\n                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``\n        :type key_name: str\n        :param plaintext: The message to be encrypted.\n        :type plaintext: bytes\n        :param authenticated_data: Optional additional authenticated data that\n                                   must also be provided to decrypt the message.\n        :type authenticated_data: bytes\n        :return: The base 64 encoded ciphertext of the original message.\n        :rtype: str",
        "code": "def foo(self, key_name, plaintext, authenticated_data=None):\n        \n        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()\n        body = {'plaintext': _b64encode(plaintext)}\n        if authenticated_data:\n            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\n\n        request = keys.encrypt(name=key_name, body=body)\n        response = request.execute(num_retries=self.num_retries)\n\n        ciphertext = response['ciphertext']\n        return ciphertext"
    },
    {
        "docstring": "Imports table from remote location to target dir. Arguments are\n        copies of direct sqoop command line arguments\n\n        :param table: Table to read\n        :param target_dir: HDFS destination dir\n        :param append: Append data to an existing dataset in HDFS\n        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".\n            Imports data to into the specified format. Defaults to text.\n        :param columns: <col,col,col\u2026> Columns to import from table\n        :param split_by: Column of the table used to split work units\n        :param where: WHERE clause to use during import\n        :param direct: Use direct connector if exists for the database\n        :param driver: Manually specify JDBC driver class to use\n        :param extra_import_options: Extra import options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.",
        "code": "def foo(self, table, target_dir=None, append=False, file_type=\"text\",\n                     columns=None, split_by=None, where=None, direct=False,\n                     driver=None, extra_import_options=None):\n        \n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n\n        cmd += [\"--table\", table]\n\n        if columns:\n            cmd += [\"--columns\", columns]\n        if where:\n            cmd += [\"--where\", where]\n\n        self.Popen(cmd)"
    },
    {
        "docstring": "Imports a specific query from the rdbms to hdfs\n\n        :param query: Free format query to run\n        :param target_dir: HDFS destination dir\n        :param append: Append data to an existing dataset in HDFS\n        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"\n            Imports data to hdfs into the specified format. Defaults to text.\n        :param split_by: Column of the table used to split work units\n        :param direct: Use direct import fast path\n        :param driver: Manually specify JDBC driver class to use\n        :param extra_import_options: Extra import options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.",
        "code": "def foo(self, query, target_dir, append=False, file_type=\"text\",\n                     split_by=None, direct=None, driver=None, extra_import_options=None):\n        \n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n        cmd += [\"--query\", query]\n\n        self.Popen(cmd)"
    },
    {
        "docstring": "Exports Hive table to remote location. Arguments are copies of direct\n        sqoop command line Arguments\n\n        :param table: Table remote destination\n        :param export_dir: Hive table to export\n        :param input_null_string: The string to be interpreted as null for\n            string columns\n        :param input_null_non_string: The string to be interpreted as null\n            for non-string columns\n        :param staging_table: The table in which data will be staged before\n            being inserted into the destination table\n        :param clear_staging_table: Indicate that any data present in the\n            staging table can be deleted\n        :param enclosed_by: Sets a required field enclosing character\n        :param escaped_by: Sets the escape character\n        :param input_fields_terminated_by: Sets the field separator character\n        :param input_lines_terminated_by: Sets the end-of-line character\n        :param input_optionally_enclosed_by: Sets a field enclosing character\n        :param batch: Use batch mode for underlying statement execution\n        :param relaxed_isolation: Transaction isolation to read uncommitted\n            for the mappers\n        :param extra_export_options: Extra export options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.",
        "code": "def foo(self, table, export_dir, input_null_string,\n                     input_null_non_string, staging_table,\n                     clear_staging_table, enclosed_by,\n                     escaped_by, input_fields_terminated_by,\n                     input_lines_terminated_by,\n                     input_optionally_enclosed_by, batch,\n                     relaxed_isolation, extra_export_options=None):\n        \n        cmd = self._export_cmd(table, export_dir, input_null_string,\n                               input_null_non_string, staging_table,\n                               clear_staging_table, enclosed_by, escaped_by,\n                               input_fields_terminated_by,\n                               input_lines_terminated_by,\n                               input_optionally_enclosed_by, batch,\n                               relaxed_isolation, extra_export_options)\n\n        self.Popen(cmd)"
    },
    {
        "docstring": "Retrieves connection to Cloud Text to Speech.\n\n        :return: Google Cloud Text to Speech client object.\n        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient",
        "code": "def foo(self):\n        \n        if not self._client:\n            self._client = TextToSpeechClient(credentials=self._get_credentials())\n        return self._client"
    },
    {
        "docstring": "Synthesizes text input\n\n        :param input_data: text input to be synthesized. See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput\n        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput\n        :param voice: configuration of voice to be used in synthesis. See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams\n        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams\n        :param audio_config: configuration of the synthesized audio. See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig\n        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig\n        :return: SynthesizeSpeechResponse See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse\n        :rtype: object\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n                requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float",
        "code": "def foo(self, input_data, voice, audio_config, retry=None, timeout=None):\n        \n        client = self.get_conn()\n        self.log.info(\"Synthesizing input: %s\" % input_data)\n        return client.synthesize_speech(\n            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout\n        )"
    },
    {
        "docstring": "Close and upload local log file to remote storage S3.",
        "code": "def foo(self):\n        \n        \n        \n        \n        \n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            \n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.s3_write(log, remote_loc)\n\n        \n        self.closed = True"
    },
    {
        "docstring": "When using git to retrieve the DAGs, use the GitSync Init Container",
        "code": "def foo(self):\n        \n        \n        if self.kube_config.dags_volume_claim or \\\n           self.kube_config.dags_volume_host or self.kube_config.dags_in_image:\n            return []\n\n        \n        init_environment = [{\n            'name': 'GIT_SYNC_REPO',\n            'value': self.kube_config.git_repo\n        }, {\n            'name': 'GIT_SYNC_BRANCH',\n            'value': self.kube_config.git_branch\n        }, {\n            'name': 'GIT_SYNC_ROOT',\n            'value': self.kube_config.git_sync_root\n        }, {\n            'name': 'GIT_SYNC_DEST',\n            'value': self.kube_config.git_sync_dest\n        }, {\n            'name': 'GIT_SYNC_DEPTH',\n            'value': '1'\n        }, {\n            'name': 'GIT_SYNC_ONE_TIME',\n            'value': 'true'\n        }]\n        if self.kube_config.git_user:\n            init_environment.append({\n                'name': 'GIT_SYNC_USERNAME',\n                'value': self.kube_config.git_user\n            })\n        if self.kube_config.git_password:\n            init_environment.append({\n                'name': 'GIT_SYNC_PASSWORD',\n                'value': self.kube_config.git_password\n            })\n\n        volume_mounts = [{\n            'mountPath': self.kube_config.git_sync_root,\n            'name': self.dags_volume_name,\n            'readOnly': False\n        }]\n        if self.kube_config.git_ssh_key_secret_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_secret_volume_name,\n                'mountPath': '/etc/git-secret/ssh',\n                'subPath': 'ssh'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_SSH_KEY_FILE',\n                    'value': '/etc/git-secret/ssh'\n                },\n                {\n                    'name': 'GIT_SYNC_SSH',\n                    'value': 'true'\n                }])\n        if self.kube_config.git_ssh_known_hosts_configmap_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_known_hosts_volume_name,\n                'mountPath': '/etc/git-secret/known_hosts',\n                'subPath': 'known_hosts'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_KNOWN_HOSTS',\n                    'value': 'true'\n                },\n                {\n                    'name': 'GIT_SSH_KNOWN_HOSTS_FILE',\n                    'value': '/etc/git-secret/known_hosts'\n                }\n            ])\n        else:\n            init_environment.append({\n                'name': 'GIT_KNOWN_HOSTS',\n                'value': 'false'\n            })\n\n        return [{\n            'name': self.kube_config.git_sync_init_container_name,\n            'image': self.kube_config.git_sync_container,\n            'securityContext': {'runAsUser': 65533},  \n            'env': init_environment,\n            'volumeMounts': volume_mounts\n        }]"
    },
    {
        "docstring": "Defines any necessary environment variables for the pod executor",
        "code": "def foo(self):\n        \n        env = {}\n\n        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):\n            env[env_var_name] = env_var_val\n\n        env[\"AIRFLOW__CORE__EXECUTOR\"] = \"LocalExecutor\"\n\n        if self.kube_config.airflow_configmap:\n            env['AIRFLOW_HOME'] = self.worker_airflow_home\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags\n        if (not self.kube_config.airflow_configmap and\n                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):\n            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(\"core\", \"SQL_ALCHEMY_CONN\")\n        if self.kube_config.git_dags_folder_mount_point:\n            \n            dag_volume_mount_path = os.path.join(\n                self.kube_config.git_dags_folder_mount_point,\n                self.kube_config.git_sync_dest,  \n                self.kube_config.git_subpath     \n            )\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = dag_volume_mount_path\n        return env"
    },
    {
        "docstring": "Defines any necessary secrets for the pod executor",
        "code": "def foo(self):\n        \n        worker_secrets = []\n\n        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):\n            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')\n            worker_secrets.append(\n                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)\n            )\n\n        if self.kube_config.env_from_secret_ref:\n            for secret_ref in self.kube_config.env_from_secret_ref.split(','):\n                worker_secrets.append(\n                    Secret('env', None, secret_ref)\n                )\n\n        return worker_secrets"
    },
    {
        "docstring": "Defines the security context",
        "code": "def foo(self):\n        \n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        \n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533\n\n        return security_context"
    },
    {
        "docstring": "Get link to qubole command result page.\n\n        :param operator: operator\n        :param dttm: datetime\n        :return: url link",
        "code": "def foo(self, operator, dttm):\n        \n        conn = BaseHook.get_connection(operator.kwargs['qubole_conn_id'])\n        if conn and conn.host:\n            host = re.sub(r'api$', 'v2/analyze?command_id=', conn.host)\n        else:\n            host = 'https://api.qubole.com/v2/analyze?command_id='\n\n        ti = TaskInstance(task=operator, execution_date=dttm)\n        qds_command_id = ti.xcom_pull(task_ids=operator.task_id, key='qbol_cmd_id')\n        url = host + str(qds_command_id) if qds_command_id else ''\n        return url"
    },
    {
        "docstring": "Heartbeats update the job's entry in the database with a timestamp\n        for the latest_heartbeat and allows for the job to be killed\n        externally. This allows at the system level to monitor what is\n        actually active.\n\n        For instance, an old heartbeat for SchedulerJob would mean something\n        is wrong.\n\n        This also allows for any job to be killed externally, regardless\n        of who is running it or on which machine it is running.\n\n        Note that if your heartbeat is set to 60 seconds and you call this\n        method after 10 seconds of processing since the last heartbeat, it\n        will sleep 50 seconds to complete the 60 seconds and keep a steady\n        heart rate. If you go over 60 seconds before calling it, it won't\n        sleep at all.",
        "code": "def foo(self):\n        \n        try:\n            with create_session() as session:\n                job = session.query(BaseJob).filter_by(id=self.id).one()\n                make_transient(job)\n                session.commit()\n\n            if job.state == State.SHUTDOWN:\n                self.kill()\n\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')\n            if not is_unit_test:\n                \n                sleep_for = 0\n                if job.latest_heartbeat:\n                    seconds_remaining = self.heartrate - \\\n                        (timezone.utcnow() - job.latest_heartbeat)\\\n                        .total_seconds()\n                    sleep_for = max(0, seconds_remaining)\n\n                sleep(sleep_for)\n\n            \n            with create_session() as session:\n                job = session.query(BaseJob).filter(BaseJob.id == self.id).first()\n                job.latest_heartbeat = timezone.utcnow()\n                session.merge(job)\n                session.commit()\n\n                self.heartbeat_callback(session=session)\n                self.log.debug('[heartbeat]')\n        except OperationalError as e:\n            self.log.error(\"Scheduler heartbeat got an exception: %s\", str(e))"
    },
    {
        "docstring": "Launch a process to process the given file.\n\n        :param result_queue: the queue to use for passing back the result\n        :type result_queue: multiprocessing.Queue\n        :param file_path: the file to process\n        :type file_path: unicode\n        :param pickle_dags: whether to pickle the DAGs found in the file and\n            save them to the DB\n        :type pickle_dags: bool\n        :param dag_id_white_list: if specified, only examine DAG ID's that are\n            in this list\n        :type dag_id_white_list: list[unicode]\n        :param thread_name: the name to use for the process that is launched\n        :type thread_name: unicode\n        :return: the process that was launched\n        :rtype: multiprocessing.Process\n        :param zombies: zombie task instances to kill\n        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]",
        "code": "def foo(result_queue,\n                        file_path,\n                        pickle_dags,\n                        dag_id_white_list,\n                        thread_name,\n                        zombies):\n        \n        def helper():\n            \n            log = logging.getLogger(\"airflow.processor\")\n\n            stdout = StreamLogWriter(log, logging.INFO)\n            stderr = StreamLogWriter(log, logging.WARN)\n\n            set_context(log, file_path)\n\n            try:\n                \n                sys.stdout = stdout\n                sys.stderr = stderr\n\n                \n                settings.configure_orm()\n\n                \n                \n                \n                threading.current_thread().name = thread_name\n                start_time = time.time()\n\n                log.info(\"Started process (PID=%s) to work on %s\",\n                         os.getpid(), file_path)\n                scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)\n                result = scheduler_job.process_file(file_path,\n                                                    zombies,\n                                                    pickle_dags)\n                result_queue.put(result)\n                end_time = time.time()\n                log.info(\n                    \"Processing %s took %.3f seconds\", file_path, end_time - start_time\n                )\n            except Exception:\n                \n                log.exception(\"Got an exception! Propagating...\")\n                raise\n            finally:\n                sys.stdout = sys.__stdout__\n                sys.stderr = sys.__stderr__\n                \n                \n                settings.dispose_orm()\n\n        p = multiprocessing.Process(target=helper,\n                                    args=(),\n                                    name=\"{}-Process\".format(thread_name))\n        p.start()\n        return p"
    },
    {
        "docstring": "Launch the process and start processing the DAG.",
        "code": "def foo(self):\n        \n        self._process = DagFileProcessor._launch_process(\n            self._result_queue,\n            self.file_path,\n            self._pickle_dags,\n            self._dag_id_white_list,\n            \"DagFileProcessor{}\".format(self._instance_id),\n            self._zombies)\n        self._start_time = timezone.utcnow()"
    },
    {
        "docstring": "Check if the process launched to process this file is done.\n\n        :return: whether the process is finished running\n        :rtype: bool",
        "code": "def foo(self):\n        \n        if self._process is None:\n            raise AirflowException(\"Tried to see if it's done before starting!\")\n\n        if self._done:\n            return True\n\n        \n        if self._result_queue and not self._result_queue.empty():\n            self._result = self._result_queue.get_nowait()\n            self._done = True\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        \n        if self._result_queue and not self._process.is_alive():\n            self._done = True\n            \n            if not self._result_queue.empty():\n                self._result = self._result_queue.get_nowait()\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        return False"
    },
    {
        "docstring": "Helper method to clean up processor_agent to avoid leaving orphan processes.",
        "code": "def foo(self, signum, frame):\n        \n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        if self.processor_agent:\n            self.processor_agent.end()\n        sys.exit(os.EX_OK)"
    },
    {
        "docstring": "For the DAGs in the given DagBag, record any associated import errors and clears\n        errors for files that no longer have them. These are usually displayed through the\n        Airflow UI so that users know that there are issues parsing DAGs.\n\n        :param session: session for ORM operations\n        :type session: sqlalchemy.orm.session.Session\n        :param dagbag: DagBag containing DAGs with import errors\n        :type dagbag: airflow.models.DagBag",
        "code": "def foo(session, dagbag):\n        \n        \n        for dagbag_file in dagbag.file_last_changed:\n            session.query(errors.ImportError).filter(\n                errors.ImportError.filename == dagbag_file\n            ).delete()\n\n        \n        for filename, stacktrace in six.iteritems(dagbag.import_errors):\n            session.add(errors.ImportError(\n                filename=filename,\n                stacktrace=stacktrace))\n        session.commit()"
    },
    {
        "docstring": "This method schedules the tasks for a single DAG by looking at the\n        active DAG runs and adding task instances that should run to the\n        queue.",
        "code": "def foo(self, dag, queue, session=None):\n        \n\n        \n        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)\n        active_dag_runs = []\n        for run in dag_runs:\n            self.log.info(\"Examining DAG run %s\", run)\n            \n            if run.execution_date > timezone.utcnow():\n                self.log.error(\n                    \"Execution date is in future: %s\",\n                    run.execution_date\n                )\n                continue\n\n            if len(active_dag_runs) >= dag.max_active_runs:\n                self.log.info(\"Number of active dag runs reached max_active_run.\")\n                break\n\n            \n            if run.is_backfill:\n                continue\n\n            \n            run.dag = dag\n            \n            run.verify_integrity(session=session)\n            run.update_state(session=session)\n            if run.state == State.RUNNING:\n                make_transient(run)\n                active_dag_runs.append(run)\n\n        for run in active_dag_runs:\n            self.log.debug(\"Examining active DAG run: %s\", run)\n            \n            tis = run.get_task_instances(state=(State.NONE,\n                                                State.UP_FOR_RETRY,\n                                                State.UP_FOR_RESCHEDULE))\n\n            \n            \n            \n            for ti in tis:\n                task = dag.get_task(ti.task_id)\n\n                \n                ti.task = task\n\n                if ti.are_dependencies_met(\n                        dep_context=DepContext(flag_upstream_failed=True),\n                        session=session):\n                    self.log.debug('Queuing task: %s', ti)\n                    queue.append(ti.key)"
    },
    {
        "docstring": "For all DAG IDs in the SimpleDagBag, look for task instances in the\n        old_states and set them to new_state if the corresponding DagRun\n        does not exist or exists but is not in the running state. This\n        normally should not happen, but it can if the state of DagRuns are\n        changed manually.\n\n        :param old_states: examine TaskInstances in this state\n        :type old_state: list[airflow.utils.state.State]\n        :param new_state: set TaskInstances to this state\n        :type new_state: airflow.utils.state.State\n        :param simple_dag_bag: TaskInstances associated with DAGs in the\n            simple_dag_bag and with states in the old_state will be examined\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag",
        "code": "def foo(self,\n                                             simple_dag_bag,\n                                             old_states,\n                                             new_state,\n                                             session=None):\n        \n        tis_changed = 0\n        query = session \\\n            .query(models.TaskInstance) \\\n            .outerjoin(models.DagRun, and_(\n                models.TaskInstance.dag_id == models.DagRun.dag_id,\n                models.TaskInstance.execution_date == models.DagRun.execution_date)) \\\n            .filter(models.TaskInstance.dag_id.in_(simple_dag_bag.dag_ids)) \\\n            .filter(models.TaskInstance.state.in_(old_states)) \\\n            .filter(or_(\n                models.DagRun.state != State.RUNNING,\n                models.DagRun.state.is_(None)))\n        if self.using_sqlite:\n            tis_to_change = query \\\n                .with_for_update() \\\n                .all()\n            for ti in tis_to_change:\n                ti.set_state(new_state, session=session)\n                tis_changed += 1\n        else:\n            subq = query.subquery()\n            tis_changed = session \\\n                .query(models.TaskInstance) \\\n                .filter(and_(\n                    models.TaskInstance.dag_id == subq.c.dag_id,\n                    models.TaskInstance.task_id == subq.c.task_id,\n                    models.TaskInstance.execution_date ==\n                    subq.c.execution_date)) \\\n                .update({models.TaskInstance.state: new_state},\n                        synchronize_session=False)\n            session.commit()\n\n        if tis_changed > 0:\n            self.log.warning(\n                \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\",\n                tis_changed, new_state\n            )"
    },
    {
        "docstring": "Get the concurrency maps.\n\n        :param states: List of states to query for\n        :type states: list[airflow.utils.state.State]\n        :return: A map from (dag_id, task_id) to # of task instances and\n         a map from (dag_id, task_id) to # of task instances in the given state list\n        :rtype: dict[tuple[str, str], int]",
        "code": "def foo(self, states, session=None):\n        \n        TI = models.TaskInstance\n        ti_concurrency_query = (\n            session\n            .query(TI.task_id, TI.dag_id, func.count('*'))\n            .filter(TI.state.in_(states))\n            .group_by(TI.task_id, TI.dag_id)\n        ).all()\n        dag_map = defaultdict(int)\n        task_map = defaultdict(int)\n        for result in ti_concurrency_query:\n            task_id, dag_id, count = result\n            dag_map[dag_id] += count\n            task_map[(dag_id, task_id)] = count\n        return dag_map, task_map"
    },
    {
        "docstring": "Changes the state of task instances in the list with one of the given states\n        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n\n        :param task_instances: TaskInstances to change the state of\n        :type task_instances: list[airflow.models.TaskInstance]\n        :param acceptable_states: Filters the TaskInstances updated to be in these states\n        :type acceptable_states: Iterable[State]\n        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]",
        "code": "def foo(self, task_instances,\n                                                    acceptable_states, session=None):\n        \n        if len(task_instances) == 0:\n            session.commit()\n            return []\n\n        TI = models.TaskInstance\n        filter_for_ti_state_change = (\n            [and_(\n                TI.dag_id == ti.dag_id,\n                TI.task_id == ti.task_id,\n                TI.execution_date == ti.execution_date)\n                for ti in task_instances])\n        ti_query = (\n            session\n            .query(TI)\n            .filter(or_(*filter_for_ti_state_change)))\n\n        if None in acceptable_states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(acceptable_states))  \n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(acceptable_states))\n\n        tis_to_set_to_queued = (\n            ti_query\n            .with_for_update()\n            .all())\n        if len(tis_to_set_to_queued) == 0:\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\n            session.commit()\n            return []\n\n        \n        for task_instance in tis_to_set_to_queued:\n            task_instance.state = State.QUEUED\n            task_instance.queued_dttm = (timezone.utcnow()\n                                         if not task_instance.queued_dttm\n                                         else task_instance.queued_dttm)\n            session.merge(task_instance)\n\n        \n        \n        simple_task_instances = [SimpleTaskInstance(ti) for ti in\n                                 tis_to_set_to_queued]\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in tis_to_set_to_queued])\n\n        session.commit()\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\n                      len(tis_to_set_to_queued), task_instance_str)\n        return simple_task_instances"
    },
    {
        "docstring": "Takes task_instances, which should have been set to queued, and enqueues them\n        with the executor.\n\n        :param simple_task_instances: TaskInstances to enqueue\n        :type simple_task_instances: list[SimpleTaskInstance]\n        :param simple_dag_bag: Should contains all of the task_instances' dags\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag",
        "code": "def foo(self, simple_dag_bag,\n                                                  simple_task_instances):\n        \n        TI = models.TaskInstance\n        \n        for simple_task_instance in simple_task_instances:\n            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)\n            command = TI.generate_command(\n                simple_task_instance.dag_id,\n                simple_task_instance.task_id,\n                simple_task_instance.execution_date,\n                local=True,\n                mark_success=False,\n                ignore_all_deps=False,\n                ignore_depends_on_past=False,\n                ignore_task_deps=False,\n                ignore_ti_state=False,\n                pool=simple_task_instance.pool,\n                file_path=simple_dag.full_filepath,\n                pickle_id=simple_dag.pickle_id)\n\n            priority = simple_task_instance.priority_weight\n            queue = simple_task_instance.queue\n            self.log.info(\n                \"Sending %s to executor with priority %s and queue %s\",\n                simple_task_instance.key, priority, queue\n            )\n\n            self.executor.queue_command(\n                simple_task_instance,\n                command,\n                priority=priority,\n                queue=queue)"
    },
    {
        "docstring": "Attempts to execute TaskInstances that should be executed by the scheduler.\n\n        There are three steps:\n        1. Pick TIs by priority with the constraint that they are in the expected states\n        and that we do exceed max_active_runs or pool limits.\n        2. Change the state for the TIs above atomically.\n        3. Enqueue the TIs in the executor.\n\n        :param simple_dag_bag: TaskInstances associated with DAGs in the\n            simple_dag_bag will be fetched from the DB and executed\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        :param states: Execute TaskInstances in these states\n        :type states: tuple[airflow.utils.state.State]\n        :return: Number of task instance with state changed.",
        "code": "def foo(self,\n                                simple_dag_bag,\n                                states,\n                                session=None):\n        \n        executable_tis = self._find_executable_task_instances(simple_dag_bag, states,\n                                                              session=session)\n\n        def query(result, items):\n            simple_tis_with_state_changed = \\\n                self._change_state_for_executable_task_instances(items,\n                                                                 states,\n                                                                 session=session)\n            self._enqueue_task_instances_with_queued_state(\n                simple_dag_bag,\n                simple_tis_with_state_changed)\n            session.commit()\n            return result + len(simple_tis_with_state_changed)\n\n        return helpers.reduce_in_chunks(query, executable_tis, 0, self.max_tis_per_query)"
    },
    {
        "docstring": "If there are tasks left over in the executor,\n        we set them back to SCHEDULED to avoid creating hanging tasks.\n\n        :param session: session for ORM operations",
        "code": "def foo(self, session):\n        \n        if self.executor.queued_tasks:\n            TI = models.TaskInstance\n            filter_for_ti_state_change = (\n                [and_(\n                    TI.dag_id == dag_id,\n                    TI.task_id == task_id,\n                    TI.execution_date == execution_date,\n                    \n                    \n                    TI._try_number == try_number - 1,\n                    TI.state == State.QUEUED)\n                    for dag_id, task_id, execution_date, try_number\n                    in self.executor.queued_tasks.keys()])\n            ti_query = (session.query(TI)\n                        .filter(or_(*filter_for_ti_state_change)))\n            tis_to_set_to_scheduled = (ti_query\n                                       .with_for_update()\n                                       .all())\n            if len(tis_to_set_to_scheduled) == 0:\n                session.commit()\n                return\n\n            \n            for task_instance in tis_to_set_to_scheduled:\n                task_instance.state = State.SCHEDULED\n\n            task_instance_str = \"\\n\\t\".join(\n                [repr(x) for x in tis_to_set_to_scheduled])\n\n            session.commit()\n            self.log.info(\"Set the following tasks to scheduled state:\\n\\t%s\", task_instance_str)"
    },
    {
        "docstring": "Respond to executor events.",
        "code": "def foo(self, simple_dag_bag, session=None):\n        \n        \n\n        TI = models.TaskInstance\n        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)\n                                   .items()):\n            dag_id, task_id, execution_date, try_number = key\n            self.log.info(\n                \"Executor reports execution of %s.%s execution_date=%s \"\n                \"exited with status %s for try_number %s\",\n                dag_id, task_id, execution_date, state, try_number\n            )\n            if state == State.FAILED or state == State.SUCCESS:\n                qry = session.query(TI).filter(TI.dag_id == dag_id,\n                                               TI.task_id == task_id,\n                                               TI.execution_date == execution_date)\n                ti = qry.first()\n                if not ti:\n                    self.log.warning(\"TaskInstance %s went missing from the database\", ti)\n                    continue\n\n                \n                if ti.try_number == try_number and ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    try:\n                        simple_dag = simple_dag_bag.get_dag(dag_id)\n                        dagbag = models.DagBag(simple_dag.full_filepath)\n                        dag = dagbag.get_dag(dag_id)\n                        ti.task = dag.get_task(task_id)\n                        ti.handle_failure(msg)\n                    except Exception:\n                        self.log.error(\"Cannot load the dag bag to handle failure for %s\"\n                                       \". Setting task to FAILED without callbacks or \"\n                                       \"retries. Do you have enough resources?\", ti)\n                        ti.state = State.FAILED\n                        session.merge(ti)\n                        session.commit()"
    },
    {
        "docstring": "Process a Python file containing Airflow DAGs.\n\n        This includes:\n\n        1. Execute the file and look for DAG objects in the namespace.\n        2. Pickle the DAG and save it to the DB (if necessary).\n        3. For each DAG, see what tasks should run and create appropriate task\n        instances in the DB.\n        4. Record any errors importing the file into ORM\n        5. Kill (in ORM) any task instances belonging to the DAGs that haven't\n        issued a heartbeat in a while.\n\n        Returns a list of SimpleDag objects that represent the DAGs found in\n        the file\n\n        :param file_path: the path to the Python file that should be executed\n        :type file_path: unicode\n        :param zombies: zombie task instances to kill.\n        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        :param pickle_dags: whether serialize the DAGs found in the file and\n            save them to the db\n        :type pickle_dags: bool\n        :return: a list of SimpleDags made from the Dags found in the file\n        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]",
        "code": "def foo(self, file_path, zombies, pickle_dags=False, session=None):\n        \n        self.log.info(\"Processing file %s for tasks to queue\", file_path)\n        \n        simple_dags = []\n\n        try:\n            dagbag = models.DagBag(file_path, include_examples=False)\n        except Exception:\n            self.log.exception(\"Failed at reloading the DAG file %s\", file_path)\n            Stats.incr('dag_file_refresh_error', 1, 1)\n            return []\n\n        if len(dagbag.dags) > 0:\n            self.log.info(\"DAG(s) %s retrieved from %s\", dagbag.dags.keys(), file_path)\n        else:\n            self.log.warning(\"No viable dags retrieved from %s\", file_path)\n            self.update_import_errors(session, dagbag)\n            return []\n\n        \n        for dag in dagbag.dags.values():\n            dag.sync_to_db()\n\n        paused_dag_ids = [dag.dag_id for dag in dagbag.dags.values()\n                          if dag.is_paused]\n\n        \n        for dag_id in dagbag.dags:\n            \n            if dag_id not in paused_dag_ids:\n                dag = dagbag.get_dag(dag_id)\n                pickle_id = None\n                if pickle_dags:\n                    pickle_id = dag.pickle(session).id\n                simple_dags.append(SimpleDag(dag, pickle_id=pickle_id))\n\n        if len(self.dag_ids) > 0:\n            dags = [dag for dag in dagbag.dags.values()\n                    if dag.dag_id in self.dag_ids and\n                    dag.dag_id not in paused_dag_ids]\n        else:\n            dags = [dag for dag in dagbag.dags.values()\n                    if not dag.parent_dag and\n                    dag.dag_id not in paused_dag_ids]\n\n        \n        \n        \n        ti_keys_to_schedule = []\n\n        self._process_dags(dagbag, dags, ti_keys_to_schedule)\n\n        for ti_key in ti_keys_to_schedule:\n            dag = dagbag.dags[ti_key[0]]\n            task = dag.get_task(ti_key[1])\n            ti = models.TaskInstance(task, ti_key[2])\n\n            ti.refresh_from_db(session=session, lock_for_update=True)\n            \n            \n            dep_context = DepContext(deps=QUEUE_DEPS, ignore_task_deps=True)\n\n            \n            \n            \n            \n            \n            \n            if ti.are_dependencies_met(\n                    dep_context=dep_context,\n                    session=session,\n                    verbose=True):\n                \n                \n                ti.state = State.SCHEDULED\n\n            \n            self.log.info(\"Creating / updating %s in ORM\", ti)\n            session.merge(ti)\n        \n        session.commit()\n\n        \n        try:\n            self.update_import_errors(session, dagbag)\n        except Exception:\n            self.log.exception(\"Error logging import errors!\")\n        try:\n            dagbag.kill_zombies(zombies)\n        except Exception:\n            self.log.exception(\"Error killing zombies!\")\n\n        return simple_dags"
    },
    {
        "docstring": "Updates the counters per state of the tasks that were running. Can re-add\n        to tasks to run in case required.\n\n        :param ti_status: the internal status of the backfill job tasks\n        :type ti_status: BackfillJob._DagRunTaskStatus",
        "code": "def foo(self, ti_status):\n        \n        for key, ti in list(ti_status.running.items()):\n            ti.refresh_from_db()\n            if ti.state == State.SUCCESS:\n                ti_status.succeeded.add(key)\n                self.log.debug(\"Task instance %s succeeded. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.SKIPPED:\n                ti_status.skipped.add(key)\n                self.log.debug(\"Task instance %s skipped. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.FAILED:\n                self.log.error(\"Task instance %s failed\", ti)\n                ti_status.failed.add(key)\n                ti_status.running.pop(key)\n                continue\n            \n            elif ti.state == State.UP_FOR_RETRY:\n                self.log.warning(\"Task instance %s is up for retry\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            \n            elif ti.state == State.UP_FOR_RESCHEDULE:\n                self.log.warning(\"Task instance %s is up for reschedule\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            \n            \n            \n            \n            \n            elif ti.state == State.NONE:\n                self.log.warning(\n                    \"FIXME: task instance %s state was set to none externally or \"\n                    \"reaching concurrency limits. Re-adding task to queue.\",\n                    ti\n                )\n                ti.set_state(State.SCHEDULED)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti"
    },
    {
        "docstring": "Checks if the executor agrees with the state of task instances\n        that are running\n\n        :param running: dict of key, task to verify",
        "code": "def foo(self, running):\n        \n        executor = self.executor\n\n        for key, state in list(executor.get_event_buffer().items()):\n            if key not in running:\n                self.log.warning(\n                    \"%s state %s not in running=%s\",\n                    key, state, running.values()\n                )\n                continue\n\n            ti = running[key]\n            ti.refresh_from_db()\n\n            self.log.debug(\"Executor state: %s task %s\", state, ti)\n\n            if state == State.FAILED or state == State.SUCCESS:\n                if ti.state == State.RUNNING or ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    ti.handle_failure(msg)"
    },
    {
        "docstring": "Returns a dag run for the given run date, which will be matched to an existing\n        dag run if available or create a new dag run otherwise. If the max_active_runs\n        limit is reached, this function will return None.\n\n        :param run_date: the execution date for the dag run\n        :type run_date: datetime.datetime\n        :param session: the database session object\n        :type session: sqlalchemy.orm.session.Session\n        :return: a DagRun in state RUNNING or None",
        "code": "def foo(self, run_date, session=None):\n        \n        run_id = BackfillJob.ID_FORMAT_PREFIX.format(run_date.isoformat())\n\n        \n        respect_dag_max_active_limit = (True\n                                        if (self.dag.schedule_interval and\n                                            not self.dag.is_subdag)\n                                        else False)\n\n        current_active_dag_count = self.dag.get_num_active_runs(external_trigger=False)\n\n        \n        \n        run = DagRun.find(dag_id=self.dag.dag_id,\n                          execution_date=run_date,\n                          session=session)\n\n        if run is not None and len(run) > 0:\n            run = run[0]\n            if run.state == State.RUNNING:\n                respect_dag_max_active_limit = False\n        else:\n            run = None\n\n        \n        \n        if (respect_dag_max_active_limit and\n                current_active_dag_count >= self.dag.max_active_runs):\n            return None\n\n        run = run or self.dag.create_dagrun(\n            run_id=run_id,\n            execution_date=run_date,\n            start_date=timezone.utcnow(),\n            state=State.RUNNING,\n            external_trigger=False,\n            session=session,\n            conf=self.conf,\n        )\n\n        \n        run.dag = self.dag\n\n        \n        run.state = State.RUNNING\n        run.run_id = run_id\n        run.verify_integrity(session=session)\n        return run"
    },
    {
        "docstring": "Returns a map of task instance key to task instance object for the tasks to\n        run in the given dag run.\n\n        :param dag_run: the dag run to get the tasks from\n        :type dag_run: airflow.models.DagRun\n        :param session: the database session object\n        :type session: sqlalchemy.orm.session.Session",
        "code": "def foo(self, dag_run, session=None):\n        \n        tasks_to_run = {}\n\n        if dag_run is None:\n            return tasks_to_run\n\n        \n        self.reset_state_for_orphaned_tasks(filter_by_dag_run=dag_run, session=session)\n\n        \n        dag_run.refresh_from_db()\n        make_transient(dag_run)\n\n        \n        for ti in dag_run.get_task_instances():\n            \n            if ti.state == State.NONE:\n                ti.set_state(State.SCHEDULED, session=session)\n            if ti.state != State.REMOVED:\n                tasks_to_run[ti.key] = ti\n\n        return tasks_to_run"
    },
    {
        "docstring": "Computes the dag runs and their respective task instances for\n        the given run dates and executes the task instances.\n        Returns a list of execution dates of the dag runs that were executed.\n\n        :param run_dates: Execution dates for dag runs\n        :type run_dates: list\n        :param ti_status: internal BackfillJob status structure to tis track progress\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to use, it must be previously started\n        :type executor: BaseExecutor\n        :param pickle_id: numeric id of the pickled dag, None if not pickled\n        :type pickle_id: int\n        :param start_date: backfill start date\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session",
        "code": "def foo(self, run_dates, ti_status, executor, pickle_id,\n                               start_date, session=None):\n        \n        for next_run_date in run_dates:\n            dag_run = self._get_dag_run(next_run_date, session=session)\n            tis_map = self._task_instances_for_dag_run(dag_run,\n                                                       session=session)\n            if dag_run is None:\n                continue\n\n            ti_status.active_runs.append(dag_run)\n            ti_status.to_run.update(tis_map or {})\n\n        processed_dag_run_dates = self._process_backfill_task_instances(\n            ti_status=ti_status,\n            executor=executor,\n            pickle_id=pickle_id,\n            start_date=start_date,\n            session=session)\n\n        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)"
    },
    {
        "docstring": "Go through the dag_runs and update the state based on the task_instance state.\n        Then set DAG runs that are not finished to failed.\n\n        :param dag_runs: DAG runs\n        :param session: session\n        :return: None",
        "code": "def foo(self, dag_runs, session=None):\n        \n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)"
    },
    {
        "docstring": "Initializes all components required to run a dag for a specified date range and\n        calls helper method to execute the tasks.",
        "code": "def foo(self, session=None):\n        \n        ti_status = BackfillJob._DagRunTaskStatus()\n\n        start_date = self.bf_start_date\n\n        \n        run_dates = self.dag.get_run_dates(start_date=start_date,\n                                           end_date=self.bf_end_date)\n        if self.run_backwards:\n            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]\n            if tasks_that_depend_on_past:\n                raise AirflowException(\n                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(\n                        \",\".join(tasks_that_depend_on_past)))\n            run_dates = run_dates[::-1]\n\n        if len(run_dates) == 0:\n            self.log.info(\"No run dates were found for the given dates and dag interval.\")\n            return\n\n        \n        pickle_id = None\n        if not self.donot_pickle and self.executor.__class__ not in (\n                executors.LocalExecutor, executors.SequentialExecutor):\n            pickle = DagPickle(self.dag)\n            session.add(pickle)\n            session.commit()\n            pickle_id = pickle.id\n\n        executor = self.executor\n        executor.start()\n\n        ti_status.total_runs = len(run_dates)  \n\n        try:\n            remaining_dates = ti_status.total_runs\n            while remaining_dates > 0:\n                dates_to_process = [run_date for run_date in run_dates\n                                    if run_date not in ti_status.executed_dag_run_dates]\n\n                self._execute_for_run_dates(run_dates=dates_to_process,\n                                            ti_status=ti_status,\n                                            executor=executor,\n                                            pickle_id=pickle_id,\n                                            start_date=start_date,\n                                            session=session)\n\n                remaining_dates = (\n                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)\n                )\n                err = self._collect_errors(ti_status=ti_status, session=session)\n                if err:\n                    raise AirflowException(err)\n\n                if remaining_dates > 0:\n                    self.log.info(\n                        \"max_active_runs limit for dag %s has been reached \"\n                        \" - waiting for other dag runs to finish\",\n                        self.dag_id\n                    )\n                    time.sleep(self.delay_on_limit_secs)\n        except (KeyboardInterrupt, SystemExit):\n            self.log.warning(\"Backfill terminated by user.\")\n\n            \n            \n            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)\n        finally:\n            session.commit()\n            executor.end()\n\n        self.log.info(\"Backfill done. Exiting.\")"
    },
    {
        "docstring": "Self destruct task if state has been moved away from running externally",
        "code": "def foo(self, session=None):\n        \n\n        if self.terminating:\n            \n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if not same_hostname:\n                self.log.warning(\"The recorded hostname %s \"\n                                 \"does not match this instance's hostname \"\n                                 \"%s\", ti.hostname, fqdn)\n                raise AirflowException(\"Hostname of job runner does not match\")\n            elif not same_process:\n                current_pid = os.getpid()\n                self.log.warning(\"Recorded pid %s does not match \"\n                                 \"the current pid %s\", ti.pid, current_pid)\n                raise AirflowException(\"PID of job runner does not match\")\n        elif (\n                self.task_runner.return_code() is None and\n                hasattr(self.task_runner, 'process')\n        ):\n            self.log.warning(\n                \"State of this instance has been externally set to %s. \"\n                \"Taking the poison pill.\",\n                ti.state\n            )\n            self.task_runner.terminate()\n            self.terminating = True"
    },
    {
        "docstring": "Provides a client for interacting with the Cloud Spanner API.\n\n        :param project_id: The ID of the  GCP project.\n        :type project_id: str\n        :return: google.cloud.spanner_v1.client.Client\n        :rtype: object",
        "code": "def foo(self, project_id):\n        \n        if not self._client:\n            self._client = Client(project=project_id, credentials=self._get_credentials())\n        return self._client"
    },
    {
        "docstring": "Gets information about a particular instance.\n\n        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner\n            database.  If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :return: google.cloud.spanner_v1.instance.Instance\n        :rtype: object",
        "code": "def foo(self, instance_id, project_id=None):\n        \n        instance = self._get_client(project_id=project_id).instance(instance_id=instance_id)\n        if not instance.exists():\n            return None\n        return instance"
    },
    {
        "docstring": "Invokes a method on a given instance by applying a specified Callable.\n\n        :param project_id: The ID of the  GCP project that owns the Cloud Spanner\n            database.\n        :type project_id: str\n        :param instance_id: The ID of the instance.\n        :type instance_id: str\n        :param configuration_name: Name of the instance configuration defining how the\n            instance will be created. Required for instances which do not yet exist.\n        :type configuration_name: str\n        :param node_count: (Optional) Number of nodes allocated to the instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the Cloud\n            Console UI. (Must be between 4 and 30 characters.) If this value is not set\n            in the constructor, will fall back to the instance ID.\n        :type display_name: str\n        :param func: Method of the instance to be called.\n        :type func: Callable",
        "code": "def foo(self, project_id, instance_id, configuration_name, node_count,\n                           display_name, func):\n        \n        \n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id, configuration_name=configuration_name,\n            node_count=node_count, display_name=display_name)\n        try:\n            operation = func(instance)  \n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)"
    },
    {
        "docstring": "Creates a new Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param configuration_name: The name of the instance configuration defining how the\n            instance will be created. Possible configuration values can be retrieved via\n            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list\n        :type configuration_name: str\n        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n            instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the GCP\n            Console. Must be between 4 and 30 characters.  If this value is not set in\n            the constructor, the name falls back to the instance ID.\n        :type display_name: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        \n        self._apply_to_instance(project_id, instance_id, configuration_name,\n                                node_count, display_name, lambda x: x.create())"
    },
    {
        "docstring": "Updates an existing Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param configuration_name: The name of the instance configuration defining how the\n            instance will be created. Possible configuration values can be retrieved via\n            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list\n        :type configuration_name: str\n        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n            instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the GCP\n            Console. Must be between 4 and 30 characters. If this value is not set in\n            the constructor, the name falls back to the instance ID.\n        :type display_name: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        \n        return self._apply_to_instance(project_id, instance_id, configuration_name,\n                                       node_count, display_name, lambda x: x.update())"
    },
    {
        "docstring": "Deletes an existing Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
        "code": "def foo(self, instance_id, project_id=None):\n        \n\n        instance = self._get_client(project_id=project_id).instance(instance_id)\n        try:\n            instance.delete()\n            return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e"
    },
    {
        "docstring": "Retrieves a database in Cloud Spanner. If the database does not exist\n        in the specified instance, it returns None.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Database object or None if database does not exist\n        :rtype: google.cloud.spanner_v1.database.Database or None",
        "code": "def foo(self, instance_id, database_id, project_id=None):\n        \n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():\n            return None\n        else:\n            return database"
    },
    {
        "docstring": "Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None",
        "code": "def foo(self, instance_id, database_id, ddl_statements, project_id=None):\n        \n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  \n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return"
    },
    {
        "docstring": "Updates DDL of a database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :param operation_id: (Optional) The unique per database operation ID that can be\n            specified to implement idempotency check.\n        :type operation_id: str\n        :return: None",
        "code": "def foo(self, instance_id, database_id, ddl_statements,\n                        project_id=None,\n                        operation_id=None):\n        \n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        try:\n            operation = database.update_ddl(\n                ddl_statements=ddl_statements, operation_id=operation_id)\n            if operation:\n                result = operation.result()\n                self.log.info(result)\n            return\n        except AlreadyExists as e:\n            if e.code == 409 and operation_id in e.message:\n                self.log.info(\"Replayed update_ddl message - the operation id %s \"\n                              \"was already done before.\", operation_id)\n                return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e"
    },
    {
        "docstring": "Drops a database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: True if everything succeeded\n        :rtype: bool",
        "code": "def foo(self, instance_id, database_id, project_id=None):\n        \n\n        instance = self._get_client(project_id=project_id).\\\n            instance(instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():\n            self.log.info(\"The database {} is already deleted from instance {}. \"\n                          \"Exiting.\".format(database_id, instance_id))\n            return\n        try:\n            operation = database.drop()  \n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return"
    },
    {
        "docstring": "Pokes for a mail attachment on the mail server.\n\n        :param context: The context that is being provided when poking.\n        :type context: dict\n        :return: True if attachment with the given name is present and False if not.\n        :rtype: bool",
        "code": "def foo(self, context):\n        \n        self.log.info('Poking for %s', self.attachment_name)\n\n        with ImapHook(imap_conn_id=self.conn_id) as imap_hook:\n            return imap_hook.has_mail_attachment(\n                name=self.attachment_name,\n                mail_folder=self.mail_folder,\n                check_regex=self.check_regex\n            )"
    },
    {
        "docstring": "Creates additional_properties parameter based on language_hints, web_detection_params and\n    additional_properties parameters specified by the user",
        "code": "def foo(additional_properties, language_hints, web_detection_params):\n    \n    if language_hints is None and web_detection_params is None:\n        return additional_properties\n\n    if additional_properties is None:\n        return {}\n\n    merged_additional_parameters = deepcopy(additional_properties)\n\n    if 'image_context' not in merged_additional_parameters:\n        merged_additional_parameters['image_context'] = {}\n\n    merged_additional_parameters['image_context']['language_hints'] = merged_additional_parameters[\n        'image_context'\n    ].get('language_hints', language_hints)\n    merged_additional_parameters['image_context']['web_detection_params'] = merged_additional_parameters[\n        'image_context'\n    ].get('web_detection_params', web_detection_params)\n\n    return merged_additional_parameters"
    },
    {
        "docstring": "Returns a cassandra Session object",
        "code": "def foo(self):\n        \n        if self.session and not self.session.is_shutdown:\n            return self.session\n        self.session = self.cluster.connect(self.keyspace)\n        return self.session"
    },
    {
        "docstring": "Checks if a table exists in Cassandra\n\n        :param table: Target Cassandra table.\n                      Use dot notation to target a specific keyspace.\n        :type table: str",
        "code": "def foo(self, table):\n        \n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        cluster_metadata = self.get_conn().cluster.metadata\n        return (keyspace in cluster_metadata.keyspaces and\n                table in cluster_metadata.keyspaces[keyspace].tables)"
    },
    {
        "docstring": "Checks if a record exists in Cassandra\n\n        :param table: Target Cassandra table.\n                      Use dot notation to target a specific keyspace.\n        :type table: str\n        :param keys: The keys and their values to check the existence.\n        :type keys: dict",
        "code": "def foo(self, table, keys):\n        \n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        ks = \" AND \".join(\"{}=%({})s\".format(key, key) for key in keys.keys())\n        cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\".format(\n            keyspace=keyspace, table=table, keys=ks)\n\n        try:\n            rs = self.get_conn().execute(cql, keys)\n            return rs.one() is not None\n        except Exception:\n            return False"
    },
    {
        "docstring": "Construct the command to poll the driver status.\n\n        :return: full command to be executed",
        "code": "def foo(self):\n        \n        connection_cmd = self._get_spark_binary_path()\n\n        \n        connection_cmd += [\"--master\", self._connection['master']]\n\n        \n        if self._driver_id:\n            connection_cmd += [\"--status\", self._driver_id]\n        else:\n            raise AirflowException(\n                \"Invalid status: attempted to poll driver \" +\n                \"status but no driver id is known. Giving up.\")\n\n        self.log.debug(\"Poll driver status cmd: %s\", connection_cmd)\n\n        return connection_cmd"
    },
    {
        "docstring": "Remote Popen to execute the spark-submit job\n\n        :param application: Submitted application, jar or py file\n        :type application: str\n        :param kwargs: extra arguments to Popen (see subprocess.Popen)",
        "code": "def foo(self, application=\"\", **kwargs):\n        \n        spark_submit_cmd = self._build_spark_submit_command(application)\n\n        if hasattr(self, '_env'):\n            env = os.environ.copy()\n            env.update(self._env)\n            kwargs[\"env\"] = env\n\n        self._submit_sp = subprocess.Popen(spark_submit_cmd,\n                                           stdout=subprocess.PIPE,\n                                           stderr=subprocess.STDOUT,\n                                           bufsize=-1,\n                                           universal_newlines=True,\n                                           **kwargs)\n\n        self._process_spark_submit_log(iter(self._submit_sp.stdout.readline, ''))\n        returncode = self._submit_sp.wait()\n\n        \n        \n        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):\n            raise AirflowException(\n                \"Cannot execute: {}. Error code is: {}.\".format(\n                    spark_submit_cmd, returncode\n                )\n            )\n\n        self.log.debug(\"Should track driver: {}\".format(self._should_track_driver_status))\n\n        \n        if self._should_track_driver_status:\n            if self._driver_id is None:\n                raise AirflowException(\n                    \"No driver id is known: something went wrong when executing \" +\n                    \"the spark submit command\"\n                )\n\n            \n            self._driver_status = \"SUBMITTED\"\n\n            \n            self._start_driver_status_tracking()\n\n            if self._driver_status != \"FINISHED\":\n                raise AirflowException(\n                    \"ERROR : Driver {} badly exited with status {}\"\n                    .format(self._driver_id, self._driver_status)\n                )"
    },
    {
        "docstring": "Processes the log files and extracts useful information out of it.\n\n        If the deploy-mode is 'client', log the output of the submit command as those\n        are the output logs of the Spark worker directly.\n\n        Remark: If the driver needs to be tracked for its status, the log-level of the\n        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)\n\n        :param itr: An iterator which iterates over the input of the subprocess",
        "code": "def foo(self, itr):\n        \n        \n        for line in itr:\n            line = line.strip()\n            \n            \n            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':\n                match = re.search('(application[0-9_]+)', line)\n                if match:\n                    self._yarn_application_id = match.groups()[0]\n                    self.log.info(\"Identified spark driver id: %s\",\n                                  self._yarn_application_id)\n\n            \n            \n            elif self._is_kubernetes:\n                match = re.search(r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)', line)\n                if match:\n                    self._kubernetes_driver_pod = match.groups()[0]\n                    self.log.info(\"Identified spark driver pod: %s\",\n                                  self._kubernetes_driver_pod)\n\n                \n                match_exit_code = re.search(r'\\s*Exit code: (\\d+)', line)\n                if match_exit_code:\n                    self._spark_exit_code = int(match_exit_code.groups()[0])\n\n            \n            \n            \n            elif self._should_track_driver_status and not self._driver_id:\n                match_driver_id = re.search(r'(driver-[0-9\\-]+)', line)\n                if match_driver_id:\n                    self._driver_id = match_driver_id.groups()[0]\n                    self.log.info(\"identified spark driver id: {}\"\n                                  .format(self._driver_id))\n\n            else:\n                self.log.info(line)\n\n            self.log.debug(\"spark submit log: {}\".format(line))"
    },
    {
        "docstring": "parses the logs of the spark driver status query process\n\n        :param itr: An iterator which iterates over the input of the subprocess",
        "code": "def foo(self, itr):\n        \n        \n        for line in itr:\n            line = line.strip()\n\n            \n            if \"driverState\" in line:\n                self._driver_status = line.split(' : ')[1] \\\n                    .replace(',', '').replace('\\\"', '').strip()\n\n            self.log.debug(\"spark driver status log: {}\".format(line))"
    },
    {
        "docstring": "Get the task runner that can be used to run the given job.\n\n    :param local_task_job: The LocalTaskJob associated with the TaskInstance\n        that needs to be executed.\n    :type local_task_job: airflow.jobs.LocalTaskJob\n    :return: The task runner to use to run the task.\n    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner",
        "code": "def foo(local_task_job):\n    \n    if _TASK_RUNNER == \"StandardTaskRunner\":\n        return StandardTaskRunner(local_task_job)\n    elif _TASK_RUNNER == \"CgroupTaskRunner\":\n        from airflow.contrib.task_runner.cgroup_task_runner import CgroupTaskRunner\n        return CgroupTaskRunner(local_task_job)\n    else:\n        raise AirflowException(\"Unknown task runner type {}\".format(_TASK_RUNNER))"
    },
    {
        "docstring": "Try to use a waiter from the below pull request\n\n            * https://github.com/boto/botocore/pull/1307\n\n        If the waiter is not available apply a exponential backoff\n\n            * docs.aws.amazon.com/general/latest/gr/api-retries.html",
        "code": "def foo(self):\n        \n        try:\n            waiter = self.client.get_waiter('job_execution_complete')\n            waiter.config.max_attempts = sys.maxsize  \n            waiter.wait(jobs=[self.jobId])\n        except ValueError:\n            \n            retry = True\n            retries = 0\n\n            while retries < self.max_retries and retry:\n                self.log.info('AWS Batch retry in the next %s seconds', retries)\n                response = self.client.describe_jobs(\n                    jobs=[self.jobId]\n                )\n                if response['jobs'][-1]['status'] in ['SUCCEEDED', 'FAILED']:\n                    retry = False\n\n                sleep(1 + pow(retries * 0.1, 2))\n                retries += 1"
    },
    {
        "docstring": "Queries mysql and returns a cursor to the results.",
        "code": "def foo(self):\n        \n        mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)\n        conn = mysql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor"
    },
    {
        "docstring": "Configure a csv writer with the file_handle and write schema\n        as headers for the new file.",
        "code": "def foo(self, file_handle, schema):\n        \n        csv_writer = csv.writer(file_handle, encoding='utf-8',\n                                delimiter=self.field_delimiter)\n        csv_writer.writerow(schema)\n        return csv_writer"
    },
    {
        "docstring": "Takes a cursor, and writes the BigQuery schema in .json format for the\n        results to a local file system.\n\n        :return: A dictionary where key is a filename to be used as an object\n            name in GCS, and values are file handles to local files that\n            contains the BigQuery schema fields in .json format.",
        "code": "def foo(self, cursor):\n        \n        schema_str = None\n        schema_file_mime_type = 'application/json'\n        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n        if self.schema is not None and isinstance(self.schema, string_types):\n            schema_str = self.schema.encode('utf-8')\n        elif self.schema is not None and isinstance(self.schema, list):\n            schema_str = json.dumps(self.schema).encode('utf-8')\n        else:\n            schema = []\n            for field in cursor.description:\n                \n                field_name = field[0]\n                field_type = self.type_map(field[1])\n                \n                \n                \n                if field[6] or field_type == 'TIMESTAMP':\n                    field_mode = 'NULLABLE'\n                else:\n                    field_mode = 'REQUIRED'\n                schema.append({\n                    'name': field_name,\n                    'type': field_type,\n                    'mode': field_mode,\n                })\n            schema_str = json.dumps(schema, sort_keys=True).encode('utf-8')\n        tmp_schema_file_handle.write(schema_str)\n\n        self.log.info('Using schema for %s: %s', self.schema_filename, schema_str)\n        schema_file_to_upload = {\n            'file_name': self.schema_filename,\n            'file_handle': tmp_schema_file_handle,\n            'file_mime_type': schema_file_mime_type\n        }\n        return schema_file_to_upload"
    },
    {
        "docstring": "Return a dict of column name and column type based on self.schema if not None.",
        "code": "def foo(self):\n        \n        schema = []\n        if isinstance(self.schema, string_types):\n            schema = json.loads(self.schema)\n        elif isinstance(self.schema, list):\n            schema = self.schema\n        elif self.schema is not None:\n            self.log.warn('Using default schema due to unexpected type.'\n                          'Should be a string or list.')\n\n        col_type_dict = {}\n        try:\n            col_type_dict = {col['name']: col['type'] for col in schema}\n        except KeyError:\n            self.log.warn('Using default schema due to missing name or type. Please '\n                          'refer to: https://cloud.google.com/bigquery/docs/schemas'\n                          '\n        return col_type_dict"
    },
    {
        "docstring": "Helper function that maps from MySQL fields to BigQuery fields. Used\n        when a schema_filename is set.",
        "code": "def foo(cls, mysql_type):\n        \n        d = {\n            FIELD_TYPE.INT24: 'INTEGER',\n            FIELD_TYPE.TINY: 'INTEGER',\n            FIELD_TYPE.BIT: 'INTEGER',\n            FIELD_TYPE.DATETIME: 'TIMESTAMP',\n            FIELD_TYPE.DATE: 'TIMESTAMP',\n            FIELD_TYPE.DECIMAL: 'FLOAT',\n            FIELD_TYPE.NEWDECIMAL: 'FLOAT',\n            FIELD_TYPE.DOUBLE: 'FLOAT',\n            FIELD_TYPE.FLOAT: 'FLOAT',\n            FIELD_TYPE.LONG: 'INTEGER',\n            FIELD_TYPE.LONGLONG: 'INTEGER',\n            FIELD_TYPE.SHORT: 'INTEGER',\n            FIELD_TYPE.TIMESTAMP: 'TIMESTAMP',\n            FIELD_TYPE.YEAR: 'INTEGER',\n        }\n        return d[mysql_type] if mysql_type in d else 'STRING'"
    },
    {
        "docstring": "Execute sqoop job",
        "code": "def foo(self, context):\n        \n        self.hook = SqoopHook(\n            conn_id=self.conn_id,\n            verbose=self.verbose,\n            num_mappers=self.num_mappers,\n            hcatalog_database=self.hcatalog_database,\n            hcatalog_table=self.hcatalog_table,\n            properties=self.properties\n        )\n\n        if self.cmd_type == 'export':\n            self.hook.export_table(\n                table=self.table,\n                export_dir=self.export_dir,\n                input_null_string=self.input_null_string,\n                input_null_non_string=self.input_null_non_string,\n                staging_table=self.staging_table,\n                clear_staging_table=self.clear_staging_table,\n                enclosed_by=self.enclosed_by,\n                escaped_by=self.escaped_by,\n                input_fields_terminated_by=self.input_fields_terminated_by,\n                input_lines_terminated_by=self.input_lines_terminated_by,\n                input_optionally_enclosed_by=self.input_optionally_enclosed_by,\n                batch=self.batch,\n                relaxed_isolation=self.relaxed_isolation,\n                extra_export_options=self.extra_export_options)\n        elif self.cmd_type == 'import':\n            \n            \n            \n            if self.create_hcatalog_table:\n                self.extra_import_options['create-hcatalog-table'] = ''\n\n            if self.table and self.query:\n                raise AirflowException(\n                    'Cannot specify query and table together. Need to specify either or.'\n                )\n\n            if self.table:\n                self.hook.import_table(\n                    table=self.table,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    columns=self.columns,\n                    split_by=self.split_by,\n                    where=self.where,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            elif self.query:\n                self.hook.import_query(\n                    query=self.query,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    split_by=self.split_by,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            else:\n                raise AirflowException(\n                    \"Provide query or table parameter to import using Sqoop\"\n                )\n        else:\n            raise AirflowException(\"cmd_type should be 'import' or 'export'\")"
    },
    {
        "docstring": "Saves the lineage to XCom and if configured to do so sends it\n    to the backend.",
        "code": "def foo(func):\n    \n    backend = _get_backend()\n\n    @wraps(func)\n    def wrapper(self, context, *args, **kwargs):\n        self.log.debug(\"Backend: %s, Lineage called with inlets: %s, outlets: %s\",\n                       backend, self.inlets, self.outlets)\n        ret_val = func(self, context, *args, **kwargs)\n\n        outlets = [x.as_dict() for x in self.outlets]\n        inlets = [x.as_dict() for x in self.inlets]\n\n        if len(self.outlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_OUTLETS,\n                           value=outlets,\n                           execution_date=context['ti'].execution_date)\n\n        if len(self.inlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_INLETS,\n                           value=inlets,\n                           execution_date=context['ti'].execution_date)\n\n        if backend:\n            backend.send_lineage(operator=self, inlets=self.inlets,\n                                 outlets=self.outlets, context=context)\n\n        return ret_val\n\n    return wrapper"
    },
    {
        "docstring": "Returns the extra property by deserializing json.",
        "code": "def foo(self):\n        \n        obj = {}\n        if self.extra:\n            try:\n                obj = json.loads(self.extra)\n            except Exception as e:\n                self.log.exception(e)\n                self.log.error(\"Failed parsing the json for conn_id %s\", self.conn_id)\n\n        return obj"
    },
    {
        "docstring": "Get a set of dates as a list based on a start, end and delta, delta\n    can be something that can be added to `datetime.datetime`\n    or a cron expression as a `str`\n\n    :Example::\n\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n            datetime.datetime(2016, 3, 1, 0, 0)]\n\n    :param start_date: anchor date to start the series from\n    :type start_date: datetime.datetime\n    :param end_date: right boundary for the date range\n    :type end_date: datetime.datetime\n    :param num: alternatively to end_date, you can specify the number of\n        number of entries you want in the range. This number can be negative,\n        output will always be sorted regardless\n    :type num: int",
        "code": "def foo(start_date, end_date=None, num=None, delta=None):\n    \n    if not delta:\n        return []\n    if end_date and start_date > end_date:\n        raise Exception(\"Wait. start_date needs to be before end_date\")\n    if end_date and num:\n        raise Exception(\"Wait. Either specify end_date OR num\")\n    if not end_date and not num:\n        end_date = timezone.utcnow()\n\n    delta_iscron = False\n    tz = start_date.tzinfo\n    if isinstance(delta, six.string_types):\n        delta_iscron = True\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n    elif isinstance(delta, timedelta):\n        delta = abs(delta)\n    dates = []\n    if end_date:\n        if timezone.is_naive(start_date):\n            end_date = timezone.make_naive(end_date, tz)\n        while start_date <= end_date:\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                start_date = cron.get_next(datetime)\n            else:\n                start_date += delta\n    else:\n        for _ in range(abs(num)):\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                if num > 0:\n                    start_date = cron.get_next(datetime)\n                else:\n                    start_date = cron.get_prev(datetime)\n            else:\n                if num > 0:\n                    start_date += delta\n                else:\n                    start_date -= delta\n    return sorted(dates)"
    },
    {
        "docstring": "Convert an array of time durations in seconds to the specified time unit.",
        "code": "def foo(time_seconds_arr, unit):\n    \n    if unit == 'minutes':\n        return list(map(lambda x: x * 1.0 / 60, time_seconds_arr))\n    elif unit == 'hours':\n        return list(map(lambda x: x * 1.0 / (60 * 60), time_seconds_arr))\n    elif unit == 'days':\n        return list(map(lambda x: x * 1.0 / (24 * 60 * 60), time_seconds_arr))\n    return time_seconds_arr"
    },
    {
        "docstring": "Get a datetime object representing `n` days ago. By default the time is\n    set to midnight.",
        "code": "def foo(n, hour=0, minute=0, second=0, microsecond=0):\n    \n    today = timezone.utcnow().replace(\n        hour=hour,\n        minute=minute,\n        second=second,\n        microsecond=microsecond)\n    return today - timedelta(days=n)"
    },
    {
        "docstring": "Initialize the role with the permissions and related view-menus.\n\n        :param role_name:\n        :param role_vms:\n        :param role_perms:\n        :return:",
        "code": "def foo(self, role_name, role_vms, role_perms):\n        \n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        role = self.find_role(role_name)\n        if not role:\n            role = self.add_role(role_name)\n\n        if len(role.permissions) == 0:\n            self.log.info('Initializing permissions for role:%s in the database.', role_name)\n            role_pvms = set()\n            for pvm in pvms:\n                if pvm.view_menu.name in role_vms and pvm.permission.name in role_perms:\n                    role_pvms.add(pvm)\n            role.permissions = list(role_pvms)\n            self.get_session.merge(role)\n            self.get_session.commit()\n        else:\n            self.log.debug('Existing permissions for the role:%s '\n                           'within the database will persist.', role_name)"
    },
    {
        "docstring": "Delete the given Role\n\n        :param role_name: the name of a role in the ab_role table",
        "code": "def foo(self, role_name):\n        \n        session = self.get_session\n        role = session.query(sqla_models.Role)\\\n                      .filter(sqla_models.Role.name == role_name)\\\n                      .first()\n        if role:\n            self.log.info(\"Deleting role '%s'\", role_name)\n            session.delete(role)\n            session.commit()\n        else:\n            raise AirflowException(\"Role named '{}' does not exist\".format(\n                role_name))"
    },
    {
        "docstring": "Get all the roles associated with the user.\n\n        :param user: the ab_user in FAB model.\n        :return: a list of roles associated with the user.",
        "code": "def foo(self, user=None):\n        \n        if user is None:\n            user = g.user\n        if user.is_anonymous:\n            public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')\n            return [appbuilder.security_manager.find_role(public_role)] \\\n                if public_role else []\n        return user.roles"
    },
    {
        "docstring": "Returns a set of tuples with the perm name and view menu name",
        "code": "def foo(self):\n        \n        perms_views = set()\n        for role in self.get_user_roles():\n            perms_views.update({(perm_view.permission.name, perm_view.view_menu.name)\n                                for perm_view in role.permissions})\n        return perms_views"
    },
    {
        "docstring": "Whether the user has this role name",
        "code": "def foo(self, role_name_or_list):\n        \n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])"
    },
    {
        "docstring": "Whether the user has this perm",
        "code": "def foo(self, permission_name, view_menu_name):\n        \n        if hasattr(self, 'perms'):\n            if (permission_name, view_menu_name) in self.perms:\n                return True\n        \n        self._get_and_cache_perms()\n        return (permission_name, view_menu_name) in self.perms"
    },
    {
        "docstring": "FAB leaves faulty permissions that need to be cleaned up",
        "code": "def foo(self):\n        \n        self.log.debug('Cleaning faulty perms')\n        sesh = self.get_session\n        pvms = (\n            sesh.query(sqla_models.PermissionView)\n            .filter(or_(\n                sqla_models.PermissionView.permission == None,  \n                sqla_models.PermissionView.view_menu == None,  \n            ))\n        )\n        deleted_count = pvms.delete()\n        sesh.commit()\n        if deleted_count:\n            self.log.info('Deleted %s faulty permissions', deleted_count)"
    },
    {
        "docstring": "Add the new permission , view_menu to ab_permission_view_role if not exists.\n        It will add the related entry to ab_permission\n        and ab_view_menu two meta tables as well.\n\n        :param permission_name: Name of the permission.\n        :type permission_name: str\n        :param view_menu_name: Name of the view-menu\n        :type view_menu_name: str\n        :return:",
        "code": "def foo(self, permission_name, view_menu_name):\n        \n        permission = self.find_permission(permission_name)\n        view_menu = self.find_view_menu(view_menu_name)\n        pv = None\n        if permission and view_menu:\n            pv = self.get_session.query(self.permissionview_model).filter_by(\n                permission=permission, view_menu=view_menu).first()\n        if not pv and permission_name and view_menu_name:\n            self.add_permission_view_menu(permission_name, view_menu_name)"
    },
    {
        "docstring": "Admin should have all the permission-views.\n        Add the missing ones to the table for admin.\n\n        :return: None.",
        "code": "def foo(self):\n        \n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        admin = self.find_role('Admin')\n        admin.permissions = list(set(admin.permissions) | set(pvms))\n\n        self.get_session.commit()"
    },
    {
        "docstring": "Set the access policy on the given DAG's ViewModel.\n\n        :param dag_id: the ID of the DAG whose permissions should be updated\n        :type dag_id: string\n        :param access_control: a dict where each key is a rolename and\n            each value is a set() of permission names (e.g.,\n            {'can_dag_read'}\n        :type access_control: dict",
        "code": "def foo(self, dag_id, access_control):\n        \n        def _get_or_create_dag_permission(perm_name):\n            dag_perm = self.find_permission_view_menu(perm_name, dag_id)\n            if not dag_perm:\n                self.log.info(\n                    \"Creating new permission '%s' on view '%s'\",\n                    perm_name, dag_id\n                )\n                dag_perm = self.add_permission_view_menu(perm_name, dag_id)\n\n            return dag_perm\n\n        def _revoke_stale_permissions(dag_view):\n            existing_dag_perms = self.find_permissions_view_menu(dag_view)\n            for perm in existing_dag_perms:\n                non_admin_roles = [role for role in perm.role\n                                   if role.name != 'Admin']\n                for role in non_admin_roles:\n                    target_perms_for_role = access_control.get(role.name, {})\n                    if perm.permission.name not in target_perms_for_role:\n                        self.log.info(\n                            \"Revoking '%s' on DAG '%s' for role '%s'\",\n                            perm.permission, dag_id, role.name\n                        )\n                        self.del_permission_role(role, perm)\n\n        dag_view = self.find_view_menu(dag_id)\n        if dag_view:\n            _revoke_stale_permissions(dag_view)\n\n        for rolename, perms in access_control.items():\n            role = self.find_role(rolename)\n            if not role:\n                raise AirflowException(\n                    \"The access_control mapping for DAG '{}' includes a role \"\n                    \"named '{}', but that role does not exist\".format(\n                        dag_id,\n                        rolename))\n\n            perms = set(perms)\n            invalid_perms = perms - self.DAG_PERMS\n            if invalid_perms:\n                raise AirflowException(\n                    \"The access_control map for DAG '{}' includes the following \"\n                    \"invalid permissions: {}; The set of valid permissions \"\n                    \"is: {}\".format(dag_id,\n                                    (perms - self.DAG_PERMS),\n                                    self.DAG_PERMS))\n\n            for perm_name in perms:\n                dag_perm = _get_or_create_dag_permission(perm_name)\n                self.add_permission_role(role, dag_perm)"
    },
    {
        "docstring": "Create perm-vm if not exist and insert into FAB security model for all-dags.",
        "code": "def foo(self):\n        \n        \n        for dag_vm in self.DAG_VMS:\n            for perm in self.DAG_PERMS:\n                self._merge_perm(permission_name=perm,\n                                 view_menu_name=dag_vm)"
    },
    {
        "docstring": "Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet",
        "code": "def foo():\n    \n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet"
    },
    {
        "docstring": "Checks for existence of the partition in the AWS Glue Catalog table",
        "code": "def foo(self, context):\n        \n        if '.' in self.table_name:\n            self.database_name, self.table_name = self.table_name.split('.')\n        self.log.info(\n            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression\n        )\n\n        return self.get_hook().check_for_partition(\n            self.database_name, self.table_name, self.expression)"
    },
    {
        "docstring": "Gets the AwsGlueCatalogHook",
        "code": "def foo(self):\n        \n        if not hasattr(self, 'hook'):\n            from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook\n            self.hook = AwsGlueCatalogHook(\n                aws_conn_id=self.aws_conn_id,\n                region_name=self.region_name)\n\n        return self.hook"
    },
    {
        "docstring": "Check for message on subscribed queue and write to xcom the message with key ``messages``\n\n        :param context: the context object\n        :type context: dict\n        :return: ``True`` if message is available or ``False``",
        "code": "def foo(self, context):\n        \n\n        sqs_hook = SQSHook(aws_conn_id=self.aws_conn_id)\n        sqs_conn = sqs_hook.get_conn()\n\n        self.log.info('SQSSensor checking for message on queue: %s', self.sqs_queue)\n\n        messages = sqs_conn.receive_message(QueueUrl=self.sqs_queue,\n                                            MaxNumberOfMessages=self.max_messages,\n                                            WaitTimeSeconds=self.wait_time_seconds)\n\n        self.log.info(\"reveived message %s\", str(messages))\n\n        if 'Messages' in messages and len(messages['Messages']) > 0:\n\n            entries = [{'Id': message['MessageId'], 'ReceiptHandle': message['ReceiptHandle']}\n                       for message in messages['Messages']]\n\n            result = sqs_conn.delete_message_batch(QueueUrl=self.sqs_queue,\n                                                   Entries=entries)\n\n            if 'Successful' in result:\n                context['ti'].xcom_push(key='messages', value=messages)\n                return True\n            else:\n                raise AirflowException(\n                    'Delete SQS Messages failed ' + str(result) + ' for messages ' + str(messages))\n\n        return False"
    },
    {
        "docstring": "Returns a snakebite HDFSClient object.",
        "code": "def foo(self):\n        \n        \n        \n        effective_user = self.proxy_user\n        autoconfig = self.autoconfig\n        use_sasl = configuration.conf.get('core', 'security') == 'kerberos'\n\n        try:\n            connections = self.get_connections(self.hdfs_conn_id)\n\n            if not effective_user:\n                effective_user = connections[0].login\n            if not autoconfig:\n                autoconfig = connections[0].extra_dejson.get('autoconfig',\n                                                             False)\n            hdfs_namenode_principal = connections[0].extra_dejson.get(\n                'hdfs_namenode_principal')\n        except AirflowException:\n            if not autoconfig:\n                raise\n\n        if autoconfig:\n            \n            client = AutoConfigClient(effective_user=effective_user,\n                                      use_sasl=use_sasl)\n        elif len(connections) == 1:\n            client = Client(connections[0].host, connections[0].port,\n                            effective_user=effective_user, use_sasl=use_sasl,\n                            hdfs_namenode_principal=hdfs_namenode_principal)\n        elif len(connections) > 1:\n            nn = [Namenode(conn.host, conn.port) for conn in connections]\n            client = HAClient(nn, effective_user=effective_user,\n                              use_sasl=use_sasl,\n                              hdfs_namenode_principal=hdfs_namenode_principal)\n        else:\n            raise HDFSHookException(\"conn_id doesn't exist in the repository \"\n                                    \"and autoconfig is not specified\")\n\n        return client"
    },
    {
        "docstring": "Establishes a connection depending on the security mode set via config or environment variable.\n\n        :return: a hdfscli InsecureClient or KerberosClient object.\n        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient",
        "code": "def foo(self):\n        \n        connections = self.get_connections(self.webhdfs_conn_id)\n\n        for connection in connections:\n            try:\n                self.log.debug('Trying namenode %s', connection.host)\n                client = self._get_client(connection)\n                client.status('/')\n                self.log.debug('Using namenode %s for hook', connection.host)\n                return client\n            except HdfsError as hdfs_error:\n                self.log.debug('Read operation on namenode %s failed with error: %s',\n                               connection.host, hdfs_error)\n\n        hosts = [connection.host for connection in connections]\n        error_message = 'Read operations failed on the namenodes below:\\n{hosts}'.format(\n            hosts='\\n'.join(hosts))\n        raise AirflowWebHDFSHookException(error_message)"
    },
    {
        "docstring": "Check for the existence of a path in HDFS by querying FileStatus.\n\n        :param hdfs_path: The path to check.\n        :type hdfs_path: str\n        :return: True if the path exists and False if not.\n        :rtype: bool",
        "code": "def foo(self, hdfs_path):\n        \n        conn = self.get_conn()\n\n        status = conn.status(hdfs_path, strict=False)\n        return bool(status)"
    },
    {
        "docstring": "r\"\"\"\n        Uploads a file to HDFS.\n\n        :param source: Local path to file or folder.\n            If it's a folder, all the files inside of it will be uploaded.\n            .. note:: This implies that folders empty of files will not be created remotely.\n\n        :type source: str\n        :param destination: PTarget HDFS path.\n            If it already exists and is a directory, files will be uploaded inside.\n        :type destination: str\n        :param overwrite: Overwrite any existing file or directory.\n        :type overwrite: bool\n        :param parallelism: Number of threads to use for parallelization.\n            A value of `0` (or negative) uses as many threads as there are files.\n        :type parallelism: int\n        :param \\**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.",
        "code": "def foo(self, source, destination, overwrite=True, parallelism=1, **kwargs):\n        r\n        conn = self.get_conn()\n\n        conn.upload(hdfs_path=destination,\n                    local_path=source,\n                    overwrite=overwrite,\n                    n_threads=parallelism,\n                    **kwargs)\n        self.log.debug(\"Uploaded file %s to %s\", source, destination)"
    },
    {
        "docstring": "Establish a connection to pinot broker through pinot dbqpi.",
        "code": "def foo(self):\n        \n        conn = self.get_connection(self.pinot_broker_conn_id)\n        pinot_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/pql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to pinot '\n                      'broker on {host}'.format(host=conn.host))\n        return pinot_broker_conn"
    },
    {
        "docstring": "Get the connection uri for pinot broker.\n\n        e.g: http://localhost:9000/pql",
        "code": "def foo(self):\n        \n        conn = self.get_connection(getattr(self, self.conn_name_attr))\n        host = conn.host\n        if conn.port is not None:\n            host += ':{port}'.format(port=conn.port)\n        conn_type = 'http' if not conn.conn_type else conn.conn_type\n        endpoint = conn.extra_dejson.get('endpoint', 'pql')\n        return '{conn_type}://{host}/{endpoint}'.format(\n            conn_type=conn_type, host=host, endpoint=endpoint)"
    },
    {
        "docstring": "Convert native python ``datetime.date`` object  to a format supported by the API",
        "code": "def foo(field_date):\n        \n        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}"
    }
]