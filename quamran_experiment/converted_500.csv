repo,path,func_name,original_string,language,code,code_tokens/0,code_tokens/1,code_tokens/2,code_tokens/3,code_tokens/4,code_tokens/5,code_tokens/6,code_tokens/7,code_tokens/8,code_tokens/9,code_tokens/10,code_tokens/11,code_tokens/12,code_tokens/13,code_tokens/14,code_tokens/15,code_tokens/16,code_tokens/17,code_tokens/18,code_tokens/19,code_tokens/20,code_tokens/21,code_tokens/22,code_tokens/23,code_tokens/24,code_tokens/25,code_tokens/26,code_tokens/27,code_tokens/28,code_tokens/29,code_tokens/30,code_tokens/31,code_tokens/32,code_tokens/33,code_tokens/34,code_tokens/35,code_tokens/36,code_tokens/37,code_tokens/38,code_tokens/39,code_tokens/40,code_tokens/41,code_tokens/42,docstring,docstring_tokens/0,docstring_tokens/1,docstring_tokens/2,docstring_tokens/3,docstring_tokens/4,docstring_tokens/5,docstring_tokens/6,docstring_tokens/7,docstring_tokens/8,docstring_tokens/9,docstring_tokens/10,docstring_tokens/11,docstring_tokens/12,docstring_tokens/13,docstring_tokens/14,sha,url,partition,code_tokens/43,code_tokens/44,code_tokens/45,code_tokens/46,code_tokens/47,code_tokens/48,code_tokens/49,code_tokens/50,code_tokens/51,code_tokens/52,code_tokens/53,code_tokens/54,code_tokens/55,code_tokens/56,code_tokens/57,code_tokens/58,code_tokens/59,code_tokens/60,code_tokens/61,code_tokens/62,code_tokens/63,code_tokens/64,code_tokens/65,code_tokens/66,code_tokens/67,code_tokens/68,code_tokens/69,code_tokens/70,code_tokens/71,code_tokens/72,docstring_tokens/15,docstring_tokens/16,docstring_tokens/17,docstring_tokens/18,docstring_tokens/19,docstring_tokens/20,docstring_tokens/21,docstring_tokens/22,docstring_tokens/23,docstring_tokens/24,docstring_tokens/25,docstring_tokens/26,docstring_tokens/27,docstring_tokens/28,docstring_tokens/29,docstring_tokens/30,docstring_tokens/31,docstring_tokens/32,docstring_tokens/33,code_tokens/73,code_tokens/74,code_tokens/75,code_tokens/76,code_tokens/77,code_tokens/78,code_tokens/79,code_tokens/80,code_tokens/81,code_tokens/82,code_tokens/83,code_tokens/84,code_tokens/85,code_tokens/86,code_tokens/87,code_tokens/88,code_tokens/89,code_tokens/90,code_tokens/91,code_tokens/92,code_tokens/93,code_tokens/94,code_tokens/95,code_tokens/96,code_tokens/97,code_tokens/98,code_tokens/99,code_tokens/100,code_tokens/101,code_tokens/102,code_tokens/103,code_tokens/104,code_tokens/105,code_tokens/106,code_tokens/107,code_tokens/108,code_tokens/109,code_tokens/110,code_tokens/111,code_tokens/112,code_tokens/113,code_tokens/114,code_tokens/115,code_tokens/116,code_tokens/117,code_tokens/118,code_tokens/119,code_tokens/120,code_tokens/121,code_tokens/122,code_tokens/123,code_tokens/124,code_tokens/125,code_tokens/126,code_tokens/127,code_tokens/128,code_tokens/129,code_tokens/130,code_tokens/131,code_tokens/132,code_tokens/133,code_tokens/134,code_tokens/135,code_tokens/136,code_tokens/137,code_tokens/138,code_tokens/139,code_tokens/140,code_tokens/141,code_tokens/142,code_tokens/143,code_tokens/144,code_tokens/145,code_tokens/146,code_tokens/147,code_tokens/148,code_tokens/149,code_tokens/150,code_tokens/151,code_tokens/152,code_tokens/153,code_tokens/154,code_tokens/155,code_tokens/156,code_tokens/157,code_tokens/158,code_tokens/159,code_tokens/160,code_tokens/161,code_tokens/162,code_tokens/163,code_tokens/164,code_tokens/165,code_tokens/166,code_tokens/167,code_tokens/168,code_tokens/169,code_tokens/170,code_tokens/171,code_tokens/172,code_tokens/173,code_tokens/174,code_tokens/175,code_tokens/176,code_tokens/177,code_tokens/178,code_tokens/179,code_tokens/180,code_tokens/181,code_tokens/182,code_tokens/183,code_tokens/184,code_tokens/185,code_tokens/186,code_tokens/187,code_tokens/188,code_tokens/189,code_tokens/190,code_tokens/191,code_tokens/192,code_tokens/193,code_tokens/194,code_tokens/195,code_tokens/196,code_tokens/197,code_tokens/198,code_tokens/199,code_tokens/200,code_tokens/201,code_tokens/202,code_tokens/203,code_tokens/204,code_tokens/205,code_tokens/206,code_tokens/207,code_tokens/208,code_tokens/209,code_tokens/210,code_tokens/211,code_tokens/212,code_tokens/213,code_tokens/214,code_tokens/215,code_tokens/216,code_tokens/217,code_tokens/218,code_tokens/219,code_tokens/220,code_tokens/221,code_tokens/222,code_tokens/223,code_tokens/224,code_tokens/225,code_tokens/226,code_tokens/227,code_tokens/228,code_tokens/229,code_tokens/230,code_tokens/231,code_tokens/232,code_tokens/233,code_tokens/234,code_tokens/235,code_tokens/236,code_tokens/237,code_tokens/238,code_tokens/239,code_tokens/240,code_tokens/241,code_tokens/242,code_tokens/243,code_tokens/244,code_tokens/245,code_tokens/246,code_tokens/247,code_tokens/248,code_tokens/249,code_tokens/250,code_tokens/251,code_tokens/252,code_tokens/253,code_tokens/254,code_tokens/255,code_tokens/256,code_tokens/257,code_tokens/258,code_tokens/259,code_tokens/260,code_tokens/261,code_tokens/262,code_tokens/263,code_tokens/264,code_tokens/265,code_tokens/266,code_tokens/267,code_tokens/268,code_tokens/269,code_tokens/270,code_tokens/271,code_tokens/272,code_tokens/273,code_tokens/274,code_tokens/275,code_tokens/276,code_tokens/277,code_tokens/278,code_tokens/279,code_tokens/280,code_tokens/281,code_tokens/282,code_tokens/283,code_tokens/284,code_tokens/285,code_tokens/286,code_tokens/287,code_tokens/288,code_tokens/289,code_tokens/290,code_tokens/291,code_tokens/292,code_tokens/293,code_tokens/294,code_tokens/295,code_tokens/296,code_tokens/297,code_tokens/298,code_tokens/299,code_tokens/300,code_tokens/301,code_tokens/302,code_tokens/303,code_tokens/304,code_tokens/305,code_tokens/306,code_tokens/307,code_tokens/308,code_tokens/309,code_tokens/310,code_tokens/311,code_tokens/312,code_tokens/313,code_tokens/314,code_tokens/315,code_tokens/316,code_tokens/317,code_tokens/318,code_tokens/319,code_tokens/320,code_tokens/321,code_tokens/322,code_tokens/323,code_tokens/324,code_tokens/325,code_tokens/326,code_tokens/327,code_tokens/328,code_tokens/329,code_tokens/330,code_tokens/331,code_tokens/332,code_tokens/333,code_tokens/334,code_tokens/335,code_tokens/336,code_tokens/337,code_tokens/338,code_tokens/339,code_tokens/340,code_tokens/341,code_tokens/342,code_tokens/343,code_tokens/344,code_tokens/345,code_tokens/346,code_tokens/347,code_tokens/348,code_tokens/349,code_tokens/350,code_tokens/351,code_tokens/352,code_tokens/353,code_tokens/354,code_tokens/355,code_tokens/356,code_tokens/357,code_tokens/358,code_tokens/359,code_tokens/360,code_tokens/361,code_tokens/362,code_tokens/363,code_tokens/364,code_tokens/365,code_tokens/366,code_tokens/367,code_tokens/368,code_tokens/369,code_tokens/370,code_tokens/371,code_tokens/372,code_tokens/373,code_tokens/374,code_tokens/375,code_tokens/376,code_tokens/377,code_tokens/378,code_tokens/379,code_tokens/380,code_tokens/381,code_tokens/382,code_tokens/383,code_tokens/384,code_tokens/385,code_tokens/386,code_tokens/387,code_tokens/388,code_tokens/389,code_tokens/390,code_tokens/391,code_tokens/392,code_tokens/393,code_tokens/394,code_tokens/395,docstring_tokens/34,docstring_tokens/35,docstring_tokens/36,docstring_tokens/37,docstring_tokens/38,docstring_tokens/39,docstring_tokens/40,docstring_tokens/41,docstring_tokens/42,docstring_tokens/43,docstring_tokens/44,code_tokens/396,code_tokens/397,code_tokens/398,code_tokens/399,code_tokens/400,code_tokens/401,code_tokens/402,code_tokens/403,code_tokens/404,code_tokens/405,code_tokens/406,code_tokens/407,code_tokens/408,code_tokens/409,code_tokens/410,code_tokens/411,code_tokens/412,code_tokens/413,code_tokens/414,code_tokens/415,code_tokens/416,code_tokens/417,code_tokens/418,code_tokens/419,code_tokens/420,code_tokens/421,code_tokens/422,code_tokens/423,code_tokens/424,code_tokens/425,code_tokens/426,code_tokens/427,code_tokens/428,code_tokens/429,code_tokens/430,code_tokens/431,code_tokens/432,code_tokens/433,code_tokens/434,code_tokens/435,code_tokens/436,code_tokens/437,code_tokens/438,code_tokens/439,code_tokens/440,code_tokens/441,code_tokens/442,code_tokens/443,code_tokens/444,code_tokens/445,code_tokens/446,code_tokens/447,code_tokens/448,code_tokens/449,code_tokens/450,code_tokens/451,code_tokens/452,code_tokens/453,code_tokens/454,code_tokens/455,code_tokens/456,code_tokens/457,code_tokens/458,code_tokens/459,code_tokens/460,code_tokens/461,code_tokens/462,code_tokens/463,code_tokens/464,code_tokens/465,code_tokens/466,code_tokens/467,code_tokens/468,code_tokens/469,docstring_tokens/45,docstring_tokens/46,docstring_tokens/47,docstring_tokens/48,docstring_tokens/49,docstring_tokens/50,docstring_tokens/51,docstring_tokens/52,docstring_tokens/53,docstring_tokens/54,docstring_tokens/55,docstring_tokens/56,docstring_tokens/57,docstring_tokens/58,docstring_tokens/59,docstring_tokens/60,docstring_tokens/61,docstring_tokens/62,docstring_tokens/63,docstring_tokens/64,docstring_tokens/65,docstring_tokens/66,docstring_tokens/67,docstring_tokens/68,docstring_tokens/69,docstring_tokens/70,docstring_tokens/71,docstring_tokens/72,docstring_tokens/73,docstring_tokens/74,docstring_tokens/75,docstring_tokens/76,docstring_tokens/77,docstring_tokens/78,docstring_tokens/79,docstring_tokens/80,docstring_tokens/81,docstring_tokens/82,docstring_tokens/83,docstring_tokens/84,docstring_tokens/85,docstring_tokens/86,docstring_tokens/87,docstring_tokens/88,docstring_tokens/89,docstring_tokens/90,docstring_tokens/91,docstring_tokens/92,docstring_tokens/93,docstring_tokens/94,docstring_tokens/95
smdabdoub/phylotoast,phylotoast/util.py,split_phylogeny,"def split_phylogeny(p, level=""s""):
    """"""
    Return either the full or truncated version of a QIIME-formatted taxonomy string.

    :type p: str
    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...

    :type level: str
    :param level: The different level of identification are kingdom (k), phylum (p),
                  class (c),order (o), family (f), genus (g) and species (s). If level is
                  not provided, the default level of identification is species.

    :rtype: str
    :return: A QIIME-formatted taxonomy string up to the classification given
            by param level.
    """"""
    level = level+""__""
    result = p.split(level)
    return result[0]+level+result[1].split("";"")[0]",python,"def split_phylogeny(p, level=""s""):
    """"""
    Return either the full or truncated version of a QIIME-formatted taxonomy string.

    :type p: str
    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...

    :type level: str
    :param level: The different level of identification are kingdom (k), phylum (p),
                  class (c),order (o), family (f), genus (g) and species (s). If level is
                  not provided, the default level of identification is species.

    :rtype: str
    :return: A QIIME-formatted taxonomy string up to the classification given
            by param level.
    """"""
    level = level+""__""
    result = p.split(level)
    return result[0]+level+result[1].split("";"")[0]",def,split_phylogeny,(,p,",",level,=,"""s""",),:,level,=,level,+,"""__""",result,=,p,.,split,(,level,),return,result,[,0,],+,level,+,result,[,1,],.,split,(,""";""",),[,0,],"Return either the full or truncated version of a QIIME-formatted taxonomy string.

    :type p: str
    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...

    :type level: str
    :param level: The different level of identification are kingdom (k), phylum (p),
                  class (c),order (o), family (f), genus (g) and species (s). If level is
                  not provided, the default level of identification is species.

    :rtype: str
    :return: A QIIME-formatted taxonomy string up to the classification given
            by param level.",Return,either,the,full,or,truncated,version,of,a,QIIME,-,formatted,taxonomy,string,.,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L159-L177,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,ensure_dir,"def ensure_dir(d):
    """"""
    Check to make sure the supplied directory path does not exist, if so, create it. The
    method catches OSError exceptions and returns a descriptive message instead of
    re-raising the error.

    :type d: str
    :param d: It is the full path to a directory.

    :return: Does not return anything, but creates a directory path if it doesn't exist
             already.
    """"""
    if not os.path.exists(d):
        try:
            os.makedirs(d)
        except OSError as oe:
            # should not happen with os.makedirs
            # ENOENT: No such file or directory
            if os.errno == errno.ENOENT:
                msg = twdd(""""""One or more directories in the path ({}) do not exist. If
                           you are specifying a new directory for output, please ensure
                           all other directories in the path currently exist."""""")
                return msg.format(d)
            else:
                msg = twdd(""""""An error occurred trying to create the output directory
                           ({}) with message: {}"""""")
                return msg.format(d, oe.strerror)",python,"def ensure_dir(d):
    """"""
    Check to make sure the supplied directory path does not exist, if so, create it. The
    method catches OSError exceptions and returns a descriptive message instead of
    re-raising the error.

    :type d: str
    :param d: It is the full path to a directory.

    :return: Does not return anything, but creates a directory path if it doesn't exist
             already.
    """"""
    if not os.path.exists(d):
        try:
            os.makedirs(d)
        except OSError as oe:
            # should not happen with os.makedirs
            # ENOENT: No such file or directory
            if os.errno == errno.ENOENT:
                msg = twdd(""""""One or more directories in the path ({}) do not exist. If
                           you are specifying a new directory for output, please ensure
                           all other directories in the path currently exist."""""")
                return msg.format(d)
            else:
                msg = twdd(""""""An error occurred trying to create the output directory
                           ({}) with message: {}"""""")
                return msg.format(d, oe.strerror)",def,ensure_dir,(,d,),:,if,not,os,.,path,.,exists,(,d,),:,try,:,os,.,makedirs,(,d,),except,OSError,as,oe,:,# should not happen with os.makedirs,# ENOENT: No such file or directory,if,os,.,errno,==,errno,.,ENOENT,:,msg,=,"Check to make sure the supplied directory path does not exist, if so, create it. The
    method catches OSError exceptions and returns a descriptive message instead of
    re-raising the error.

    :type d: str
    :param d: It is the full path to a directory.

    :return: Does not return anything, but creates a directory path if it doesn't exist
             already.",Check,to,make,sure,the,supplied,directory,path,does,not,exist,if,so,create,it,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L180-L206,train,twdd,(,"""""""One or more directories in the path ({}) do not exist. If
                           you are specifying a new directory for output, please ensure
                           all other directories in the path currently exist.""""""",),return,msg,.,format,(,d,),else,:,msg,=,twdd,(,"""""""An error occurred trying to create the output directory
                           ({}) with message: {}""""""",),return,msg,.,format,(,d,",",oe,.,strerror,),.,The,method,catches,OSError,exceptions,and,returns,a,descriptive,message,instead,of,re,-,raising,the,error,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,file_handle,"def file_handle(fnh, mode=""rU""):
    """"""
    Takes either a file path or an open file handle, checks validity and returns an open
    file handle or raises an appropriate Exception.

    :type fnh: str
    :param fnh: It is the full path to a file, or open file handle

    :type mode: str
    :param mode: The way in which this file will be used, for example to read or write or
                 both. By default, file will be opened in rU mode.

    :return: Returns an opened file for appropriate usage.
    """"""
    handle = None
    if isinstance(fnh, file):
        if fnh.closed:
            raise ValueError(""Input file is closed."")
        handle = fnh
    elif isinstance(fnh, str):
        handle = open(fnh, mode)

    return handle",python,"def file_handle(fnh, mode=""rU""):
    """"""
    Takes either a file path or an open file handle, checks validity and returns an open
    file handle or raises an appropriate Exception.

    :type fnh: str
    :param fnh: It is the full path to a file, or open file handle

    :type mode: str
    :param mode: The way in which this file will be used, for example to read or write or
                 both. By default, file will be opened in rU mode.

    :return: Returns an opened file for appropriate usage.
    """"""
    handle = None
    if isinstance(fnh, file):
        if fnh.closed:
            raise ValueError(""Input file is closed."")
        handle = fnh
    elif isinstance(fnh, str):
        handle = open(fnh, mode)

    return handle",def,file_handle,(,fnh,",",mode,=,"""rU""",),:,handle,=,None,if,isinstance,(,fnh,",",file,),:,if,fnh,.,closed,:,raise,ValueError,(,"""Input file is closed.""",),handle,=,fnh,elif,isinstance,(,fnh,",",str,),:,handle,"Takes either a file path or an open file handle, checks validity and returns an open
    file handle or raises an appropriate Exception.

    :type fnh: str
    :param fnh: It is the full path to a file, or open file handle

    :type mode: str
    :param mode: The way in which this file will be used, for example to read or write or
                 both. By default, file will be opened in rU mode.

    :return: Returns an opened file for appropriate usage.",Takes,either,a,file,path,or,an,open,file,handle,checks,validity,and,returns,an,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L209-L231,train,=,open,(,fnh,",",mode,),return,handle,,,,,,,,,,,,,,,,,,,,,,open,file,handle,or,raises,an,appropriate,Exception,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,gather_categories,"def gather_categories(imap, header, categories=None):
    """"""
    Find the user specified categories in the map and create a dictionary to contain the
    relevant data for each type within the categories. Multiple categories will have their
    types combined such that each possible combination will have its own entry in the
    dictionary.

    :type imap: dict
    :param imap: The input mapping file data keyed by SampleID
    :type header: list
    :param header: The header line from the input mapping file. This will be searched for
                   the user-specified categories
    :type categories: list
    :param categories: The list of user-specified category column name from mapping file
    :rtype: dict
    :return: A sorted dictionary keyed on the combinations of all the types found within
             the user-specified categories. Each entry will contain an empty DataCategory
             namedtuple. If no categories are specified, a single entry with the key
             'default' will be returned
    """"""
    # If no categories provided, return all SampleIDs
    if categories is None:
        return {""default"": DataCategory(set(imap.keys()), {})}

    cat_ids = [header.index(cat)
               for cat in categories if cat in header and ""="" not in cat]

    table = OrderedDict()
    conditions = defaultdict(set)
    for i, cat in enumerate(categories):
        if ""="" in cat and cat.split(""="")[0] in header:
            cat_name = header[header.index(cat.split(""="")[0])]
            conditions[cat_name].add(cat.split(""="")[1])

    # If invalid categories or conditions identified, return all SampleIDs
    if not cat_ids and not conditions:
        return {""default"": DataCategory(set(imap.keys()), {})}

    #If only category column given, return column-wise SampleIDs
    if cat_ids and not conditions:
        for sid, row in imap.items():
            cat_name = ""_"".join([row[cid] for cid in cat_ids])
            if cat_name not in table:
                table[cat_name] = DataCategory(set(), {})
            table[cat_name].sids.add(sid)
        return table

    # Collect all condition names
    cond_ids = set()
    for k in conditions:
        try:
            cond_ids.add(header.index(k))
        except ValueError:
            continue
    idx_to_test = set(cat_ids).union(cond_ids)

    # If column name and condition given, return overlapping SampleIDs of column and
    # condition combinations
    for sid, row in imap.items():
        if all([row[header.index(c)] in conditions[c] for c in conditions]):
            key = ""_"".join([row[idx] for idx in idx_to_test])
            try:
                assert key in table.keys()
            except AssertionError:
                table[key] = DataCategory(set(), {})
            table[key].sids.add(sid)
    try:
        assert len(table) > 0
    except AssertionError:
        return {""default"": DataCategory(set(imap.keys()), {})}
    else:
        return table",python,"def gather_categories(imap, header, categories=None):
    """"""
    Find the user specified categories in the map and create a dictionary to contain the
    relevant data for each type within the categories. Multiple categories will have their
    types combined such that each possible combination will have its own entry in the
    dictionary.

    :type imap: dict
    :param imap: The input mapping file data keyed by SampleID
    :type header: list
    :param header: The header line from the input mapping file. This will be searched for
                   the user-specified categories
    :type categories: list
    :param categories: The list of user-specified category column name from mapping file
    :rtype: dict
    :return: A sorted dictionary keyed on the combinations of all the types found within
             the user-specified categories. Each entry will contain an empty DataCategory
             namedtuple. If no categories are specified, a single entry with the key
             'default' will be returned
    """"""
    # If no categories provided, return all SampleIDs
    if categories is None:
        return {""default"": DataCategory(set(imap.keys()), {})}

    cat_ids = [header.index(cat)
               for cat in categories if cat in header and ""="" not in cat]

    table = OrderedDict()
    conditions = defaultdict(set)
    for i, cat in enumerate(categories):
        if ""="" in cat and cat.split(""="")[0] in header:
            cat_name = header[header.index(cat.split(""="")[0])]
            conditions[cat_name].add(cat.split(""="")[1])

    # If invalid categories or conditions identified, return all SampleIDs
    if not cat_ids and not conditions:
        return {""default"": DataCategory(set(imap.keys()), {})}

    #If only category column given, return column-wise SampleIDs
    if cat_ids and not conditions:
        for sid, row in imap.items():
            cat_name = ""_"".join([row[cid] for cid in cat_ids])
            if cat_name not in table:
                table[cat_name] = DataCategory(set(), {})
            table[cat_name].sids.add(sid)
        return table

    # Collect all condition names
    cond_ids = set()
    for k in conditions:
        try:
            cond_ids.add(header.index(k))
        except ValueError:
            continue
    idx_to_test = set(cat_ids).union(cond_ids)

    # If column name and condition given, return overlapping SampleIDs of column and
    # condition combinations
    for sid, row in imap.items():
        if all([row[header.index(c)] in conditions[c] for c in conditions]):
            key = ""_"".join([row[idx] for idx in idx_to_test])
            try:
                assert key in table.keys()
            except AssertionError:
                table[key] = DataCategory(set(), {})
            table[key].sids.add(sid)
    try:
        assert len(table) > 0
    except AssertionError:
        return {""default"": DataCategory(set(imap.keys()), {})}
    else:
        return table",def,gather_categories,(,imap,",",header,",",categories,=,None,),:,"# If no categories provided, return all SampleIDs",if,categories,is,None,:,return,{,"""default""",:,DataCategory,(,set,(,imap,.,keys,(,),),",",{,},),},cat_ids,=,[,header,.,index,"Find the user specified categories in the map and create a dictionary to contain the
    relevant data for each type within the categories. Multiple categories will have their
    types combined such that each possible combination will have its own entry in the
    dictionary.

    :type imap: dict
    :param imap: The input mapping file data keyed by SampleID
    :type header: list
    :param header: The header line from the input mapping file. This will be searched for
                   the user-specified categories
    :type categories: list
    :param categories: The list of user-specified category column name from mapping file
    :rtype: dict
    :return: A sorted dictionary keyed on the combinations of all the types found within
             the user-specified categories. Each entry will contain an empty DataCategory
             namedtuple. If no categories are specified, a single entry with the key
             'default' will be returned",Find,the,user,specified,categories,in,the,map,and,create,a,dictionary,to,contain,the,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L238-L309,train,(,cat,),for,cat,in,categories,if,cat,in,header,and,"""=""",not,in,cat,],table,=,OrderedDict,(,),conditions,=,defaultdict,(,set,),for,i,relevant,data,for,each,type,within,the,categories,.,Multiple,categories,will,have,their,types,combined,such,that,each,",",cat,in,enumerate,(,categories,),:,if,"""=""",in,cat,and,cat,.,split,(,"""=""",),[,0,],in,header,:,cat_name,=,header,[,header,.,index,(,cat,.,split,(,"""=""",),[,0,],),],conditions,[,cat_name,],.,add,(,cat,.,split,(,"""=""",),[,1,],),"# If invalid categories or conditions identified, return all SampleIDs",if,not,cat_ids,and,not,conditions,:,return,{,"""default""",:,DataCategory,(,set,(,imap,.,keys,(,),),",",{,},),},"#If only category column given, return column-wise SampleIDs",if,cat_ids,and,not,conditions,:,for,sid,",",row,in,imap,.,items,(,),:,cat_name,=,"""_""",.,join,(,[,row,[,cid,],for,cid,in,cat_ids,],),if,cat_name,not,in,table,:,table,[,cat_name,],=,DataCategory,(,set,(,),",",{,},),table,[,cat_name,],.,sids,.,add,(,sid,),return,table,# Collect all condition names,cond_ids,=,set,(,),for,k,in,conditions,:,try,:,cond_ids,.,add,(,header,.,index,(,k,),),except,ValueError,:,continue,idx_to_test,=,set,(,cat_ids,),.,union,(,cond_ids,),"# If column name and condition given, return overlapping SampleIDs of column and",# condition combinations,for,sid,",",row,in,imap,.,items,(,),:,if,all,(,[,row,[,header,.,index,(,c,),],in,conditions,[,c,],for,c,in,conditions,],),:,key,=,"""_""",.,join,(,[,row,[,idx,],for,idx,in,idx_to_test,],),try,:,assert,key,in,table,.,keys,(,),except,AssertionError,:,table,[,key,],=,DataCategory,(,set,(,),",",{,},),table,[,key,],.,sids,.,add,(,sid,),try,:,assert,len,(,table,),>,0,except,AssertionError,:,return,{,"""default""",:,DataCategory,(,set,(,imap,.,keys,(,),),",",{,},),},else,:,return,table,possible,combination,will,have,its,own,entry,in,the,dictionary,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,parse_unifrac,"def parse_unifrac(unifracFN):
    """"""
    Parses the unifrac results file into a dictionary

    :type unifracFN: str
    :param unifracFN: The path to the unifrac results file

    :rtype: dict
    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a
             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and
             'varexp' (variation explained)
    """"""
    with open(unifracFN, ""rU"") as uF:
        first = uF.next().split(""\t"")
        lines = [line.strip() for line in uF]

    unifrac = {""pcd"": OrderedDict(), ""eigvals"": [], ""varexp"": []}
    if first[0] == ""pc vector number"":
        return parse_unifrac_v1_8(unifrac, lines)
    elif first[0] == ""Eigvals"":
        return parse_unifrac_v1_9(unifrac, lines)
    else:
        raise ValueError(""File format not supported/recognized. Please check input ""
                         ""unifrac file."")",python,"def parse_unifrac(unifracFN):
    """"""
    Parses the unifrac results file into a dictionary

    :type unifracFN: str
    :param unifracFN: The path to the unifrac results file

    :rtype: dict
    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a
             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and
             'varexp' (variation explained)
    """"""
    with open(unifracFN, ""rU"") as uF:
        first = uF.next().split(""\t"")
        lines = [line.strip() for line in uF]

    unifrac = {""pcd"": OrderedDict(), ""eigvals"": [], ""varexp"": []}
    if first[0] == ""pc vector number"":
        return parse_unifrac_v1_8(unifrac, lines)
    elif first[0] == ""Eigvals"":
        return parse_unifrac_v1_9(unifrac, lines)
    else:
        raise ValueError(""File format not supported/recognized. Please check input ""
                         ""unifrac file."")",def,parse_unifrac,(,unifracFN,),:,with,open,(,unifracFN,",","""rU""",),as,uF,:,first,=,uF,.,next,(,),.,split,(,"""\t""",),lines,=,[,line,.,strip,(,),for,line,in,uF,],unifrac,=,"Parses the unifrac results file into a dictionary

    :type unifracFN: str
    :param unifracFN: The path to the unifrac results file

    :rtype: dict
    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a
             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and
             'varexp' (variation explained)",Parses,the,unifrac,results,file,into,a,dictionary,,,,,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L311-L334,train,{,"""pcd""",:,OrderedDict,(,),",","""eigvals""",:,[,],",","""varexp""",:,[,],},if,first,[,0,],==,"""pc vector number""",:,return,parse_unifrac_v1_8,(,unifrac,",",,,,,,,,,,,,,,,,,,,,lines,),elif,first,[,0,],==,"""Eigvals""",:,return,parse_unifrac_v1_9,(,unifrac,",",lines,),else,:,raise,ValueError,(,"""File format not supported/recognized. Please check input ""","""unifrac file.""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,parse_unifrac_v1_8,"def parse_unifrac_v1_8(unifrac, file_data):
    """"""
    Function to parse data from older version of unifrac file obtained from Qiime version
    1.8 and earlier.

    :type unifrac: dict
    :param unifracFN: The path to the unifrac results file

    :type file_data: list
    :param file_data: Unifrac data lines after stripping whitespace characters.
    """"""
    for line in file_data:
        if line == """":
            break
        line = line.split(""\t"")
        unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]

    unifrac[""eigvals""] = [float(entry) for entry in file_data[-2].split(""\t"")[1:]]
    unifrac[""varexp""] = [float(entry) for entry in file_data[-1].split(""\t"")[1:]]
    return unifrac",python,"def parse_unifrac_v1_8(unifrac, file_data):
    """"""
    Function to parse data from older version of unifrac file obtained from Qiime version
    1.8 and earlier.

    :type unifrac: dict
    :param unifracFN: The path to the unifrac results file

    :type file_data: list
    :param file_data: Unifrac data lines after stripping whitespace characters.
    """"""
    for line in file_data:
        if line == """":
            break
        line = line.split(""\t"")
        unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]

    unifrac[""eigvals""] = [float(entry) for entry in file_data[-2].split(""\t"")[1:]]
    unifrac[""varexp""] = [float(entry) for entry in file_data[-1].split(""\t"")[1:]]
    return unifrac",def,parse_unifrac_v1_8,(,unifrac,",",file_data,),:,for,line,in,file_data,:,if,line,==,"""""",:,break,line,=,line,.,split,(,"""\t""",),unifrac,[,"""pcd""",],[,line,[,0,],],=,[,float,(,e,),"Function to parse data from older version of unifrac file obtained from Qiime version
    1.8 and earlier.

    :type unifrac: dict
    :param unifracFN: The path to the unifrac results file

    :type file_data: list
    :param file_data: Unifrac data lines after stripping whitespace characters.",Function,to,parse,data,from,older,version,of,unifrac,file,obtained,from,Qiime,version,1,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L337-L356,train,for,e,in,line,[,1,:,],],unifrac,[,"""eigvals""",],=,[,float,(,entry,),for,entry,in,file_data,[,-,2,],.,split,(,.,8,and,earlier,.,,,,,,,,,,,,,,,"""\t""",),[,1,:,],],unifrac,[,"""varexp""",],=,[,float,(,entry,),for,entry,in,file_data,[,-,1,],.,split,(,"""\t""",),[,1,:,],],return,unifrac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,parse_unifrac_v1_9,"def parse_unifrac_v1_9(unifrac, file_data):
    """"""
    Function to parse data from newer version of unifrac file obtained from Qiime version
    1.9 and later.

    :type unifracFN: str
    :param unifracFN: The path to the unifrac results file

    :type file_data: list
    :param file_data: Unifrac data lines after stripping whitespace characters.
    """"""
    unifrac[""eigvals""] = [float(entry) for entry in file_data[0].split(""\t"")]
    unifrac[""varexp""] = [float(entry)*100 for entry in file_data[3].split(""\t"")]

    for line in file_data[8:]:
        if line == """":
            break
        line = line.split(""\t"")
        unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]
    return unifrac",python,"def parse_unifrac_v1_9(unifrac, file_data):
    """"""
    Function to parse data from newer version of unifrac file obtained from Qiime version
    1.9 and later.

    :type unifracFN: str
    :param unifracFN: The path to the unifrac results file

    :type file_data: list
    :param file_data: Unifrac data lines after stripping whitespace characters.
    """"""
    unifrac[""eigvals""] = [float(entry) for entry in file_data[0].split(""\t"")]
    unifrac[""varexp""] = [float(entry)*100 for entry in file_data[3].split(""\t"")]

    for line in file_data[8:]:
        if line == """":
            break
        line = line.split(""\t"")
        unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]
    return unifrac",def,parse_unifrac_v1_9,(,unifrac,",",file_data,),:,unifrac,[,"""eigvals""",],=,[,float,(,entry,),for,entry,in,file_data,[,0,],.,split,(,"""\t""",),],unifrac,[,"""varexp""",],=,[,float,(,entry,),*,100,"Function to parse data from newer version of unifrac file obtained from Qiime version
    1.9 and later.

    :type unifracFN: str
    :param unifracFN: The path to the unifrac results file

    :type file_data: list
    :param file_data: Unifrac data lines after stripping whitespace characters.",Function,to,parse,data,from,newer,version,of,unifrac,file,obtained,from,Qiime,version,1,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L359-L378,train,for,entry,in,file_data,[,3,],.,split,(,"""\t""",),],for,line,in,file_data,[,8,:,],:,if,line,==,"""""",:,break,line,=,.,9,and,later,.,,,,,,,,,,,,,,,line,.,split,(,"""\t""",),unifrac,[,"""pcd""",],[,line,[,0,],],=,[,float,(,e,),for,e,in,line,[,1,:,],],return,unifrac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/util.py,color_mapping,"def color_mapping(sample_map, header, group_column, color_column=None):
    """"""
    Determine color-category mapping. If color_column was specified, then map the category
    names to color values. Otherwise, use the palettable colors to automatically generate
    a set of colors for the group values.

    :type sample_map: dict
    :param unifracFN: Map associating each line of the mapping file with the appropriate
                      sample ID (each value of the map also contains the sample ID)

    :type header: tuple
    :param A tuple of header line for mapping file

    :type group_column: str
    :param group_column: String denoting the column name for sample groups.

    :type color_column: str
    :param color_column: String denoting the column name for sample colors.

    :type return: dict
    :param return: {SampleID: Color}
    """"""
    group_colors = OrderedDict()
    group_gather = gather_categories(sample_map, header, [group_column])

    if color_column is not None:
        color_gather = gather_categories(sample_map, header, [color_column])
        # match sample IDs between color_gather and group_gather
        for group in group_gather:
            for color in color_gather:
                # allow incomplete assignment of colors, if group sids overlap at
                # all with the color sids, consider it a match
                if group_gather[group].sids.intersection(color_gather[color].sids):
                    group_colors[group] = color
    else:
        bcolors = itertools.cycle(Set3_12.hex_colors)
        for group in group_gather:
            group_colors[group] = bcolors.next()

    return group_colors",python,"def color_mapping(sample_map, header, group_column, color_column=None):
    """"""
    Determine color-category mapping. If color_column was specified, then map the category
    names to color values. Otherwise, use the palettable colors to automatically generate
    a set of colors for the group values.

    :type sample_map: dict
    :param unifracFN: Map associating each line of the mapping file with the appropriate
                      sample ID (each value of the map also contains the sample ID)

    :type header: tuple
    :param A tuple of header line for mapping file

    :type group_column: str
    :param group_column: String denoting the column name for sample groups.

    :type color_column: str
    :param color_column: String denoting the column name for sample colors.

    :type return: dict
    :param return: {SampleID: Color}
    """"""
    group_colors = OrderedDict()
    group_gather = gather_categories(sample_map, header, [group_column])

    if color_column is not None:
        color_gather = gather_categories(sample_map, header, [color_column])
        # match sample IDs between color_gather and group_gather
        for group in group_gather:
            for color in color_gather:
                # allow incomplete assignment of colors, if group sids overlap at
                # all with the color sids, consider it a match
                if group_gather[group].sids.intersection(color_gather[color].sids):
                    group_colors[group] = color
    else:
        bcolors = itertools.cycle(Set3_12.hex_colors)
        for group in group_gather:
            group_colors[group] = bcolors.next()

    return group_colors",def,color_mapping,(,sample_map,",",header,",",group_column,",",color_column,=,None,),:,group_colors,=,OrderedDict,(,),group_gather,=,gather_categories,(,sample_map,",",header,",",[,group_column,],),if,color_column,is,not,None,:,color_gather,=,gather_categories,(,sample_map,",","Determine color-category mapping. If color_column was specified, then map the category
    names to color values. Otherwise, use the palettable colors to automatically generate
    a set of colors for the group values.

    :type sample_map: dict
    :param unifracFN: Map associating each line of the mapping file with the appropriate
                      sample ID (each value of the map also contains the sample ID)

    :type header: tuple
    :param A tuple of header line for mapping file

    :type group_column: str
    :param group_column: String denoting the column name for sample groups.

    :type color_column: str
    :param color_column: String denoting the column name for sample colors.

    :type return: dict
    :param return: {SampleID: Color}",Determine,color,-,category,mapping,.,If,color_column,was,specified,then,map,the,category,names,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L380-L419,train,header,",",[,color_column,],),# match sample IDs between color_gather and group_gather,for,group,in,group_gather,:,for,color,in,color_gather,:,"# allow incomplete assignment of colors, if group sids overlap at","# all with the color sids, consider it a match",if,group_gather,[,group,],.,sids,.,intersection,(,color_gather,to,color,values,.,Otherwise,use,the,palettable,colors,to,automatically,generate,a,set,of,colors,for,the,group,[,color,],.,sids,),:,group_colors,[,group,],=,color,else,:,bcolors,=,itertools,.,cycle,(,Set3_12,.,hex_colors,),for,group,in,group_gather,:,group_colors,[,group,],=,bcolors,.,next,(,),return,group_colors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,values,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/shuffle_genome.py,rev_c,"def rev_c(read):
    """"""
    return reverse completment of read
    """"""
    rc = []
    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}
    for base in read:
        rc.extend(rc_nucs[base.upper()])
    return rc[::-1]",python,"def rev_c(read):
    """"""
    return reverse completment of read
    """"""
    rc = []
    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}
    for base in read:
        rc.extend(rc_nucs[base.upper()])
    return rc[::-1]",def,rev_c,(,read,),:,rc,=,[,],rc_nucs,=,{,'A',:,'T',",",'T',:,'A',",",'G',:,'C',",",'C',:,'G',",",'N',:,'N',},for,base,in,read,:,rc,.,extend,(,rc_nucs,return reverse completment of read,return,reverse,completment,of,read,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/shuffle_genome.py#L27-L35,train,[,base,.,upper,(,),],),return,rc,[,:,:,-,1,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/shuffle_genome.py,shuffle_genome,"def shuffle_genome(genome, cat, fraction = float(100), plot = True, \
        alpha = 0.1, beta = 100000, \
        min_length = 1000, max_length = 200000):
    """"""
    randomly shuffle genome
    """"""
    header = '>randomized_%s' % (genome.name)
    sequence = list(''.join([i[1] for i in parse_fasta(genome)]))
    length = len(sequence)
    shuffled = []
    # break genome into pieces
    while sequence is not False:
        s = int(random.gammavariate(alpha, beta))
        if s <= min_length or s >= max_length:
            continue
        if len(sequence) < s:
            seq = sequence[0:]
        else:
            seq = sequence[0:s]
        sequence = sequence[s:]
#        if bool(random.getrandbits(1)) is True:
#            seq = rev_c(seq)
#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)
#        else:
#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)
        shuffled.append(''.join(seq))
        if sequence == []:
            break
    # shuffle pieces
    random.shuffle(shuffled)
    # subset fragments
    if fraction == float(100):
        subset = shuffled
    else:
        max_pieces = int(length * fraction/100)
        subset, total = [], 0
        for fragment in shuffled:
            length = len(fragment)
            if total + length <= max_pieces:
                subset.append(fragment)
                total += length
            else:
                diff = max_pieces - total
                subset.append(fragment[0:diff])
                break
    # combine sequences, if requested
    if cat is True:
        yield [header, ''.join(subset)]
    else:
        for i, seq in enumerate(subset):
            yield ['%s fragment:%s' % (header, i), seq]",python,"def shuffle_genome(genome, cat, fraction = float(100), plot = True, \
        alpha = 0.1, beta = 100000, \
        min_length = 1000, max_length = 200000):
    """"""
    randomly shuffle genome
    """"""
    header = '>randomized_%s' % (genome.name)
    sequence = list(''.join([i[1] for i in parse_fasta(genome)]))
    length = len(sequence)
    shuffled = []
    # break genome into pieces
    while sequence is not False:
        s = int(random.gammavariate(alpha, beta))
        if s <= min_length or s >= max_length:
            continue
        if len(sequence) < s:
            seq = sequence[0:]
        else:
            seq = sequence[0:s]
        sequence = sequence[s:]
#        if bool(random.getrandbits(1)) is True:
#            seq = rev_c(seq)
#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)
#        else:
#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)
        shuffled.append(''.join(seq))
        if sequence == []:
            break
    # shuffle pieces
    random.shuffle(shuffled)
    # subset fragments
    if fraction == float(100):
        subset = shuffled
    else:
        max_pieces = int(length * fraction/100)
        subset, total = [], 0
        for fragment in shuffled:
            length = len(fragment)
            if total + length <= max_pieces:
                subset.append(fragment)
                total += length
            else:
                diff = max_pieces - total
                subset.append(fragment[0:diff])
                break
    # combine sequences, if requested
    if cat is True:
        yield [header, ''.join(subset)]
    else:
        for i, seq in enumerate(subset):
            yield ['%s fragment:%s' % (header, i), seq]",def,shuffle_genome,(,genome,",",cat,",",fraction,=,float,(,100,),",",plot,=,True,",",alpha,=,0.1,",",beta,=,100000,",",min_length,=,1000,",",max_length,=,200000,),:,header,=,'>randomized_%s',%,(,genome,.,name,randomly shuffle genome,randomly,shuffle,genome,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/shuffle_genome.py#L37-L87,train,),sequence,=,list,(,'',.,join,(,[,i,[,1,],for,i,in,parse_fasta,(,genome,),],),),length,=,len,(,sequence,),,,,,,,,,,,,,,,,,,,,shuffled,=,[,],# break genome into pieces,while,sequence,is,not,False,:,s,=,int,(,random,.,gammavariate,(,alpha,",",beta,),),if,s,<=,min_length,or,s,>=,max_length,:,continue,if,len,(,sequence,),<,s,:,seq,=,sequence,[,0,:,],else,:,seq,=,sequence,[,0,:,s,],sequence,=,sequence,[,s,:,],#        if bool(random.getrandbits(1)) is True:,#            seq = rev_c(seq),"#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)",#        else:,"#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)",shuffled,.,append,(,'',.,join,(,seq,),),if,sequence,==,[,],:,break,# shuffle pieces,random,.,shuffle,(,shuffled,),# subset fragments,if,fraction,==,float,(,100,),:,subset,=,shuffled,else,:,max_pieces,=,int,(,length,*,fraction,/,100,),subset,",",total,=,[,],",",0,for,fragment,in,shuffled,:,length,=,len,(,fragment,),if,total,+,length,<=,max_pieces,:,subset,.,append,(,fragment,),total,+=,length,else,:,diff,=,max_pieces,-,total,subset,.,append,(,fragment,[,0,:,diff,],),break,"# combine sequences, if requested",if,cat,is,True,:,yield,[,header,",",'',.,join,(,subset,),],else,:,for,i,",",seq,in,enumerate,(,subset,),:,yield,[,'%s fragment:%s',%,(,header,",",i,),",",seq,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/regression.py,MultiVarLinReg._prune,"def _prune(self, fit, p_max):
        """"""
        If the fit contains statistically insignificant parameters, remove them.
        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max

        Parameters
        ----------
        fit: fm.ols fit object
            Can contain insignificant parameters
        p_max : float
            Maximum allowed probability of the t-statistic

        Returns
        -------
        fit: fm.ols fit object
            Won't contain any insignificant parameters

        """"""

        def remove_from_model_desc(x, model_desc):
            """"""
            Return a model_desc without x
            """"""

            rhs_termlist = []
            for t in model_desc.rhs_termlist:
                if not t.factors:
                    # intercept, add anyway
                    rhs_termlist.append(t)
                elif not x == t.factors[0]._varname:
                    # this is not the term with x
                    rhs_termlist.append(t)

            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)
            return md

        corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])
        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
        try:
            pars_to_prune.remove('Intercept')
        except:
            pass
        while pars_to_prune:
            corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)
            fit = fm.ols(corrected_model_desc, data=self.df).fit()
            pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
            try:
                pars_to_prune.remove('Intercept')
            except:
                pass
        return fit",python,"def _prune(self, fit, p_max):
        """"""
        If the fit contains statistically insignificant parameters, remove them.
        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max

        Parameters
        ----------
        fit: fm.ols fit object
            Can contain insignificant parameters
        p_max : float
            Maximum allowed probability of the t-statistic

        Returns
        -------
        fit: fm.ols fit object
            Won't contain any insignificant parameters

        """"""

        def remove_from_model_desc(x, model_desc):
            """"""
            Return a model_desc without x
            """"""

            rhs_termlist = []
            for t in model_desc.rhs_termlist:
                if not t.factors:
                    # intercept, add anyway
                    rhs_termlist.append(t)
                elif not x == t.factors[0]._varname:
                    # this is not the term with x
                    rhs_termlist.append(t)

            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)
            return md

        corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])
        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
        try:
            pars_to_prune.remove('Intercept')
        except:
            pass
        while pars_to_prune:
            corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)
            fit = fm.ols(corrected_model_desc, data=self.df).fit()
            pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
            try:
                pars_to_prune.remove('Intercept')
            except:
                pass
        return fit",def,_prune,(,self,",",fit,",",p_max,),:,def,remove_from_model_desc,(,x,",",model_desc,),:,"""""""
            Return a model_desc without x
            """"""",rhs_termlist,=,[,],for,t,in,model_desc,.,rhs_termlist,:,if,not,t,.,factors,:,"# intercept, add anyway",rhs_termlist,.,append,(,t,),"If the fit contains statistically insignificant parameters, remove them.
        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max

        Parameters
        ----------
        fit: fm.ols fit object
            Can contain insignificant parameters
        p_max : float
            Maximum allowed probability of the t-statistic

        Returns
        -------
        fit: fm.ols fit object
            Won't contain any insignificant parameters",If,the,fit,contains,statistically,insignificant,parameters,remove,them,.,Returns,a,pruned,fit,where,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/regression.py#L222-L272,train,elif,not,x,==,t,.,factors,[,0,],.,_varname,:,# this is not the term with x,rhs_termlist,.,append,(,t,),md,=,ModelDesc,(,model_desc,.,lhs_termlist,",",rhs_termlist,),all,parameters,have,p,-,values,of,the,t,-,statistic,below,p_max,,,,,,,return,md,corrected_model_desc,=,ModelDesc,(,fit,.,model,.,formula,.,lhs_termlist,[,:,],",",fit,.,model,.,formula,.,rhs_termlist,[,:,],),pars_to_prune,=,fit,.,pvalues,.,where,(,fit,.,pvalues,>,p_max,),.,dropna,(,),.,index,.,tolist,(,),try,:,pars_to_prune,.,remove,(,'Intercept',),except,:,pass,while,pars_to_prune,:,corrected_model_desc,=,remove_from_model_desc,(,pars_to_prune,[,0,],",",corrected_model_desc,),fit,=,fm,.,ols,(,corrected_model_desc,",",data,=,self,.,df,),.,fit,(,),pars_to_prune,=,fit,.,pvalues,.,where,(,fit,.,pvalues,>,p_max,),.,dropna,(,),.,index,.,tolist,(,),try,:,pars_to_prune,.,remove,(,'Intercept',),except,:,pass,return,fit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/regression.py,MultiVarLinReg.find_best_rsquared,"def find_best_rsquared(list_of_fits):
        """"""Return the best fit, based on rsquared""""""
        res = sorted(list_of_fits, key=lambda x: x.rsquared)
        return res[-1]",python,"def find_best_rsquared(list_of_fits):
        """"""Return the best fit, based on rsquared""""""
        res = sorted(list_of_fits, key=lambda x: x.rsquared)
        return res[-1]",def,find_best_rsquared,(,list_of_fits,),:,res,=,sorted,(,list_of_fits,",",key,=,lambda,x,:,x,.,rsquared,),return,res,[,-,1,],,,,,,,,,,,,,,,,,"Return the best fit, based on rsquared",Return,the,best,fit,based,on,rsquared,,,,,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/regression.py#L275-L278,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/regression.py,MultiVarLinReg._predict,"def _predict(self, fit, df):
        """"""
        Return a df with predictions and confidence interval

        Notes
        -----
        The df will contain the following columns:
        - 'predicted': the model output
        - 'interval_u', 'interval_l': upper and lower confidence bounds.

        The result will depend on the following attributes of self:
        confint : float (default=0.95)
            Confidence level for two-sided hypothesis
        allow_negative_predictions : bool (default=True)
            If False, correct negative predictions to zero (typically for energy consumption predictions)

        Parameters
        ----------
        fit : Statsmodels fit
        df : pandas DataFrame or None (default)
            If None, use self.df


        Returns
        -------
        df_res : pandas DataFrame
            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'
        """"""

        # Add model results to data as column 'predictions'
        df_res = df.copy()
        if 'Intercept' in fit.model.exog_names:
            df_res['Intercept'] = 1.0
        df_res['predicted'] = fit.predict(df_res)
        if not self.allow_negative_predictions:
            df_res.loc[df_res['predicted'] < 0, 'predicted'] = 0

        prstd, interval_l, interval_u = wls_prediction_std(fit,
                                                           df_res[fit.model.exog_names],
                                                           alpha=1 - self.confint)
        df_res['interval_l'] = interval_l
        df_res['interval_u'] = interval_u

        if 'Intercept' in df_res:
            df_res.drop(labels=['Intercept'], axis=1, inplace=True)

        return df_res",python,"def _predict(self, fit, df):
        """"""
        Return a df with predictions and confidence interval

        Notes
        -----
        The df will contain the following columns:
        - 'predicted': the model output
        - 'interval_u', 'interval_l': upper and lower confidence bounds.

        The result will depend on the following attributes of self:
        confint : float (default=0.95)
            Confidence level for two-sided hypothesis
        allow_negative_predictions : bool (default=True)
            If False, correct negative predictions to zero (typically for energy consumption predictions)

        Parameters
        ----------
        fit : Statsmodels fit
        df : pandas DataFrame or None (default)
            If None, use self.df


        Returns
        -------
        df_res : pandas DataFrame
            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'
        """"""

        # Add model results to data as column 'predictions'
        df_res = df.copy()
        if 'Intercept' in fit.model.exog_names:
            df_res['Intercept'] = 1.0
        df_res['predicted'] = fit.predict(df_res)
        if not self.allow_negative_predictions:
            df_res.loc[df_res['predicted'] < 0, 'predicted'] = 0

        prstd, interval_l, interval_u = wls_prediction_std(fit,
                                                           df_res[fit.model.exog_names],
                                                           alpha=1 - self.confint)
        df_res['interval_l'] = interval_l
        df_res['interval_u'] = interval_u

        if 'Intercept' in df_res:
            df_res.drop(labels=['Intercept'], axis=1, inplace=True)

        return df_res",def,_predict,(,self,",",fit,",",df,),:,# Add model results to data as column 'predictions',df_res,=,df,.,copy,(,),if,'Intercept',in,fit,.,model,.,exog_names,:,df_res,[,'Intercept',],=,1.0,df_res,[,'predicted',],=,fit,.,predict,(,df_res,"Return a df with predictions and confidence interval

        Notes
        -----
        The df will contain the following columns:
        - 'predicted': the model output
        - 'interval_u', 'interval_l': upper and lower confidence bounds.

        The result will depend on the following attributes of self:
        confint : float (default=0.95)
            Confidence level for two-sided hypothesis
        allow_negative_predictions : bool (default=True)
            If False, correct negative predictions to zero (typically for energy consumption predictions)

        Parameters
        ----------
        fit : Statsmodels fit
        df : pandas DataFrame or None (default)
            If None, use self.df


        Returns
        -------
        df_res : pandas DataFrame
            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'",Return,a,df,with,predictions,and,confidence,interval,,,,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/regression.py#L292-L338,train,),if,not,self,.,allow_negative_predictions,:,df_res,.,loc,[,df_res,[,'predicted',],<,0,",",'predicted',],=,0,prstd,",",interval_l,",",interval_u,=,wls_prediction_std,(,,,,,,,,,,,,,,,,,,,,fit,",",df_res,[,fit,.,model,.,exog_names,],",",alpha,=,1,-,self,.,confint,),df_res,[,'interval_l',],=,interval_l,df_res,[,'interval_u',],=,interval_u,if,'Intercept',in,df_res,:,df_res,.,drop,(,labels,=,[,'Intercept',],",",axis,=,1,",",inplace,=,True,),return,df_res,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/biom_calc.py,relative_abundance,"def relative_abundance(biomf, sampleIDs=None):
    """"""
    Calculate the relative abundance of each OTUID in a Sample.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :rtype: dict
    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on
             OTUID's and their values represent the relative abundance of that OTUID in
             that SampleID.
    """"""
    if sampleIDs is None:
        sampleIDs = biomf.ids()
    else:
        try:
            for sid in sampleIDs:
                assert sid in biomf.ids()
        except AssertionError:
            raise ValueError(
                ""\nError while calculating relative abundances: The sampleIDs provided do""
                "" not match the sampleIDs in biom file. Please double check the sampleIDs""
                "" provided.\n"")
    otuIDs = biomf.ids(axis=""observation"")
    norm_biomf = biomf.norm(inplace=False)

    return {sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample)
                     for otuID in otuIDs} for sample in sampleIDs}",python,"def relative_abundance(biomf, sampleIDs=None):
    """"""
    Calculate the relative abundance of each OTUID in a Sample.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :rtype: dict
    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on
             OTUID's and their values represent the relative abundance of that OTUID in
             that SampleID.
    """"""
    if sampleIDs is None:
        sampleIDs = biomf.ids()
    else:
        try:
            for sid in sampleIDs:
                assert sid in biomf.ids()
        except AssertionError:
            raise ValueError(
                ""\nError while calculating relative abundances: The sampleIDs provided do""
                "" not match the sampleIDs in biom file. Please double check the sampleIDs""
                "" provided.\n"")
    otuIDs = biomf.ids(axis=""observation"")
    norm_biomf = biomf.norm(inplace=False)

    return {sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample)
                     for otuID in otuIDs} for sample in sampleIDs}",def,relative_abundance,(,biomf,",",sampleIDs,=,None,),:,if,sampleIDs,is,None,:,sampleIDs,=,biomf,.,ids,(,),else,:,try,:,for,sid,in,sampleIDs,:,assert,sid,in,biomf,.,ids,(,),except,AssertionError,:,raise,"Calculate the relative abundance of each OTUID in a Sample.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :rtype: dict
    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on
             OTUID's and their values represent the relative abundance of that OTUID in
             that SampleID.",Calculate,the,relative,abundance,of,each,OTUID,in,a,Sample,.,,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L11-L41,train,ValueError,(,"""\nError while calculating relative abundances: The sampleIDs provided do""",""" not match the sampleIDs in biom file. Please double check the sampleIDs""",""" provided.\n""",),otuIDs,=,biomf,.,ids,(,axis,=,"""observation""",),norm_biomf,=,biomf,.,norm,(,inplace,=,False,),return,{,sample,:,,,,,,,,,,,,,,,,,,,,{,otuID,:,norm_biomf,.,get_value_by_ids,(,otuID,",",sample,),for,otuID,in,otuIDs,},for,sample,in,sampleIDs,},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/biom_calc.py,mean_otu_pct_abundance,"def mean_otu_pct_abundance(ra, otuIDs):
    """"""
    Calculate the mean OTU abundance percentage.

    :type ra: Dict
    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are
               dictionaries keyed on OTUID's and their values represent the relative
               abundance of that OTUID in that SampleID. 'ra' is the output of
               relative_abundance() function.

    :type otuIDs: List
    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be
                   measured.

    :rtype: dict
    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.
    """"""
    sids = ra.keys()
    otumeans = defaultdict(int)

    for oid in otuIDs:
        otumeans[oid] = sum([ra[sid][oid] for sid in sids
                             if oid in ra[sid]]) / len(sids) * 100
    return otumeans",python,"def mean_otu_pct_abundance(ra, otuIDs):
    """"""
    Calculate the mean OTU abundance percentage.

    :type ra: Dict
    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are
               dictionaries keyed on OTUID's and their values represent the relative
               abundance of that OTUID in that SampleID. 'ra' is the output of
               relative_abundance() function.

    :type otuIDs: List
    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be
                   measured.

    :rtype: dict
    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.
    """"""
    sids = ra.keys()
    otumeans = defaultdict(int)

    for oid in otuIDs:
        otumeans[oid] = sum([ra[sid][oid] for sid in sids
                             if oid in ra[sid]]) / len(sids) * 100
    return otumeans",def,mean_otu_pct_abundance,(,ra,",",otuIDs,),:,sids,=,ra,.,keys,(,),otumeans,=,defaultdict,(,int,),for,oid,in,otuIDs,:,otumeans,[,oid,],=,sum,(,[,ra,[,sid,],[,oid,],for,sid,"Calculate the mean OTU abundance percentage.

    :type ra: Dict
    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are
               dictionaries keyed on OTUID's and their values represent the relative
               abundance of that OTUID in that SampleID. 'ra' is the output of
               relative_abundance() function.

    :type otuIDs: List
    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be
                   measured.

    :rtype: dict
    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.",Calculate,the,mean,OTU,abundance,percentage,.,,,,,,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L44-L67,train,in,sids,if,oid,in,ra,[,sid,],],),/,len,(,sids,),*,100,return,otumeans,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/biom_calc.py,MRA,"def MRA(biomf, sampleIDs=None, transform=None):
    """"""
    Calculate the mean relative abundance percentage.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :param transform: Mathematical function which is used to transform smax to another
                      format. By default, the function has been set to None.

    :rtype: dict
    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given
             number of sampleIDs.
    """"""
    ra = relative_abundance(biomf, sampleIDs)
    if transform is not None:
        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}
              for sample in ra.keys()}
    otuIDs = biomf.ids(axis=""observation"")
    return mean_otu_pct_abundance(ra, otuIDs)",python,"def MRA(biomf, sampleIDs=None, transform=None):
    """"""
    Calculate the mean relative abundance percentage.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :param transform: Mathematical function which is used to transform smax to another
                      format. By default, the function has been set to None.

    :rtype: dict
    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given
             number of sampleIDs.
    """"""
    ra = relative_abundance(biomf, sampleIDs)
    if transform is not None:
        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}
              for sample in ra.keys()}
    otuIDs = biomf.ids(axis=""observation"")
    return mean_otu_pct_abundance(ra, otuIDs)",def,MRA,(,biomf,",",sampleIDs,=,None,",",transform,=,None,),:,ra,=,relative_abundance,(,biomf,",",sampleIDs,),if,transform,is,not,None,:,ra,=,{,sample,:,{,otuID,:,transform,(,abd,),for,otuID,",","Calculate the mean relative abundance percentage.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :param transform: Mathematical function which is used to transform smax to another
                      format. By default, the function has been set to None.

    :rtype: dict
    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given
             number of sampleIDs.",Calculate,the,mean,relative,abundance,percentage,.,,,,,,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L70-L92,train,abd,in,ra,[,sample,],.,items,(,),},for,sample,in,ra,.,keys,(,),},otuIDs,=,biomf,.,ids,(,axis,=,"""observation""",),,,,,,,,,,,,,,,,,,,,return,mean_otu_pct_abundance,(,ra,",",otuIDs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/biom_calc.py,raw_abundance,"def raw_abundance(biomf, sampleIDs=None, sample_abd=True):
    """"""
    Calculate the total number of sequences in each OTU or SampleID.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: List
    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the
                      list has been set to None.

    :type sample_abd: Boolean
    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By
                       default, the output will be provided for SampleID's.

    :rtype: dict
    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their
             respective abundance as values.
    """"""
    results = defaultdict(int)
    if sampleIDs is None:
        sampleIDs = biomf.ids()
    else:
        try:
            for sid in sampleIDs:
                assert sid in biomf.ids()
        except AssertionError:
            raise ValueError(
                ""\nError while calculating raw total abundances: The sampleIDs provided ""
                ""do not match the sampleIDs in biom file. Please double check the ""
                ""sampleIDs provided.\n"")
    otuIDs = biomf.ids(axis=""observation"")

    for sampleID in sampleIDs:
        for otuID in otuIDs:
            abd = biomf.get_value_by_ids(otuID, sampleID)
            if sample_abd:
                results[sampleID] += abd
            else:
                results[otuID] += abd
    return results",python,"def raw_abundance(biomf, sampleIDs=None, sample_abd=True):
    """"""
    Calculate the total number of sequences in each OTU or SampleID.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: List
    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the
                      list has been set to None.

    :type sample_abd: Boolean
    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By
                       default, the output will be provided for SampleID's.

    :rtype: dict
    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their
             respective abundance as values.
    """"""
    results = defaultdict(int)
    if sampleIDs is None:
        sampleIDs = biomf.ids()
    else:
        try:
            for sid in sampleIDs:
                assert sid in biomf.ids()
        except AssertionError:
            raise ValueError(
                ""\nError while calculating raw total abundances: The sampleIDs provided ""
                ""do not match the sampleIDs in biom file. Please double check the ""
                ""sampleIDs provided.\n"")
    otuIDs = biomf.ids(axis=""observation"")

    for sampleID in sampleIDs:
        for otuID in otuIDs:
            abd = biomf.get_value_by_ids(otuID, sampleID)
            if sample_abd:
                results[sampleID] += abd
            else:
                results[otuID] += abd
    return results",def,raw_abundance,(,biomf,",",sampleIDs,=,None,",",sample_abd,=,True,),:,results,=,defaultdict,(,int,),if,sampleIDs,is,None,:,sampleIDs,=,biomf,.,ids,(,),else,:,try,:,for,sid,in,sampleIDs,:,assert,sid,"Calculate the total number of sequences in each OTU or SampleID.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: List
    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the
                      list has been set to None.

    :type sample_abd: Boolean
    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By
                       default, the output will be provided for SampleID's.

    :rtype: dict
    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their
             respective abundance as values.",Calculate,the,total,number,of,sequences,in,each,OTU,or,SampleID,.,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L95-L135,train,in,biomf,.,ids,(,),except,AssertionError,:,raise,ValueError,(,"""\nError while calculating raw total abundances: The sampleIDs provided ""","""do not match the sampleIDs in biom file. Please double check the ""","""sampleIDs provided.\n""",),otuIDs,=,biomf,.,ids,(,axis,=,"""observation""",),for,sampleID,in,sampleIDs,,,,,,,,,,,,,,,,,,,,:,for,otuID,in,otuIDs,:,abd,=,biomf,.,get_value_by_ids,(,otuID,",",sampleID,),if,sample_abd,:,results,[,sampleID,],+=,abd,else,:,results,[,otuID,],+=,abd,return,results,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,phylotoast/biom_calc.py,transform_raw_abundance,"def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):
    """"""
    Function to transform the total abundance calculation for each sample ID to another
    format based on user given transformation function.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :param fn: Mathematical function which is used to transform smax to another format.
               By default, the function has been given as base 10 logarithm.

    :rtype: dict
    :return: Returns a dictionary similar to output of raw_abundance function but with
             the abundance values modified by the mathematical operation. By default, the
             operation performed on the abundances is base 10 logarithm.
    """"""
    totals = raw_abundance(biomf, sampleIDs, sample_abd)
    return {sid: fn(abd) for sid, abd in totals.items()}",python,"def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):
    """"""
    Function to transform the total abundance calculation for each sample ID to another
    format based on user given transformation function.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :param fn: Mathematical function which is used to transform smax to another format.
               By default, the function has been given as base 10 logarithm.

    :rtype: dict
    :return: Returns a dictionary similar to output of raw_abundance function but with
             the abundance values modified by the mathematical operation. By default, the
             operation performed on the abundances is base 10 logarithm.
    """"""
    totals = raw_abundance(biomf, sampleIDs, sample_abd)
    return {sid: fn(abd) for sid, abd in totals.items()}",def,transform_raw_abundance,(,biomf,",",fn,=,math,.,log10,",",sampleIDs,=,None,",",sample_abd,=,True,),:,totals,=,raw_abundance,(,biomf,",",sampleIDs,",",sample_abd,),return,{,sid,:,fn,(,abd,),for,sid,",",abd,in,"Function to transform the total abundance calculation for each sample ID to another
    format based on user given transformation function.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :param fn: Mathematical function which is used to transform smax to another format.
               By default, the function has been given as base 10 logarithm.

    :rtype: dict
    :return: Returns a dictionary similar to output of raw_abundance function but with
             the abundance values modified by the mathematical operation. By default, the
             operation performed on the abundances is base 10 logarithm.",Function,to,transform,the,total,abundance,calculation,for,each,sample,ID,to,another,format,based,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L138-L155,train,totals,.,items,(,),},,,,,,,,,,,,,,,,,,,,,,,,,on,user,given,transformation,function,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/diversity.py,print_MannWhitneyU,"def print_MannWhitneyU(div_calc):
    """"""
    Compute the Mann-Whitney U test for unequal group sample sizes.
    """"""
    try:
        x = div_calc.values()[0].values()
        y = div_calc.values()[1].values()
    except:
        return ""Error setting up input arrays for Mann-Whitney U Test. Skipping ""\
               ""significance testing.""
    T, p = stats.mannwhitneyu(x, y)
    print ""\nMann-Whitney U test statistic:"", T
    print ""Two-tailed p-value: {}"".format(2 * p)",python,"def print_MannWhitneyU(div_calc):
    """"""
    Compute the Mann-Whitney U test for unequal group sample sizes.
    """"""
    try:
        x = div_calc.values()[0].values()
        y = div_calc.values()[1].values()
    except:
        return ""Error setting up input arrays for Mann-Whitney U Test. Skipping ""\
               ""significance testing.""
    T, p = stats.mannwhitneyu(x, y)
    print ""\nMann-Whitney U test statistic:"", T
    print ""Two-tailed p-value: {}"".format(2 * p)",def,print_MannWhitneyU,(,div_calc,),:,try,:,x,=,div_calc,.,values,(,),[,0,],.,values,(,),y,=,div_calc,.,values,(,),[,1,],.,values,(,),except,:,return,"""Error setting up input arrays for Mann-Whitney U Test. Skipping ""","""significance testing.""",T,",",Compute the Mann-Whitney U test for unequal group sample sizes.,Compute,the,Mann,-,Whitney,U,test,for,unequal,group,sample,sizes,.,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L54-L66,train,p,=,stats,.,mannwhitneyu,(,x,",",y,),print,"""\nMann-Whitney U test statistic:""",",",T,print,"""Two-tailed p-value: {}""",.,format,(,2,*,p,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/diversity.py,print_KruskalWallisH,"def print_KruskalWallisH(div_calc):
    """"""
    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that
    each group must have at least 5 measurements.
    """"""
    calc = defaultdict(list)
    try:
        for k1, v1 in div_calc.iteritems():
            for k2, v2 in v1.iteritems():
                calc[k1].append(v2)
    except:
        return ""Error setting up input arrays for Kruskal-Wallis H-Test. Skipping ""\
               ""significance testing.""
    h, p = stats.kruskal(*calc.values())
    print ""\nKruskal-Wallis H-test statistic for {} groups: {}"".format(str(len(div_calc)), h)
    print ""p-value: {}"".format(p)",python,"def print_KruskalWallisH(div_calc):
    """"""
    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that
    each group must have at least 5 measurements.
    """"""
    calc = defaultdict(list)
    try:
        for k1, v1 in div_calc.iteritems():
            for k2, v2 in v1.iteritems():
                calc[k1].append(v2)
    except:
        return ""Error setting up input arrays for Kruskal-Wallis H-Test. Skipping ""\
               ""significance testing.""
    h, p = stats.kruskal(*calc.values())
    print ""\nKruskal-Wallis H-test statistic for {} groups: {}"".format(str(len(div_calc)), h)
    print ""p-value: {}"".format(p)",def,print_KruskalWallisH,(,div_calc,),:,calc,=,defaultdict,(,list,),try,:,for,k1,",",v1,in,div_calc,.,iteritems,(,),:,for,k2,",",v2,in,v1,.,iteritems,(,),:,calc,[,k1,],.,append,(,"Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that
    each group must have at least 5 measurements.",Compute,the,Kruskal,-,Wallis,H,-,test,for,independent,samples,.,A,typical,rule,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L69-L84,train,v2,),except,:,return,"""Error setting up input arrays for Kruskal-Wallis H-Test. Skipping ""","""significance testing.""",h,",",p,=,stats,.,kruskal,(,*,calc,.,values,(,),),print,"""\nKruskal-Wallis H-test statistic for {} groups: {}""",.,format,(,str,(,len,is,that,each,group,must,have,at,least,5,measurements,.,,,,,,,,,(,div_calc,),),",",h,),print,"""p-value: {}""",.,format,(,p,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/diversity.py,handle_program_options,"def handle_program_options():
    """"""Parses the given options passed in at the command line.""""""
    parser = argparse.ArgumentParser(description=""Calculate the alpha diversity\
                                     of a set of samples using one or more \
                                     metrics and output a kernal density \
                                     estimator-smoothed histogram of the \
                                     results."")
    parser.add_argument(""-m"", ""--map_file"",
                        help=""QIIME mapping file."")
    parser.add_argument(""-i"", ""--biom_fp"",
                        help=""Path to the BIOM table"")
    parser.add_argument(""-c"", ""--category"",
                        help=""Specific category from the mapping file."")
    parser.add_argument(""-d"", ""--diversity"", default=[""shannon""], nargs=""+"",
                        help=""The alpha diversity metric. Default \
                             value is 'shannon', which will calculate the Shannon\
                             entropy. Multiple metrics can be specified (space separated).\
                             The full list of metrics is available at:\
                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\
                             Beta diversity metrics will be supported in the future."")
    parser.add_argument(""--x_label"", default=[None], nargs=""+"",
                        help=""The name of the diversity metric to be displayed on the\
                        plot as the X-axis label. If multiple metrics are specified,\
                        then multiple entries for the X-axis label should be given."")
    parser.add_argument(""--color_by"",
                        help=""A column name in the mapping file containing\
                              hexadecimal (#FF0000) color values that will\
                              be used to color the groups. Each sample ID must\
                              have a color entry."")
    parser.add_argument(""--plot_title"", default="""",
                        help=""A descriptive title that will appear at the top \
                        of the output plot. Surround with quotes if there are\
                        spaces in the title."")
    parser.add_argument(""-o"", ""--output_dir"", default=""."",
                        help=""The directory plots will be saved to."")
    parser.add_argument(""--image_type"", default=""png"",
                        help=""The type of image to save: png, svg, pdf, eps, etc..."")
    parser.add_argument(""--save_calculations"",
                        help=""Path and name of text file to store the calculated ""
                        ""diversity metrics."")
    parser.add_argument(""--suppress_stats"", action=""store_true"", help=""Do not display ""
                        ""significance testing results which are shown by default."")
    parser.add_argument(""--show_available_metrics"", action=""store_true"",
                        help=""Supply this parameter to see which alpha diversity metrics ""
                             "" are available for usage. No calculations will be performed""
                             "" if this parameter is provided."")
    return parser.parse_args()",python,"def handle_program_options():
    """"""Parses the given options passed in at the command line.""""""
    parser = argparse.ArgumentParser(description=""Calculate the alpha diversity\
                                     of a set of samples using one or more \
                                     metrics and output a kernal density \
                                     estimator-smoothed histogram of the \
                                     results."")
    parser.add_argument(""-m"", ""--map_file"",
                        help=""QIIME mapping file."")
    parser.add_argument(""-i"", ""--biom_fp"",
                        help=""Path to the BIOM table"")
    parser.add_argument(""-c"", ""--category"",
                        help=""Specific category from the mapping file."")
    parser.add_argument(""-d"", ""--diversity"", default=[""shannon""], nargs=""+"",
                        help=""The alpha diversity metric. Default \
                             value is 'shannon', which will calculate the Shannon\
                             entropy. Multiple metrics can be specified (space separated).\
                             The full list of metrics is available at:\
                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\
                             Beta diversity metrics will be supported in the future."")
    parser.add_argument(""--x_label"", default=[None], nargs=""+"",
                        help=""The name of the diversity metric to be displayed on the\
                        plot as the X-axis label. If multiple metrics are specified,\
                        then multiple entries for the X-axis label should be given."")
    parser.add_argument(""--color_by"",
                        help=""A column name in the mapping file containing\
                              hexadecimal (#FF0000) color values that will\
                              be used to color the groups. Each sample ID must\
                              have a color entry."")
    parser.add_argument(""--plot_title"", default="""",
                        help=""A descriptive title that will appear at the top \
                        of the output plot. Surround with quotes if there are\
                        spaces in the title."")
    parser.add_argument(""-o"", ""--output_dir"", default=""."",
                        help=""The directory plots will be saved to."")
    parser.add_argument(""--image_type"", default=""png"",
                        help=""The type of image to save: png, svg, pdf, eps, etc..."")
    parser.add_argument(""--save_calculations"",
                        help=""Path and name of text file to store the calculated ""
                        ""diversity metrics."")
    parser.add_argument(""--suppress_stats"", action=""store_true"", help=""Do not display ""
                        ""significance testing results which are shown by default."")
    parser.add_argument(""--show_available_metrics"", action=""store_true"",
                        help=""Supply this parameter to see which alpha diversity metrics ""
                             "" are available for usage. No calculations will be performed""
                             "" if this parameter is provided."")
    return parser.parse_args()",def,handle_program_options,(,),:,parser,=,argparse,.,ArgumentParser,(,description,=,"""Calculate the alpha diversity\
                                     of a set of samples using one or more \
                                     metrics and output a kernal density \
                                     estimator-smoothed histogram of the \
                                     results.""",),parser,.,add_argument,(,"""-m""",",","""--map_file""",",",help,=,"""QIIME mapping file.""",),parser,.,add_argument,(,"""-i""",",","""--biom_fp""",",",help,=,"""Path to the BIOM table""",),parser,.,add_argument,(,Parses the given options passed in at the command line.,Parses,the,given,options,passed,in,at,the,command,line,.,,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L122-L168,train,"""-c""",",","""--category""",",",help,=,"""Specific category from the mapping file.""",),parser,.,add_argument,(,"""-d""",",","""--diversity""",",",default,=,[,"""shannon""",],",",nargs,=,"""+""",",",help,=,"""The alpha diversity metric. Default \
                             value is 'shannon', which will calculate the Shannon\
                             entropy. Multiple metrics can be specified (space separated).\
                             The full list of metrics is available at:\
                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\
                             Beta diversity metrics will be supported in the future.""",),,,,,,,,,,,,,,,,,,,,parser,.,add_argument,(,"""--x_label""",",",default,=,[,None,],",",nargs,=,"""+""",",",help,=,"""The name of the diversity metric to be displayed on the\
                        plot as the X-axis label. If multiple metrics are specified,\
                        then multiple entries for the X-axis label should be given.""",),parser,.,add_argument,(,"""--color_by""",",",help,=,"""A column name in the mapping file containing\
                              hexadecimal (#FF0000) color values that will\
                              be used to color the groups. Each sample ID must\
                              have a color entry.""",),parser,.,add_argument,(,"""--plot_title""",",",default,=,"""""",",",help,=,"""A descriptive title that will appear at the top \
                        of the output plot. Surround with quotes if there are\
                        spaces in the title.""",),parser,.,add_argument,(,"""-o""",",","""--output_dir""",",",default,=,""".""",",",help,=,"""The directory plots will be saved to.""",),parser,.,add_argument,(,"""--image_type""",",",default,=,"""png""",",",help,=,"""The type of image to save: png, svg, pdf, eps, etc...""",),parser,.,add_argument,(,"""--save_calculations""",",",help,=,"""Path and name of text file to store the calculated ""","""diversity metrics.""",),parser,.,add_argument,(,"""--suppress_stats""",",",action,=,"""store_true""",",",help,=,"""Do not display ""","""significance testing results which are shown by default.""",),parser,.,add_argument,(,"""--show_available_metrics""",",",action,=,"""store_true""",",",help,=,"""Supply this parameter to see which alpha diversity metrics """,""" are available for usage. No calculations will be performed""",""" if this parameter is provided.""",),return,parser,.,parse_args,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/search.py,blastdb,"def blastdb(fasta, maxfile = 10000000):
    """"""
    make blast db
    """"""
    db = fasta.rsplit('.', 1)[0]
    type = check_type(fasta)
    if type == 'nucl':
        type = ['nhr', type]
    else:
        type = ['phr', type]
    if os.path.exists('%s.%s' % (db, type[0])) is False \
            and os.path.exists('%s.00.%s' % (db, type[0])) is False:
        print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)
        os.system('makeblastdb \
                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \
                % (fasta, db, type[1], maxfile))
    else:
        print('# ... database found for: %s' % (fasta), file=sys.stderr)
    return db",python,"def blastdb(fasta, maxfile = 10000000):
    """"""
    make blast db
    """"""
    db = fasta.rsplit('.', 1)[0]
    type = check_type(fasta)
    if type == 'nucl':
        type = ['nhr', type]
    else:
        type = ['phr', type]
    if os.path.exists('%s.%s' % (db, type[0])) is False \
            and os.path.exists('%s.00.%s' % (db, type[0])) is False:
        print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)
        os.system('makeblastdb \
                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \
                % (fasta, db, type[1], maxfile))
    else:
        print('# ... database found for: %s' % (fasta), file=sys.stderr)
    return db",def,blastdb,(,fasta,",",maxfile,=,10000000,),:,db,=,fasta,.,rsplit,(,'.',",",1,),[,0,],type,=,check_type,(,fasta,),if,type,==,'nucl',:,type,=,[,'nhr',",",type,],else,:,make blast db,make,blast,db,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/search.py#L28-L46,train,type,=,[,'phr',",",type,],if,os,.,path,.,exists,(,'%s.%s',%,(,db,",",type,[,0,],),),is,False,and,os,.,,,,,,,,,,,,,,,,,,,,path,.,exists,(,'%s.00.%s',%,(,db,",",type,[,0,],),),is,False,:,print,(,'# ... making blastdb for: %s',%,(,fasta,),",",file,=,sys,.,stderr,),os,.,system,(,"'makeblastdb \
                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt'",%,(,fasta,",",db,",",type,[,1,],",",maxfile,),),else,:,print,(,'# ... database found for: %s',%,(,fasta,),",",file,=,sys,.,stderr,),return,db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/search.py,usearchdb,"def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):
    """"""
    make usearch db
    """"""
    if '.udb' in fasta:
        print('# ... database found: %s' % (fasta), file=sys.stderr)
        return fasta
    type = check_type(fasta)
    db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)
    if os.path.exists(db) is False:
        print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)
        if alignment == 'local':
            os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))
        elif alignment == 'global':
            os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))
    else:
        print('# ... database found for: %s' % (fasta), file=sys.stderr)
    return db",python,"def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):
    """"""
    make usearch db
    """"""
    if '.udb' in fasta:
        print('# ... database found: %s' % (fasta), file=sys.stderr)
        return fasta
    type = check_type(fasta)
    db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)
    if os.path.exists(db) is False:
        print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)
        if alignment == 'local':
            os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))
        elif alignment == 'global':
            os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))
    else:
        print('# ... database found for: %s' % (fasta), file=sys.stderr)
    return db",def,usearchdb,(,fasta,",",alignment,=,'local',",",usearch_loc,=,'usearch',),:,if,'.udb',in,fasta,:,print,(,'# ... database found: %s',%,(,fasta,),",",file,=,sys,.,stderr,),return,fasta,type,=,check_type,(,fasta,),db,=,make usearch db,make,usearch,db,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/search.py#L68-L85,train,'%s.%s.udb',%,(,fasta,.,rsplit,(,'.',",",1,),[,0,],",",type,),if,os,.,path,.,exists,(,db,),is,False,:,print,,,,,,,,,,,,,,,,,,,,(,'# ... making usearch db for: %s',%,(,fasta,),",",file,=,sys,.,stderr,),if,alignment,==,'local',:,os,.,system,(,'%s -makeudb_ublast %s -output %s >> log.txt',%,(,usearch_loc,",",fasta,",",db,),),elif,alignment,==,'global',:,os,.,system,(,'%s -makeudb_usearch %s -output %s >> log.txt',%,(,usearch_loc,",",fasta,",",db,),),else,:,print,(,'# ... database found for: %s',%,(,fasta,),",",file,=,sys,.,stderr,),return,db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/control.py,_pp,"def _pp(dict_data):
    """"""Pretty print.""""""
    for key, val in dict_data.items():
        # pylint: disable=superfluous-parens
        print('{0:<11}: {1}'.format(key, val))",python,"def _pp(dict_data):
    """"""Pretty print.""""""
    for key, val in dict_data.items():
        # pylint: disable=superfluous-parens
        print('{0:<11}: {1}'.format(key, val))",def,_pp,(,dict_data,),:,for,key,",",val,in,dict_data,.,items,(,),:,# pylint: disable=superfluous-parens,print,(,'{0:<11}: {1}',.,format,(,key,",",val,),),,,,,,,,,,,,,,,Pretty print.,Pretty,print,.,,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L11-L15,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/control.py,print_licences,"def print_licences(params, metadata):
    """"""Print licenses.

    :param argparse.Namespace params: parameter
    :param bootstrap_py.classifier.Classifiers metadata: package metadata
    """"""
    if hasattr(params, 'licenses'):
        if params.licenses:
            _pp(metadata.licenses_desc())
        sys.exit(0)",python,"def print_licences(params, metadata):
    """"""Print licenses.

    :param argparse.Namespace params: parameter
    :param bootstrap_py.classifier.Classifiers metadata: package metadata
    """"""
    if hasattr(params, 'licenses'):
        if params.licenses:
            _pp(metadata.licenses_desc())
        sys.exit(0)",def,print_licences,(,params,",",metadata,),:,if,hasattr,(,params,",",'licenses',),:,if,params,.,licenses,:,_pp,(,metadata,.,licenses_desc,(,),),sys,.,exit,(,0,),,,,,,,,,"Print licenses.

    :param argparse.Namespace params: parameter
    :param bootstrap_py.classifier.Classifiers metadata: package metadata",Print,licenses,.,,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L27-L36,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/control.py,check_repository_existence,"def check_repository_existence(params):
    """"""Check repository existence.

    :param argparse.Namespace params: parameters
    """"""
    repodir = os.path.join(params.outdir, params.name)
    if os.path.isdir(repodir):
        raise Conflict(
            'Package repository ""{0}"" has already exists.'.format(repodir))",python,"def check_repository_existence(params):
    """"""Check repository existence.

    :param argparse.Namespace params: parameters
    """"""
    repodir = os.path.join(params.outdir, params.name)
    if os.path.isdir(repodir):
        raise Conflict(
            'Package repository ""{0}"" has already exists.'.format(repodir))",def,check_repository_existence,(,params,),:,repodir,=,os,.,path,.,join,(,params,.,outdir,",",params,.,name,),if,os,.,path,.,isdir,(,repodir,),:,raise,Conflict,(,"'Package repository ""{0}"" has already exists.'",.,format,(,repodir,),),,"Check repository existence.

    :param argparse.Namespace params: parameters",Check,repository,existence,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L39-L47,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/control.py,generate_package,"def generate_package(params):
    """"""Generate package repository.

    :param argparse.Namespace params: parameters
    """"""
    pkg_data = package.PackageData(params)
    pkg_tree = package.PackageTree(pkg_data)
    pkg_tree.generate()
    pkg_tree.move()
    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)",python,"def generate_package(params):
    """"""Generate package repository.

    :param argparse.Namespace params: parameters
    """"""
    pkg_data = package.PackageData(params)
    pkg_tree = package.PackageTree(pkg_data)
    pkg_tree.generate()
    pkg_tree.move()
    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)",def,generate_package,(,params,),:,pkg_data,=,package,.,PackageData,(,params,),pkg_tree,=,package,.,PackageTree,(,pkg_data,),pkg_tree,.,generate,(,),pkg_tree,.,move,(,),VCS,(,os,.,path,.,join,(,pkg_tree,.,outdir,"Generate package repository.

    :param argparse.Namespace params: parameters",Generate,package,repository,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L59-L68,train,",",pkg_tree,.,name,),",",pkg_tree,.,pkg_data,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/sam2fastq.py,print_single,"def print_single(line, rev):
    """"""
    print single reads to stderr
    """"""
    if rev is True:
        seq = rc(['', line[9]])[1]
        qual = line[10][::-1]
    else:
        seq = line[9]
        qual = line[10]
    fq = ['@%s' % line[0], seq, '+%s' % line[0], qual]
    print('\n'.join(fq), file = sys.stderr)",python,"def print_single(line, rev):
    """"""
    print single reads to stderr
    """"""
    if rev is True:
        seq = rc(['', line[9]])[1]
        qual = line[10][::-1]
    else:
        seq = line[9]
        qual = line[10]
    fq = ['@%s' % line[0], seq, '+%s' % line[0], qual]
    print('\n'.join(fq), file = sys.stderr)",def,print_single,(,line,",",rev,),:,if,rev,is,True,:,seq,=,rc,(,[,'',",",line,[,9,],],),[,1,],qual,=,line,[,10,],[,:,:,-,1,],else,:,print single reads to stderr,print,single,reads,to,stderr,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/sam2fastq.py#L13-L24,train,seq,=,line,[,9,],qual,=,line,[,10,],fq,=,[,'@%s',%,line,[,0,],",",seq,",",'+%s',%,line,[,0,],,,,,,,,,,,,,,,,,,,,",",qual,],print,(,'\n',.,join,(,fq,),",",file,=,sys,.,stderr,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/sam2fastq.py,sam2fastq,"def sam2fastq(sam, singles = False, force = False):
    """"""
    convert sam to fastq
    """"""
    L, R = None, None
    for line in sam:
        if line.startswith('@') is True:
            continue
        line = line.strip().split()
        bit = [True if i == '1' else False \
                for i in bin(int(line[1])).split('b')[1][::-1]]
        while len(bit) < 8:
            bit.append(False)
        pair, proper, na, nap, rev, mrev, left, right = bit
        # make sure read is paired
        if pair is False:
            if singles is True:
                print_single(line, rev)
            continue
        # check if sequence is reverse-complemented
        if rev is True:
            seq = rc(['', line[9]])[1]
            qual = line[10][::-1]
        else:
            seq = line[9]
            qual = line[10]
        # check if read is forward or reverse, return when both have been found
        if left is True:
            if L is not None and force is False:
                print('sam file is not sorted', file = sys.stderr)
                print('\te.g.: %s' % (line[0]), file = sys.stderr)
                exit()
            if L is not None:
                L = None
                continue
            L = ['@%s' % line[0], seq, '+%s' % line[0], qual]
            if R is not None:
                yield L
                yield R
                L, R = None, None
        if right is True:
            if R is not None and force is False:
                print('sam file is not sorted', file = sys.stderr)
                print('\te.g.: %s' % (line[0]), file = sys.stderr)
                exit()
            if R is not None:
                R = None
                continue
            R = ['@%s' % line[0], seq, '+%s' % line[0], qual]
            if L is not None:
                yield L
                yield R
                L, R = None, None",python,"def sam2fastq(sam, singles = False, force = False):
    """"""
    convert sam to fastq
    """"""
    L, R = None, None
    for line in sam:
        if line.startswith('@') is True:
            continue
        line = line.strip().split()
        bit = [True if i == '1' else False \
                for i in bin(int(line[1])).split('b')[1][::-1]]
        while len(bit) < 8:
            bit.append(False)
        pair, proper, na, nap, rev, mrev, left, right = bit
        # make sure read is paired
        if pair is False:
            if singles is True:
                print_single(line, rev)
            continue
        # check if sequence is reverse-complemented
        if rev is True:
            seq = rc(['', line[9]])[1]
            qual = line[10][::-1]
        else:
            seq = line[9]
            qual = line[10]
        # check if read is forward or reverse, return when both have been found
        if left is True:
            if L is not None and force is False:
                print('sam file is not sorted', file = sys.stderr)
                print('\te.g.: %s' % (line[0]), file = sys.stderr)
                exit()
            if L is not None:
                L = None
                continue
            L = ['@%s' % line[0], seq, '+%s' % line[0], qual]
            if R is not None:
                yield L
                yield R
                L, R = None, None
        if right is True:
            if R is not None and force is False:
                print('sam file is not sorted', file = sys.stderr)
                print('\te.g.: %s' % (line[0]), file = sys.stderr)
                exit()
            if R is not None:
                R = None
                continue
            R = ['@%s' % line[0], seq, '+%s' % line[0], qual]
            if L is not None:
                yield L
                yield R
                L, R = None, None",def,sam2fastq,(,sam,",",singles,=,False,",",force,=,False,),:,L,",",R,=,None,",",None,for,line,in,sam,:,if,line,.,startswith,(,'@',),is,True,:,continue,line,=,line,.,strip,(,convert sam to fastq,convert,sam,to,fastq,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/sam2fastq.py#L26-L78,train,),.,split,(,),bit,=,[,True,if,i,==,'1',else,False,for,i,in,bin,(,int,(,line,[,1,],),),.,split,,,,,,,,,,,,,,,,,,,,(,'b',),[,1,],[,:,:,-,1,],],while,len,(,bit,),<,8,:,bit,.,append,(,False,),pair,",",proper,",",na,",",nap,",",rev,",",mrev,",",left,",",right,=,bit,# make sure read is paired,if,pair,is,False,:,if,singles,is,True,:,print_single,(,line,",",rev,),continue,# check if sequence is reverse-complemented,if,rev,is,True,:,seq,=,rc,(,[,'',",",line,[,9,],],),[,1,],qual,=,line,[,10,],[,:,:,-,1,],else,:,seq,=,line,[,9,],qual,=,line,[,10,],"# check if read is forward or reverse, return when both have been found",if,left,is,True,:,if,L,is,not,None,and,force,is,False,:,print,(,'sam file is not sorted',",",file,=,sys,.,stderr,),print,(,'\te.g.: %s',%,(,line,[,0,],),",",file,=,sys,.,stderr,),exit,(,),if,L,is,not,None,:,L,=,None,continue,L,=,[,'@%s',%,line,[,0,],",",seq,",",'+%s',%,line,[,0,],",",qual,],if,R,is,not,None,:,yield,L,yield,R,L,",",R,=,None,",",None,if,right,is,True,:,if,R,is,not,None,and,force,is,False,:,print,(,'sam file is not sorted',",",file,=,sys,.,stderr,),print,(,'\te.g.: %s',%,(,line,[,0,],),",",file,=,sys,.,stderr,),exit,(,),if,R,is,not,None,:,R,=,None,continue,R,=,[,'@%s',%,line,[,0,],",",seq,",",'+%s',%,line,[,0,],",",qual,],if,L,is,not,None,:,yield,L,yield,R,L,",",R,=,None,",",None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/subset_sam.py,sort_sam,"def sort_sam(sam, sort):
    """"""
    sort sam file
    """"""
    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])
    if sort is True:
        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])
        if sam != '-':
            if os.path.exists(mapping) is False:
                os.system(""\
                    sort -k1 --buffer-size=%sG -T %s -o %s %s\
                    "" % (sbuffer, tempdir, mapping, sam)) 
        else:
            mapping = 'stdin-sam.sorted.sam'
            p = Popen(""sort -k1 --buffer-size=%sG -T %s -o %s"" \
                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True) 
            p.communicate()
        mapping = open(mapping)
    else:
        if sam == '-':
            mapping = sys.stdin
        else:
            mapping = open(sam)
    return mapping",python,"def sort_sam(sam, sort):
    """"""
    sort sam file
    """"""
    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])
    if sort is True:
        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])
        if sam != '-':
            if os.path.exists(mapping) is False:
                os.system(""\
                    sort -k1 --buffer-size=%sG -T %s -o %s %s\
                    "" % (sbuffer, tempdir, mapping, sam)) 
        else:
            mapping = 'stdin-sam.sorted.sam'
            p = Popen(""sort -k1 --buffer-size=%sG -T %s -o %s"" \
                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True) 
            p.communicate()
        mapping = open(mapping)
    else:
        if sam == '-':
            mapping = sys.stdin
        else:
            mapping = open(sam)
    return mapping",def,sort_sam,(,sam,",",sort,),:,tempdir,=,'%s/',%,(,os,.,path,.,abspath,(,sam,),.,rsplit,(,'/',",",1,),[,0,],),if,sort,is,True,:,mapping,=,'%s.sorted.sam',%,(,sam,sort sam file,sort,sam,file,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/subset_sam.py#L14-L37,train,.,rsplit,(,'.',",",1,),[,0,],),if,sam,!=,'-',:,if,os,.,path,.,exists,(,mapping,),is,False,:,os,.,,,,,,,,,,,,,,,,,,,,system,(,"""\
                    sort -k1 --buffer-size=%sG -T %s -o %s %s\
                    """,%,(,sbuffer,",",tempdir,",",mapping,",",sam,),),else,:,mapping,=,'stdin-sam.sorted.sam',p,=,Popen,(,"""sort -k1 --buffer-size=%sG -T %s -o %s""",%,(,sbuffer,",",tempdir,",",mapping,),",",stdin,=,sys,.,stdin,",",shell,=,True,),p,.,communicate,(,),mapping,=,open,(,mapping,),else,:,if,sam,==,'-',:,mapping,=,sys,.,stdin,else,:,mapping,=,open,(,sam,),return,mapping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/subset_sam.py,sub_sam,"def sub_sam(sam, percent, sort = True, sbuffer = False):
    """"""
    randomly subset sam file
    """"""
    mapping = sort_sam(sam, sort)
    pool = [1 for i in range(0, percent)] + [0 for i in range(0, 100 - percent)]
    c = cycle([1, 2])
    for line in mapping:
        line = line.strip().split()
        if line[0].startswith('@'): # get the sam header
            yield line
            continue
        if int(line[1]) <= 20: # is this from a single read?
            if random.choice(pool) == 1:
                yield line
        else:
            n = next(c)
            if n == 1:
                prev = line
            if n == 2 and random.choice(pool) == 1:
                yield prev
                yield line",python,"def sub_sam(sam, percent, sort = True, sbuffer = False):
    """"""
    randomly subset sam file
    """"""
    mapping = sort_sam(sam, sort)
    pool = [1 for i in range(0, percent)] + [0 for i in range(0, 100 - percent)]
    c = cycle([1, 2])
    for line in mapping:
        line = line.strip().split()
        if line[0].startswith('@'): # get the sam header
            yield line
            continue
        if int(line[1]) <= 20: # is this from a single read?
            if random.choice(pool) == 1:
                yield line
        else:
            n = next(c)
            if n == 1:
                prev = line
            if n == 2 and random.choice(pool) == 1:
                yield prev
                yield line",def,sub_sam,(,sam,",",percent,",",sort,=,True,",",sbuffer,=,False,),:,mapping,=,sort_sam,(,sam,",",sort,),pool,=,[,1,for,i,in,range,(,0,",",percent,),],+,[,0,for,i,randomly subset sam file,randomly,subset,sam,file,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/subset_sam.py#L39-L60,train,in,range,(,0,",",100,-,percent,),],c,=,cycle,(,[,1,",",2,],),for,line,in,mapping,:,line,=,line,.,strip,,,,,,,,,,,,,,,,,,,,(,),.,split,(,),if,line,[,0,],.,startswith,(,'@',),:,# get the sam header,yield,line,continue,if,int,(,line,[,1,],),<=,20,:,# is this from a single read?,if,random,.,choice,(,pool,),==,1,:,yield,line,else,:,n,=,next,(,c,),if,n,==,1,:,prev,=,line,if,n,==,2,and,random,.,choice,(,pool,),==,1,:,yield,prev,yield,line,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/fastq2fasta.py,fq2fa,"def fq2fa(fq):
    """"""
    convert fq to fa
    """"""
    c = cycle([1, 2, 3, 4])
    for line in fq:
        n = next(c)
        if n == 1:
            seq = ['>%s' % (line.strip().split('@', 1)[1])]
        if n == 2:
            seq.append(line.strip())
            yield seq",python,"def fq2fa(fq):
    """"""
    convert fq to fa
    """"""
    c = cycle([1, 2, 3, 4])
    for line in fq:
        n = next(c)
        if n == 1:
            seq = ['>%s' % (line.strip().split('@', 1)[1])]
        if n == 2:
            seq.append(line.strip())
            yield seq",def,fq2fa,(,fq,),:,c,=,cycle,(,[,1,",",2,",",3,",",4,],),for,line,in,fq,:,n,=,next,(,c,),if,n,==,1,:,seq,=,[,'>%s',%,(,line,convert fq to fa,convert,fq,to,fa,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/fastq2fasta.py#L11-L22,train,.,strip,(,),.,split,(,'@',",",1,),[,1,],),],if,n,==,2,:,seq,.,append,(,line,.,strip,(,),,,,,,,,,,,,,,,,,,,,),yield,seq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
elbow-jason/Uno-deprecated,uno/decorators.py,change_return_type,"def change_return_type(f):
    """"""
    Converts the returned value of wrapped function to the type of the
    first arg or to the type specified by a kwarg key return_type's value.
    """"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        if kwargs.has_key('return_type'):
            return_type = kwargs['return_type']
            kwargs.pop('return_type')
            return return_type(f(*args, **kwargs))
        elif len(args) > 0:
            return_type = type(args[0])
            return return_type(f(*args, **kwargs))
        else:
            return f(*args, **kwargs)
    return wrapper",python,"def change_return_type(f):
    """"""
    Converts the returned value of wrapped function to the type of the
    first arg or to the type specified by a kwarg key return_type's value.
    """"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        if kwargs.has_key('return_type'):
            return_type = kwargs['return_type']
            kwargs.pop('return_type')
            return return_type(f(*args, **kwargs))
        elif len(args) > 0:
            return_type = type(args[0])
            return return_type(f(*args, **kwargs))
        else:
            return f(*args, **kwargs)
    return wrapper",def,change_return_type,(,f,),:,@,wraps,(,f,),def,wrapper,(,*,args,",",*,*,kwargs,),:,if,kwargs,.,has_key,(,'return_type',),:,return_type,=,kwargs,[,'return_type',],kwargs,.,pop,(,'return_type',),return,"Converts the returned value of wrapped function to the type of the
    first arg or to the type specified by a kwarg key return_type's value.",Converts,the,returned,value,of,wrapped,function,to,the,type,of,the,first,arg,or,4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4,https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/decorators.py#L11-L27,train,return_type,(,f,(,*,args,",",*,*,kwargs,),),elif,len,(,args,),>,0,:,return_type,=,type,(,args,[,0,],),return,to,the,type,specified,by,a,kwarg,key,return_type,s,value,.,,,,,,,,return_type,(,f,(,*,args,",",*,*,kwargs,),),else,:,return,f,(,*,args,",",*,*,kwargs,),return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
elbow-jason/Uno-deprecated,uno/decorators.py,convert_args_to_sets,"def convert_args_to_sets(f):
    """"""
    Converts all args to 'set' type via self.setify function.
    """"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        args = (setify(x) for x in args)
        return f(*args, **kwargs)
    return wrapper",python,"def convert_args_to_sets(f):
    """"""
    Converts all args to 'set' type via self.setify function.
    """"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        args = (setify(x) for x in args)
        return f(*args, **kwargs)
    return wrapper",def,convert_args_to_sets,(,f,),:,@,wraps,(,f,),def,wrapper,(,*,args,",",*,*,kwargs,),:,args,=,(,setify,(,x,),for,x,in,args,),return,f,(,*,args,",",*,*,kwargs,Converts all args to 'set' type via self.setify function.,Converts,all,args,to,set,type,via,self,.,setify,function,.,,,,4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4,https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/decorators.py#L30-L38,train,),return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,KBBI._init_entri,"def _init_entri(self, laman):
        """"""Membuat objek-objek entri dari laman yang diambil.

        :param laman: Laman respons yang dikembalikan oleh KBBI daring.
        :type laman: Response
        """"""

        sup = BeautifulSoup(laman.text, 'html.parser')
        estr = ''
        for label in sup.find('hr').next_siblings:
            if label.name == 'hr':
                self.entri.append(Entri(estr))
                break
            if label.name == 'h2':
                if estr:
                    self.entri.append(Entri(estr))
                estr = ''
            estr += str(label).strip()",python,"def _init_entri(self, laman):
        """"""Membuat objek-objek entri dari laman yang diambil.

        :param laman: Laman respons yang dikembalikan oleh KBBI daring.
        :type laman: Response
        """"""

        sup = BeautifulSoup(laman.text, 'html.parser')
        estr = ''
        for label in sup.find('hr').next_siblings:
            if label.name == 'hr':
                self.entri.append(Entri(estr))
                break
            if label.name == 'h2':
                if estr:
                    self.entri.append(Entri(estr))
                estr = ''
            estr += str(label).strip()",def,_init_entri,(,self,",",laman,),:,sup,=,BeautifulSoup,(,laman,.,text,",",'html.parser',),estr,=,'',for,label,in,sup,.,find,(,'hr',),.,next_siblings,:,if,label,.,name,==,'hr',:,self,.,entri,"Membuat objek-objek entri dari laman yang diambil.

        :param laman: Laman respons yang dikembalikan oleh KBBI daring.
        :type laman: Response",Membuat,objek,-,objek,entri,dari,laman,yang,diambil,.,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L46-L63,train,.,append,(,Entri,(,estr,),),break,if,label,.,name,==,'h2',:,if,estr,:,self,.,entri,.,append,(,Entri,(,estr,),),,,,,,,,,,,,,,,,,,,,estr,=,'',estr,+=,str,(,label,),.,strip,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Entri._init_kata_dasar,"def _init_kata_dasar(self, dasar):
        """"""Memproses kata dasar yang ada dalam nama entri.

        :param dasar: ResultSet untuk label HTML dengan class=""rootword""
        :type dasar: ResultSet
        """"""

        for tiap in dasar:
            kata = tiap.find('a')
            dasar_no = kata.find('sup')
            kata = ambil_teks_dalam_label(kata)
            self.kata_dasar.append(
                kata + ' [{}]'.format(dasar_no.text.strip()) if dasar_no else kata
            )",python,"def _init_kata_dasar(self, dasar):
        """"""Memproses kata dasar yang ada dalam nama entri.

        :param dasar: ResultSet untuk label HTML dengan class=""rootword""
        :type dasar: ResultSet
        """"""

        for tiap in dasar:
            kata = tiap.find('a')
            dasar_no = kata.find('sup')
            kata = ambil_teks_dalam_label(kata)
            self.kata_dasar.append(
                kata + ' [{}]'.format(dasar_no.text.strip()) if dasar_no else kata
            )",def,_init_kata_dasar,(,self,",",dasar,),:,for,tiap,in,dasar,:,kata,=,tiap,.,find,(,'a',),dasar_no,=,kata,.,find,(,'sup',),kata,=,ambil_teks_dalam_label,(,kata,),self,.,kata_dasar,.,append,(,kata,+,"Memproses kata dasar yang ada dalam nama entri.

        :param dasar: ResultSet untuk label HTML dengan class=""rootword""
        :type dasar: ResultSet",Memproses,kata,dasar,yang,ada,dalam,nama,entri,.,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L126-L139,train,' [{}]',.,format,(,dasar_no,.,text,.,strip,(,),),if,dasar_no,else,kata,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Entri.serialisasi,"def serialisasi(self):
        """"""Mengembalikan hasil serialisasi objek Entri ini.

        :returns: Dictionary hasil serialisasi
        :rtype: dict
        """"""

        return {
            ""nama"": self.nama,
            ""nomor"": self.nomor,
            ""kata_dasar"": self.kata_dasar,
            ""pelafalan"": self.pelafalan,
            ""bentuk_tidak_baku"": self.bentuk_tidak_baku,
            ""varian"": self.varian,
            ""makna"": [makna.serialisasi() for makna in self.makna]
        }",python,"def serialisasi(self):
        """"""Mengembalikan hasil serialisasi objek Entri ini.

        :returns: Dictionary hasil serialisasi
        :rtype: dict
        """"""

        return {
            ""nama"": self.nama,
            ""nomor"": self.nomor,
            ""kata_dasar"": self.kata_dasar,
            ""pelafalan"": self.pelafalan,
            ""bentuk_tidak_baku"": self.bentuk_tidak_baku,
            ""varian"": self.varian,
            ""makna"": [makna.serialisasi() for makna in self.makna]
        }",def,serialisasi,(,self,),:,return,{,"""nama""",:,self,.,nama,",","""nomor""",:,self,.,nomor,",","""kata_dasar""",:,self,.,kata_dasar,",","""pelafalan""",:,self,.,pelafalan,",","""bentuk_tidak_baku""",:,self,.,bentuk_tidak_baku,",","""varian""",:,self,.,varian,"Mengembalikan hasil serialisasi objek Entri ini.

        :returns: Dictionary hasil serialisasi
        :rtype: dict",Mengembalikan,hasil,serialisasi,objek,Entri,ini,.,,,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L141-L156,train,",","""makna""",:,[,makna,.,serialisasi,(,),for,makna,in,self,.,makna,],},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Entri._makna,"def _makna(self):
        """"""Mengembalikan representasi string untuk semua makna entri ini.

        :returns: String representasi makna-makna
        :rtype: str
        """"""

        if len(self.makna) > 1:
            return '\n'.join(
                str(i) + "". "" + str(makna)
                for i, makna in enumerate(self.makna, 1)
            )
        return str(self.makna[0])",python,"def _makna(self):
        """"""Mengembalikan representasi string untuk semua makna entri ini.

        :returns: String representasi makna-makna
        :rtype: str
        """"""

        if len(self.makna) > 1:
            return '\n'.join(
                str(i) + "". "" + str(makna)
                for i, makna in enumerate(self.makna, 1)
            )
        return str(self.makna[0])",def,_makna,(,self,),:,if,len,(,self,.,makna,),>,1,:,return,'\n',.,join,(,str,(,i,),+,""". """,+,str,(,makna,),for,i,",",makna,in,enumerate,(,self,.,makna,",","Mengembalikan representasi string untuk semua makna entri ini.

        :returns: String representasi makna-makna
        :rtype: str",Mengembalikan,representasi,string,untuk,semua,makna,entri,ini,.,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L158-L170,train,1,),),return,str,(,self,.,makna,[,0,],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Entri._nama,"def _nama(self):
        """"""Mengembalikan representasi string untuk nama entri ini.

        :returns: String representasi nama entri
        :rtype: str
        """"""

        hasil = self.nama
        if self.nomor:
            hasil += "" [{}]"".format(self.nomor)
        if self.kata_dasar:
            hasil = ""  "".join(self.kata_dasar) + ""  "" + hasil
        return hasil",python,"def _nama(self):
        """"""Mengembalikan representasi string untuk nama entri ini.

        :returns: String representasi nama entri
        :rtype: str
        """"""

        hasil = self.nama
        if self.nomor:
            hasil += "" [{}]"".format(self.nomor)
        if self.kata_dasar:
            hasil = ""  "".join(self.kata_dasar) + ""  "" + hasil
        return hasil",def,_nama,(,self,),:,hasil,=,self,.,nama,if,self,.,nomor,:,hasil,+=,""" [{}]""",.,format,(,self,.,nomor,),if,self,.,kata_dasar,:,hasil,=,"""  "".",j,oin(,s,elf.,k,ata_dasar), , ,"  "" +","Mengembalikan representasi string untuk nama entri ini.

        :returns: String representasi nama entri
        :rtype: str",Mengembalikan,representasi,string,untuk,nama,entri,ini,.,,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L172-L184,train,h,sil,return,hasil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Entri._varian,"def _varian(self, varian):
        """"""Mengembalikan representasi string untuk varian entri ini.
        Dapat digunakan untuk ""Varian"" maupun ""Bentuk tidak baku"".

        :param varian: List bentuk tidak baku atau varian
        :type varian: list
        :returns: String representasi varian atau bentuk tidak baku
        :rtype: str
        """"""

        if varian == self.bentuk_tidak_baku:
            nama = ""Bentuk tidak baku""
        elif varian == self.varian:
            nama = ""Varian""
        else:
            return ''
        return nama + ': ' + ', '.join(varian)",python,"def _varian(self, varian):
        """"""Mengembalikan representasi string untuk varian entri ini.
        Dapat digunakan untuk ""Varian"" maupun ""Bentuk tidak baku"".

        :param varian: List bentuk tidak baku atau varian
        :type varian: list
        :returns: String representasi varian atau bentuk tidak baku
        :rtype: str
        """"""

        if varian == self.bentuk_tidak_baku:
            nama = ""Bentuk tidak baku""
        elif varian == self.varian:
            nama = ""Varian""
        else:
            return ''
        return nama + ': ' + ', '.join(varian)",def,_varian,(,self,",",varian,),:,if,varian,==,self,.,bentuk_tidak_baku,:,nama,=,"""Bentuk tidak baku""",elif,varian,==,self,.,varian,:,nama,=,"""Varian""",else,:,return,'',return,nama,+,': ',+,"', '",.,join,(,varian,),"Mengembalikan representasi string untuk varian entri ini.
        Dapat digunakan untuk ""Varian"" maupun ""Bentuk tidak baku"".

        :param varian: List bentuk tidak baku atau varian
        :type varian: list
        :returns: String representasi varian atau bentuk tidak baku
        :rtype: str",Mengembalikan,representasi,string,untuk,varian,entri,ini,.,Dapat,digunakan,untuk,Varian,maupun,Bentuk,tidak,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L186-L202,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,baku,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Makna._init_kelas,"def _init_kelas(self, makna_label):
        """"""Memproses kelas kata yang ada dalam makna.

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup
        """"""

        kelas = makna_label.find(color='red')
        lain = makna_label.find(color='darkgreen')
        info = makna_label.find(color='green')
        if kelas:
            kelas = kelas.find_all('span')
        if lain:
            self.kelas = {lain.text.strip(): lain['title'].strip()}
            self.submakna = lain.next_sibling.strip()
            self.submakna += ' ' + makna_label.find(color='grey').text.strip()
        else:
            self.kelas = {
                k.text.strip(): k['title'].strip() for k in kelas
            } if kelas else {}
        self.info = info.text.strip() if info else ''",python,"def _init_kelas(self, makna_label):
        """"""Memproses kelas kata yang ada dalam makna.

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup
        """"""

        kelas = makna_label.find(color='red')
        lain = makna_label.find(color='darkgreen')
        info = makna_label.find(color='green')
        if kelas:
            kelas = kelas.find_all('span')
        if lain:
            self.kelas = {lain.text.strip(): lain['title'].strip()}
            self.submakna = lain.next_sibling.strip()
            self.submakna += ' ' + makna_label.find(color='grey').text.strip()
        else:
            self.kelas = {
                k.text.strip(): k['title'].strip() for k in kelas
            } if kelas else {}
        self.info = info.text.strip() if info else ''",def,_init_kelas,(,self,",",makna_label,),:,kelas,=,makna_label,.,find,(,color,=,'red',),lain,=,makna_label,.,find,(,color,=,'darkgreen',),info,=,makna_label,.,find,(,color,=,'green',),if,kelas,:,kelas,=,"Memproses kelas kata yang ada dalam makna.

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup",Memproses,kelas,kata,yang,ada,dalam,makna,.,,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L239-L259,train,kelas,.,find_all,(,'span',),if,lain,:,self,.,kelas,=,{,lain,.,text,.,strip,(,),:,lain,[,'title',],.,strip,(,),,,,,,,,,,,,,,,,,,,,},self,.,submakna,=,lain,.,next_sibling,.,strip,(,),self,.,submakna,+=,' ',+,makna_label,.,find,(,color,=,'grey',),.,text,.,strip,(,),else,:,self,.,kelas,=,{,k,.,text,.,strip,(,),:,k,[,'title',],.,strip,(,),for,k,in,kelas,},if,kelas,else,{,},self,.,info,=,info,.,text,.,strip,(,),if,info,else,'',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Makna._init_contoh,"def _init_contoh(self, makna_label):
        """"""Memproses contoh yang ada dalam makna.

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup
        """"""

        indeks = makna_label.text.find(': ')
        if indeks != -1:
            contoh = makna_label.text[indeks + 2:].strip()
            self.contoh = contoh.split('; ')
        else:
            self.contoh = []",python,"def _init_contoh(self, makna_label):
        """"""Memproses contoh yang ada dalam makna.

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup
        """"""

        indeks = makna_label.text.find(': ')
        if indeks != -1:
            contoh = makna_label.text[indeks + 2:].strip()
            self.contoh = contoh.split('; ')
        else:
            self.contoh = []",def,_init_contoh,(,self,",",makna_label,),:,indeks,=,makna_label,.,text,.,find,(,': ',),if,indeks,!=,-,1,:,contoh,=,makna_label,.,text,[,indeks,+,2,:,],.,strip,(,),self,.,contoh,=,"Memproses contoh yang ada dalam makna.

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup",Memproses,contoh,yang,ada,dalam,makna,.,,,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L261-L273,train,contoh,.,split,(,'; ',),else,:,self,.,contoh,=,[,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
laymonage/kbbi-python,kbbi/kbbi.py,Makna.serialisasi,"def serialisasi(self):
        """"""Mengembalikan hasil serialisasi objek Makna ini.

        :returns: Dictionary hasil serialisasi
        :rtype: dict
        """"""

        return {
            ""kelas"": self.kelas,
            ""submakna"": self.submakna,
            ""info"": self.info,
            ""contoh"": self.contoh
        }",python,"def serialisasi(self):
        """"""Mengembalikan hasil serialisasi objek Makna ini.

        :returns: Dictionary hasil serialisasi
        :rtype: dict
        """"""

        return {
            ""kelas"": self.kelas,
            ""submakna"": self.submakna,
            ""info"": self.info,
            ""contoh"": self.contoh
        }",def,serialisasi,(,self,),:,return,{,"""kelas""",:,self,.,kelas,",","""submakna""",:,self,.,submakna,",","""info""",:,self,.,info,",","""contoh""",:,self,.,contoh,},,,,,,,,,,,,"Mengembalikan hasil serialisasi objek Makna ini.

        :returns: Dictionary hasil serialisasi
        :rtype: dict",Mengembalikan,hasil,serialisasi,objek,Makna,ini,.,,,,,,,,,1a52ba8bcc6dc4c5c1215f9e00207aca264287d6,https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L275-L287,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/docs.py,build_sphinx,"def build_sphinx(pkg_data, projectdir):
    """"""Build sphinx documentation.

    :rtype: int
    :return: subprocess.call return code

    :param `bootstrap_py.control.PackageData` pkg_data: package meta data
    :param str projectdir: project root directory
    """"""
    try:
        version, _minor_version = pkg_data.version.rsplit('.', 1)
    except ValueError:
        version = pkg_data.version
    args = ' '.join(('sphinx-quickstart',
                     '--sep',
                     '-q',
                     '-p ""{name}""',
                     '-a ""{author}""',
                     '-v ""{version}""',
                     '-r ""{release}""',
                     '-l en',
                     '--suffix=.rst',
                     '--master=index',
                     '--ext-autodoc',
                     '--ext-viewcode',
                     '--makefile',
                     '{projectdir}')).format(name=pkg_data.name,
                                             author=pkg_data.author,
                                             version=version,
                                             release=pkg_data.version,
                                             projectdir=projectdir)
    if subprocess.call(shlex.split(args)) == 0:
        _touch_gitkeep(projectdir)",python,"def build_sphinx(pkg_data, projectdir):
    """"""Build sphinx documentation.

    :rtype: int
    :return: subprocess.call return code

    :param `bootstrap_py.control.PackageData` pkg_data: package meta data
    :param str projectdir: project root directory
    """"""
    try:
        version, _minor_version = pkg_data.version.rsplit('.', 1)
    except ValueError:
        version = pkg_data.version
    args = ' '.join(('sphinx-quickstart',
                     '--sep',
                     '-q',
                     '-p ""{name}""',
                     '-a ""{author}""',
                     '-v ""{version}""',
                     '-r ""{release}""',
                     '-l en',
                     '--suffix=.rst',
                     '--master=index',
                     '--ext-autodoc',
                     '--ext-viewcode',
                     '--makefile',
                     '{projectdir}')).format(name=pkg_data.name,
                                             author=pkg_data.author,
                                             version=version,
                                             release=pkg_data.version,
                                             projectdir=projectdir)
    if subprocess.call(shlex.split(args)) == 0:
        _touch_gitkeep(projectdir)",def,build_sphinx,(,pkg_data,",",projectdir,),:,try,:,version,",",_minor_version,=,pkg_data,.,version,.,rsplit,(,'.',",",1,),except,ValueError,:,version,=,pkg_data,.,version,args,=,' ',.,join,(,(,'sphinx-quickstart',",",'--sep',",","Build sphinx documentation.

    :rtype: int
    :return: subprocess.call return code

    :param `bootstrap_py.control.PackageData` pkg_data: package meta data
    :param str projectdir: project root directory",Build,sphinx,documentation,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/docs.py#L8-L40,train,'-q',",","'-p ""{name}""'",",","'-a ""{author}""'",",","'-v ""{version}""'",",","'-r ""{release}""'",",",'-l en',",",'--suffix=.rst',",",'--master=index',",",'--ext-autodoc',",",'--ext-viewcode',",",'--makefile',",",'{projectdir}',),),.,format,(,name,=,,,,,,,,,,,,,,,,,,,,pkg_data,.,name,",",author,=,pkg_data,.,author,",",version,=,version,",",release,=,pkg_data,.,version,",",projectdir,=,projectdir,),if,subprocess,.,call,(,shlex,.,split,(,args,),),==,0,:,_touch_gitkeep,(,projectdir,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/crossmap.py,bowtiedb,"def bowtiedb(fa, keepDB):
    """"""
    make bowtie db
    """"""
    btdir = '%s/bt2' % (os.getcwd())
    # make directory for
    if not os.path.exists(btdir):
        os.mkdir(btdir)
    btdb = '%s/%s' % (btdir, fa.rsplit('/', 1)[-1])
    if keepDB is True:
        if os.path.exists('%s.1.bt2' % (btdb)):
            return btdb
    p = subprocess.Popen('bowtie2-build -q %s %s' \
        % (fa, btdb), shell = True)
    p.communicate()
    return btdb",python,"def bowtiedb(fa, keepDB):
    """"""
    make bowtie db
    """"""
    btdir = '%s/bt2' % (os.getcwd())
    # make directory for
    if not os.path.exists(btdir):
        os.mkdir(btdir)
    btdb = '%s/%s' % (btdir, fa.rsplit('/', 1)[-1])
    if keepDB is True:
        if os.path.exists('%s.1.bt2' % (btdb)):
            return btdb
    p = subprocess.Popen('bowtie2-build -q %s %s' \
        % (fa, btdb), shell = True)
    p.communicate()
    return btdb",def,bowtiedb,(,fa,",",keepDB,),:,btdir,=,'%s/bt2',%,(,os,.,getcwd,(,),),# make directory for,if,not,os,.,path,.,exists,(,btdir,),:,os,.,mkdir,(,btdir,),btdb,=,'%s/%s',%,(,btdir,make bowtie db,make,bowtie,db,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/crossmap.py#L16-L31,train,",",fa,.,rsplit,(,'/',",",1,),[,-,1,],),if,keepDB,is,True,:,if,os,.,path,.,exists,(,'%s.1.bt2',%,(,btdb,,,,,,,,,,,,,,,,,,,,),),:,return,btdb,p,=,subprocess,.,Popen,(,'bowtie2-build -q %s %s',%,(,fa,",",btdb,),",",shell,=,True,),p,.,communicate,(,),return,btdb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/crossmap.py,bowtie,"def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):
    """"""
    generate bowtie2 command
    """"""
    bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)
    if f is not False:
        bt2 += '-1 %s -2 %s ' % (f, r)
    if u is not False:
        bt2 += '-U %s ' % (u)
    bt2 += opt
    if no_shrink is False:
        if f is False:
            bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)
        else:
            bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)
    else:
        bt2 += ' > %s.sam' % (sam)
    return bt2",python,"def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):
    """"""
    generate bowtie2 command
    """"""
    bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)
    if f is not False:
        bt2 += '-1 %s -2 %s ' % (f, r)
    if u is not False:
        bt2 += '-U %s ' % (u)
    bt2 += opt
    if no_shrink is False:
        if f is False:
            bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)
        else:
            bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)
    else:
        bt2 += ' > %s.sam' % (sam)
    return bt2",def,bowtie,(,sam,",",btd,",",f,",",r,",",u,",",opt,",",no_shrink,",",threads,),:,bt2,=,'bowtie2 -x %s -p %s ',%,(,btd,",",threads,),if,f,is,not,False,:,bt2,+=,'-1 %s -2 %s ',%,(,f,",",r,generate bowtie2 command,generate,bowtie2,command,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/crossmap.py#L33-L50,train,),if,u,is,not,False,:,bt2,+=,'-U %s ',%,(,u,),bt2,+=,opt,if,no_shrink,is,False,:,if,f,is,False,:,bt2,+=,' | shrinksam -u -k %s-shrunk.sam ',,,,,,,,,,,,,,,,,,,,%,(,sam,),else,:,bt2,+=,' | shrinksam -k %s-shrunk.sam ',%,(,sam,),else,:,bt2,+=,' > %s.sam',%,(,sam,),return,bt2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/crossmap.py,crossmap,"def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):
    """"""
    map all read sets against all fasta files
    """"""
    if cluster is True:
        threads = '48'
    btc = []
    for fa in fas:
        btd = bowtiedb(fa, keepDB)
        F, R, U = reads
        if F is not False:
            if U is False:
                u = False
            for i, f in enumerate(F):
                r = R[i]
                if U is not False:
                    u = U[i]
                sam = '%s/%s-vs-%s' % (os.getcwd(), \
                        fa.rsplit('/', 1)[-1], f.rsplit('/', 1)[-1].rsplit('.', 3)[0])
                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))
        else:
            f = False
            r = False
            for u in U:
                sam = '%s/%s-vs-%s' % (os.getcwd(), \
                        fa.rsplit('/', 1)[-1], u.rsplit('/', 1)[-1].rsplit('.', 3)[0])
                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))
    if cluster is False:
        for i in btc:
            p = subprocess.Popen(i, shell = True)
            p.communicate()
    else:
        ID = ''.join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))
        for node, commands in enumerate(chunks(btc, nodes), 1):
            bs = open('%s/crossmap-qsub.%s.%s.sh' % (os.getcwd(), ID, node), 'w')
            print('\n'.join(commands), file=bs)
            bs.close()
            p = subprocess.Popen(\
                    'qsub -V -N crossmap %s' \
                        % (bs.name), \
                    shell = True)
            p.communicate()",python,"def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):
    """"""
    map all read sets against all fasta files
    """"""
    if cluster is True:
        threads = '48'
    btc = []
    for fa in fas:
        btd = bowtiedb(fa, keepDB)
        F, R, U = reads
        if F is not False:
            if U is False:
                u = False
            for i, f in enumerate(F):
                r = R[i]
                if U is not False:
                    u = U[i]
                sam = '%s/%s-vs-%s' % (os.getcwd(), \
                        fa.rsplit('/', 1)[-1], f.rsplit('/', 1)[-1].rsplit('.', 3)[0])
                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))
        else:
            f = False
            r = False
            for u in U:
                sam = '%s/%s-vs-%s' % (os.getcwd(), \
                        fa.rsplit('/', 1)[-1], u.rsplit('/', 1)[-1].rsplit('.', 3)[0])
                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))
    if cluster is False:
        for i in btc:
            p = subprocess.Popen(i, shell = True)
            p.communicate()
    else:
        ID = ''.join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))
        for node, commands in enumerate(chunks(btc, nodes), 1):
            bs = open('%s/crossmap-qsub.%s.%s.sh' % (os.getcwd(), ID, node), 'w')
            print('\n'.join(commands), file=bs)
            bs.close()
            p = subprocess.Popen(\
                    'qsub -V -N crossmap %s' \
                        % (bs.name), \
                    shell = True)
            p.communicate()",def,crossmap,(,fas,",",reads,",",options,",",no_shrink,",",keepDB,",",threads,",",cluster,",",nodes,),:,if,cluster,is,True,:,threads,=,'48',btc,=,[,],for,fa,in,fas,:,btd,=,bowtiedb,(,fa,",",map all read sets against all fasta files,map,all,read,sets,against,all,fasta,files,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/crossmap.py#L55-L96,train,keepDB,),F,",",R,",",U,=,reads,if,F,is,not,False,:,if,U,is,False,:,u,=,False,for,i,",",f,in,enumerate,(,,,,,,,,,,,,,,,,,,,,F,),:,r,=,R,[,i,],if,U,is,not,False,:,u,=,U,[,i,],sam,=,'%s/%s-vs-%s',%,(,os,.,getcwd,(,),",",fa,.,rsplit,(,'/',",",1,),[,-,1,],",",f,.,rsplit,(,'/',",",1,),[,-,1,],.,rsplit,(,'.',",",3,),[,0,],),btc,.,append,(,bowtie,(,sam,",",btd,",",f,",",r,",",u,",",options,",",no_shrink,",",threads,),),else,:,f,=,False,r,=,False,for,u,in,U,:,sam,=,'%s/%s-vs-%s',%,(,os,.,getcwd,(,),",",fa,.,rsplit,(,'/',",",1,),[,-,1,],",",u,.,rsplit,(,'/',",",1,),[,-,1,],.,rsplit,(,'.',",",3,),[,0,],),btc,.,append,(,bowtie,(,sam,",",btd,",",f,",",r,",",u,",",options,",",no_shrink,",",threads,),),if,cluster,is,False,:,for,i,in,btc,:,p,=,subprocess,.,Popen,(,i,",",shell,=,True,),p,.,communicate,(,),else,:,ID,=,'',.,join,(,random,.,choice,(,[,str,(,i,),for,i,in,range,(,0,",",9,),],),for,_,in,range,(,5,),),for,node,",",commands,in,enumerate,(,chunks,(,btc,",",nodes,),",",1,),:,bs,=,open,(,'%s/crossmap-qsub.%s.%s.sh',%,(,os,.,getcwd,(,),",",ID,",",node,),",",'w',),print,(,'\n',.,join,(,commands,),",",file,=,bs,),bs,.,close,(,),p,=,subprocess,.,Popen,(,'qsub -V -N crossmap %s',%,(,bs,.,name,),",",shell,=,True,),p,.,communicate,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/base.py,BaseCluster.get_conn,"def get_conn(self, *args, **kwargs):
        """"""
        Returns a connection object from the router given ``args``.

        Useful in cases where a connection cannot be automatically determined
        during all steps of the process. An example of this would be
        Redis pipelines.
        """"""
        connections = self.__connections_for('get_conn', args=args, kwargs=kwargs)

        if len(connections) is 1:
            return connections[0]
        else:
            return connections",python,"def get_conn(self, *args, **kwargs):
        """"""
        Returns a connection object from the router given ``args``.

        Useful in cases where a connection cannot be automatically determined
        during all steps of the process. An example of this would be
        Redis pipelines.
        """"""
        connections = self.__connections_for('get_conn', args=args, kwargs=kwargs)

        if len(connections) is 1:
            return connections[0]
        else:
            return connections",def,get_conn,(,self,",",*,args,",",*,*,kwargs,),:,connections,=,self,.,__connections_for,(,'get_conn',",",args,=,args,",",kwargs,=,kwargs,),if,len,(,connections,),is,1,:,return,connections,[,0,],else,"Returns a connection object from the router given ``args``.

        Useful in cases where a connection cannot be automatically determined
        during all steps of the process. An example of this would be
        Redis pipelines.",Returns,a,connection,object,from,the,router,given,args,.,,,,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/base.py#L100-L113,train,:,return,connections,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/_crc_algorithms.py,Crc.__get_nondirect_init,"def __get_nondirect_init(self, init):
        """"""
        return the non-direct init if the direct algorithm has been selected.
        """"""
        crc = init
        for i in range(self.Width):
            bit = crc & 0x01
            if bit:
                crc^= self.Poly
            crc >>= 1
            if bit:
                crc |= self.MSB_Mask
        return crc & self.Mask",python,"def __get_nondirect_init(self, init):
        """"""
        return the non-direct init if the direct algorithm has been selected.
        """"""
        crc = init
        for i in range(self.Width):
            bit = crc & 0x01
            if bit:
                crc^= self.Poly
            crc >>= 1
            if bit:
                crc |= self.MSB_Mask
        return crc & self.Mask",def,__get_nondirect_init,(,self,",",init,),:,crc,=,init,for,i,in,range,(,self,.,Width,),:,bit,=,crc,&,0x01,if,bit,:,crc,^=,self,.,Poly,crc,>>=,1,if,bit,:,crc,|=,self,return the non-direct init if the direct algorithm has been selected.,return,the,non,-,direct,init,if,the,direct,algorithm,has,been,selected,.,,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L98-L110,train,.,MSB_Mask,return,crc,&,self,.,Mask,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/_crc_algorithms.py,Crc.reflect,"def reflect(self, data, width):
        """"""
        reflect a data word, i.e. reverts the bit order.
        """"""
        x = data & 0x01
        for i in range(width - 1):
            data >>= 1
            x = (x << 1) | (data & 0x01)
        return x",python,"def reflect(self, data, width):
        """"""
        reflect a data word, i.e. reverts the bit order.
        """"""
        x = data & 0x01
        for i in range(width - 1):
            data >>= 1
            x = (x << 1) | (data & 0x01)
        return x",def,reflect,(,self,",",data,",",width,),:,x,=,data,&,0x01,for,i,in,range,(,width,-,1,),:,data,>>=,1,x,=,(,x,<<,1,),|,(,data,&,0x01,),return,x,"reflect a data word, i.e. reverts the bit order.",reflect,a,data,word,i,.,e,.,reverts,the,bit,order,.,,,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L115-L123,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/_crc_algorithms.py,Crc.bit_by_bit,"def bit_by_bit(self, in_data):
        """"""
        Classic simple and slow CRC implementation.  This function iterates bit
        by bit over the augmented input message and returns the calculated CRC
        value at the end.
        """"""
        # If the input data is a string, convert to bytes.
        if isinstance(in_data, str):
            in_data = [ord(c) for c in in_data]

        register = self.NonDirectInit
        for octet in in_data:
            if self.ReflectIn:
                octet = self.reflect(octet, 8)
            for i in range(8):
                topbit = register & self.MSB_Mask
                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)
                if topbit:
                    register ^= self.Poly

        for i in range(self.Width):
            topbit = register & self.MSB_Mask
            register = ((register << 1) & self.Mask)
            if topbit:
                register ^= self.Poly

        if self.ReflectOut:
            register = self.reflect(register, self.Width)
        return register ^ self.XorOut",python,"def bit_by_bit(self, in_data):
        """"""
        Classic simple and slow CRC implementation.  This function iterates bit
        by bit over the augmented input message and returns the calculated CRC
        value at the end.
        """"""
        # If the input data is a string, convert to bytes.
        if isinstance(in_data, str):
            in_data = [ord(c) for c in in_data]

        register = self.NonDirectInit
        for octet in in_data:
            if self.ReflectIn:
                octet = self.reflect(octet, 8)
            for i in range(8):
                topbit = register & self.MSB_Mask
                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)
                if topbit:
                    register ^= self.Poly

        for i in range(self.Width):
            topbit = register & self.MSB_Mask
            register = ((register << 1) & self.Mask)
            if topbit:
                register ^= self.Poly

        if self.ReflectOut:
            register = self.reflect(register, self.Width)
        return register ^ self.XorOut",def,bit_by_bit,(,self,",",in_data,),:,"# If the input data is a string, convert to bytes.",if,isinstance,(,in_data,",",str,),:,in_data,=,[,ord,(,c,),for,c,in,in_data,],register,=,self,.,NonDirectInit,for,octet,in,in_data,:,if,self,.,ReflectIn,"Classic simple and slow CRC implementation.  This function iterates bit
        by bit over the augmented input message and returns the calculated CRC
        value at the end.",Classic,simple,and,slow,CRC,implementation,.,This,function,iterates,bit,by,bit,over,the,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L128-L156,train,:,octet,=,self,.,reflect,(,octet,",",8,),for,i,in,range,(,8,),:,topbit,=,register,&,self,.,MSB_Mask,register,=,(,(,augmented,input,message,and,returns,the,calculated,CRC,value,at,the,end,.,,,,,,,register,<<,1,),&,self,.,Mask,),|,(,(,octet,>>,(,7,-,i,),),&,0x01,),if,topbit,:,register,^=,self,.,Poly,for,i,in,range,(,self,.,Width,),:,topbit,=,register,&,self,.,MSB_Mask,register,=,(,(,register,<<,1,),&,self,.,Mask,),if,topbit,:,register,^=,self,.,Poly,if,self,.,ReflectOut,:,register,=,self,.,reflect,(,register,",",self,.,Width,),return,register,^,self,.,XorOut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/_crc_algorithms.py,Crc.gen_table,"def gen_table(self):
        """"""
        This function generates the CRC table used for the table_driven CRC
        algorithm.  The Python version cannot handle tables of an index width
        other than 8.  See the generated C code for tables with different sizes
        instead.
        """"""
        table_length = 1 << self.TableIdxWidth
        tbl = [0] * table_length
        for i in range(table_length):
            register = i
            if self.ReflectIn:
                register = self.reflect(register, self.TableIdxWidth)
            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)
            for j in range(self.TableIdxWidth):
                if register & (self.MSB_Mask << self.CrcShift) != 0:
                    register = (register << 1) ^ (self.Poly << self.CrcShift)
                else:
                    register = (register << 1)
            if self.ReflectIn:
                register = self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift
            tbl[i] = register & (self.Mask << self.CrcShift)
        return tbl",python,"def gen_table(self):
        """"""
        This function generates the CRC table used for the table_driven CRC
        algorithm.  The Python version cannot handle tables of an index width
        other than 8.  See the generated C code for tables with different sizes
        instead.
        """"""
        table_length = 1 << self.TableIdxWidth
        tbl = [0] * table_length
        for i in range(table_length):
            register = i
            if self.ReflectIn:
                register = self.reflect(register, self.TableIdxWidth)
            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)
            for j in range(self.TableIdxWidth):
                if register & (self.MSB_Mask << self.CrcShift) != 0:
                    register = (register << 1) ^ (self.Poly << self.CrcShift)
                else:
                    register = (register << 1)
            if self.ReflectIn:
                register = self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift
            tbl[i] = register & (self.Mask << self.CrcShift)
        return tbl",def,gen_table,(,self,),:,table_length,=,1,<<,self,.,TableIdxWidth,tbl,=,[,0,],*,table_length,for,i,in,range,(,table_length,),:,register,=,i,if,self,.,ReflectIn,:,register,=,self,.,reflect,(,register,"This function generates the CRC table used for the table_driven CRC
        algorithm.  The Python version cannot handle tables of an index width
        other than 8.  See the generated C code for tables with different sizes
        instead.",This,function,generates,the,CRC,table,used,for,the,table_driven,CRC,algorithm,.,The,Python,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L190-L212,train,",",self,.,TableIdxWidth,),register,=,register,<<,(,self,.,Width,-,self,.,TableIdxWidth,+,self,.,CrcShift,),for,j,in,range,(,self,.,TableIdxWidth,version,cannot,handle,tables,of,an,index,width,other,than,8,.,See,the,generated,C,code,for,tables,),:,if,register,&,(,self,.,MSB_Mask,<<,self,.,CrcShift,),!=,0,:,register,=,(,register,<<,1,),^,(,self,.,Poly,<<,self,.,CrcShift,),else,:,register,=,(,register,<<,1,),if,self,.,ReflectIn,:,register,=,self,.,reflect,(,register,>>,self,.,CrcShift,",",self,.,Width,),<<,self,.,CrcShift,tbl,[,i,],=,register,&,(,self,.,Mask,<<,self,.,CrcShift,),return,tbl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,with,different,sizes,instead,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/_crc_algorithms.py,Crc.table_driven,"def table_driven(self, in_data):
        """"""
        The Standard table_driven CRC algorithm.
        """"""
        # If the input data is a string, convert to bytes.
        if isinstance(in_data, str):
            in_data = [ord(c) for c in in_data]

        tbl = self.gen_table()

        register = self.DirectInit << self.CrcShift
        if not self.ReflectIn:
            for octet in in_data:
                tblidx = ((register >> (self.Width - self.TableIdxWidth + self.CrcShift)) ^ octet) & 0xff
                register = ((register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)
            register = register >> self.CrcShift
        else:
            register = self.reflect(register, self.Width + self.CrcShift) << self.CrcShift
            for octet in in_data:
                tblidx = ((register >> self.CrcShift) ^ octet) & 0xff
                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)
            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask

        if self.ReflectOut:
            register = self.reflect(register, self.Width)
        return register ^ self.XorOut",python,"def table_driven(self, in_data):
        """"""
        The Standard table_driven CRC algorithm.
        """"""
        # If the input data is a string, convert to bytes.
        if isinstance(in_data, str):
            in_data = [ord(c) for c in in_data]

        tbl = self.gen_table()

        register = self.DirectInit << self.CrcShift
        if not self.ReflectIn:
            for octet in in_data:
                tblidx = ((register >> (self.Width - self.TableIdxWidth + self.CrcShift)) ^ octet) & 0xff
                register = ((register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)
            register = register >> self.CrcShift
        else:
            register = self.reflect(register, self.Width + self.CrcShift) << self.CrcShift
            for octet in in_data:
                tblidx = ((register >> self.CrcShift) ^ octet) & 0xff
                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)
            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask

        if self.ReflectOut:
            register = self.reflect(register, self.Width)
        return register ^ self.XorOut",def,table_driven,(,self,",",in_data,),:,"# If the input data is a string, convert to bytes.",if,isinstance,(,in_data,",",str,),:,in_data,=,[,ord,(,c,),for,c,in,in_data,],tbl,=,self,.,gen_table,(,),register,=,self,.,DirectInit,<<,self,The Standard table_driven CRC algorithm.,The,Standard,table_driven,CRC,algorithm,.,,,,,,,,,,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L217-L242,train,.,CrcShift,if,not,self,.,ReflectIn,:,for,octet,in,in_data,:,tblidx,=,(,(,register,>>,(,self,.,Width,-,self,.,TableIdxWidth,+,self,.,,,,,,,,,,,,,,,,,,,,CrcShift,),),^,octet,),&,0xff,register,=,(,(,register,<<,(,self,.,TableIdxWidth,-,self,.,CrcShift,),),^,tbl,[,tblidx,],),&,(,self,.,Mask,<<,self,.,CrcShift,),register,=,register,>>,self,.,CrcShift,else,:,register,=,self,.,reflect,(,register,",",self,.,Width,+,self,.,CrcShift,),<<,self,.,CrcShift,for,octet,in,in_data,:,tblidx,=,(,(,register,>>,self,.,CrcShift,),^,octet,),&,0xff,register,=,(,(,register,>>,self,.,TableIdxWidth,),^,tbl,[,tblidx,],),&,(,self,.,Mask,<<,self,.,CrcShift,),register,=,self,.,reflect,(,register,",",self,.,Width,+,self,.,CrcShift,),&,self,.,Mask,if,self,.,ReflectOut,:,register,=,self,.,reflect,(,register,",",self,.,Width,),return,register,^,self,.,XorOut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/strip_masked.py,parse_masked,"def parse_masked(seq, min_len):
    """"""
    parse masked sequence into non-masked and masked regions
    """"""
    nm, masked = [], [[]]
    prev = None
    for base in seq[1]:
        if base.isupper():
            nm.append(base)
            if masked != [[]] and len(masked[-1]) < min_len:
                nm.extend(masked[-1])
                del masked[-1]
            prev = False
        elif base.islower():
            if prev is False:
                masked.append([])
            masked[-1].append(base)
            prev = True
    return nm, masked",python,"def parse_masked(seq, min_len):
    """"""
    parse masked sequence into non-masked and masked regions
    """"""
    nm, masked = [], [[]]
    prev = None
    for base in seq[1]:
        if base.isupper():
            nm.append(base)
            if masked != [[]] and len(masked[-1]) < min_len:
                nm.extend(masked[-1])
                del masked[-1]
            prev = False
        elif base.islower():
            if prev is False:
                masked.append([])
            masked[-1].append(base)
            prev = True
    return nm, masked",def,parse_masked,(,seq,",",min_len,),:,nm,",",masked,=,[,],",",[,[,],],prev,=,None,for,base,in,seq,[,1,],:,if,base,.,isupper,(,),:,nm,.,append,(,base,),parse masked sequence into non-masked and masked regions,parse,masked,sequence,into,non,-,masked,and,masked,regions,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_masked.py#L13-L31,train,if,masked,!=,[,[,],],and,len,(,masked,[,-,1,],),<,min_len,:,nm,.,extend,(,masked,[,-,1,],),del,,,,,,,,,,,,,,,,,,,,masked,[,-,1,],prev,=,False,elif,base,.,islower,(,),:,if,prev,is,False,:,masked,.,append,(,[,],),masked,[,-,1,],.,append,(,base,),prev,=,True,return,nm,",",masked,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/strip_masked.py,strip_masked,"def strip_masked(fasta, min_len, print_masked):
    """"""
    remove masked regions from fasta file as long as
    they are longer than min_len
    """"""
    for seq in parse_fasta(fasta):
        nm, masked = parse_masked(seq, min_len)
        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]
        yield [0, nm]
        if print_masked is True:
            for i, m in enumerate([i for i in masked if i != []], 1):
                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]
                yield [1, m]",python,"def strip_masked(fasta, min_len, print_masked):
    """"""
    remove masked regions from fasta file as long as
    they are longer than min_len
    """"""
    for seq in parse_fasta(fasta):
        nm, masked = parse_masked(seq, min_len)
        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]
        yield [0, nm]
        if print_masked is True:
            for i, m in enumerate([i for i in masked if i != []], 1):
                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]
                yield [1, m]",def,strip_masked,(,fasta,",",min_len,",",print_masked,),:,for,seq,in,parse_fasta,(,fasta,),:,nm,",",masked,=,parse_masked,(,seq,",",min_len,),nm,=,[,'%s removed_masked >=%s',%,(,seq,[,0,],",",min_len,),",",'',"remove masked regions from fasta file as long as
    they are longer than min_len",remove,masked,regions,from,fasta,file,as,long,as,they,are,longer,than,min_len,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_masked.py#L33-L45,train,.,join,(,nm,),],yield,[,0,",",nm,],if,print_masked,is,True,:,for,i,",",m,in,enumerate,(,[,i,for,i,in,masked,,,,,,,,,,,,,,,,,,,,if,i,!=,[,],],",",1,),:,m,=,[,'%s insertion:%s',%,(,seq,[,0,],",",i,),",",'',.,join,(,m,),],yield,[,1,",",m,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/network_plots_gephi.py,get_relative_abundance,"def get_relative_abundance(biomfile):
    """"""
    Return arcsine transformed relative abundance from a BIOM format file.

    :type biomfile: BIOM format file
    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in
                     a SampleID, which are used as node sizes in network plots.

    :type return: Dictionary of dictionaries.
    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name
             whose value is the arc sine tranfsormed relative abundance value for that
             SampleID-OTU Name pair.
    """"""
    biomf = biom.load_table(biomfile)
    norm_biomf = biomf.norm(inplace=False)
    rel_abd = {}
    for sid in norm_biomf.ids():
        rel_abd[sid] = {}
        for otuid in norm_biomf.ids(""observation""):
            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=""observation"")[""taxonomy""])
            otuname = "" "".join(otuname.split(""_""))
            abd = norm_biomf.get_value_by_ids(otuid, sid)
            rel_abd[sid][otuname] = abd
    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)
    return ast_rel_abd",python,"def get_relative_abundance(biomfile):
    """"""
    Return arcsine transformed relative abundance from a BIOM format file.

    :type biomfile: BIOM format file
    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in
                     a SampleID, which are used as node sizes in network plots.

    :type return: Dictionary of dictionaries.
    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name
             whose value is the arc sine tranfsormed relative abundance value for that
             SampleID-OTU Name pair.
    """"""
    biomf = biom.load_table(biomfile)
    norm_biomf = biomf.norm(inplace=False)
    rel_abd = {}
    for sid in norm_biomf.ids():
        rel_abd[sid] = {}
        for otuid in norm_biomf.ids(""observation""):
            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=""observation"")[""taxonomy""])
            otuname = "" "".join(otuname.split(""_""))
            abd = norm_biomf.get_value_by_ids(otuid, sid)
            rel_abd[sid][otuname] = abd
    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)
    return ast_rel_abd",def,get_relative_abundance,(,biomfile,),:,biomf,=,biom,.,load_table,(,biomfile,),norm_biomf,=,biomf,.,norm,(,inplace,=,False,),rel_abd,=,{,},for,sid,in,norm_biomf,.,ids,(,),:,rel_abd,[,sid,],=,{,"Return arcsine transformed relative abundance from a BIOM format file.

    :type biomfile: BIOM format file
    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in
                     a SampleID, which are used as node sizes in network plots.

    :type return: Dictionary of dictionaries.
    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name
             whose value is the arc sine tranfsormed relative abundance value for that
             SampleID-OTU Name pair.",Return,arcsine,transformed,relative,abundance,from,a,BIOM,format,file,.,,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/network_plots_gephi.py#L33-L57,train,},for,otuid,in,norm_biomf,.,ids,(,"""observation""",),:,otuname,=,oc,.,otu_name,(,norm_biomf,.,metadata,(,otuid,",",axis,=,"""observation""",),[,"""taxonomy""",],,,,,,,,,,,,,,,,,,,,),otuname,=,""" """,.,join,(,otuname,.,split,(,"""_""",),),abd,=,norm_biomf,.,get_value_by_ids,(,otuid,",",sid,),rel_abd,[,sid,],[,otuname,],=,abd,ast_rel_abd,=,bc,.,arcsine_sqrt_transform,(,rel_abd,),return,ast_rel_abd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/iTol.py,find_otu,"def find_otu(otuid, tree):
    """"""
    Find an OTU ID in a Newick-format tree.
    Return the starting position of the ID or None if not found.
    """"""
    for m in re.finditer(otuid, tree):
        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]
        if before in [""("", "","", "")""] and after in ["":"", "";""]:
            return m.start()
    return None",python,"def find_otu(otuid, tree):
    """"""
    Find an OTU ID in a Newick-format tree.
    Return the starting position of the ID or None if not found.
    """"""
    for m in re.finditer(otuid, tree):
        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]
        if before in [""("", "","", "")""] and after in ["":"", "";""]:
            return m.start()
    return None",def,find_otu,(,otuid,",",tree,),:,for,m,in,re,.,finditer,(,otuid,",",tree,),:,before,",",after,=,tree,[,m,.,start,(,),-,1,],",",tree,[,m,.,start,(,),+,"Find an OTU ID in a Newick-format tree.
    Return the starting position of the ID or None if not found.",Find,an,OTU,ID,in,a,Newick,-,format,tree,.,Return,the,starting,position,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/iTol.py#L17-L26,train,len,(,otuid,),],if,before,in,[,"""(""",",",""",""",",",""")""",],and,after,in,[,""":""",",",""";""",],:,return,m,.,start,(,),of,the,ID,or,None,if,not,found,.,,,,,,,,,,,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/iTol.py,newick_replace_otuids,"def newick_replace_otuids(tree, biomf):
    """"""
    Replace the OTU ids in the Newick phylogenetic tree format with truncated
    OTU names
    """"""
    for val, id_, md in biomf.iter(axis=""observation""):
        otu_loc = find_otu(id_, tree)
        if otu_loc is not None:
            tree = tree[:otu_loc] + \
                   oc.otu_name(md[""taxonomy""]) + \
                   tree[otu_loc + len(id_):]
    return tree",python,"def newick_replace_otuids(tree, biomf):
    """"""
    Replace the OTU ids in the Newick phylogenetic tree format with truncated
    OTU names
    """"""
    for val, id_, md in biomf.iter(axis=""observation""):
        otu_loc = find_otu(id_, tree)
        if otu_loc is not None:
            tree = tree[:otu_loc] + \
                   oc.otu_name(md[""taxonomy""]) + \
                   tree[otu_loc + len(id_):]
    return tree",def,newick_replace_otuids,(,tree,",",biomf,),:,for,val,",",id_,",",md,in,biomf,.,iter,(,axis,=,"""observation""",),:,otu_loc,=,find_otu,(,id_,",",tree,),if,otu_loc,is,not,None,:,tree,=,tree,[,:,"Replace the OTU ids in the Newick phylogenetic tree format with truncated
    OTU names",Replace,the,OTU,ids,in,the,Newick,phylogenetic,tree,format,with,truncated,OTU,names,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/iTol.py#L29-L40,train,otu_loc,],+,oc,.,otu_name,(,md,[,"""taxonomy""",],),+,tree,[,otu_loc,+,len,(,id_,),:,],return,tree,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/cluster_ani.py,genome_info,"def genome_info(genome, info):
    """"""
    return genome info for choosing representative

    if ggKbase table provided - choose rep based on SCGs and genome length
        - priority for most SCGs - extra SCGs, then largest genome

    otherwise, based on largest genome
    """"""
    try:
        scg       = info['#SCGs']
        dups      = info['#SCG duplicates']
        length    = info['genome size (bp)']
        return [scg - dups, length, genome]
    except:
        return [False, False, info['genome size (bp)'], genome]",python,"def genome_info(genome, info):
    """"""
    return genome info for choosing representative

    if ggKbase table provided - choose rep based on SCGs and genome length
        - priority for most SCGs - extra SCGs, then largest genome

    otherwise, based on largest genome
    """"""
    try:
        scg       = info['#SCGs']
        dups      = info['#SCG duplicates']
        length    = info['genome size (bp)']
        return [scg - dups, length, genome]
    except:
        return [False, False, info['genome size (bp)'], genome]",def,genome_info,(,genome,",",info,),:,try,:,scg,=,info,[,'#SCGs',],dups,=,info,[,'#SCG duplicates',],length,=,info,[,'genome size (bp)',],return,[,scg,-,dups,",",length,",",genome,],except,:,return,[,False,"return genome info for choosing representative

    if ggKbase table provided - choose rep based on SCGs and genome length
        - priority for most SCGs - extra SCGs, then largest genome

    otherwise, based on largest genome",return,genome,info,for,choosing,representative,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L97-L112,train,",",False,",",info,[,'genome size (bp)',],",",genome,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/cluster_ani.py,print_clusters,"def print_clusters(fastas, info, ANI):
    """"""
    choose represenative genome and
    print cluster information

    *if ggKbase table is provided, use SCG info to choose best genome
    """"""
    header = ['#cluster', 'num. genomes', 'rep.', 'genome', '#SCGs', '#SCG duplicates', \
            'genome size (bp)', 'fragments', 'list']
    yield header
    in_cluster = []
    for cluster_num, cluster in enumerate(connected_components(ANI)):
        cluster = sorted([genome_info(genome, info[genome]) \
                            for genome in cluster], \
                            key = lambda x: x[0:], reverse = True)
        rep = cluster[0][-1]
        cluster = [i[-1] for i in cluster]
        size = len(cluster)
        for genome in cluster:
            in_cluster.append(genome)
            try:
                stats = [size, rep, genome, \
                            info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \
                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]
            except:
                stats = [size, rep, genome, \
                            'n/a', 'n/a', \
                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]
            if rep == genome:
                stats = ['*%s' % (cluster_num)] + stats
            else:
                stats = [cluster_num] + stats
            yield stats
    # print singletons
    try:
        start = cluster_num + 1
    except:
        start = 0
    fastas = set([i.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0] for i in fastas])
    for cluster_num, genome in \
            enumerate(fastas.difference(set(in_cluster)), start):
        try:
            stats = ['*%s' % (cluster_num), 1, genome, genome, \
                        info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \
                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]
        except:
            stats = ['*%s' % (cluster_num), 1, genome, genome, \
                        'n/a', 'n/a', \
                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]
        yield stats",python,"def print_clusters(fastas, info, ANI):
    """"""
    choose represenative genome and
    print cluster information

    *if ggKbase table is provided, use SCG info to choose best genome
    """"""
    header = ['#cluster', 'num. genomes', 'rep.', 'genome', '#SCGs', '#SCG duplicates', \
            'genome size (bp)', 'fragments', 'list']
    yield header
    in_cluster = []
    for cluster_num, cluster in enumerate(connected_components(ANI)):
        cluster = sorted([genome_info(genome, info[genome]) \
                            for genome in cluster], \
                            key = lambda x: x[0:], reverse = True)
        rep = cluster[0][-1]
        cluster = [i[-1] for i in cluster]
        size = len(cluster)
        for genome in cluster:
            in_cluster.append(genome)
            try:
                stats = [size, rep, genome, \
                            info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \
                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]
            except:
                stats = [size, rep, genome, \
                            'n/a', 'n/a', \
                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]
            if rep == genome:
                stats = ['*%s' % (cluster_num)] + stats
            else:
                stats = [cluster_num] + stats
            yield stats
    # print singletons
    try:
        start = cluster_num + 1
    except:
        start = 0
    fastas = set([i.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0] for i in fastas])
    for cluster_num, genome in \
            enumerate(fastas.difference(set(in_cluster)), start):
        try:
            stats = ['*%s' % (cluster_num), 1, genome, genome, \
                        info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \
                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]
        except:
            stats = ['*%s' % (cluster_num), 1, genome, genome, \
                        'n/a', 'n/a', \
                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]
        yield stats",def,print_clusters,(,fastas,",",info,",",ANI,),:,header,=,[,'#cluster',",",'num. genomes',",",'rep.',",",'genome',",",'#SCGs',",",'#SCG duplicates',",",'genome size (bp)',",",'fragments',",",'list',],yield,header,in_cluster,=,[,],for,cluster_num,",",cluster,in,enumerate,"choose represenative genome and
    print cluster information

    *if ggKbase table is provided, use SCG info to choose best genome",choose,represenative,genome,and,print,cluster,information,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L114-L163,train,(,connected_components,(,ANI,),),:,cluster,=,sorted,(,[,genome_info,(,genome,",",info,[,genome,],),for,genome,in,cluster,],",",key,=,lambda,,,,,,,,,,,,,,,,,,,,x,:,x,[,0,:,],",",reverse,=,True,),rep,=,cluster,[,0,],[,-,1,],cluster,=,[,i,[,-,1,],for,i,in,cluster,],size,=,len,(,cluster,),for,genome,in,cluster,:,in_cluster,.,append,(,genome,),try,:,stats,=,[,size,",",rep,",",genome,",",info,[,genome,],[,'#SCGs',],",",info,[,genome,],[,'#SCG duplicates',],",",info,[,genome,],[,'genome size (bp)',],",",info,[,genome,],[,'# contigs',],",",cluster,],except,:,stats,=,[,size,",",rep,",",genome,",",'n/a',",",'n/a',",",info,[,genome,],[,'genome size (bp)',],",",info,[,genome,],[,'# contigs',],",",cluster,],if,rep,==,genome,:,stats,=,[,'*%s',%,(,cluster_num,),],+,stats,else,:,stats,=,[,cluster_num,],+,stats,yield,stats,# print singletons,try,:,start,=,cluster_num,+,1,except,:,start,=,0,fastas,=,set,(,[,i,.,rsplit,(,'.',",",1,),[,0,],.,rsplit,(,'/',",",1,),[,-,1,],.,rsplit,(,'.contigs',),[,0,],for,i,in,fastas,],),for,cluster_num,",",genome,in,enumerate,(,fastas,.,difference,(,set,(,in_cluster,),),",",start,),:,try,:,stats,=,[,'*%s',%,(,cluster_num,),",",1,",",genome,",",genome,",",info,[,genome,],[,'#SCGs',],",",info,[,genome,],[,'#SCG duplicates',],",",info,[,genome,],[,'genome size (bp)',],",",info,[,genome,],[,'# contigs',],",",[,genome,],],except,:,stats,=,[,'*%s',%,(,cluster_num,),",",1,",",genome,",",genome,",",'n/a',",",'n/a',",",info,[,genome,],[,'genome size (bp)',],",",info,[,genome,],[,'# contigs',],",",[,genome,,,,,,,,,,,,],],yield,stats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/cluster_ani.py,parse_ggKbase_tables,"def parse_ggKbase_tables(tables, id_type):
    """"""
    convert ggKbase genome info tables to dictionary
    """"""
    g2info = {}
    for table in tables:
        for line in open(table):
            line = line.strip().split('\t')
            if line[0].startswith('name'):
                header = line
                header[4] = 'genome size (bp)'
                header[12] = '#SCGs'
                header[13] = '#SCG duplicates'
                continue
            name, code, info = line[0], line[1], line
            info = [to_int(i) for i in info]
            if id_type is False: # try to use name and code ID
                if 'UNK' in code or 'unknown' in code:
                    code = name
                if (name != code) and (name and code in g2info):
                    print('# duplicate name or code in table(s)', file=sys.stderr)
                    print('# %s and/or %s' % (name, code), file=sys.stderr)
                    exit()
                if name not in g2info:
                    g2info[name] = {item:stat for item, stat in zip(header, info)}
                if code not in g2info:
                    g2info[code] = {item:stat for item, stat in zip(header, info)}
            else:
                if id_type == 'name':
                    ID = name
                elif id_type == 'code':
                    ID = code
                else:
                    print('# specify name or code column using -id', file=sys.stderr)
                    exit()
                ID = ID.replace(' ', '')
                g2info[ID] = {item:stat for item, stat in zip(header, info)}
                if g2info[ID]['genome size (bp)'] == '':
                    g2info[ID]['genome size (bp)'] = 0
    return g2info",python,"def parse_ggKbase_tables(tables, id_type):
    """"""
    convert ggKbase genome info tables to dictionary
    """"""
    g2info = {}
    for table in tables:
        for line in open(table):
            line = line.strip().split('\t')
            if line[0].startswith('name'):
                header = line
                header[4] = 'genome size (bp)'
                header[12] = '#SCGs'
                header[13] = '#SCG duplicates'
                continue
            name, code, info = line[0], line[1], line
            info = [to_int(i) for i in info]
            if id_type is False: # try to use name and code ID
                if 'UNK' in code or 'unknown' in code:
                    code = name
                if (name != code) and (name and code in g2info):
                    print('# duplicate name or code in table(s)', file=sys.stderr)
                    print('# %s and/or %s' % (name, code), file=sys.stderr)
                    exit()
                if name not in g2info:
                    g2info[name] = {item:stat for item, stat in zip(header, info)}
                if code not in g2info:
                    g2info[code] = {item:stat for item, stat in zip(header, info)}
            else:
                if id_type == 'name':
                    ID = name
                elif id_type == 'code':
                    ID = code
                else:
                    print('# specify name or code column using -id', file=sys.stderr)
                    exit()
                ID = ID.replace(' ', '')
                g2info[ID] = {item:stat for item, stat in zip(header, info)}
                if g2info[ID]['genome size (bp)'] == '':
                    g2info[ID]['genome size (bp)'] = 0
    return g2info",def,parse_ggKbase_tables,(,tables,",",id_type,),:,g2info,=,{,},for,table,in,tables,:,for,line,in,open,(,table,),:,line,=,line,.,strip,(,),.,split,(,'\t',),if,line,[,0,],.,convert ggKbase genome info tables to dictionary,convert,ggKbase,genome,info,tables,to,dictionary,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L174-L213,train,startswith,(,'name',),:,header,=,line,header,[,4,],=,'genome size (bp)',header,[,12,],=,'#SCGs',header,[,13,],=,'#SCG duplicates',continue,name,",",code,,,,,,,,,,,,,,,,,,,,",",info,=,line,[,0,],",",line,[,1,],",",line,info,=,[,to_int,(,i,),for,i,in,info,],if,id_type,is,False,:,# try to use name and code ID,if,'UNK',in,code,or,'unknown',in,code,:,code,=,name,if,(,name,!=,code,),and,(,name,and,code,in,g2info,),:,print,(,'# duplicate name or code in table(s)',",",file,=,sys,.,stderr,),print,(,'# %s and/or %s',%,(,name,",",code,),",",file,=,sys,.,stderr,),exit,(,),if,name,not,in,g2info,:,g2info,[,name,],=,{,item,:,stat,for,item,",",stat,in,zip,(,header,",",info,),},if,code,not,in,g2info,:,g2info,[,code,],=,{,item,:,stat,for,item,",",stat,in,zip,(,header,",",info,),},else,:,if,id_type,==,'name',:,ID,=,name,elif,id_type,==,'code',:,ID,=,code,else,:,print,(,'# specify name or code column using -id',",",file,=,sys,.,stderr,),exit,(,),ID,=,ID,.,replace,(,' ',",",'',),g2info,[,ID,],=,{,item,:,stat,for,item,",",stat,in,zip,(,header,",",info,),},if,g2info,[,ID,],[,'genome size (bp)',],==,'',:,g2info,[,ID,],[,'genome size (bp)',],=,0,return,g2info,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/cluster_ani.py,parse_checkM_tables,"def parse_checkM_tables(tables):
    """"""
    convert checkM genome info tables to dictionary
    """"""
    g2info = {}
    for table in tables:
        for line in open(table):
            line = line.strip().split('\t')
            if line[0].startswith('Bin Id'):
                header = line
                header[8] = 'genome size (bp)'
                header[5] = '#SCGs'
                header[6] = '#SCG duplicates'
                continue
            ID, info = line[0], line
            info = [to_int(i) for i in info]
            ID = ID.replace(' ', '')
            g2info[ID] = {item:stat for item, stat in zip(header, info)}
            if g2info[ID]['genome size (bp)'] == '':
                g2info[ID]['genome size (bp)'] = 0
    return g2info",python,"def parse_checkM_tables(tables):
    """"""
    convert checkM genome info tables to dictionary
    """"""
    g2info = {}
    for table in tables:
        for line in open(table):
            line = line.strip().split('\t')
            if line[0].startswith('Bin Id'):
                header = line
                header[8] = 'genome size (bp)'
                header[5] = '#SCGs'
                header[6] = '#SCG duplicates'
                continue
            ID, info = line[0], line
            info = [to_int(i) for i in info]
            ID = ID.replace(' ', '')
            g2info[ID] = {item:stat for item, stat in zip(header, info)}
            if g2info[ID]['genome size (bp)'] == '':
                g2info[ID]['genome size (bp)'] = 0
    return g2info",def,parse_checkM_tables,(,tables,),:,g2info,=,{,},for,table,in,tables,:,for,line,in,open,(,table,),:,line,=,line,.,strip,(,),.,split,(,'\t',),if,line,[,0,],.,startswith,(,convert checkM genome info tables to dictionary,convert,checkM,genome,info,tables,to,dictionary,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L215-L235,train,'Bin Id',),:,header,=,line,header,[,8,],=,'genome size (bp)',header,[,5,],=,'#SCGs',header,[,6,],=,'#SCG duplicates',continue,ID,",",info,=,line,,,,,,,,,,,,,,,,,,,,[,0,],",",line,info,=,[,to_int,(,i,),for,i,in,info,],ID,=,ID,.,replace,(,' ',",",'',),g2info,[,ID,],=,{,item,:,stat,for,item,",",stat,in,zip,(,header,",",info,),},if,g2info,[,ID,],[,'genome size (bp)',],==,'',:,g2info,[,ID,],[,'genome size (bp)',],=,0,return,g2info,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/cluster_ani.py,genome_lengths,"def genome_lengths(fastas, info):
    """"""
    get genome lengths
    """"""
    if info is False:
        info = {}
    for genome in fastas:
        name = genome.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]
        if name in info:
            continue
        length = 0
        fragments = 0
        for seq in parse_fasta(genome):
            length += len(seq[1])
            fragments += 1
        info[name] = {'genome size (bp)':length, '# contigs':fragments}
    return info",python,"def genome_lengths(fastas, info):
    """"""
    get genome lengths
    """"""
    if info is False:
        info = {}
    for genome in fastas:
        name = genome.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]
        if name in info:
            continue
        length = 0
        fragments = 0
        for seq in parse_fasta(genome):
            length += len(seq[1])
            fragments += 1
        info[name] = {'genome size (bp)':length, '# contigs':fragments}
    return info",def,genome_lengths,(,fastas,",",info,),:,if,info,is,False,:,info,=,{,},for,genome,in,fastas,:,name,=,genome,.,rsplit,(,'.',",",1,),[,0,],.,rsplit,(,'/',",",1,),[,get genome lengths,get,genome,lengths,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L237-L253,train,-,1,],.,rsplit,(,'.contigs',),[,0,],if,name,in,info,:,continue,length,=,0,fragments,=,0,for,seq,in,parse_fasta,(,genome,),,,,,,,,,,,,,,,,,,,,:,length,+=,len,(,seq,[,1,],),fragments,+=,1,info,[,name,],=,{,'genome size (bp)',:,length,",",'# contigs',:,fragments,},return,info,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/routers/base.py,BaseRouter.get_dbs,"def get_dbs(self, attr, args, kwargs, **fkwargs):
        """"""
        Returns a list of db keys to route the given call to.

        :param attr: Name of attribute being called on the connection.
        :param args: List of arguments being passed to ``attr``.
        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.

        >>> redis = Cluster(router=BaseRouter)
        >>> router = redis.router
        >>> router.get_dbs('incr', args=('key name', 1))
        [0,1,2]

        """"""
        if not self._ready:
            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):
                raise self.UnableToSetupRouter()

        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)
        if retval is not None:
            args, kwargs = retval

        if not (args or kwargs):
            return self.cluster.hosts.keys()

        try:
            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)
        except Exception as e:
            self._handle_exception(e)
            db_nums = []

        return self._post_routing(attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs)",python,"def get_dbs(self, attr, args, kwargs, **fkwargs):
        """"""
        Returns a list of db keys to route the given call to.

        :param attr: Name of attribute being called on the connection.
        :param args: List of arguments being passed to ``attr``.
        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.

        >>> redis = Cluster(router=BaseRouter)
        >>> router = redis.router
        >>> router.get_dbs('incr', args=('key name', 1))
        [0,1,2]

        """"""
        if not self._ready:
            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):
                raise self.UnableToSetupRouter()

        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)
        if retval is not None:
            args, kwargs = retval

        if not (args or kwargs):
            return self.cluster.hosts.keys()

        try:
            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)
        except Exception as e:
            self._handle_exception(e)
            db_nums = []

        return self._post_routing(attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs)",def,get_dbs,(,self,",",attr,",",args,",",kwargs,",",*,*,fkwargs,),:,if,not,self,.,_ready,:,if,not,self,.,setup_router,(,args,=,args,",",kwargs,=,kwargs,",",*,*,fkwargs,),:,raise,self,"Returns a list of db keys to route the given call to.

        :param attr: Name of attribute being called on the connection.
        :param args: List of arguments being passed to ``attr``.
        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.

        >>> redis = Cluster(router=BaseRouter)
        >>> router = redis.router
        >>> router.get_dbs('incr', args=('key name', 1))
        [0,1,2]",Returns,a,list,of,db,keys,to,route,the,given,call,to,.,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L50-L81,train,.,UnableToSetupRouter,(,),retval,=,self,.,_pre_routing,(,attr,=,attr,",",args,=,args,",",kwargs,=,kwargs,",",*,*,fkwargs,),if,retval,is,not,,,,,,,,,,,,,,,,,,,,None,:,args,",",kwargs,=,retval,if,not,(,args,or,kwargs,),:,return,self,.,cluster,.,hosts,.,keys,(,),try,:,db_nums,=,self,.,_route,(,attr,=,attr,",",args,=,args,",",kwargs,=,kwargs,",",*,*,fkwargs,),except,Exception,as,e,:,self,.,_handle_exception,(,e,),db_nums,=,[,],return,self,.,_post_routing,(,attr,=,attr,",",db_nums,=,db_nums,",",args,=,args,",",kwargs,=,kwargs,",",*,*,fkwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/routers/base.py,BaseRouter.setup_router,"def setup_router(self, args, kwargs, **fkwargs):
        """"""
        Call method to perform any setup
        """"""
        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)

        return self._ready",python,"def setup_router(self, args, kwargs, **fkwargs):
        """"""
        Call method to perform any setup
        """"""
        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)

        return self._ready",def,setup_router,(,self,",",args,",",kwargs,",",*,*,fkwargs,),:,self,.,_ready,=,self,.,_setup_router,(,args,=,args,",",kwargs,=,kwargs,",",*,*,fkwargs,),return,self,.,_ready,,,,,,Call method to perform any setup,Call,method,to,perform,any,setup,,,,,,,,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L87-L93,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/routers/base.py,BaseRouter._route,"def _route(self, attr, args, kwargs, **fkwargs):
        """"""
        Perform routing and return db_nums
        """"""
        return self.cluster.hosts.keys()",python,"def _route(self, attr, args, kwargs, **fkwargs):
        """"""
        Perform routing and return db_nums
        """"""
        return self.cluster.hosts.keys()",def,_route,(,self,",",attr,",",args,",",kwargs,",",*,*,fkwargs,),:,return,self,.,cluster,.,hosts,.,keys,(,),,,,,,,,,,,,,,,,,,Perform routing and return db_nums,Perform,routing,and,return,db_nums,,,,,,,,,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L111-L115,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/routers/base.py,RoundRobinRouter.check_down_connections,"def check_down_connections(self):
        """"""
        Iterates through all connections which were previously listed as unavailable
        and marks any that have expired their retry_timeout as being up.
        """"""
        now = time.time()

        for db_num, marked_down_at in self._down_connections.items():
            if marked_down_at + self.retry_timeout <= now:
                self.mark_connection_up(db_num)",python,"def check_down_connections(self):
        """"""
        Iterates through all connections which were previously listed as unavailable
        and marks any that have expired their retry_timeout as being up.
        """"""
        now = time.time()

        for db_num, marked_down_at in self._down_connections.items():
            if marked_down_at + self.retry_timeout <= now:
                self.mark_connection_up(db_num)",def,check_down_connections,(,self,),:,now,=,time,.,time,(,),for,db_num,",",marked_down_at,in,self,.,_down_connections,.,items,(,),:,if,marked_down_at,+,self,.,retry_timeout,<=,now,:,self,.,mark_connection_up,(,db_num,),,,"Iterates through all connections which were previously listed as unavailable
        and marks any that have expired their retry_timeout as being up.",Iterates,through,all,connections,which,were,previously,listed,as,unavailable,and,marks,any,that,have,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L175-L184,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,expired,their,retry_timeout,as,being,up,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/routers/base.py,RoundRobinRouter.flush_down_connections,"def flush_down_connections(self):
        """"""
        Marks all connections which were previously listed as unavailable as being up.
        """"""
        self._get_db_attempts = 0
        for db_num in self._down_connections.keys():
            self.mark_connection_up(db_num)",python,"def flush_down_connections(self):
        """"""
        Marks all connections which were previously listed as unavailable as being up.
        """"""
        self._get_db_attempts = 0
        for db_num in self._down_connections.keys():
            self.mark_connection_up(db_num)",def,flush_down_connections,(,self,),:,self,.,_get_db_attempts,=,0,for,db_num,in,self,.,_down_connections,.,keys,(,),:,self,.,mark_connection_up,(,db_num,),,,,,,,,,,,,,,,,Marks all connections which were previously listed as unavailable as being up.,Marks,all,connections,which,were,previously,listed,as,unavailable,as,being,up,.,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L186-L192,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/analysis.py,standby,"def standby(df, resolution='24h', time_window=None):
    """"""
    Compute standby power

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Electricity Power
    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    df : pandas.Series with DateTimeIndex in the given resolution
    """"""

    if df.empty:
        raise EmptyDataFrame()

    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame
    def parse_time(t):
        if isinstance(t, numbers.Number):
            return pd.Timestamp.utcfromtimestamp(t).time()
        else:
            return pd.Timestamp(t).time()


    # first filter based on the time-window
    if time_window is not None:
        t_start = parse_time(time_window[0])
        t_end = parse_time(time_window[1])
        if t_start > t_end:
            # start before midnight
            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]
        else:
            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]

    return df.resample(resolution).min()",python,"def standby(df, resolution='24h', time_window=None):
    """"""
    Compute standby power

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Electricity Power
    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    df : pandas.Series with DateTimeIndex in the given resolution
    """"""

    if df.empty:
        raise EmptyDataFrame()

    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame
    def parse_time(t):
        if isinstance(t, numbers.Number):
            return pd.Timestamp.utcfromtimestamp(t).time()
        else:
            return pd.Timestamp(t).time()


    # first filter based on the time-window
    if time_window is not None:
        t_start = parse_time(time_window[0])
        t_end = parse_time(time_window[1])
        if t_start > t_end:
            # start before midnight
            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]
        else:
            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]

    return df.resample(resolution).min()",def,standby,(,df,",",resolution,=,'24h',",",time_window,=,None,),:,if,df,.,empty,:,raise,EmptyDataFrame,(,),df,=,pd,.,DataFrame,(,df,),"# if df was a pd.Series, convert to DataFrame",def,parse_time,(,t,),:,if,isinstance,(,t,",","Compute standby power

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Electricity Power
    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    df : pandas.Series with DateTimeIndex in the given resolution",Compute,standby,power,,,,,,,,,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L72-L115,train,numbers,.,Number,),:,return,pd,.,Timestamp,.,utcfromtimestamp,(,t,),.,time,(,),else,:,return,pd,.,Timestamp,(,t,),.,time,(,,,,,,,,,,,,,,,,,,,,),# first filter based on the time-window,if,time_window,is,not,None,:,t_start,=,parse_time,(,time_window,[,0,],),t_end,=,parse_time,(,time_window,[,1,],),if,t_start,>,t_end,:,# start before midnight,df,=,df,[,(,df,.,index,.,time,>=,t_start,),|,(,df,.,index,.,time,<,t_end,),],else,:,df,=,df,[,(,df,.,index,.,time,>=,t_start,),&,(,df,.,index,.,time,<,t_end,),],return,df,.,resample,(,resolution,),.,min,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/analysis.py,share_of_standby,"def share_of_standby(df, resolution='24h', time_window=None):
    """"""
    Compute the share of the standby power in the total consumption.

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Power (typically electricity, can be anything)
    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    fraction : float between 0-1 with the share of the standby consumption
    """"""

    p_sb = standby(df, resolution, time_window)
    df = df.resample(resolution).mean()
    p_tot = df.sum()
    p_standby = p_sb.sum()
    share_standby = p_standby / p_tot
    res = share_standby.iloc[0]
    return res",python,"def share_of_standby(df, resolution='24h', time_window=None):
    """"""
    Compute the share of the standby power in the total consumption.

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Power (typically electricity, can be anything)
    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    fraction : float between 0-1 with the share of the standby consumption
    """"""

    p_sb = standby(df, resolution, time_window)
    df = df.resample(resolution).mean()
    p_tot = df.sum()
    p_standby = p_sb.sum()
    share_standby = p_standby / p_tot
    res = share_standby.iloc[0]
    return res",def,share_of_standby,(,df,",",resolution,=,'24h',",",time_window,=,None,),:,p_sb,=,standby,(,df,",",resolution,",",time_window,),df,=,df,.,resample,(,resolution,),.,mean,(,),p_tot,=,df,.,sum,(,),"Compute the share of the standby power in the total consumption.

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Power (typically electricity, can be anything)
    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'
    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    fraction : float between 0-1 with the share of the standby consumption",Compute,the,share,of,the,standby,power,in,the,total,consumption,.,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L118-L146,train,p_standby,=,p_sb,.,sum,(,),share_standby,=,p_standby,/,p_tot,res,=,share_standby,.,iloc,[,0,],return,res,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/analysis.py,count_peaks,"def count_peaks(ts):
    """"""
    Toggle counter for gas boilers

    Counts the number of times the gas consumption increases with more than 3kW

    Parameters
    ----------
    ts: Pandas Series
        Gas consumption in minute resolution

    Returns
    -------
    int
    """"""

    on_toggles = ts.diff() > 3000
    shifted = np.logical_not(on_toggles.shift(1))
    result = on_toggles & shifted
    count = result.sum()
    return count",python,"def count_peaks(ts):
    """"""
    Toggle counter for gas boilers

    Counts the number of times the gas consumption increases with more than 3kW

    Parameters
    ----------
    ts: Pandas Series
        Gas consumption in minute resolution

    Returns
    -------
    int
    """"""

    on_toggles = ts.diff() > 3000
    shifted = np.logical_not(on_toggles.shift(1))
    result = on_toggles & shifted
    count = result.sum()
    return count",def,count_peaks,(,ts,),:,on_toggles,=,ts,.,diff,(,),>,3000,shifted,=,np,.,logical_not,(,on_toggles,.,shift,(,1,),),result,=,on_toggles,&,shifted,count,=,result,.,sum,(,),return,count,,"Toggle counter for gas boilers

    Counts the number of times the gas consumption increases with more than 3kW

    Parameters
    ----------
    ts: Pandas Series
        Gas consumption in minute resolution

    Returns
    -------
    int",Toggle,counter,for,gas,boilers,,,,,,,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L149-L169,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/analysis.py,load_factor,"def load_factor(ts, resolution=None, norm=None):
    """"""
    Calculate the ratio of input vs. norm over a given interval.

    Parameters
    ----------
    ts : pandas.Series
        timeseries
    resolution : str, optional
        interval over which to calculate the ratio
        default: resolution of the input timeseries
    norm : int | float, optional
        denominator of the ratio
        default: the maximum of the input timeseries

    Returns
    -------
    pandas.Series
    """"""
    if norm is None:
        norm = ts.max()

    if resolution is not None:
        ts = ts.resample(rule=resolution).mean()

    lf = ts / norm

    return lf",python,"def load_factor(ts, resolution=None, norm=None):
    """"""
    Calculate the ratio of input vs. norm over a given interval.

    Parameters
    ----------
    ts : pandas.Series
        timeseries
    resolution : str, optional
        interval over which to calculate the ratio
        default: resolution of the input timeseries
    norm : int | float, optional
        denominator of the ratio
        default: the maximum of the input timeseries

    Returns
    -------
    pandas.Series
    """"""
    if norm is None:
        norm = ts.max()

    if resolution is not None:
        ts = ts.resample(rule=resolution).mean()

    lf = ts / norm

    return lf",def,load_factor,(,ts,",",resolution,=,None,",",norm,=,None,),:,if,norm,is,None,:,norm,=,ts,.,max,(,),if,resolution,is,not,None,:,ts,=,ts,.,resample,(,rule,=,resolution,),.,"Calculate the ratio of input vs. norm over a given interval.

    Parameters
    ----------
    ts : pandas.Series
        timeseries
    resolution : str, optional
        interval over which to calculate the ratio
        default: resolution of the input timeseries
    norm : int | float, optional
        denominator of the ratio
        default: the maximum of the input timeseries

    Returns
    -------
    pandas.Series",Calculate,the,ratio,of,input,vs,.,norm,over,a,given,interval,.,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L172-L199,train,mean,(,),lf,=,ts,/,norm,return,lf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/besthits.py,top_hits,"def top_hits(hits, num, column, reverse):
    """"""
    get top hits after sorting by column number
    """"""
    hits.sort(key = itemgetter(column), reverse = reverse)
    for hit in hits[0:num]:
        yield hit",python,"def top_hits(hits, num, column, reverse):
    """"""
    get top hits after sorting by column number
    """"""
    hits.sort(key = itemgetter(column), reverse = reverse)
    for hit in hits[0:num]:
        yield hit",def,top_hits,(,hits,",",num,",",column,",",reverse,),:,hits,.,sort,(,key,=,itemgetter,(,column,),",",reverse,=,reverse,),for,hit,in,hits,[,0,:,num,],:,yield,hit,,,,,get top hits after sorting by column number,get,top,hits,after,sorting,by,column,number,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L17-L23,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/besthits.py,numBlast_sort,"def numBlast_sort(blast, numHits, evalueT, bitT):
    """"""
    parse b6 output with sorting
    """"""
    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',
              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']
    yield header
    hmm = {h:[] for h in header}
    for line in blast:
        if line.startswith('#'):
            continue
        line = line.strip().split('\t')
        # Evalue and Bitscore thresholds
        line[10], line[11] = float(line[10]), float(line[11])
        evalue, bit = line[10], line[11]
        if evalueT is not False and evalue > evalueT:
            continue
        if bitT is not False and bit < bitT:
            continue
        for i, h in zip(line, header):
            hmm[h].append(i)
    hmm = pd.DataFrame(hmm)
    for query, df in hmm.groupby(by = ['#query']):
        df = df.sort_values(by = ['bitscore'], ascending = False)
        for hit in df[header].values[0:numHits]:
            yield hit",python,"def numBlast_sort(blast, numHits, evalueT, bitT):
    """"""
    parse b6 output with sorting
    """"""
    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',
              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']
    yield header
    hmm = {h:[] for h in header}
    for line in blast:
        if line.startswith('#'):
            continue
        line = line.strip().split('\t')
        # Evalue and Bitscore thresholds
        line[10], line[11] = float(line[10]), float(line[11])
        evalue, bit = line[10], line[11]
        if evalueT is not False and evalue > evalueT:
            continue
        if bitT is not False and bit < bitT:
            continue
        for i, h in zip(line, header):
            hmm[h].append(i)
    hmm = pd.DataFrame(hmm)
    for query, df in hmm.groupby(by = ['#query']):
        df = df.sort_values(by = ['bitscore'], ascending = False)
        for hit in df[header].values[0:numHits]:
            yield hit",def,numBlast_sort,(,blast,",",numHits,",",evalueT,",",bitT,),:,header,=,[,'#query',",",'target',",",'pident',",",'alen',",",'mismatch',",",'gapopen',",",'qstart',",",'qend',",",'tstart',",",'tend',",",'evalue',",",'bitscore',],yield,header,hmm,=,parse b6 output with sorting,parse,b6,output,with,sorting,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L25-L50,train,{,h,:,[,],for,h,in,header,},for,line,in,blast,:,if,line,.,startswith,(,'#',),:,continue,line,=,line,.,strip,(,,,,,,,,,,,,,,,,,,,,),.,split,(,'\t',),# Evalue and Bitscore thresholds,line,[,10,],",",line,[,11,],=,float,(,line,[,10,],),",",float,(,line,[,11,],),evalue,",",bit,=,line,[,10,],",",line,[,11,],if,evalueT,is,not,False,and,evalue,>,evalueT,:,continue,if,bitT,is,not,False,and,bit,<,bitT,:,continue,for,i,",",h,in,zip,(,line,",",header,),:,hmm,[,h,],.,append,(,i,),hmm,=,pd,.,DataFrame,(,hmm,),for,query,",",df,in,hmm,.,groupby,(,by,=,[,'#query',],),:,df,=,df,.,sort_values,(,by,=,[,'bitscore',],",",ascending,=,False,),for,hit,in,df,[,header,],.,values,[,0,:,numHits,],:,yield,hit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/besthits.py,numBlast,"def numBlast(blast, numHits, evalueT = False, bitT = False, sort = False):
    """"""
    parse b6 output
    """"""
    if sort is True:
        for hit in numBlast_sort(blast, numHits, evalueT, bitT):
            yield hit
        return
    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',
              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']
    yield header
    prev, hits = None, []
    for line in blast:
        line = line.strip().split('\t')
        ID = line[0]
        line[10], line[11] = float(line[10]), float(line[11])
        evalue, bit = line[10], line[11]
        if ID != prev:
            if len(hits) > 0:
                # column is 1 + line index
                for hit in top_hits(hits, numHits, 11, True):
                    yield hit
            hits = []
        if evalueT == False and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bit >= bitT:
            hits.append(line)
        elif evalueT == False and bit >= bitT:
            hits.append(line)
        prev = ID
    for hit in top_hits(hits, numHits, 11, True):
        yield hit",python,"def numBlast(blast, numHits, evalueT = False, bitT = False, sort = False):
    """"""
    parse b6 output
    """"""
    if sort is True:
        for hit in numBlast_sort(blast, numHits, evalueT, bitT):
            yield hit
        return
    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',
              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']
    yield header
    prev, hits = None, []
    for line in blast:
        line = line.strip().split('\t')
        ID = line[0]
        line[10], line[11] = float(line[10]), float(line[11])
        evalue, bit = line[10], line[11]
        if ID != prev:
            if len(hits) > 0:
                # column is 1 + line index
                for hit in top_hits(hits, numHits, 11, True):
                    yield hit
            hits = []
        if evalueT == False and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bit >= bitT:
            hits.append(line)
        elif evalueT == False and bit >= bitT:
            hits.append(line)
        prev = ID
    for hit in top_hits(hits, numHits, 11, True):
        yield hit",def,numBlast,(,blast,",",numHits,",",evalueT,=,False,",",bitT,=,False,",",sort,=,False,),:,if,sort,is,True,:,for,hit,in,numBlast_sort,(,blast,",",numHits,",",evalueT,",",bitT,),:,yield,hit,return,header,parse b6 output,parse,b6,output,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L52-L85,train,=,[,'#query',",",'target',",",'pident',",",'alen',",",'mismatch',",",'gapopen',",",'qstart',",",'qend',",",'tstart',",",'tend',",",'evalue',",",'bitscore',],yield,header,prev,",",,,,,,,,,,,,,,,,,,,,hits,=,None,",",[,],for,line,in,blast,:,line,=,line,.,strip,(,),.,split,(,'\t',),ID,=,line,[,0,],line,[,10,],",",line,[,11,],=,float,(,line,[,10,],),",",float,(,line,[,11,],),evalue,",",bit,=,line,[,10,],",",line,[,11,],if,ID,!=,prev,:,if,len,(,hits,),>,0,:,# column is 1 + line index,for,hit,in,top_hits,(,hits,",",numHits,",",11,",",True,),:,yield,hit,hits,=,[,],if,evalueT,==,False,and,bitT,==,False,:,hits,.,append,(,line,),elif,evalue,<=,evalueT,and,bitT,==,False,:,hits,.,append,(,line,),elif,evalue,<=,evalueT,and,bit,>=,bitT,:,hits,.,append,(,line,),elif,evalueT,==,False,and,bit,>=,bitT,:,hits,.,append,(,line,),prev,=,ID,for,hit,in,top_hits,(,hits,",",numHits,",",11,",",True,),:,yield,hit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/besthits.py,numDomtblout,"def numDomtblout(domtblout, numHits, evalueT, bitT, sort):
    """"""
    parse hmm domain table output
    this version is faster but does not work unless the table is sorted
    """"""
    if sort is True:
        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):
            yield hit
        return
    header = ['#target name', 'target accession', 'tlen',
              'query name', 'query accession', 'qlen',
              'full E-value', 'full score', 'full bias',
              'domain #', '# domains',
              'domain c-Evalue', 'domain i-Evalue', 'domain score', 'domain bias',
              'hmm from', 'hmm to', 'seq from', 'seq to', 'env from', 'env to',
              'acc', 'target description']
    yield header
    prev, hits = None, []
    for line in domtblout:
        if line.startswith('#'):
            continue
        # parse line and get description
        line = line.strip().split()
        desc = ' '.join(line[18:])
        line = line[0:18]
        line.append(desc)
        # create ID based on query name and domain number
        ID = line[0] + line[9]
        # domain c-Evalue and domain score thresholds
        line[11], line[13] = float(line[11]), float(line[13])
        evalue, bitscore = line[11], line[13]
        line[11], line[13] = evalue, bitscore
        if ID != prev:
            if len(hits) > 0:
                for hit in top_hits(hits, numHits, 13, True):
                    yield hit
            hits = []
        if evalueT == False and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bit >= bitT:
            hits.append(line)
        elif evalueT == False and bit >= bitT:
            hits.append(line)
        prev = ID
    for hit in top_hits(hits, numHits, 13, True):
        yield hit",python,"def numDomtblout(domtblout, numHits, evalueT, bitT, sort):
    """"""
    parse hmm domain table output
    this version is faster but does not work unless the table is sorted
    """"""
    if sort is True:
        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):
            yield hit
        return
    header = ['#target name', 'target accession', 'tlen',
              'query name', 'query accession', 'qlen',
              'full E-value', 'full score', 'full bias',
              'domain #', '# domains',
              'domain c-Evalue', 'domain i-Evalue', 'domain score', 'domain bias',
              'hmm from', 'hmm to', 'seq from', 'seq to', 'env from', 'env to',
              'acc', 'target description']
    yield header
    prev, hits = None, []
    for line in domtblout:
        if line.startswith('#'):
            continue
        # parse line and get description
        line = line.strip().split()
        desc = ' '.join(line[18:])
        line = line[0:18]
        line.append(desc)
        # create ID based on query name and domain number
        ID = line[0] + line[9]
        # domain c-Evalue and domain score thresholds
        line[11], line[13] = float(line[11]), float(line[13])
        evalue, bitscore = line[11], line[13]
        line[11], line[13] = evalue, bitscore
        if ID != prev:
            if len(hits) > 0:
                for hit in top_hits(hits, numHits, 13, True):
                    yield hit
            hits = []
        if evalueT == False and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bitT == False:
            hits.append(line)
        elif evalue <= evalueT and bit >= bitT:
            hits.append(line)
        elif evalueT == False and bit >= bitT:
            hits.append(line)
        prev = ID
    for hit in top_hits(hits, numHits, 13, True):
        yield hit",def,numDomtblout,(,domtblout,",",numHits,",",evalueT,",",bitT,",",sort,),:,if,sort,is,True,:,for,hit,in,numDomtblout_sort,(,domtblout,",",numHits,",",evalueT,",",bitT,),:,yield,hit,return,header,=,[,'#target name',",",'target accession',",","parse hmm domain table output
    this version is faster but does not work unless the table is sorted",parse,hmm,domain,table,output,this,version,is,faster,but,does,not,work,unless,the,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L121-L168,train,'tlen',",",'query name',",",'query accession',",",'qlen',",",'full E-value',",",'full score',",",'full bias',",",'domain #',",",'# domains',",",'domain c-Evalue',",",'domain i-Evalue',",",'domain score',",",'domain bias',",",'hmm from',",",'hmm to',",",table,is,sorted,,,,,,,,,,,,,,,,,'seq from',",",'seq to',",",'env from',",",'env to',",",'acc',",",'target description',],yield,header,prev,",",hits,=,None,",",[,],for,line,in,domtblout,:,if,line,.,startswith,(,'#',),:,continue,# parse line and get description,line,=,line,.,strip,(,),.,split,(,),desc,=,' ',.,join,(,line,[,18,:,],),line,=,line,[,0,:,18,],line,.,append,(,desc,),# create ID based on query name and domain number,ID,=,line,[,0,],+,line,[,9,],# domain c-Evalue and domain score thresholds,line,[,11,],",",line,[,13,],=,float,(,line,[,11,],),",",float,(,line,[,13,],),evalue,",",bitscore,=,line,[,11,],",",line,[,13,],line,[,11,],",",line,[,13,],=,evalue,",",bitscore,if,ID,!=,prev,:,if,len,(,hits,),>,0,:,for,hit,in,top_hits,(,hits,",",numHits,",",13,",",True,),:,yield,hit,hits,=,[,],if,evalueT,==,False,and,bitT,==,False,:,hits,.,append,(,line,),elif,evalue,<=,evalueT,and,bitT,==,False,:,hits,.,append,(,line,),elif,evalue,<=,evalueT,and,bit,>=,bitT,:,hits,.,append,(,line,),elif,evalueT,==,False,and,bit,>=,bitT,:,hits,.,append,(,line,),prev,=,ID,for,hit,in,top_hits,(,hits,",",numHits,",",13,",",True,),:,yield,hit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/stockholm2fa.py,stock2fa,"def stock2fa(stock):
    """"""
    convert stockholm to fasta
    """"""
    seqs = {}
    for line in stock:
        if line.startswith('#') is False and line.startswith(' ') is False and len(line) > 3:
            id, seq = line.strip().split()
            id = id.rsplit('/', 1)[0]
            id = re.split('[0-9]\|', id, 1)[-1]
            if id not in seqs:
                seqs[id] = []
            seqs[id].append(seq)
        if line.startswith('//'):
            break
    return seqs",python,"def stock2fa(stock):
    """"""
    convert stockholm to fasta
    """"""
    seqs = {}
    for line in stock:
        if line.startswith('#') is False and line.startswith(' ') is False and len(line) > 3:
            id, seq = line.strip().split()
            id = id.rsplit('/', 1)[0]
            id = re.split('[0-9]\|', id, 1)[-1]
            if id not in seqs:
                seqs[id] = []
            seqs[id].append(seq)
        if line.startswith('//'):
            break
    return seqs",def,stock2fa,(,stock,),:,seqs,=,{,},for,line,in,stock,:,if,line,.,startswith,(,'#',),is,False,and,line,.,startswith,(,' ',),is,False,and,len,(,line,),>,3,:,id,",",convert stockholm to fasta,convert,stockholm,to,fasta,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/stockholm2fa.py#L11-L26,train,seq,=,line,.,strip,(,),.,split,(,),id,=,id,.,rsplit,(,'/',",",1,),[,0,],id,=,re,.,split,(,,,,,,,,,,,,,,,,,,,,'[0-9]\|',",",id,",",1,),[,-,1,],if,id,not,in,seqs,:,seqs,[,id,],=,[,],seqs,[,id,],.,append,(,seq,),if,line,.,startswith,(,'//',),:,break,return,seqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/utils.py,week_schedule,"def week_schedule(index, on_time=None, off_time=None, off_days=None):
    """""" Return boolean time series following given week schedule.

    Parameters
    ----------
    index : pandas.DatetimeIndex
        Datetime index
    on_time : str or datetime.time
        Daily opening time. Default: '09:00'
    off_time : str or datetime.time
        Daily closing time. Default: '17:00'
    off_days : list of str
        List of weekdays. Default: ['Sunday', 'Monday']

    Returns
    -------
    pandas.Series of bool
        True when on, False otherwise for given datetime index

    Examples
    --------
    >>> import pandas as pd
    >>> from opengrid.library.utils import week_schedule
    >>> index = pd.date_range('20170701', '20170710', freq='H')
    >>> week_schedule(index)
    """"""
    if on_time is None:
        on_time = '9:00'
    if off_time is None:
        off_time = '17:00'
    if off_days is None:
        off_days = ['Sunday', 'Monday']
    if not isinstance(on_time, datetime.time):
        on_time = pd.to_datetime(on_time, format='%H:%M').time()
    if not isinstance(off_time, datetime.time):
        off_time = pd.to_datetime(off_time, format='%H:%M').time()
    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))
    return pd.Series(times, index=index)",python,"def week_schedule(index, on_time=None, off_time=None, off_days=None):
    """""" Return boolean time series following given week schedule.

    Parameters
    ----------
    index : pandas.DatetimeIndex
        Datetime index
    on_time : str or datetime.time
        Daily opening time. Default: '09:00'
    off_time : str or datetime.time
        Daily closing time. Default: '17:00'
    off_days : list of str
        List of weekdays. Default: ['Sunday', 'Monday']

    Returns
    -------
    pandas.Series of bool
        True when on, False otherwise for given datetime index

    Examples
    --------
    >>> import pandas as pd
    >>> from opengrid.library.utils import week_schedule
    >>> index = pd.date_range('20170701', '20170710', freq='H')
    >>> week_schedule(index)
    """"""
    if on_time is None:
        on_time = '9:00'
    if off_time is None:
        off_time = '17:00'
    if off_days is None:
        off_days = ['Sunday', 'Monday']
    if not isinstance(on_time, datetime.time):
        on_time = pd.to_datetime(on_time, format='%H:%M').time()
    if not isinstance(off_time, datetime.time):
        off_time = pd.to_datetime(off_time, format='%H:%M').time()
    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))
    return pd.Series(times, index=index)",def,week_schedule,(,index,",",on_time,=,None,",",off_time,=,None,",",off_days,=,None,),:,if,on_time,is,None,:,on_time,=,'9:00',if,off_time,is,None,:,off_time,=,'17:00',if,off_days,is,None,:,off_days,=,[,'Sunday',"Return boolean time series following given week schedule.

    Parameters
    ----------
    index : pandas.DatetimeIndex
        Datetime index
    on_time : str or datetime.time
        Daily opening time. Default: '09:00'
    off_time : str or datetime.time
        Daily closing time. Default: '17:00'
    off_days : list of str
        List of weekdays. Default: ['Sunday', 'Monday']

    Returns
    -------
    pandas.Series of bool
        True when on, False otherwise for given datetime index

    Examples
    --------
    >>> import pandas as pd
    >>> from opengrid.library.utils import week_schedule
    >>> index = pd.date_range('20170701', '20170710', freq='H')
    >>> week_schedule(index)",Return,boolean,time,series,following,given,week,schedule,.,,,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/utils.py#L10-L47,train,",",'Monday',],if,not,isinstance,(,on_time,",",datetime,.,time,),:,on_time,=,pd,.,to_datetime,(,on_time,",",format,=,'%H:%M',),.,time,(,),,,,,,,,,,,,,,,,,,,,if,not,isinstance,(,off_time,",",datetime,.,time,),:,off_time,=,pd,.,to_datetime,(,off_time,",",format,=,'%H:%M',),.,time,(,),times,=,(,index,.,time,>=,on_time,),&,(,index,.,time,<,off_time,),&,(,~,index,.,weekday_name,.,isin,(,off_days,),),return,pd,.,Series,(,times,",",index,=,index,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/plotting.py,carpet,"def carpet(timeseries, **kwargs):
    """"""
    Draw a carpet plot of a pandas timeseries.

    The carpet plot reads like a letter. Every day one line is added to the
    bottom of the figure, minute for minute moving from left (morning) to right
    (evening).
    The color denotes the level of consumption and is scaled logarithmically.
    If vmin and vmax are not provided as inputs, the minimum and maximum of the
    colorbar represent the minimum and maximum of the (resampled) timeseries.

    Parameters
    ----------
    timeseries : pandas.Series
    vmin, vmax : If not None, either or both of these values determine the range
    of the z axis. If None, the range is given by the minimum and/or maximum
    of the (resampled) timeseries.
    zlabel, title : If not None, these determine the labels of z axis and/or
    title. If None, the name of the timeseries is used if defined.
    cmap : matplotlib.cm instance, default coolwarm

    Examples
    --------
    >>> import numpy as np
    >>> import pandas as pd
    >>> from opengrid.library import plotting
    >>> plt = plotting.plot_style()
    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')
    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')
    >>> im = plotting.carpet(ser)
    """"""

    # define optional input parameters
    cmap = kwargs.pop('cmap', cm.coolwarm)
    norm = kwargs.pop('norm', LogNorm())
    interpolation = kwargs.pop('interpolation', 'nearest')
    cblabel = kwargs.pop('zlabel', timeseries.name if timeseries.name else '')
    title = kwargs.pop('title', 'carpet plot: ' + timeseries.name if timeseries.name else '')

    # data preparation
    if timeseries.dropna().empty:
        print('skipped {} - no data'.format(title))
        return
    ts = timeseries.resample('15min').interpolate()
    vmin = max(0.1, kwargs.pop('vmin', ts[ts > 0].min()))
    vmax = max(vmin, kwargs.pop('vmax', ts.quantile(.999)))

    # convert to dataframe with date as index and time as columns by
    # first replacing the index by a MultiIndex
    mpldatetimes = date2num(ts.index.to_pydatetime())
    ts.index = pd.MultiIndex.from_arrays(
        [np.floor(mpldatetimes), 2 + mpldatetimes % 1])  # '2 +': matplotlib bug workaround.
    # and then unstacking the second index level to columns
    df = ts.unstack()

    # data plotting

    fig, ax = plt.subplots()
    # define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks)
    extent = [df.columns[0], df.columns[-1], df.index[-1] + 0.5, df.index[0] - 0.5]
    im = plt.imshow(df, vmin=vmin, vmax=vmax, extent=extent, cmap=cmap, aspect='auto', norm=norm,
                    interpolation=interpolation, **kwargs)

    # figure formatting

    # x axis
    ax.xaxis_date()
    ax.xaxis.set_major_locator(HourLocator(interval=2))
    ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))
    ax.xaxis.grid(True)
    plt.xlabel('UTC Time')

    # y axis
    ax.yaxis_date()
    dmin, dmax = ax.yaxis.get_data_interval()
    number_of_days = (num2date(dmax) - num2date(dmin)).days
    # AutoDateLocator is not suited in case few data is available
    if abs(number_of_days) <= 35:
        ax.yaxis.set_major_locator(DayLocator())
    else:
        ax.yaxis.set_major_locator(AutoDateLocator())
    ax.yaxis.set_major_formatter(DateFormatter(""%a, %d %b %Y""))

    # plot colorbar
    cbticks = np.logspace(np.log10(vmin), np.log10(vmax), 11, endpoint=True)
    cb = plt.colorbar(format='%.0f', ticks=cbticks)
    cb.set_label(cblabel)

    # plot title
    plt.title(title)

    return im",python,"def carpet(timeseries, **kwargs):
    """"""
    Draw a carpet plot of a pandas timeseries.

    The carpet plot reads like a letter. Every day one line is added to the
    bottom of the figure, minute for minute moving from left (morning) to right
    (evening).
    The color denotes the level of consumption and is scaled logarithmically.
    If vmin and vmax are not provided as inputs, the minimum and maximum of the
    colorbar represent the minimum and maximum of the (resampled) timeseries.

    Parameters
    ----------
    timeseries : pandas.Series
    vmin, vmax : If not None, either or both of these values determine the range
    of the z axis. If None, the range is given by the minimum and/or maximum
    of the (resampled) timeseries.
    zlabel, title : If not None, these determine the labels of z axis and/or
    title. If None, the name of the timeseries is used if defined.
    cmap : matplotlib.cm instance, default coolwarm

    Examples
    --------
    >>> import numpy as np
    >>> import pandas as pd
    >>> from opengrid.library import plotting
    >>> plt = plotting.plot_style()
    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')
    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')
    >>> im = plotting.carpet(ser)
    """"""

    # define optional input parameters
    cmap = kwargs.pop('cmap', cm.coolwarm)
    norm = kwargs.pop('norm', LogNorm())
    interpolation = kwargs.pop('interpolation', 'nearest')
    cblabel = kwargs.pop('zlabel', timeseries.name if timeseries.name else '')
    title = kwargs.pop('title', 'carpet plot: ' + timeseries.name if timeseries.name else '')

    # data preparation
    if timeseries.dropna().empty:
        print('skipped {} - no data'.format(title))
        return
    ts = timeseries.resample('15min').interpolate()
    vmin = max(0.1, kwargs.pop('vmin', ts[ts > 0].min()))
    vmax = max(vmin, kwargs.pop('vmax', ts.quantile(.999)))

    # convert to dataframe with date as index and time as columns by
    # first replacing the index by a MultiIndex
    mpldatetimes = date2num(ts.index.to_pydatetime())
    ts.index = pd.MultiIndex.from_arrays(
        [np.floor(mpldatetimes), 2 + mpldatetimes % 1])  # '2 +': matplotlib bug workaround.
    # and then unstacking the second index level to columns
    df = ts.unstack()

    # data plotting

    fig, ax = plt.subplots()
    # define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks)
    extent = [df.columns[0], df.columns[-1], df.index[-1] + 0.5, df.index[0] - 0.5]
    im = plt.imshow(df, vmin=vmin, vmax=vmax, extent=extent, cmap=cmap, aspect='auto', norm=norm,
                    interpolation=interpolation, **kwargs)

    # figure formatting

    # x axis
    ax.xaxis_date()
    ax.xaxis.set_major_locator(HourLocator(interval=2))
    ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))
    ax.xaxis.grid(True)
    plt.xlabel('UTC Time')

    # y axis
    ax.yaxis_date()
    dmin, dmax = ax.yaxis.get_data_interval()
    number_of_days = (num2date(dmax) - num2date(dmin)).days
    # AutoDateLocator is not suited in case few data is available
    if abs(number_of_days) <= 35:
        ax.yaxis.set_major_locator(DayLocator())
    else:
        ax.yaxis.set_major_locator(AutoDateLocator())
    ax.yaxis.set_major_formatter(DateFormatter(""%a, %d %b %Y""))

    # plot colorbar
    cbticks = np.logspace(np.log10(vmin), np.log10(vmax), 11, endpoint=True)
    cb = plt.colorbar(format='%.0f', ticks=cbticks)
    cb.set_label(cblabel)

    # plot title
    plt.title(title)

    return im",def,carpet,(,timeseries,",",*,*,kwargs,),:,# define optional input parameters,cmap,=,kwargs,.,pop,(,'cmap',",",cm,.,coolwarm,),norm,=,kwargs,.,pop,(,'norm',",",LogNorm,(,),),interpolation,=,kwargs,.,pop,(,'interpolation',",","Draw a carpet plot of a pandas timeseries.

    The carpet plot reads like a letter. Every day one line is added to the
    bottom of the figure, minute for minute moving from left (morning) to right
    (evening).
    The color denotes the level of consumption and is scaled logarithmically.
    If vmin and vmax are not provided as inputs, the minimum and maximum of the
    colorbar represent the minimum and maximum of the (resampled) timeseries.

    Parameters
    ----------
    timeseries : pandas.Series
    vmin, vmax : If not None, either or both of these values determine the range
    of the z axis. If None, the range is given by the minimum and/or maximum
    of the (resampled) timeseries.
    zlabel, title : If not None, these determine the labels of z axis and/or
    title. If None, the name of the timeseries is used if defined.
    cmap : matplotlib.cm instance, default coolwarm

    Examples
    --------
    >>> import numpy as np
    >>> import pandas as pd
    >>> from opengrid.library import plotting
    >>> plt = plotting.plot_style()
    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')
    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')
    >>> im = plotting.carpet(ser)",Draw,a,carpet,plot,of,a,pandas,timeseries,.,,,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/plotting.py#L34-L125,train,'nearest',),cblabel,=,kwargs,.,pop,(,'zlabel',",",timeseries,.,name,if,timeseries,.,name,else,'',),title,=,kwargs,.,pop,(,'title',",",'carpet plot: ',+,,,,,,,,,,,,,,,,,,,,timeseries,.,name,if,timeseries,.,name,else,'',),# data preparation,if,timeseries,.,dropna,(,),.,empty,:,print,(,'skipped {} - no data',.,format,(,title,),),return,ts,=,timeseries,.,resample,(,'15min',),.,interpolate,(,),vmin,=,max,(,0.1,",",kwargs,.,pop,(,'vmin',",",ts,[,ts,>,0,],.,min,(,),),),vmax,=,max,(,vmin,",",kwargs,.,pop,(,'vmax',",",ts,.,quantile,(,.999,),),),# convert to dataframe with date as index and time as columns by,# first replacing the index by a MultiIndex,mpldatetimes,=,date2num,(,ts,.,index,.,to_pydatetime,(,),),ts,.,index,=,pd,.,MultiIndex,.,from_arrays,(,[,np,.,floor,(,mpldatetimes,),",",2,+,mpldatetimes,%,1,],),# '2 +': matplotlib bug workaround.,# and then unstacking the second index level to columns,df,=,ts,.,unstack,(,),# data plotting,fig,",",ax,=,plt,.,subplots,(,),# define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks),extent,=,[,df,.,columns,[,0,],",",df,.,columns,[,-,1,],",",df,.,index,[,-,1,],+,0.5,",",df,.,index,[,0,],-,0.5,],im,=,plt,.,imshow,(,df,",",vmin,=,vmin,",",vmax,=,vmax,",",extent,=,extent,",",cmap,=,cmap,",",aspect,=,'auto',",",norm,=,norm,",",interpolation,=,interpolation,",",*,*,kwargs,),# figure formatting,# x axis,ax,.,xaxis_date,(,),ax,.,xaxis,.,set_major_locator,(,HourLocator,(,interval,=,2,),),ax,.,xaxis,.,set_major_formatter,(,DateFormatter,(,'%H:%M',),),ax,.,xaxis,.,grid,(,True,),plt,.,xlabel,(,'UTC Time',),# y axis,ax,.,yaxis_date,(,),dmin,",",dmax,=,ax,.,yaxis,.,get_data_interval,(,),number_of_days,=,(,num2date,(,dmax,),-,num2date,(,dmin,),),.,days,# AutoDateLocator is not suited in case few data is available,if,abs,(,number_of_days,),<=,35,:,ax,.,yaxis,.,set_major_locator,(,DayLocator,(,),),else,:,ax,.,yaxis,,,,,,,,,,,,.,set_major_locator,(,AutoDateLocator,(,),),ax,.,yaxis,.,set_major_formatter,(,DateFormatter,(,"""%a, %d %b %Y""",),),# plot colorbar,cbticks,=,np,.,logspace,(,np,.,log10,(,vmin,),",",np,.,log10,(,vmax,),",",11,",",endpoint,=,True,),cb,=,plt,.,colorbar,(,format,=,'%.0f',",",ticks,=,cbticks,),cb,.,set_label,(,cblabel,),# plot title,plt,.,title,(,title,),return,im,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,calc_pident_ignore_gaps,"def calc_pident_ignore_gaps(a, b):
    """"""
    calculate percent identity
    """"""
    m = 0 # matches
    mm = 0 # mismatches
    for A, B in zip(list(a), list(b)):
        if A == '-' or A == '.' or B == '-' or B == '.':
            continue
        if A == B:
            m += 1
        else:
            mm += 1
    try:
        return float(float(m)/float((m + mm))) * 100
    except:
        return 0",python,"def calc_pident_ignore_gaps(a, b):
    """"""
    calculate percent identity
    """"""
    m = 0 # matches
    mm = 0 # mismatches
    for A, B in zip(list(a), list(b)):
        if A == '-' or A == '.' or B == '-' or B == '.':
            continue
        if A == B:
            m += 1
        else:
            mm += 1
    try:
        return float(float(m)/float((m + mm))) * 100
    except:
        return 0",def,calc_pident_ignore_gaps,(,a,",",b,),:,m,=,0,# matches,mm,=,0,# mismatches,for,A,",",B,in,zip,(,list,(,a,),",",list,(,b,),),:,if,A,==,'-',or,A,==,'.',or,calculate percent identity,calculate,percent,identity,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L34-L50,train,B,==,'-',or,B,==,'.',:,continue,if,A,==,B,:,m,+=,1,else,:,mm,+=,1,try,:,return,float,(,float,(,m,,,,,,,,,,,,,,,,,,,,),/,float,(,(,m,+,mm,),),),*,100,except,:,return,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,remove_gaps,"def remove_gaps(A, B):
    """"""
    skip column if either is a gap
    """"""
    a_seq, b_seq = [], []
    for a, b in zip(list(A), list(B)):
        if a == '-' or a == '.' or b == '-' or b == '.':
            continue
        a_seq.append(a)
        b_seq.append(b)
    return ''.join(a_seq), ''.join(b_seq)",python,"def remove_gaps(A, B):
    """"""
    skip column if either is a gap
    """"""
    a_seq, b_seq = [], []
    for a, b in zip(list(A), list(B)):
        if a == '-' or a == '.' or b == '-' or b == '.':
            continue
        a_seq.append(a)
        b_seq.append(b)
    return ''.join(a_seq), ''.join(b_seq)",def,remove_gaps,(,A,",",B,),:,a_seq,",",b_seq,=,[,],",",[,],for,a,",",b,in,zip,(,list,(,A,),",",list,(,B,),),:,if,a,==,'-',or,a,==,'.',skip column if either is a gap,skip,column,if,either,is,a,gap,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L52-L62,train,or,b,==,'-',or,b,==,'.',:,continue,a_seq,.,append,(,a,),b_seq,.,append,(,b,),return,'',.,join,(,a_seq,),",",,,,,,,,,,,,,,,,,,,,'',.,join,(,b_seq,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,compare_seqs,"def compare_seqs(seqs):
    """"""
    compare pairs of sequences
    """"""
    A, B, ignore_gaps = seqs
    a, b = A[1], B[1] # actual sequences
    if len(a) != len(b):
        print('# reads are not the same length', file=sys.stderr)
        exit()
    if ignore_gaps is True:
        pident = calc_pident_ignore_gaps(a, b)
    else:
        pident = calc_pident(a, b)
    return A[0], B[0], pident",python,"def compare_seqs(seqs):
    """"""
    compare pairs of sequences
    """"""
    A, B, ignore_gaps = seqs
    a, b = A[1], B[1] # actual sequences
    if len(a) != len(b):
        print('# reads are not the same length', file=sys.stderr)
        exit()
    if ignore_gaps is True:
        pident = calc_pident_ignore_gaps(a, b)
    else:
        pident = calc_pident(a, b)
    return A[0], B[0], pident",def,compare_seqs,(,seqs,),:,A,",",B,",",ignore_gaps,=,seqs,a,",",b,=,A,[,1,],",",B,[,1,],# actual sequences,if,len,(,a,),!=,len,(,b,),:,print,(,'# reads are not the same length',",",file,compare pairs of sequences,compare,pairs,of,sequences,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L64-L77,train,=,sys,.,stderr,),exit,(,),if,ignore_gaps,is,True,:,pident,=,calc_pident_ignore_gaps,(,a,",",b,),else,:,pident,=,calc_pident,(,a,",",b,,,,,,,,,,,,,,,,,,,,),return,A,[,0,],",",B,[,0,],",",pident,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,compare_seqs_leven,"def compare_seqs_leven(seqs):
    """"""
    calculate Levenshtein ratio of sequences
    """"""
    A, B, ignore_gaps = seqs
    a, b = remove_gaps(A[1], B[1]) # actual sequences
    if len(a) != len(b):
        print('# reads are not the same length', file=sys.stderr)
        exit()
    pident = lr(a, b) * 100
    return A[0], B[0], pident",python,"def compare_seqs_leven(seqs):
    """"""
    calculate Levenshtein ratio of sequences
    """"""
    A, B, ignore_gaps = seqs
    a, b = remove_gaps(A[1], B[1]) # actual sequences
    if len(a) != len(b):
        print('# reads are not the same length', file=sys.stderr)
        exit()
    pident = lr(a, b) * 100
    return A[0], B[0], pident",def,compare_seqs_leven,(,seqs,),:,A,",",B,",",ignore_gaps,=,seqs,a,",",b,=,remove_gaps,(,A,[,1,],",",B,[,1,],),# actual sequences,if,len,(,a,),!=,len,(,b,),:,print,(,calculate Levenshtein ratio of sequences,calculate,Levenshtein,ratio,of,sequences,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L79-L89,train,'# reads are not the same length',",",file,=,sys,.,stderr,),exit,(,),pident,=,lr,(,a,",",b,),*,100,return,A,[,0,],",",B,[,0,,,,,,,,,,,,,,,,,,,,],",",pident,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,pairwise_compare,"def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):
    """"""
    make pairwise sequence comparisons between aligned sequences
    """"""
    # load sequences into dictionary
    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index = True)}
    num_seqs = len(seqs)
    # define all pairs
    pairs = ((i[0], i[1], ignore_gaps) for i in itertools.combinations(list(seqs.values()), 2))
    pool = multithread(threads)
    # calc percent identity between all pairs - parallelize
    if leven is True:
        pident = pool.map(compare_seqs_leven, pairs)
    else:
        compare = pool.imap_unordered(compare_seqs, pairs)
        pident = [i for i in tqdm(compare, total = (num_seqs*num_seqs)/2)]
    pool.close()
    pool.terminate()
    pool.join()
    return to_dictionary(pident, print_list)",python,"def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):
    """"""
    make pairwise sequence comparisons between aligned sequences
    """"""
    # load sequences into dictionary
    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index = True)}
    num_seqs = len(seqs)
    # define all pairs
    pairs = ((i[0], i[1], ignore_gaps) for i in itertools.combinations(list(seqs.values()), 2))
    pool = multithread(threads)
    # calc percent identity between all pairs - parallelize
    if leven is True:
        pident = pool.map(compare_seqs_leven, pairs)
    else:
        compare = pool.imap_unordered(compare_seqs, pairs)
        pident = [i for i in tqdm(compare, total = (num_seqs*num_seqs)/2)]
    pool.close()
    pool.terminate()
    pool.join()
    return to_dictionary(pident, print_list)",def,pairwise_compare,(,afa,",",leven,",",threads,",",print_list,",",ignore_gaps,),:,# load sequences into dictionary,seqs,=,{,seq,[,0,],:,seq,for,seq,in,nr_fasta,(,[,afa,],",",append_index,=,True,),},num_seqs,=,len,(,seqs,make pairwise sequence comparisons between aligned sequences,make,pairwise,sequence,comparisons,between,aligned,sequences,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L91-L110,train,),# define all pairs,pairs,=,(,(,i,[,0,],",",i,[,1,],",",ignore_gaps,),for,i,in,itertools,.,combinations,(,list,(,seqs,.,values,,,,,,,,,,,,,,,,,,,,(,),),",",2,),),pool,=,multithread,(,threads,),# calc percent identity between all pairs - parallelize,if,leven,is,True,:,pident,=,pool,.,map,(,compare_seqs_leven,",",pairs,),else,:,compare,=,pool,.,imap_unordered,(,compare_seqs,",",pairs,),pident,=,[,i,for,i,in,tqdm,(,compare,",",total,=,(,num_seqs,*,num_seqs,),/,2,),],pool,.,close,(,),pool,.,terminate,(,),pool,.,join,(,),return,to_dictionary,(,pident,",",print_list,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,print_pairwise,"def print_pairwise(pw, median = False):
    """"""
    print matrix of pidents to stdout
    """"""
    names = sorted(set([i for i in pw]))
    if len(names) != 0:
        if '>' in names[0]:
            yield ['#'] + [i.split('>')[1] for i in names if '>' in i]
        else:
            yield ['#'] + names
        for a in names:
            if '>' in a:
                yield [a.split('>')[1]] + [pw[a][b] for b in names]
            else:
                out = []
                for b in names:
                    if b in pw[a]:
                        if median is False:
                            out.append(max(pw[a][b]))
                        else:
                            out.append(np.median(pw[a][b]))
                    else:
                        out.append('-')
                yield [a] + out",python,"def print_pairwise(pw, median = False):
    """"""
    print matrix of pidents to stdout
    """"""
    names = sorted(set([i for i in pw]))
    if len(names) != 0:
        if '>' in names[0]:
            yield ['#'] + [i.split('>')[1] for i in names if '>' in i]
        else:
            yield ['#'] + names
        for a in names:
            if '>' in a:
                yield [a.split('>')[1]] + [pw[a][b] for b in names]
            else:
                out = []
                for b in names:
                    if b in pw[a]:
                        if median is False:
                            out.append(max(pw[a][b]))
                        else:
                            out.append(np.median(pw[a][b]))
                    else:
                        out.append('-')
                yield [a] + out",def,print_pairwise,(,pw,",",median,=,False,),:,names,=,sorted,(,set,(,[,i,for,i,in,pw,],),),if,len,(,names,),!=,0,:,if,'>',in,names,[,0,],:,yield,[,print matrix of pidents to stdout,print,matrix,of,pidents,to,stdout,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L132-L155,train,'#',],+,[,i,.,split,(,'>',),[,1,],for,i,in,names,if,'>',in,i,],else,:,yield,[,'#',],+,names,,,,,,,,,,,,,,,,,,,,for,a,in,names,:,if,'>',in,a,:,yield,[,a,.,split,(,'>',),[,1,],],+,[,pw,[,a,],[,b,],for,b,in,names,],else,:,out,=,[,],for,b,in,names,:,if,b,in,pw,[,a,],:,if,median,is,False,:,out,.,append,(,max,(,pw,[,a,],[,b,],),),else,:,out,.,append,(,np,.,median,(,pw,[,a,],[,b,],),),else,:,out,.,append,(,'-',),yield,[,a,],+,out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,print_comps,"def print_comps(comps):
    """"""
    print stats for comparisons
    """"""
    if comps == []:
        print('n/a')
    else:
        print('# min: %s, max: %s, mean: %s' % \
            (min(comps), max(comps), np.mean(comps)))",python,"def print_comps(comps):
    """"""
    print stats for comparisons
    """"""
    if comps == []:
        print('n/a')
    else:
        print('# min: %s, max: %s, mean: %s' % \
            (min(comps), max(comps), np.mean(comps)))",def,print_comps,(,comps,),:,if,comps,==,[,],:,print,(,'n/a',),else,:,print,(,"'# min: %s, max: %s, mean: %s'",%,(,min,(,comps,),",",max,(,comps,),",",np,.,mean,(,comps,),),),,,print stats for comparisons,print,stats,for,comparisons,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L157-L165,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,compare_clades,"def compare_clades(pw):
    """"""
    print min. pident within each clade and then matrix of between-clade max.
    """"""
    names = sorted(set([i for i in pw]))
    for i in range(0, 4):
        wi, bt = {}, {}
        for a in names:
            for b in pw[a]:
                if ';' not in a or ';' not in b:
                    continue
                pident = pw[a][b]
                cA, cB = a.split(';')[i], b.split(';')[i]
                if i == 0 and '_' in cA and '_' in cB:
                    cA = cA.rsplit('_', 1)[1]
                    cB = cB.rsplit('_', 1)[1]
                elif '>' in cA or '>' in cB:
                    cA = cA.split('>')[1]
                    cB = cB.split('>')[1]
                if cA == cB:
                    if cA not in wi:
                        wi[cA] = []
                    wi[cA].append(pident)
                else:
                    if cA not in bt:
                        bt[cA] = {}
                    if cB not in bt[cA]:
                        bt[cA][cB] = []
                    bt[cA][cB].append(pident)
        print('\n# min. within')
        for clade, pidents in list(wi.items()):
            print('\t'.join(['wi:%s' % str(i), clade, str(min(pidents))]))
        # print matrix of maximum between groups
        comps = []
        print('\n# max. between')
        for comp in print_pairwise(bt):
            if comp is not None:
                print('\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))
                if comp[0] != '#':
                    comps.extend([j for j in comp[1:] if j != '-'])
        print_comps(comps)
        # print matrix of median between groups
        comps = []
        print('\n# median between')
        for comp in print_pairwise(bt, median = True):
            if comp is not None:
                print('\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))
                if comp[0] != '#':
                    comps.extend([j for j in comp[1:] if j != '-'])
        print_comps(comps)",python,"def compare_clades(pw):
    """"""
    print min. pident within each clade and then matrix of between-clade max.
    """"""
    names = sorted(set([i for i in pw]))
    for i in range(0, 4):
        wi, bt = {}, {}
        for a in names:
            for b in pw[a]:
                if ';' not in a or ';' not in b:
                    continue
                pident = pw[a][b]
                cA, cB = a.split(';')[i], b.split(';')[i]
                if i == 0 and '_' in cA and '_' in cB:
                    cA = cA.rsplit('_', 1)[1]
                    cB = cB.rsplit('_', 1)[1]
                elif '>' in cA or '>' in cB:
                    cA = cA.split('>')[1]
                    cB = cB.split('>')[1]
                if cA == cB:
                    if cA not in wi:
                        wi[cA] = []
                    wi[cA].append(pident)
                else:
                    if cA not in bt:
                        bt[cA] = {}
                    if cB not in bt[cA]:
                        bt[cA][cB] = []
                    bt[cA][cB].append(pident)
        print('\n# min. within')
        for clade, pidents in list(wi.items()):
            print('\t'.join(['wi:%s' % str(i), clade, str(min(pidents))]))
        # print matrix of maximum between groups
        comps = []
        print('\n# max. between')
        for comp in print_pairwise(bt):
            if comp is not None:
                print('\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))
                if comp[0] != '#':
                    comps.extend([j for j in comp[1:] if j != '-'])
        print_comps(comps)
        # print matrix of median between groups
        comps = []
        print('\n# median between')
        for comp in print_pairwise(bt, median = True):
            if comp is not None:
                print('\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))
                if comp[0] != '#':
                    comps.extend([j for j in comp[1:] if j != '-'])
        print_comps(comps)",def,compare_clades,(,pw,),:,names,=,sorted,(,set,(,[,i,for,i,in,pw,],),),for,i,in,range,(,0,",",4,),:,wi,",",bt,=,{,},",",{,},for,a,in,print min. pident within each clade and then matrix of between-clade max.,print,min,.,pident,within,each,clade,and,then,matrix,of,between,-,clade,max,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L167-L216,train,names,:,for,b,in,pw,[,a,],:,if,';',not,in,a,or,';',not,in,b,:,continue,pident,=,pw,[,a,],[,b,.,,,,,,,,,,,,,,,,,,,],cA,",",cB,=,a,.,split,(,';',),[,i,],",",b,.,split,(,';',),[,i,],if,i,==,0,and,'_',in,cA,and,'_',in,cB,:,cA,=,cA,.,rsplit,(,'_',",",1,),[,1,],cB,=,cB,.,rsplit,(,'_',",",1,),[,1,],elif,'>',in,cA,or,'>',in,cB,:,cA,=,cA,.,split,(,'>',),[,1,],cB,=,cB,.,split,(,'>',),[,1,],if,cA,==,cB,:,if,cA,not,in,wi,:,wi,[,cA,],=,[,],wi,[,cA,],.,append,(,pident,),else,:,if,cA,not,in,bt,:,bt,[,cA,],=,{,},if,cB,not,in,bt,[,cA,],:,bt,[,cA,],[,cB,],=,[,],bt,[,cA,],[,cB,],.,append,(,pident,),print,(,'\n# min. within',),for,clade,",",pidents,in,list,(,wi,.,items,(,),),:,print,(,'\t',.,join,(,[,'wi:%s',%,str,(,i,),",",clade,",",str,(,min,(,pidents,),),],),),# print matrix of maximum between groups,comps,=,[,],print,(,'\n# max. between',),for,comp,in,print_pairwise,(,bt,),:,if,comp,is,not,None,:,print,(,'\t',.,join,(,[,'bt:%s',%,str,(,i,),],+,[,str,(,j,),for,j,in,comp,],),),if,comp,[,0,],!=,'#',:,comps,.,extend,(,[,j,for,j,in,comp,[,1,:,],if,j,!=,'-',],),print_comps,(,comps,),# print matrix of median between groups,comps,=,[,],print,(,'\n# median between',),for,comp,in,print_pairwise,(,bt,",",median,=,True,),:,if,comp,is,not,None,:,print,(,'\t',,,,,,,,,,,,.,join,(,[,'bt:%s',%,str,(,i,),],+,[,str,(,j,),for,j,in,comp,],),),if,comp,[,0,],!=,'#',:,comps,.,extend,(,[,j,for,j,in,comp,[,1,:,],if,j,!=,'-',],),print_comps,(,comps,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/compare_aligned.py,matrix2dictionary,"def matrix2dictionary(matrix):
    """"""
    convert matrix to dictionary of comparisons
    """"""
    pw = {}
    for line in matrix:
        line = line.strip().split('\t')
        if line[0].startswith('#'):
            names = line[1:]
            continue
        a = line[0]
        for i, pident in enumerate(line[1:]):
            b = names[i]
            if a not in pw:
                pw[a] = {}
            if b not in pw:
                pw[b] = {}
            if pident != '-':
                pident = float(pident)
            pw[a][b] = pident
            pw[b][a] = pident
    return pw",python,"def matrix2dictionary(matrix):
    """"""
    convert matrix to dictionary of comparisons
    """"""
    pw = {}
    for line in matrix:
        line = line.strip().split('\t')
        if line[0].startswith('#'):
            names = line[1:]
            continue
        a = line[0]
        for i, pident in enumerate(line[1:]):
            b = names[i]
            if a not in pw:
                pw[a] = {}
            if b not in pw:
                pw[b] = {}
            if pident != '-':
                pident = float(pident)
            pw[a][b] = pident
            pw[b][a] = pident
    return pw",def,matrix2dictionary,(,matrix,),:,pw,=,{,},for,line,in,matrix,:,line,=,line,.,strip,(,),.,split,(,'\t',),if,line,[,0,],.,startswith,(,'#',),:,names,=,line,[,1,convert matrix to dictionary of comparisons,convert,matrix,to,dictionary,of,comparisons,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L218-L239,train,:,],continue,a,=,line,[,0,],for,i,",",pident,in,enumerate,(,line,[,1,:,],),:,b,=,names,[,i,],if,,,,,,,,,,,,,,,,,,,,a,not,in,pw,:,pw,[,a,],=,{,},if,b,not,in,pw,:,pw,[,b,],=,{,},if,pident,!=,'-',:,pident,=,float,(,pident,),pw,[,a,],[,b,],=,pident,pw,[,b,],[,a,],=,pident,return,pw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/commands.py,setoption,"def setoption(parser, metadata=None):
    """"""Set argument parser option.""""""
    parser.add_argument('-v', action='version',
                        version=__version__)
    subparsers = parser.add_subparsers(help='sub commands help')
    create_cmd = subparsers.add_parser('create')
    create_cmd.add_argument('name',
                            help='Specify Python package name.')
    create_cmd.add_argument('-d', dest='description', action='store',
                            help='Short description about your package.')
    create_cmd.add_argument('-a', dest='author', action='store',
                            required=True,
                            help='Python package author name.')
    create_cmd.add_argument('-e', dest='email', action='store',
                            required=True,
                            help='Python package author email address.')
    create_cmd.add_argument('-l', dest='license',
                            choices=metadata.licenses().keys(),
                            default='GPLv3+',
                            help='Specify license. (default: %(default)s)')
    create_cmd.add_argument('-s', dest='status',
                            choices=metadata.status().keys(),
                            default='Alpha',
                            help=('Specify development status. '
                                  '(default: %(default)s)'))
    create_cmd.add_argument('--no-check', action='store_true',
                            help='No checking package name in PyPI.')
    create_cmd.add_argument('--with-samples', action='store_true',
                            help='Generate package with sample code.')
    group = create_cmd.add_mutually_exclusive_group(required=True)
    group.add_argument('-U', dest='username', action='store',
                       help='Specify GitHub username.')
    group.add_argument('-u', dest='url', action='store', type=valid_url,
                       help='Python package homepage url.')
    create_cmd.add_argument('-o', dest='outdir', action='store',
                            default=os.path.abspath(os.path.curdir),
                            help='Specify output directory. (default: $PWD)')
    list_cmd = subparsers.add_parser('list')
    list_cmd.add_argument('-l', dest='licenses', action='store_true',
                          help='show license choices.')",python,"def setoption(parser, metadata=None):
    """"""Set argument parser option.""""""
    parser.add_argument('-v', action='version',
                        version=__version__)
    subparsers = parser.add_subparsers(help='sub commands help')
    create_cmd = subparsers.add_parser('create')
    create_cmd.add_argument('name',
                            help='Specify Python package name.')
    create_cmd.add_argument('-d', dest='description', action='store',
                            help='Short description about your package.')
    create_cmd.add_argument('-a', dest='author', action='store',
                            required=True,
                            help='Python package author name.')
    create_cmd.add_argument('-e', dest='email', action='store',
                            required=True,
                            help='Python package author email address.')
    create_cmd.add_argument('-l', dest='license',
                            choices=metadata.licenses().keys(),
                            default='GPLv3+',
                            help='Specify license. (default: %(default)s)')
    create_cmd.add_argument('-s', dest='status',
                            choices=metadata.status().keys(),
                            default='Alpha',
                            help=('Specify development status. '
                                  '(default: %(default)s)'))
    create_cmd.add_argument('--no-check', action='store_true',
                            help='No checking package name in PyPI.')
    create_cmd.add_argument('--with-samples', action='store_true',
                            help='Generate package with sample code.')
    group = create_cmd.add_mutually_exclusive_group(required=True)
    group.add_argument('-U', dest='username', action='store',
                       help='Specify GitHub username.')
    group.add_argument('-u', dest='url', action='store', type=valid_url,
                       help='Python package homepage url.')
    create_cmd.add_argument('-o', dest='outdir', action='store',
                            default=os.path.abspath(os.path.curdir),
                            help='Specify output directory. (default: $PWD)')
    list_cmd = subparsers.add_parser('list')
    list_cmd.add_argument('-l', dest='licenses', action='store_true',
                          help='show license choices.')",def,setoption,(,parser,",",metadata,=,None,),:,parser,.,add_argument,(,'-v',",",action,=,'version',",",version,=,__version__,),subparsers,=,parser,.,add_subparsers,(,help,=,'sub commands help',),create_cmd,=,subparsers,.,add_parser,(,'create',),create_cmd,Set argument parser option.,Set,argument,parser,option,.,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/commands.py#L12-L51,train,.,add_argument,(,'name',",",help,=,'Specify Python package name.',),create_cmd,.,add_argument,(,'-d',",",dest,=,'description',",",action,=,'store',",",help,=,'Short description about your package.',),create_cmd,.,add_argument,,,,,,,,,,,,,,,,,,,,(,'-a',",",dest,=,'author',",",action,=,'store',",",required,=,True,",",help,=,'Python package author name.',),create_cmd,.,add_argument,(,'-e',",",dest,=,'email',",",action,=,'store',",",required,=,True,",",help,=,'Python package author email address.',),create_cmd,.,add_argument,(,'-l',",",dest,=,'license',",",choices,=,metadata,.,licenses,(,),.,keys,(,),",",default,=,'GPLv3+',",",help,=,'Specify license. (default: %(default)s)',),create_cmd,.,add_argument,(,'-s',",",dest,=,'status',",",choices,=,metadata,.,status,(,),.,keys,(,),",",default,=,'Alpha',",",help,=,(,'Specify development status. ','(default: %(default)s)',),),create_cmd,.,add_argument,(,'--no-check',",",action,=,'store_true',",",help,=,'No checking package name in PyPI.',),create_cmd,.,add_argument,(,'--with-samples',",",action,=,'store_true',",",help,=,'Generate package with sample code.',),group,=,create_cmd,.,add_mutually_exclusive_group,(,required,=,True,),group,.,add_argument,(,'-U',",",dest,=,'username',",",action,=,'store',",",help,=,'Specify GitHub username.',),group,.,add_argument,(,'-u',",",dest,=,'url',",",action,=,'store',",",type,=,valid_url,",",help,=,'Python package homepage url.',),create_cmd,.,add_argument,(,'-o',",",dest,=,'outdir',",",action,=,'store',",",default,=,os,.,path,.,abspath,(,os,.,path,.,curdir,),",",help,=,'Specify output directory. (default: $PWD)',),list_cmd,=,subparsers,.,add_parser,(,'list',),list_cmd,.,add_argument,(,'-l',",",dest,=,'licenses',",",action,=,'store_true',",",help,=,'show license choices.',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/commands.py,parse_options,"def parse_options(metadata):
    """"""Parse argument options.""""""
    parser = argparse.ArgumentParser(description='%(prog)s usage:',
                                     prog=__prog__)
    setoption(parser, metadata=metadata)
    return parser",python,"def parse_options(metadata):
    """"""Parse argument options.""""""
    parser = argparse.ArgumentParser(description='%(prog)s usage:',
                                     prog=__prog__)
    setoption(parser, metadata=metadata)
    return parser",def,parse_options,(,metadata,),:,parser,=,argparse,.,ArgumentParser,(,description,=,'%(prog)s usage:',",",prog,=,__prog__,),setoption,(,parser,",",metadata,=,metadata,),return,parser,,,,,,,,,,,,,,Parse argument options.,Parse,argument,options,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/commands.py#L72-L77,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/commands.py,main,"def main():
    """"""Execute main processes.""""""
    try:
        pkg_version = Update()
        if pkg_version.updatable():
            pkg_version.show_message()
        metadata = control.retreive_metadata()
        parser = parse_options(metadata)
        argvs = sys.argv
        if len(argvs) <= 1:
            parser.print_help()
            sys.exit(1)
        args = parser.parse_args()
        control.print_licences(args, metadata)
        control.check_repository_existence(args)
        control.check_package_existence(args)
        control.generate_package(args)
    except (RuntimeError, BackendFailure, Conflict) as exc:
        sys.stderr.write('{0}\n'.format(exc))
        sys.exit(1)",python,"def main():
    """"""Execute main processes.""""""
    try:
        pkg_version = Update()
        if pkg_version.updatable():
            pkg_version.show_message()
        metadata = control.retreive_metadata()
        parser = parse_options(metadata)
        argvs = sys.argv
        if len(argvs) <= 1:
            parser.print_help()
            sys.exit(1)
        args = parser.parse_args()
        control.print_licences(args, metadata)
        control.check_repository_existence(args)
        control.check_package_existence(args)
        control.generate_package(args)
    except (RuntimeError, BackendFailure, Conflict) as exc:
        sys.stderr.write('{0}\n'.format(exc))
        sys.exit(1)",def,main,(,),:,try,:,pkg_version,=,Update,(,),if,pkg_version,.,updatable,(,),:,pkg_version,.,show_message,(,),metadata,=,control,.,retreive_metadata,(,),parser,=,parse_options,(,metadata,),argvs,=,sys,.,argv,if,Execute main processes.,Execute,main,processes,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/commands.py#L80-L99,train,len,(,argvs,),<=,1,:,parser,.,print_help,(,),sys,.,exit,(,1,),args,=,parser,.,parse_args,(,),control,.,print_licences,(,args,,,,,,,,,,,,,,,,,,,,",",metadata,),control,.,check_repository_existence,(,args,),control,.,check_package_existence,(,args,),control,.,generate_package,(,args,),except,(,RuntimeError,",",BackendFailure,",",Conflict,),as,exc,:,sys,.,stderr,.,write,(,'{0}\n',.,format,(,exc,),),sys,.,exit,(,1,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/package.py,PackageData._check_or_set_default_params,"def _check_or_set_default_params(self):
        """"""Check key and set default vaule when it does not exists.""""""
        if not hasattr(self, 'date'):
            self._set_param('date', datetime.utcnow().strftime('%Y-%m-%d'))
        if not hasattr(self, 'version'):
            self._set_param('version', self.default_version)
        # pylint: disable=no-member
        if not hasattr(self, 'description') or self.description is None:
            getattr(self, '_set_param')('description', self.warning_message)",python,"def _check_or_set_default_params(self):
        """"""Check key and set default vaule when it does not exists.""""""
        if not hasattr(self, 'date'):
            self._set_param('date', datetime.utcnow().strftime('%Y-%m-%d'))
        if not hasattr(self, 'version'):
            self._set_param('version', self.default_version)
        # pylint: disable=no-member
        if not hasattr(self, 'description') or self.description is None:
            getattr(self, '_set_param')('description', self.warning_message)",def,_check_or_set_default_params,(,self,),:,if,not,hasattr,(,self,",",'date',),:,self,.,_set_param,(,'date',",",datetime,.,utcnow,(,),.,strftime,(,'%Y-%m-%d',),),if,not,hasattr,(,self,",",'version',),:,self,.,Check key and set default vaule when it does not exists.,Check,key,and,set,default,vaule,when,it,does,not,exists,.,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/package.py#L44-L52,train,_set_param,(,'version',",",self,.,default_version,),# pylint: disable=no-member,if,not,hasattr,(,self,",",'description',),or,self,.,description,is,None,:,getattr,(,self,",",'_set_param',),,,,,,,,,,,,,,,,,,,,(,'description',",",self,.,warning_message,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/package.py,PackageTree.move,"def move(self):
        """"""Move directory from working directory to output directory.""""""
        if not os.path.isdir(self.outdir):
            os.makedirs(self.outdir)
        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))",python,"def move(self):
        """"""Move directory from working directory to output directory.""""""
        if not os.path.isdir(self.outdir):
            os.makedirs(self.outdir)
        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))",def,move,(,self,),:,if,not,os,.,path,.,isdir,(,self,.,outdir,),:,os,.,makedirs,(,self,.,outdir,),shutil,.,move,(,self,.,tmpdir,",",os,.,path,.,join,(,self,.,Move directory from working directory to output directory.,Move,directory,from,working,directory,to,output,directory,.,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/package.py#L169-L173,train,outdir,",",self,.,name,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/package.py,PackageTree.vcs_init,"def vcs_init(self):
        """"""Initialize VCS repository.""""""
        VCS(os.path.join(self.outdir, self.name), self.pkg_data)",python,"def vcs_init(self):
        """"""Initialize VCS repository.""""""
        VCS(os.path.join(self.outdir, self.name), self.pkg_data)",def,vcs_init,(,self,),:,VCS,(,os,.,path,.,join,(,self,.,outdir,",",self,.,name,),",",self,.,pkg_data,),,,,,,,,,,,,,,,,,Initialize VCS repository.,Initialize,VCS,repository,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/package.py#L185-L187,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/winutils.py,find_steam_location,"def find_steam_location():
  """"""
  Finds the location of the current Steam installation on Windows machines.
  Returns None for any non-Windows machines, or for Windows machines where
  Steam is not installed.
  """"""
  if registry is None:
    return None

  key = registry.CreateKey(registry.HKEY_CURRENT_USER,""Software\Valve\Steam"")
  return registry.QueryValueEx(key,""SteamPath"")[0]",python,"def find_steam_location():
  """"""
  Finds the location of the current Steam installation on Windows machines.
  Returns None for any non-Windows machines, or for Windows machines where
  Steam is not installed.
  """"""
  if registry is None:
    return None

  key = registry.CreateKey(registry.HKEY_CURRENT_USER,""Software\Valve\Steam"")
  return registry.QueryValueEx(key,""SteamPath"")[0]",def,find_steam_location,(,),:,if,registry,is,None,:,return,None,key,=,registry,.,CreateKey,(,registry,.,HKEY_CURRENT_USER,",","""Software\Valve\Steam""",),return,registry,.,QueryValueEx,(,key,",","""SteamPath""",),[,0,],,,,,,,,"Finds the location of the current Steam installation on Windows machines.
  Returns None for any non-Windows machines, or for Windows machines where
  Steam is not installed.",Finds,the,location,of,the,current,Steam,installation,on,Windows,machines,.,Returns,None,for,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/winutils.py#L10-L20,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,any,non,-,Windows,machines,or,for,Windows,machines,where,Steam,is,not,installed,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/PCoA_bubble.py,plot_PCoA,"def plot_PCoA(cat_data, otu_name, unifrac, names, colors, xr, yr, outDir,
              save_as, plot_style):
    """"""
    Plot PCoA principal coordinates scaled by the relative abundances of
    otu_name.
    """"""
    fig = plt.figure(figsize=(14, 8))
    ax = fig.add_subplot(111)

    for i, cat in enumerate(cat_data):
        plt.scatter(cat_data[cat][""pc1""], cat_data[cat][""pc2""], cat_data[cat][""size""],
                    color=colors[cat], alpha=0.85, marker=""o"", edgecolor=""black"",
                    label=cat)
    lgnd = plt.legend(loc=""best"", scatterpoints=3, fontsize=13)
    for i in range(len(colors.keys())):
        lgnd.legendHandles[i]._sizes = [80]  # Change the legend marker size manually
    plt.title("" "".join(otu_name.split(""_"")), style=""italic"")
    plt.ylabel(""PC2 (Percent Explained Variance {:.3f}%)"".format(float(unifrac[""varexp""][1])))
    plt.xlabel(""PC1 (Percent Explained Variance {:.3f}%)"".format(float(unifrac[""varexp""][0])))
    plt.xlim(round(xr[0]*1.5, 1), round(xr[1]*1.5, 1))
    plt.ylim(round(yr[0]*1.5, 1), round(yr[1]*1.5, 1))
    if plot_style:
        gu.ggplot2_style(ax)
        fc = ""0.8""
    else:
        fc = ""none""
    fig.savefig(os.path.join(outDir, ""_"".join(otu_name.split())) + ""."" + save_as,
                facecolor=fc, edgecolor=""none"", format=save_as,
                bbox_inches=""tight"", pad_inches=0.2)
    plt.close(fig)",python,"def plot_PCoA(cat_data, otu_name, unifrac, names, colors, xr, yr, outDir,
              save_as, plot_style):
    """"""
    Plot PCoA principal coordinates scaled by the relative abundances of
    otu_name.
    """"""
    fig = plt.figure(figsize=(14, 8))
    ax = fig.add_subplot(111)

    for i, cat in enumerate(cat_data):
        plt.scatter(cat_data[cat][""pc1""], cat_data[cat][""pc2""], cat_data[cat][""size""],
                    color=colors[cat], alpha=0.85, marker=""o"", edgecolor=""black"",
                    label=cat)
    lgnd = plt.legend(loc=""best"", scatterpoints=3, fontsize=13)
    for i in range(len(colors.keys())):
        lgnd.legendHandles[i]._sizes = [80]  # Change the legend marker size manually
    plt.title("" "".join(otu_name.split(""_"")), style=""italic"")
    plt.ylabel(""PC2 (Percent Explained Variance {:.3f}%)"".format(float(unifrac[""varexp""][1])))
    plt.xlabel(""PC1 (Percent Explained Variance {:.3f}%)"".format(float(unifrac[""varexp""][0])))
    plt.xlim(round(xr[0]*1.5, 1), round(xr[1]*1.5, 1))
    plt.ylim(round(yr[0]*1.5, 1), round(yr[1]*1.5, 1))
    if plot_style:
        gu.ggplot2_style(ax)
        fc = ""0.8""
    else:
        fc = ""none""
    fig.savefig(os.path.join(outDir, ""_"".join(otu_name.split())) + ""."" + save_as,
                facecolor=fc, edgecolor=""none"", format=save_as,
                bbox_inches=""tight"", pad_inches=0.2)
    plt.close(fig)",def,plot_PCoA,(,cat_data,",",otu_name,",",unifrac,",",names,",",colors,",",xr,",",yr,",",outDir,",",save_as,",",plot_style,),:,fig,=,plt,.,figure,(,figsize,=,(,14,",",8,),),ax,=,fig,.,add_subplot,"Plot PCoA principal coordinates scaled by the relative abundances of
    otu_name.",Plot,PCoA,principal,coordinates,scaled,by,the,relative,abundances,of,otu_name,.,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/PCoA_bubble.py#L36-L65,train,(,111,),for,i,",",cat,in,enumerate,(,cat_data,),:,plt,.,scatter,(,cat_data,[,cat,],[,"""pc1""",],",",cat_data,[,cat,],[,,,,,,,,,,,,,,,,,,,,"""pc2""",],",",cat_data,[,cat,],[,"""size""",],",",color,=,colors,[,cat,],",",alpha,=,0.85,",",marker,=,"""o""",",",edgecolor,=,"""black""",",",label,=,cat,),lgnd,=,plt,.,legend,(,loc,=,"""best""",",",scatterpoints,=,3,",",fontsize,=,13,),for,i,in,range,(,len,(,colors,.,keys,(,),),),:,lgnd,.,legendHandles,[,i,],.,_sizes,=,[,80,],# Change the legend marker size manually,plt,.,title,(,""" """,.,join,(,otu_name,.,split,(,"""_""",),),",",style,=,"""italic""",),plt,.,ylabel,(,"""PC2 (Percent Explained Variance {:.3f}%)""",.,format,(,float,(,unifrac,[,"""varexp""",],[,1,],),),),plt,.,xlabel,(,"""PC1 (Percent Explained Variance {:.3f}%)""",.,format,(,float,(,unifrac,[,"""varexp""",],[,0,],),),),plt,.,xlim,(,round,(,xr,[,0,],*,1.5,",",1,),",",round,(,xr,[,1,],*,1.5,",",1,),),plt,.,ylim,(,round,(,yr,[,0,],*,1.5,",",1,),",",round,(,yr,[,1,],*,1.5,",",1,),),if,plot_style,:,gu,.,ggplot2_style,(,ax,),fc,=,"""0.8""",else,:,fc,=,"""none""",fig,.,savefig,(,os,.,path,.,join,(,outDir,",","""_""",.,join,(,otu_name,.,split,(,),),),+,""".""",+,save_as,",",facecolor,=,fc,",",edgecolor,=,"""none""",",",format,=,save_as,",",bbox_inches,=,"""tight""",",",pad_inches,=,0.2,),plt,.,close,(,fig,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/transpose_biom.py,split_by_category,"def split_by_category(biom_cols, mapping, category_id):
    """"""
    Split up the column data in a biom table by mapping category value.
    """"""
    columns = defaultdict(list)
    for i, col in enumerate(biom_cols):
        columns[mapping[col['id']][category_id]].append((i, col))

    return columns",python,"def split_by_category(biom_cols, mapping, category_id):
    """"""
    Split up the column data in a biom table by mapping category value.
    """"""
    columns = defaultdict(list)
    for i, col in enumerate(biom_cols):
        columns[mapping[col['id']][category_id]].append((i, col))

    return columns",def,split_by_category,(,biom_cols,",",mapping,",",category_id,),:,columns,=,defaultdict,(,list,),for,i,",",col,in,enumerate,(,biom_cols,),:,columns,[,mapping,[,col,[,'id',],],[,category_id,],],.,append,(,(,Split up the column data in a biom table by mapping category value.,Split,up,the,column,data,in,a,biom,table,by,mapping,category,value,.,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/transpose_biom.py#L17-L25,train,i,",",col,),),return,columns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/stockholm2oneline.py,print_line,"def print_line(l):
    """"""
    print line if starts with ...
    """"""
    print_lines = ['# STOCKHOLM', '#=GF', '#=GS', ' ']
    if len(l.split()) == 0:
        return True
    for start in print_lines:
        if l.startswith(start):
            return True
    return False",python,"def print_line(l):
    """"""
    print line if starts with ...
    """"""
    print_lines = ['# STOCKHOLM', '#=GF', '#=GS', ' ']
    if len(l.split()) == 0:
        return True
    for start in print_lines:
        if l.startswith(start):
            return True
    return False",def,print_line,(,l,),:,print_lines,=,[,'# STOCKHOLM',",",'#=GF',",",'#=GS',",",' ',],if,len,(,l,.,split,(,),),==,0,:,return,True,for,start,in,print_lines,:,if,l,.,startswith,(,start,),print line if starts with ...,print,line,if,starts,with,...,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/stockholm2oneline.py#L11-L21,train,:,return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/stockholm2oneline.py,stock2one,"def stock2one(stock):
    """"""
    convert stockholm to single line format
    """"""
    lines = {}
    for line in stock:
        line = line.strip()
        if print_line(line) is True:
            yield line
            continue
        if line.startswith('//'):
            continue
        ID, seq = line.rsplit(' ', 1)
        if ID not in lines:
            lines[ID] = ''
        else:
            # remove preceding white space
            seq = seq.strip()
        lines[ID] += seq
    for ID, line in lines.items():
        yield '\t'.join([ID, line])
    yield '\n//'",python,"def stock2one(stock):
    """"""
    convert stockholm to single line format
    """"""
    lines = {}
    for line in stock:
        line = line.strip()
        if print_line(line) is True:
            yield line
            continue
        if line.startswith('//'):
            continue
        ID, seq = line.rsplit(' ', 1)
        if ID not in lines:
            lines[ID] = ''
        else:
            # remove preceding white space
            seq = seq.strip()
        lines[ID] += seq
    for ID, line in lines.items():
        yield '\t'.join([ID, line])
    yield '\n//'",def,stock2one,(,stock,),:,lines,=,{,},for,line,in,stock,:,line,=,line,.,strip,(,),if,print_line,(,line,),is,True,:,yield,line,continue,if,line,.,startswith,(,'//',),:,continue,ID,convert stockholm to single line format,convert,stockholm,to,single,line,format,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/stockholm2oneline.py#L23-L44,train,",",seq,=,line,.,rsplit,(,' ',",",1,),if,ID,not,in,lines,:,lines,[,ID,],=,'',else,:,# remove preceding white space,seq,=,seq,.,,,,,,,,,,,,,,,,,,,,strip,(,),lines,[,ID,],+=,seq,for,ID,",",line,in,lines,.,items,(,),:,yield,'\t',.,join,(,[,ID,",",line,],),yield,'\n//',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
elbow-jason/Uno-deprecated,uno/helpers.py,math_func,"def math_func(f):
    """"""
    Statics the methods. wut.
    """"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        if len(args) > 0:
            return_type = type(args[0])
        if kwargs.has_key('return_type'):
            return_type = kwargs['return_type']
            kwargs.pop('return_type')
            return return_type(f(*args, **kwargs))
        args = list((setify(x) for x in args))
        return return_type(f(*args, **kwargs))
    return wrapper",python,"def math_func(f):
    """"""
    Statics the methods. wut.
    """"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        if len(args) > 0:
            return_type = type(args[0])
        if kwargs.has_key('return_type'):
            return_type = kwargs['return_type']
            kwargs.pop('return_type')
            return return_type(f(*args, **kwargs))
        args = list((setify(x) for x in args))
        return return_type(f(*args, **kwargs))
    return wrapper",def,math_func,(,f,),:,@,wraps,(,f,),def,wrapper,(,*,args,",",*,*,kwargs,),:,if,len,(,args,),>,0,:,return_type,=,type,(,args,[,0,],),if,kwargs,.,has_key,Statics the methods. wut.,Statics,the,methods,.,wut,.,,,,,,,,,,4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4,https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/helpers.py#L8-L22,train,(,'return_type',),:,return_type,=,kwargs,[,'return_type',],kwargs,.,pop,(,'return_type',),return,return_type,(,f,(,*,args,",",*,*,kwargs,),),args,,,,,,,,,,,,,,,,,,,,=,list,(,(,setify,(,x,),for,x,in,args,),),return,return_type,(,f,(,*,args,",",*,*,kwargs,),),return,wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jay-johnson/network-pipeline,network_pipeline/scripts/icmp_send_msg.py,dump_stats,"def dump_stats(myStats):
    """"""
    Show stats when pings are done
    """"""
    print(""\n----%s PYTHON PING Statistics----"" % (myStats.thisIP))

    if myStats.pktsSent > 0:
        myStats.fracLoss = (myStats.pktsSent - myStats.pktsRcvd) \
                / myStats.pktsSent

    print((""%d packets transmitted, %d packets received, ""
           ""%0.1f%% packet loss"") % (
        myStats.pktsSent,
        myStats.pktsRcvd,
        100.0 * myStats.fracLoss
    ))

    if myStats.pktsRcvd > 0:
        print(""round-trip (ms)  min/avg/max = %d/%0.1f/%d"" % (
            myStats.minTime,
            myStats.totTime / myStats.pktsRcvd,
            myStats.maxTime
        ))

    print("""")
    return",python,"def dump_stats(myStats):
    """"""
    Show stats when pings are done
    """"""
    print(""\n----%s PYTHON PING Statistics----"" % (myStats.thisIP))

    if myStats.pktsSent > 0:
        myStats.fracLoss = (myStats.pktsSent - myStats.pktsRcvd) \
                / myStats.pktsSent

    print((""%d packets transmitted, %d packets received, ""
           ""%0.1f%% packet loss"") % (
        myStats.pktsSent,
        myStats.pktsRcvd,
        100.0 * myStats.fracLoss
    ))

    if myStats.pktsRcvd > 0:
        print(""round-trip (ms)  min/avg/max = %d/%0.1f/%d"" % (
            myStats.minTime,
            myStats.totTime / myStats.pktsRcvd,
            myStats.maxTime
        ))

    print("""")
    return",def,dump_stats,(,myStats,),:,print,(,"""\n----%s PYTHON PING Statistics----""",%,(,myStats,.,thisIP,),),if,myStats,.,pktsSent,>,0,:,myStats,.,fracLoss,=,(,myStats,.,pktsSent,-,myStats,.,pktsRcvd,),/,myStats,.,pktsSent,print,(,(,Show stats when pings are done,Show,stats,when,pings,are,done,,,,,,,,,,4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa,https://github.com/jay-johnson/network-pipeline/blob/4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa/network_pipeline/scripts/icmp_send_msg.py#L470-L495,train,"""%d packets transmitted, %d packets received, ""","""%0.1f%% packet loss""",),%,(,myStats,.,pktsSent,",",myStats,.,pktsRcvd,",",100.0,*,myStats,.,fracLoss,),),if,myStats,.,pktsRcvd,>,0,:,print,(,"""round-trip (ms)  min/avg/max = %d/%0.1f/%d""",,,,,,,,,,,,,,,,,,,,%,(,myStats,.,minTime,",",myStats,.,totTime,/,myStats,.,pktsRcvd,",",myStats,.,maxTime,),),print,(,"""""",),return,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/update.py,Update.updatable,"def updatable(self):
        """"""bootstrap-py package updatable?.""""""
        if self.latest_version > self.current_version:
            updatable_version = self.latest_version
        else:
            updatable_version = False
        return updatable_version",python,"def updatable(self):
        """"""bootstrap-py package updatable?.""""""
        if self.latest_version > self.current_version:
            updatable_version = self.latest_version
        else:
            updatable_version = False
        return updatable_version",def,updatable,(,self,),:,if,self,.,latest_version,>,self,.,current_version,:,updatable_version,=,self,.,latest_version,else,:,updatable_version,=,False,return,updatable_version,,,,,,,,,,,,,,,,,bootstrap-py package updatable?.,bootstrap,-,py,package,updatable?,.,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/update.py#L29-L35,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/update.py,Update.show_message,"def show_message(self):
        """"""Show message updatable.""""""
        print(
            'current version: {current_version}\n'
            'latest version : {latest_version}'.format(
                current_version=self.current_version,
                latest_version=self.latest_version))",python,"def show_message(self):
        """"""Show message updatable.""""""
        print(
            'current version: {current_version}\n'
            'latest version : {latest_version}'.format(
                current_version=self.current_version,
                latest_version=self.latest_version))",def,show_message,(,self,),:,print,(,'current version: {current_version}\n','latest version : {latest_version}',.,format,(,current_version,=,self,.,current_version,",",latest_version,=,self,.,latest_version,),),,,,,,,,,,,,,,,,,,Show message updatable.,Show,message,updatable,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/update.py#L37-L43,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/pick_otus_condense.py,condense_otus,"def condense_otus(otuF, nuniqueF):
    """"""
    Traverse the input otu-sequence file, collect the non-unique OTU IDs and
    file the sequences associated with then under the unique OTU ID as defined
    by the input matrix.

    :@type otuF: file
    :@param otuF: The output file from QIIME's pick_otus.py
    :@type nuniqueF: file
    :@param nuniqueF: The matrix of unique OTU IDs associated to the list of
                      non-unique OTU IDs they replaced.

    :@rtype: dict
    :@return: The new condensed table of unique OTU IDs and the sequence IDs
              associated with them.
    """"""
    uniqueOTUs = set()
    nuOTUs = {}

    # parse non-unique otu matrix
    for line in nuniqueF:
        line = line.split()
        uOTU = line[0]
        for nuOTU in line[1:]:
            nuOTUs[nuOTU] = uOTU
        uniqueOTUs.add(uOTU)

    otuFilter = defaultdict(list)
    # parse otu sequence file
    for line in otuF:
        line = line.split()
        otuID, seqIDs = line[0], line[1:]
        if otuID in uniqueOTUs:
            otuFilter[otuID].extend(seqIDs)
        elif otuID in nuOTUs:
            otuFilter[nuOTUs[otuID]].extend(seqIDs)

    return otuFilter",python,"def condense_otus(otuF, nuniqueF):
    """"""
    Traverse the input otu-sequence file, collect the non-unique OTU IDs and
    file the sequences associated with then under the unique OTU ID as defined
    by the input matrix.

    :@type otuF: file
    :@param otuF: The output file from QIIME's pick_otus.py
    :@type nuniqueF: file
    :@param nuniqueF: The matrix of unique OTU IDs associated to the list of
                      non-unique OTU IDs they replaced.

    :@rtype: dict
    :@return: The new condensed table of unique OTU IDs and the sequence IDs
              associated with them.
    """"""
    uniqueOTUs = set()
    nuOTUs = {}

    # parse non-unique otu matrix
    for line in nuniqueF:
        line = line.split()
        uOTU = line[0]
        for nuOTU in line[1:]:
            nuOTUs[nuOTU] = uOTU
        uniqueOTUs.add(uOTU)

    otuFilter = defaultdict(list)
    # parse otu sequence file
    for line in otuF:
        line = line.split()
        otuID, seqIDs = line[0], line[1:]
        if otuID in uniqueOTUs:
            otuFilter[otuID].extend(seqIDs)
        elif otuID in nuOTUs:
            otuFilter[nuOTUs[otuID]].extend(seqIDs)

    return otuFilter",def,condense_otus,(,otuF,",",nuniqueF,),:,uniqueOTUs,=,set,(,),nuOTUs,=,{,},# parse non-unique otu matrix,for,line,in,nuniqueF,:,line,=,line,.,split,(,),uOTU,=,line,[,0,],for,nuOTU,in,line,[,1,:,"Traverse the input otu-sequence file, collect the non-unique OTU IDs and
    file the sequences associated with then under the unique OTU ID as defined
    by the input matrix.

    :@type otuF: file
    :@param otuF: The output file from QIIME's pick_otus.py
    :@type nuniqueF: file
    :@param nuniqueF: The matrix of unique OTU IDs associated to the list of
                      non-unique OTU IDs they replaced.

    :@rtype: dict
    :@return: The new condensed table of unique OTU IDs and the sequence IDs
              associated with them.",Traverse,the,input,otu,-,sequence,file,collect,the,non,-,unique,OTU,IDs,and,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/pick_otus_condense.py#L14-L51,train,],:,nuOTUs,[,nuOTU,],=,uOTU,uniqueOTUs,.,add,(,uOTU,),otuFilter,=,defaultdict,(,list,),# parse otu sequence file,for,line,in,otuF,:,line,=,line,.,file,the,sequences,associated,with,then,under,the,unique,OTU,ID,as,defined,by,the,input,matrix,.,,split,(,),otuID,",",seqIDs,=,line,[,0,],",",line,[,1,:,],if,otuID,in,uniqueOTUs,:,otuFilter,[,otuID,],.,extend,(,seqIDs,),elif,otuID,in,nuOTUs,:,otuFilter,[,nuOTUs,[,otuID,],],.,extend,(,seqIDs,),return,otuFilter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_copies.py,rna_bases,"def rna_bases(rna_cov, scaffold, bases, line):
    """"""
    determine if read overlaps with rna, if so count bases
    """"""
    start = int(line[3])
    stop = start + bases - 1
    if scaffold not in rna_cov:
        return rna_cov
    for pos in rna_cov[scaffold][2]:
        ol = get_overlap([start, stop], pos)
        rna_cov[scaffold][0] += ol
    return rna_cov",python,"def rna_bases(rna_cov, scaffold, bases, line):
    """"""
    determine if read overlaps with rna, if so count bases
    """"""
    start = int(line[3])
    stop = start + bases - 1
    if scaffold not in rna_cov:
        return rna_cov
    for pos in rna_cov[scaffold][2]:
        ol = get_overlap([start, stop], pos)
        rna_cov[scaffold][0] += ol
    return rna_cov",def,rna_bases,(,rna_cov,",",scaffold,",",bases,",",line,),:,start,=,int,(,line,[,3,],),stop,=,start,+,bases,-,1,if,scaffold,not,in,rna_cov,:,return,rna_cov,for,pos,in,rna_cov,[,scaffold,],"determine if read overlaps with rna, if so count bases",determine,if,read,overlaps,with,rna,if,so,count,bases,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_copies.py#L18-L29,train,[,2,],:,ol,=,get_overlap,(,[,start,",",stop,],",",pos,),rna_cov,[,scaffold,],[,0,],+=,ol,return,rna_cov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_copies.py,parse_s2bins,"def parse_s2bins(s2bins):
    """"""
    parse ggKbase scaffold-to-bin mapping
        - scaffolds-to-bins and bins-to-scaffolds
    """"""
    s2b = {}
    b2s = {}
    for line in s2bins:
        line = line.strip().split()
        s, b = line[0], line[1]
        if 'UNK' in b:
            continue
        if len(line) > 2:
            g = ' '.join(line[2:])
        else:
            g = 'n/a'
        b = '%s\t%s' % (b, g)
        s2b[s] = b 
        if b not in b2s:
           b2s[b] = []
        b2s[b].append(s)
    return s2b, b2s",python,"def parse_s2bins(s2bins):
    """"""
    parse ggKbase scaffold-to-bin mapping
        - scaffolds-to-bins and bins-to-scaffolds
    """"""
    s2b = {}
    b2s = {}
    for line in s2bins:
        line = line.strip().split()
        s, b = line[0], line[1]
        if 'UNK' in b:
            continue
        if len(line) > 2:
            g = ' '.join(line[2:])
        else:
            g = 'n/a'
        b = '%s\t%s' % (b, g)
        s2b[s] = b 
        if b not in b2s:
           b2s[b] = []
        b2s[b].append(s)
    return s2b, b2s",def,parse_s2bins,(,s2bins,),:,s2b,=,{,},b2s,=,{,},for,line,in,s2bins,:,line,=,line,.,strip,(,),.,split,(,),s,",",b,=,line,[,0,],",",line,[,1,],"parse ggKbase scaffold-to-bin mapping
        - scaffolds-to-bins and bins-to-scaffolds",parse,ggKbase,scaffold,-,to,-,bin,mapping,-,scaffolds,-,to,-,bins,and,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_copies.py#L31-L52,train,if,'UNK',in,b,:,continue,if,len,(,line,),>,2,:,g,=,' ',.,join,(,line,[,2,:,],),else,:,g,=,bins,-,to,-,scaffolds,,,,,,,,,,,,,,,'n/a',b,=,'%s\t%s',%,(,b,",",g,),s2b,[,s,],=,b,if,b,not,in,b2s,:,b2s,[,b,],=,[,],b2s,[,b,],.,append,(,s,),return,s2b,",",b2s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_copies.py,filter_missing_rna,"def filter_missing_rna(s2bins, bins2s, rna_cov):
    """"""
    remove any bins that don't have 16S
    """"""
    for bin, scaffolds in list(bins2s.items()):
        c = 0
        for s in scaffolds:
            if s in rna_cov:
                c += 1
        if c == 0:
            del bins2s[bin]
    for scaffold, bin in list(s2bins.items()):
        if bin not in bins2s:
            del s2bins[scaffold]
    return s2bins, bins2s",python,"def filter_missing_rna(s2bins, bins2s, rna_cov):
    """"""
    remove any bins that don't have 16S
    """"""
    for bin, scaffolds in list(bins2s.items()):
        c = 0
        for s in scaffolds:
            if s in rna_cov:
                c += 1
        if c == 0:
            del bins2s[bin]
    for scaffold, bin in list(s2bins.items()):
        if bin not in bins2s:
            del s2bins[scaffold]
    return s2bins, bins2s",def,filter_missing_rna,(,s2bins,",",bins2s,",",rna_cov,),:,for,bin,",",scaffolds,in,list,(,bins2s,.,items,(,),),:,c,=,0,for,s,in,scaffolds,:,if,s,in,rna_cov,:,c,+=,1,if,c,==,remove any bins that don't have 16S,remove,any,bins,that,don,t,have,16S,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_copies.py#L76-L90,train,0,:,del,bins2s,[,bin,],for,scaffold,",",bin,in,list,(,s2bins,.,items,(,),),:,if,bin,not,in,bins2s,:,del,s2bins,[,,,,,,,,,,,,,,,,,,,,scaffold,],return,s2bins,",",bins2s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_copies.py,calc_bin_cov,"def calc_bin_cov(scaffolds, cov):
    """"""
    calculate bin coverage
    """"""
    bases = sum([cov[i][0] for i in scaffolds if i in cov])
    length = sum([cov[i][1] for i in scaffolds if i in cov])
    if length == 0:
        return 0
    return float(float(bases)/float(length))",python,"def calc_bin_cov(scaffolds, cov):
    """"""
    calculate bin coverage
    """"""
    bases = sum([cov[i][0] for i in scaffolds if i in cov])
    length = sum([cov[i][1] for i in scaffolds if i in cov])
    if length == 0:
        return 0
    return float(float(bases)/float(length))",def,calc_bin_cov,(,scaffolds,",",cov,),:,bases,=,sum,(,[,cov,[,i,],[,0,],for,i,in,scaffolds,if,i,in,cov,],),length,=,sum,(,[,cov,[,i,],[,1,],for,calculate bin coverage,calculate,bin,coverage,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_copies.py#L92-L100,train,i,in,scaffolds,if,i,in,cov,],),if,length,==,0,:,return,0,return,float,(,float,(,bases,),/,float,(,length,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dokterbob/django-multilingual-model,multilingual_model/forms.py,TranslationFormSet.clean,"def clean(self):
        """"""
        Make sure there is at least a translation has been filled in. If a
        default language has been specified, make sure that it exists amongst
        translations.
        """"""

        # First make sure the super's clean method is called upon.
        super(TranslationFormSet, self).clean()

        if settings.HIDE_LANGUAGE:
            return

        if len(self.forms) > 0:
            # If a default language has been provided, make sure a translation
            # is available

            if settings.DEFAULT_LANGUAGE and not any(self.errors):
                # Don't bother validating the formset unless each form is
                # valid on its own. Reference:
                # http://docs.djangoproject.com/en/dev/topics/forms/formsets/#custom-formset-validation

                for form in self.forms:
                    language_code = form.cleaned_data.get(
                        'language_code', None
                    )

                    if language_code == settings.DEFAULT_LANGUAGE:

                        # All is good, don't bother checking any further
                        return

                raise forms.ValidationError(_(
                    'No translation provided for default language \'%s\'.'
                ) % settings.DEFAULT_LANGUAGE)

        else:
            raise forms.ValidationError(
                _('At least one translation should be provided.')
            )",python,"def clean(self):
        """"""
        Make sure there is at least a translation has been filled in. If a
        default language has been specified, make sure that it exists amongst
        translations.
        """"""

        # First make sure the super's clean method is called upon.
        super(TranslationFormSet, self).clean()

        if settings.HIDE_LANGUAGE:
            return

        if len(self.forms) > 0:
            # If a default language has been provided, make sure a translation
            # is available

            if settings.DEFAULT_LANGUAGE and not any(self.errors):
                # Don't bother validating the formset unless each form is
                # valid on its own. Reference:
                # http://docs.djangoproject.com/en/dev/topics/forms/formsets/#custom-formset-validation

                for form in self.forms:
                    language_code = form.cleaned_data.get(
                        'language_code', None
                    )

                    if language_code == settings.DEFAULT_LANGUAGE:

                        # All is good, don't bother checking any further
                        return

                raise forms.ValidationError(_(
                    'No translation provided for default language \'%s\'.'
                ) % settings.DEFAULT_LANGUAGE)

        else:
            raise forms.ValidationError(
                _('At least one translation should be provided.')
            )",def,clean,(,self,),:,# First make sure the super's clean method is called upon.,super,(,TranslationFormSet,",",self,),.,clean,(,),if,settings,.,HIDE_LANGUAGE,:,return,if,len,(,self,.,forms,),>,0,:,"# If a default language has been provided, make sure a translation",# is available,if,settings,.,DEFAULT_LANGUAGE,and,not,any,(,"Make sure there is at least a translation has been filled in. If a
        default language has been specified, make sure that it exists amongst
        translations.",Make,sure,there,is,at,least,a,translation,has,been,filled,in,.,If,a,2479b2c3d6f7b697e95aa1e082c8bc8699f1f638,https://github.com/dokterbob/django-multilingual-model/blob/2479b2c3d6f7b697e95aa1e082c8bc8699f1f638/multilingual_model/forms.py#L19-L58,train,self,.,errors,),:,# Don't bother validating the formset unless each form is,# valid on its own. Reference:,# http://docs.djangoproject.com/en/dev/topics/forms/formsets/#custom-formset-validation,for,form,in,self,.,forms,:,language_code,=,form,.,cleaned_data,.,get,(,'language_code',",",None,),if,language_code,==,default,language,has,been,specified,make,sure,that,it,exists,amongst,translations,.,,,,,,,settings,.,DEFAULT_LANGUAGE,:,"# All is good, don't bother checking any further",return,raise,forms,.,ValidationError,(,_,(,'No translation provided for default language \'%s\'.',),%,settings,.,DEFAULT_LANGUAGE,),else,:,raise,forms,.,ValidationError,(,_,(,'At least one translation should be provided.',),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dokterbob/django-multilingual-model,multilingual_model/forms.py,TranslationFormSet._get_default_language,"def _get_default_language(self):
        """"""
        If a default language has been set, and is still available in
        `self.available_languages`, return it and remove it from the list.

        If not, simply pop the first available language.
        """"""

        assert hasattr(self, 'available_languages'), \
            'No available languages have been generated.'
        assert len(self.available_languages) > 0, \
            'No available languages to select from.'

        if (
            settings.DEFAULT_LANGUAGE and
            settings.DEFAULT_LANGUAGE in self.available_languages
        ) or (
            'language_code' not in self.form.base_fields
        ):
            # Default language still available

            self.available_languages.remove(settings.DEFAULT_LANGUAGE)
            return settings.DEFAULT_LANGUAGE

        else:
            # Select the first item and return it
            return self.available_languages.pop(0)",python,"def _get_default_language(self):
        """"""
        If a default language has been set, and is still available in
        `self.available_languages`, return it and remove it from the list.

        If not, simply pop the first available language.
        """"""

        assert hasattr(self, 'available_languages'), \
            'No available languages have been generated.'
        assert len(self.available_languages) > 0, \
            'No available languages to select from.'

        if (
            settings.DEFAULT_LANGUAGE and
            settings.DEFAULT_LANGUAGE in self.available_languages
        ) or (
            'language_code' not in self.form.base_fields
        ):
            # Default language still available

            self.available_languages.remove(settings.DEFAULT_LANGUAGE)
            return settings.DEFAULT_LANGUAGE

        else:
            # Select the first item and return it
            return self.available_languages.pop(0)",def,_get_default_language,(,self,),:,assert,hasattr,(,self,",",'available_languages',),",",'No available languages have been generated.',assert,len,(,self,.,available_languages,),>,0,",",'No available languages to select from.',if,(,settings,.,DEFAULT_LANGUAGE,and,settings,.,DEFAULT_LANGUAGE,in,self,.,available_languages,),or,(,'language_code',"If a default language has been set, and is still available in
        `self.available_languages`, return it and remove it from the list.

        If not, simply pop the first available language.",If,a,default,language,has,been,set,and,is,still,available,in,self,.,available_languages,2479b2c3d6f7b697e95aa1e082c8bc8699f1f638,https://github.com/dokterbob/django-multilingual-model/blob/2479b2c3d6f7b697e95aa1e082c8bc8699f1f638/multilingual_model/forms.py#L68-L94,train,not,in,self,.,form,.,base_fields,),:,# Default language still available,self,.,available_languages,.,remove,(,settings,.,DEFAULT_LANGUAGE,),return,settings,.,DEFAULT_LANGUAGE,else,:,# Select the first item and return it,return,self,.,return,it,and,remove,it,from,the,list,.,,,,,,,,,,,available_languages,.,pop,(,0,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dokterbob/django-multilingual-model,multilingual_model/forms.py,TranslationFormSet._construct_form,"def _construct_form(self, i, **kwargs):
        """"""
        Construct the form, overriding the initial value for `language_code`.
        """"""
        if not settings.HIDE_LANGUAGE:
            self._construct_available_languages()

        form = super(TranslationFormSet, self)._construct_form(i, **kwargs)

        if settings.HIDE_LANGUAGE:
            form.instance.language_code = settings.DEFAULT_LANGUAGE
        else:
            language_code = form.instance.language_code

            if language_code:
                logger.debug(
                    u'Removing translation choice %s for instance %s'
                    u' in form %d', language_code, form.instance, i
                )

                self.available_languages.remove(language_code)

            else:
                initial_language_code = self._get_default_language()

                logger.debug(
                    u'Preselecting language code %s for form %d',
                    initial_language_code, i
                )

                form.initial['language_code'] = initial_language_code

        return form",python,"def _construct_form(self, i, **kwargs):
        """"""
        Construct the form, overriding the initial value for `language_code`.
        """"""
        if not settings.HIDE_LANGUAGE:
            self._construct_available_languages()

        form = super(TranslationFormSet, self)._construct_form(i, **kwargs)

        if settings.HIDE_LANGUAGE:
            form.instance.language_code = settings.DEFAULT_LANGUAGE
        else:
            language_code = form.instance.language_code

            if language_code:
                logger.debug(
                    u'Removing translation choice %s for instance %s'
                    u' in form %d', language_code, form.instance, i
                )

                self.available_languages.remove(language_code)

            else:
                initial_language_code = self._get_default_language()

                logger.debug(
                    u'Preselecting language code %s for form %d',
                    initial_language_code, i
                )

                form.initial['language_code'] = initial_language_code

        return form",def,_construct_form,(,self,",",i,",",*,*,kwargs,),:,if,not,settings,.,HIDE_LANGUAGE,:,self,.,_construct_available_languages,(,),form,=,super,(,TranslationFormSet,",",self,),.,_construct_form,(,i,",",*,*,kwargs,),if,settings,.,"Construct the form, overriding the initial value for `language_code`.",Construct,the,form,overriding,the,initial,value,for,language_code,.,,,,,,2479b2c3d6f7b697e95aa1e082c8bc8699f1f638,https://github.com/dokterbob/django-multilingual-model/blob/2479b2c3d6f7b697e95aa1e082c8bc8699f1f638/multilingual_model/forms.py#L96-L128,train,HIDE_LANGUAGE,:,form,.,instance,.,language_code,=,settings,.,DEFAULT_LANGUAGE,else,:,language_code,=,form,.,instance,.,language_code,if,language_code,:,logger,.,debug,(,u'Removing translation choice %s for instance %s',u' in form %d',",",,,,,,,,,,,,,,,,,,,,language_code,",",form,.,instance,",",i,),self,.,available_languages,.,remove,(,language_code,),else,:,initial_language_code,=,self,.,_get_default_language,(,),logger,.,debug,(,u'Preselecting language code %s for form %d',",",initial_language_code,",",i,),form,.,initial,[,'language_code',],=,initial_language_code,return,form,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/fastq_merge.py,fq_merge,"def fq_merge(R1, R2):
    """"""
    merge separate fastq files
    """"""
    c = itertools.cycle([1, 2, 3, 4])
    for r1, r2 in zip(R1, R2):
        n = next(c)
        if n == 1:
            pair = [[], []]
        pair[0].append(r1.strip())
        pair[1].append(r2.strip())
        if n == 4:
            yield pair",python,"def fq_merge(R1, R2):
    """"""
    merge separate fastq files
    """"""
    c = itertools.cycle([1, 2, 3, 4])
    for r1, r2 in zip(R1, R2):
        n = next(c)
        if n == 1:
            pair = [[], []]
        pair[0].append(r1.strip())
        pair[1].append(r2.strip())
        if n == 4:
            yield pair",def,fq_merge,(,R1,",",R2,),:,c,=,itertools,.,cycle,(,[,1,",",2,",",3,",",4,],),for,r1,",",r2,in,zip,(,R1,",",R2,),:,n,=,next,(,c,),if,merge separate fastq files,merge,separate,fastq,files,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/fastq_merge.py#L13-L25,train,n,==,1,:,pair,=,[,[,],",",[,],],pair,[,0,],.,append,(,r1,.,strip,(,),),pair,[,1,],,,,,,,,,,,,,,,,,,,,.,append,(,r2,.,strip,(,),),if,n,==,4,:,yield,pair,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/contrib/ketama.py,Ketama._build_circle,"def _build_circle(self):
        """"""
            Creates hash ring.
        """"""
        total_weight = 0
        for node in self._nodes:
            total_weight += self._weights.get(node, 1)

        for node in self._nodes:
            weight = self._weights.get(node, 1)

            ks = math.floor((40 * len(self._nodes) * weight) / total_weight)

            for i in xrange(0, int(ks)):
                b_key = self._md5_digest('%s-%s-salt' % (node, i))

                for l in xrange(0, 4):
                    key = ((b_key[3 + l * 4] << 24)
                           | (b_key[2 + l * 4] << 16)
                           | (b_key[1 + l * 4] << 8)
                           | b_key[l * 4])

                    self._hashring[key] = node
                    self._sorted_keys.append(key)

        self._sorted_keys.sort()",python,"def _build_circle(self):
        """"""
            Creates hash ring.
        """"""
        total_weight = 0
        for node in self._nodes:
            total_weight += self._weights.get(node, 1)

        for node in self._nodes:
            weight = self._weights.get(node, 1)

            ks = math.floor((40 * len(self._nodes) * weight) / total_weight)

            for i in xrange(0, int(ks)):
                b_key = self._md5_digest('%s-%s-salt' % (node, i))

                for l in xrange(0, 4):
                    key = ((b_key[3 + l * 4] << 24)
                           | (b_key[2 + l * 4] << 16)
                           | (b_key[1 + l * 4] << 8)
                           | b_key[l * 4])

                    self._hashring[key] = node
                    self._sorted_keys.append(key)

        self._sorted_keys.sort()",def,_build_circle,(,self,),:,total_weight,=,0,for,node,in,self,.,_nodes,:,total_weight,+=,self,.,_weights,.,get,(,node,",",1,),for,node,in,self,.,_nodes,:,weight,=,self,.,_weights,.,get,(,Creates hash ring.,Creates,hash,ring,.,,,,,,,,,,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/contrib/ketama.py#L35-L60,train,node,",",1,),ks,=,math,.,floor,(,(,40,*,len,(,self,.,_nodes,),*,weight,),/,total_weight,),for,i,in,xrange,(,,,,,,,,,,,,,,,,,,,,0,",",int,(,ks,),),:,b_key,=,self,.,_md5_digest,(,'%s-%s-salt',%,(,node,",",i,),),for,l,in,xrange,(,0,",",4,),:,key,=,(,(,b_key,[,3,+,l,*,4,],<<,24,),|,(,b_key,[,2,+,l,*,4,],<<,16,),|,(,b_key,[,1,+,l,*,4,],<<,8,),|,b_key,[,l,*,4,],),self,.,_hashring,[,key,],=,node,self,.,_sorted_keys,.,append,(,key,),self,.,_sorted_keys,.,sort,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/contrib/ketama.py,Ketama._gen_key,"def _gen_key(self, key):
        """"""
            Return long integer for a given key, that represent it place on
            the hash ring.
        """"""
        b_key = self._md5_digest(key)
        return self._hashi(b_key, lambda x: x)",python,"def _gen_key(self, key):
        """"""
            Return long integer for a given key, that represent it place on
            the hash ring.
        """"""
        b_key = self._md5_digest(key)
        return self._hashi(b_key, lambda x: x)",def,_gen_key,(,self,",",key,),:,b_key,=,self,.,_md5_digest,(,key,),return,self,.,_hashi,(,b_key,",",lambda,x,:,x,),,,,,,,,,,,,,,,,"Return long integer for a given key, that represent it place on
            the hash ring.",Return,long,integer,for,a,given,key,that,represent,it,place,on,the,hash,ring,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/contrib/ketama.py#L78-L84,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/grid.py,has_custom_image,"def has_custom_image(user_context, app_id):
  """"""Returns True if there exists a custom image for app_id.""""""
  possible_paths = _valid_custom_image_paths(user_context, app_id)
  return any(map(os.path.exists, possible_paths))",python,"def has_custom_image(user_context, app_id):
  """"""Returns True if there exists a custom image for app_id.""""""
  possible_paths = _valid_custom_image_paths(user_context, app_id)
  return any(map(os.path.exists, possible_paths))",def,has_custom_image,(,user_context,",",app_id,),:,possible_paths,=,_valid_custom_image_paths,(,user_context,",",app_id,),return,any,(,map,(,os,.,path,.,exists,",",possible_paths,),),,,,,,,,,,,,,,Returns True if there exists a custom image for app_id.,Returns,True,if,there,exists,a,custom,image,for,app_id,.,,,,,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/grid.py#L32-L35,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/grid.py,get_custom_image,"def get_custom_image(user_context, app_id):
  """"""Returns the custom image associated with a given app. If there are
  multiple candidate images on disk, one is chosen arbitrarily.""""""
  possible_paths = _valid_custom_image_paths(user_context, app_id)
  existing_images = filter(os.path.exists, possible_paths)
  if len(existing_images) > 0:
    return existing_images[0]",python,"def get_custom_image(user_context, app_id):
  """"""Returns the custom image associated with a given app. If there are
  multiple candidate images on disk, one is chosen arbitrarily.""""""
  possible_paths = _valid_custom_image_paths(user_context, app_id)
  existing_images = filter(os.path.exists, possible_paths)
  if len(existing_images) > 0:
    return existing_images[0]",def,get_custom_image,(,user_context,",",app_id,),:,possible_paths,=,_valid_custom_image_paths,(,user_context,",",app_id,),existing_images,=,filter,(,os,.,path,.,exists,",",possible_paths,),if,len,(,existing_images,),>,0,:,return,existing_images,[,0,],,,"Returns the custom image associated with a given app. If there are
  multiple candidate images on disk, one is chosen arbitrarily.",Returns,the,custom,image,associated,with,a,given,app,.,If,there,are,multiple,candidate,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/grid.py#L37-L43,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,images,on,disk,one,is,chosen,arbitrarily,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/grid.py,set_custom_image,"def set_custom_image(user_context, app_id, image_path):
  """"""Sets the custom image for `app_id` to be the image located at
  `image_path`. If there already exists a custom image for `app_id` it will
  be deleted. Returns True is setting the image was successful.""""""
  if image_path is None:
    return False

  if not os.path.exists(image_path):
    return False

  (root, ext) = os.path.splitext(image_path)
  if not is_valid_extension(ext):
    # TODO: Maybe log that this happened?
    return False
  # If we don't remove the old image then theres no guarantee that Steam will
  # show our new image when it launches.
  if has_custom_image(user_context, app_id):
    img = get_custom_image(user_context, app_id)
    assert(img is not None)
    os.remove(img)
  
  # Set the new image
  parent_dir = paths.custom_images_directory(user_context)
  new_path = os.path.join(parent_dir, app_id + ext)
  shutil.copyfile(image_path, new_path)
  return True",python,"def set_custom_image(user_context, app_id, image_path):
  """"""Sets the custom image for `app_id` to be the image located at
  `image_path`. If there already exists a custom image for `app_id` it will
  be deleted. Returns True is setting the image was successful.""""""
  if image_path is None:
    return False

  if not os.path.exists(image_path):
    return False

  (root, ext) = os.path.splitext(image_path)
  if not is_valid_extension(ext):
    # TODO: Maybe log that this happened?
    return False
  # If we don't remove the old image then theres no guarantee that Steam will
  # show our new image when it launches.
  if has_custom_image(user_context, app_id):
    img = get_custom_image(user_context, app_id)
    assert(img is not None)
    os.remove(img)
  
  # Set the new image
  parent_dir = paths.custom_images_directory(user_context)
  new_path = os.path.join(parent_dir, app_id + ext)
  shutil.copyfile(image_path, new_path)
  return True",def,set_custom_image,(,user_context,",",app_id,",",image_path,),:,if,image_path,is,None,:,return,False,if,not,os,.,path,.,exists,(,image_path,),:,return,False,(,root,",",ext,),=,os,.,path,.,splitext,(,image_path,"Sets the custom image for `app_id` to be the image located at
  `image_path`. If there already exists a custom image for `app_id` it will
  be deleted. Returns True is setting the image was successful.",Sets,the,custom,image,for,app_id,to,be,the,image,located,at,image_path,.,If,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/grid.py#L45-L70,train,),if,not,is_valid_extension,(,ext,),:,# TODO: Maybe log that this happened?,return,False,# If we don't remove the old image then theres no guarantee that Steam will,# show our new image when it launches.,if,has_custom_image,(,user_context,",",app_id,),:,img,=,get_custom_image,(,user_context,",",app_id,),assert,there,already,exists,a,custom,image,for,app_id,it,will,be,deleted,.,Returns,True,is,setting,the,image,(,img,is,not,None,),os,.,remove,(,img,),# Set the new image,parent_dir,=,paths,.,custom_images_directory,(,user_context,),new_path,=,os,.,path,.,join,(,parent_dir,",",app_id,+,ext,),shutil,.,copyfile,(,image_path,",",new_path,),return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,was,successful,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cldf/segments,src/segments/profile.py,Profile.from_file,"def from_file(cls, fname, form=None):
        """"""
        Read an orthography profile from a metadata file or a default tab-separated profile file.
        """"""
        try:
            tg = TableGroup.from_file(fname)
            opfname = None
        except JSONDecodeError:
            tg = TableGroup.fromvalue(cls.MD)
            opfname = fname
        if len(tg.tables) != 1:
            raise ValueError('profile description must contain exactly one table')
        metadata = tg.common_props
        metadata.update(fname=Path(fname), form=form)
        return cls(
            *[{k: None if (k != cls.GRAPHEME_COL and v == cls.NULL) else v for k, v in d.items()}
              for d in tg.tables[0].iterdicts(fname=opfname)],
            **metadata)",python,"def from_file(cls, fname, form=None):
        """"""
        Read an orthography profile from a metadata file or a default tab-separated profile file.
        """"""
        try:
            tg = TableGroup.from_file(fname)
            opfname = None
        except JSONDecodeError:
            tg = TableGroup.fromvalue(cls.MD)
            opfname = fname
        if len(tg.tables) != 1:
            raise ValueError('profile description must contain exactly one table')
        metadata = tg.common_props
        metadata.update(fname=Path(fname), form=form)
        return cls(
            *[{k: None if (k != cls.GRAPHEME_COL and v == cls.NULL) else v for k, v in d.items()}
              for d in tg.tables[0].iterdicts(fname=opfname)],
            **metadata)",def,from_file,(,cls,",",fname,",",form,=,None,),:,try,:,tg,=,TableGroup,.,from_file,(,fname,),opfname,=,None,except,JSONDecodeError,:,tg,=,TableGroup,.,fromvalue,(,cls,.,MD,),opfname,=,fname,if,len,Read an orthography profile from a metadata file or a default tab-separated profile file.,Read,an,orthography,profile,from,a,metadata,file,or,a,default,tab,-,separated,profile,9136a4ec89555bf9b574399ffbb07f3cc9a9f45f,https://github.com/cldf/segments/blob/9136a4ec89555bf9b574399ffbb07f3cc9a9f45f/src/segments/profile.py#L100-L117,train,(,tg,.,tables,),!=,1,:,raise,ValueError,(,'profile description must contain exactly one table',),metadata,=,tg,.,common_props,metadata,.,update,(,fname,=,Path,(,fname,),",",form,file,.,,,,,,,,,,,,,,,,,,=,form,),return,cls,(,*,[,{,k,:,None,if,(,k,!=,cls,.,GRAPHEME_COL,and,v,==,cls,.,NULL,),else,v,for,k,",",v,in,d,.,items,(,),},for,d,in,tg,.,tables,[,0,],.,iterdicts,(,fname,=,opfname,),],",",*,*,metadata,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cldf/segments,src/segments/profile.py,Profile.from_text,"def from_text(cls, text, mapping='mapping'):
        """"""
        Create a Profile instance from the Unicode graphemes found in `text`.

        Parameters
        ----------
        text
        mapping

        Returns
        -------
        A Profile instance.

        """"""
        graphemes = Counter(grapheme_pattern.findall(text))
        specs = [
            OrderedDict([
                (cls.GRAPHEME_COL, grapheme),
                ('frequency', frequency),
                (mapping, grapheme)])
            for grapheme, frequency in graphemes.most_common()]
        return cls(*specs)",python,"def from_text(cls, text, mapping='mapping'):
        """"""
        Create a Profile instance from the Unicode graphemes found in `text`.

        Parameters
        ----------
        text
        mapping

        Returns
        -------
        A Profile instance.

        """"""
        graphemes = Counter(grapheme_pattern.findall(text))
        specs = [
            OrderedDict([
                (cls.GRAPHEME_COL, grapheme),
                ('frequency', frequency),
                (mapping, grapheme)])
            for grapheme, frequency in graphemes.most_common()]
        return cls(*specs)",def,from_text,(,cls,",",text,",",mapping,=,'mapping',),:,graphemes,=,Counter,(,grapheme_pattern,.,findall,(,text,),),specs,=,[,OrderedDict,(,[,(,cls,.,GRAPHEME_COL,",",grapheme,),",",(,'frequency',",",frequency,),",","Create a Profile instance from the Unicode graphemes found in `text`.

        Parameters
        ----------
        text
        mapping

        Returns
        -------
        A Profile instance.",Create,a,Profile,instance,from,the,Unicode,graphemes,found,in,text,.,,,,9136a4ec89555bf9b574399ffbb07f3cc9a9f45f,https://github.com/cldf/segments/blob/9136a4ec89555bf9b574399ffbb07f3cc9a9f45f/src/segments/profile.py#L120-L141,train,(,mapping,",",grapheme,),],),for,grapheme,",",frequency,in,graphemes,.,most_common,(,),],return,cls,(,*,specs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/name2fasta.py,split_fasta,"def split_fasta(f, id2f):
    """"""
    split fasta file into separate fasta files based on list of scaffolds
    that belong to each separate file
    """"""
    opened = {}
    for seq in parse_fasta(f):
        id = seq[0].split('>')[1].split()[0]
        if id not in id2f:
            continue
        fasta = id2f[id]
        if fasta not in opened:
            opened[fasta] = '%s.fa' % fasta
        seq[1] += '\n'
        with open(opened[fasta], 'a+') as f_out:
            f_out.write('\n'.join(seq))",python,"def split_fasta(f, id2f):
    """"""
    split fasta file into separate fasta files based on list of scaffolds
    that belong to each separate file
    """"""
    opened = {}
    for seq in parse_fasta(f):
        id = seq[0].split('>')[1].split()[0]
        if id not in id2f:
            continue
        fasta = id2f[id]
        if fasta not in opened:
            opened[fasta] = '%s.fa' % fasta
        seq[1] += '\n'
        with open(opened[fasta], 'a+') as f_out:
            f_out.write('\n'.join(seq))",def,split_fasta,(,f,",",id2f,),:,opened,=,{,},for,seq,in,parse_fasta,(,f,),:,id,=,seq,[,0,],.,split,(,'>',),[,1,],.,split,(,),[,0,],if,id,"split fasta file into separate fasta files based on list of scaffolds
    that belong to each separate file",split,fasta,file,into,separate,fasta,files,based,on,list,of,scaffolds,that,belong,to,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/name2fasta.py#L7-L22,train,not,in,id2f,:,continue,fasta,=,id2f,[,id,],if,fasta,not,in,opened,:,opened,[,fasta,],=,'%s.fa',%,fasta,seq,[,1,],+=,each,separate,file,,,,,,,,,,,,,,,,,'\n',with,open,(,opened,[,fasta,],",",'a+',),as,f_out,:,f_out,.,write,(,'\n',.,join,(,seq,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/legacy/steam.py,Steam._is_user_directory,"def _is_user_directory(self, pathname):
      """"""Check whether `pathname` is a valid user data directory

      This method is meant to be called on the contents of the userdata dir.
      As such, it will return True when `pathname` refers to a directory name
      that can be interpreted as a users' userID.
      """"""
      fullpath = os.path.join(self.userdata_location(), pathname)
      # SteamOS puts a directory named 'anonymous' in the userdata directory
      # by default. Since we assume that pathname is a userID, ignore any name
      # that can't be converted to a number
      return os.path.isdir(fullpath) and pathname.isdigit()",python,"def _is_user_directory(self, pathname):
      """"""Check whether `pathname` is a valid user data directory

      This method is meant to be called on the contents of the userdata dir.
      As such, it will return True when `pathname` refers to a directory name
      that can be interpreted as a users' userID.
      """"""
      fullpath = os.path.join(self.userdata_location(), pathname)
      # SteamOS puts a directory named 'anonymous' in the userdata directory
      # by default. Since we assume that pathname is a userID, ignore any name
      # that can't be converted to a number
      return os.path.isdir(fullpath) and pathname.isdigit()",def,_is_user_directory,(,self,",",pathname,),:,fullpath,=,os,.,path,.,join,(,self,.,userdata_location,(,),",",pathname,),# SteamOS puts a directory named 'anonymous' in the userdata directory,"# by default. Since we assume that pathname is a userID, ignore any name",# that can't be converted to a number,return,os,.,path,.,isdir,(,fullpath,),and,pathname,.,isdigit,(,),,"Check whether `pathname` is a valid user data directory

      This method is meant to be called on the contents of the userdata dir.
      As such, it will return True when `pathname` refers to a directory name
      that can be interpreted as a users' userID.",Check,whether,pathname,is,a,valid,user,data,directory,,,,,,,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/legacy/steam.py#L47-L58,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/legacy/steam.py,Steam.local_users,"def local_users(self):
        """"""Returns an array of user ids for users on the filesystem""""""
        # Any users on the machine will have an entry inside of the userdata
        # folder. As such, the easiest way to find a list of all users on the
        # machine is to just list the folders inside userdata
        userdirs = filter(self._is_user_directory, os.listdir(self.userdata_location()))
        # Exploits the fact that the directory is named the same as the user id
        return map(lambda userdir: user.User(self, int(userdir)), userdirs)",python,"def local_users(self):
        """"""Returns an array of user ids for users on the filesystem""""""
        # Any users on the machine will have an entry inside of the userdata
        # folder. As such, the easiest way to find a list of all users on the
        # machine is to just list the folders inside userdata
        userdirs = filter(self._is_user_directory, os.listdir(self.userdata_location()))
        # Exploits the fact that the directory is named the same as the user id
        return map(lambda userdir: user.User(self, int(userdir)), userdirs)",def,local_users,(,self,),:,# Any users on the machine will have an entry inside of the userdata,"# folder. As such, the easiest way to find a list of all users on the",# machine is to just list the folders inside userdata,userdirs,=,filter,(,self,.,_is_user_directory,",",os,.,listdir,(,self,.,userdata_location,(,),),),# Exploits the fact that the directory is named the same as the user id,return,map,(,lambda,userdir,:,user,.,User,(,self,",",int,(,Returns an array of user ids for users on the filesystem,Returns,an,array,of,user,ids,for,users,on,the,filesystem,,,,,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/legacy/steam.py#L80-L87,train,userdir,),),",",userdirs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opengridcc/opengrid,opengrid/library/weather.py,_calculate_degree_days,"def _calculate_degree_days(temperature_equivalent, base_temperature, cooling=False):
    """"""
    Calculates degree days, starting with a series of temperature equivalent values

    Parameters
    ----------
    temperature_equivalent : Pandas Series
    base_temperature : float
    cooling : bool
        Set True if you want cooling degree days instead of heating degree days

    Returns
    -------
    Pandas Series called HDD_base_temperature for heating degree days or
    CDD_base_temperature for cooling degree days.
    """"""

    if cooling:
        ret = temperature_equivalent - base_temperature
    else:
        ret = base_temperature - temperature_equivalent

    # degree days cannot be negative
    ret[ret < 0] = 0

    prefix = 'CDD' if cooling else 'HDD'
    ret.name = '{}_{}'.format(prefix, base_temperature)

    return ret",python,"def _calculate_degree_days(temperature_equivalent, base_temperature, cooling=False):
    """"""
    Calculates degree days, starting with a series of temperature equivalent values

    Parameters
    ----------
    temperature_equivalent : Pandas Series
    base_temperature : float
    cooling : bool
        Set True if you want cooling degree days instead of heating degree days

    Returns
    -------
    Pandas Series called HDD_base_temperature for heating degree days or
    CDD_base_temperature for cooling degree days.
    """"""

    if cooling:
        ret = temperature_equivalent - base_temperature
    else:
        ret = base_temperature - temperature_equivalent

    # degree days cannot be negative
    ret[ret < 0] = 0

    prefix = 'CDD' if cooling else 'HDD'
    ret.name = '{}_{}'.format(prefix, base_temperature)

    return ret",def,_calculate_degree_days,(,temperature_equivalent,",",base_temperature,",",cooling,=,False,),:,if,cooling,:,ret,=,temperature_equivalent,-,base_temperature,else,:,ret,=,base_temperature,-,temperature_equivalent,# degree days cannot be negative,ret,[,ret,<,0,],=,0,prefix,=,'CDD',if,cooling,else,'HDD',"Calculates degree days, starting with a series of temperature equivalent values

    Parameters
    ----------
    temperature_equivalent : Pandas Series
    base_temperature : float
    cooling : bool
        Set True if you want cooling degree days instead of heating degree days

    Returns
    -------
    Pandas Series called HDD_base_temperature for heating degree days or
    CDD_base_temperature for cooling degree days.",Calculates,degree,days,starting,with,a,series,of,temperature,equivalent,values,,,,,69b8da3c8fcea9300226c45ef0628cd6d4307651,https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/weather.py#L31-L59,train,ret,.,name,=,'{}_{}',.,format,(,prefix,",",base_temperature,),return,ret,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/classifiers.py,Classifiers.status,"def status(self):
        """"""Development status.""""""
        return {self._acronym_status(l): l for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_status)}",python,"def status(self):
        """"""Development status.""""""
        return {self._acronym_status(l): l for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_status)}",def,status,(,self,),:,return,{,self,.,_acronym_status,(,l,),:,l,for,l,in,self,.,resp_text,.,split,(,'\n',),if,l,.,startswith,(,self,.,prefix_status,),},,,,,,,Development status.,Development,status,.,,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/classifiers.py#L33-L36,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/classifiers.py,Classifiers.licenses,"def licenses(self):
        """"""OSI Approved license.""""""
        return {self._acronym_lic(l): l for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_lic)}",python,"def licenses(self):
        """"""OSI Approved license.""""""
        return {self._acronym_lic(l): l for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_lic)}",def,licenses,(,self,),:,return,{,self,.,_acronym_lic,(,l,),:,l,for,l,in,self,.,resp_text,.,split,(,'\n',),if,l,.,startswith,(,self,.,prefix_lic,),},,,,,,,OSI Approved license.,OSI,Approved,license,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/classifiers.py#L43-L46,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/classifiers.py,Classifiers.licenses_desc,"def licenses_desc(self):
        """"""Remove prefix.""""""
        return {self._acronym_lic(l): l.split(self.prefix_lic)[1]
                for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_lic)}",python,"def licenses_desc(self):
        """"""Remove prefix.""""""
        return {self._acronym_lic(l): l.split(self.prefix_lic)[1]
                for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_lic)}",def,licenses_desc,(,self,),:,return,{,self,.,_acronym_lic,(,l,),:,l,.,split,(,self,.,prefix_lic,),[,1,],for,l,in,self,.,resp_text,.,split,(,'\n',),if,l,.,startswith,(,self,Remove prefix.,Remove,prefix,.,,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/classifiers.py#L48-L52,train,.,prefix_lic,),},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/classifiers.py,Classifiers._acronym_lic,"def _acronym_lic(self, license_statement):
        """"""Convert license acronym.""""""
        pat = re.compile(r'\(([\w+\W?\s?]+)\)')
        if pat.search(license_statement):
            lic = pat.search(license_statement).group(1)
            if lic.startswith('CNRI'):
                acronym_licence = lic[:4]
            else:
                acronym_licence = lic.replace(' ', '')
        else:
            acronym_licence = ''.join(
                [w[0]
                 for w in license_statement.split(self.prefix_lic)[1].split()])
        return acronym_licence",python,"def _acronym_lic(self, license_statement):
        """"""Convert license acronym.""""""
        pat = re.compile(r'\(([\w+\W?\s?]+)\)')
        if pat.search(license_statement):
            lic = pat.search(license_statement).group(1)
            if lic.startswith('CNRI'):
                acronym_licence = lic[:4]
            else:
                acronym_licence = lic.replace(' ', '')
        else:
            acronym_licence = ''.join(
                [w[0]
                 for w in license_statement.split(self.prefix_lic)[1].split()])
        return acronym_licence",def,_acronym_lic,(,self,",",license_statement,),:,pat,=,re,.,compile,(,r'\(([\w+\W?\s?]+)\)',),if,pat,.,search,(,license_statement,),:,lic,=,pat,.,search,(,license_statement,),.,group,(,1,),if,lic,.,startswith,(,'CNRI',Convert license acronym.,Convert,license,acronym,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/classifiers.py#L54-L67,train,),:,acronym_licence,=,lic,[,:,4,],else,:,acronym_licence,=,lic,.,replace,(,' ',",",'',),else,:,acronym_licence,=,'',.,join,(,[,,,,,,,,,,,,,,,,,,,,w,[,0,],for,w,in,license_statement,.,split,(,self,.,prefix_lic,),[,1,],.,split,(,),],),return,acronym_licence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,calcMD5,"def calcMD5(path):
    """"""
    calc MD5 based on path
    """"""
    # check that file exists
    if os.path.exists(path) is False:
        yield False
    else:
        command = ['md5sum', path]
        p = Popen(command, stdout = PIPE)
        for line in p.communicate()[0].splitlines():
            yield line.decode('ascii').strip().split()[0]
        p.wait()
        yield False",python,"def calcMD5(path):
    """"""
    calc MD5 based on path
    """"""
    # check that file exists
    if os.path.exists(path) is False:
        yield False
    else:
        command = ['md5sum', path]
        p = Popen(command, stdout = PIPE)
        for line in p.communicate()[0].splitlines():
            yield line.decode('ascii').strip().split()[0]
        p.wait()
        yield False",def,calcMD5,(,path,),:,# check that file exists,if,os,.,path,.,exists,(,path,),is,False,:,yield,False,else,:,command,=,[,'md5sum',",",path,],p,=,Popen,(,command,",",stdout,=,PIPE,),for,line,in,calc MD5 based on path,calc,MD5,based,on,path,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L18-L31,train,p,.,communicate,(,),[,0,],.,splitlines,(,),:,yield,line,.,decode,(,'ascii',),.,strip,(,),.,split,(,),[,0,,,,,,,,,,,,,,,,,,,,],p,.,wait,(,),yield,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,wget,"def wget(ftp, f = False, exclude = False, name = False, md5 = False, tries = 10):
    """"""
    download files with wget
    """"""
    # file name
    if f is False:
        f = ftp.rsplit('/', 1)[-1]
    # downloaded file if it does not already exist
    # check md5s on server (optional)
    t = 0
    while md5check(f, ftp, md5, exclude) is not True:
        t += 1
        if name is not False:
            print('# downloading:', name, f)
        if exclude is False:
            command = 'wget -q --random-wait %s' % (ftp)
        else:
            command = 'wget -q --random-wait -R %s %s' % (exclude, ftp)
        p = Popen(command, shell = True)
        p.communicate()
        if t >= tries:
            print('not downloaded:', name, f)
            return [f, False]
    return [f, True]",python,"def wget(ftp, f = False, exclude = False, name = False, md5 = False, tries = 10):
    """"""
    download files with wget
    """"""
    # file name
    if f is False:
        f = ftp.rsplit('/', 1)[-1]
    # downloaded file if it does not already exist
    # check md5s on server (optional)
    t = 0
    while md5check(f, ftp, md5, exclude) is not True:
        t += 1
        if name is not False:
            print('# downloading:', name, f)
        if exclude is False:
            command = 'wget -q --random-wait %s' % (ftp)
        else:
            command = 'wget -q --random-wait -R %s %s' % (exclude, ftp)
        p = Popen(command, shell = True)
        p.communicate()
        if t >= tries:
            print('not downloaded:', name, f)
            return [f, False]
    return [f, True]",def,wget,(,ftp,",",f,=,False,",",exclude,=,False,",",name,=,False,",",md5,=,False,",",tries,=,10,),:,# file name,if,f,is,False,:,f,=,ftp,.,rsplit,(,'/',",",1,),[,download files with wget,download,files,with,wget,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L74-L97,train,-,1,],# downloaded file if it does not already exist,# check md5s on server (optional),t,=,0,while,md5check,(,f,",",ftp,",",md5,",",exclude,),is,not,True,:,t,+=,1,if,name,is,not,,,,,,,,,,,,,,,,,,,,False,:,print,(,'# downloading:',",",name,",",f,),if,exclude,is,False,:,command,=,'wget -q --random-wait %s',%,(,ftp,),else,:,command,=,'wget -q --random-wait -R %s %s',%,(,exclude,",",ftp,),p,=,Popen,(,command,",",shell,=,True,),p,.,communicate,(,),if,t,>=,tries,:,print,(,'not downloaded:',",",name,",",f,),return,[,f,",",False,],return,[,f,",",True,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,check,"def check(line, queries):
    """"""
    check that at least one of
    queries is in list, l
    """"""
    line = line.strip()
    spLine = line.replace('.', ' ').split()
    matches = set(spLine).intersection(queries)
    if len(matches) > 0:
        return matches, line.split('\t')
    return matches, False",python,"def check(line, queries):
    """"""
    check that at least one of
    queries is in list, l
    """"""
    line = line.strip()
    spLine = line.replace('.', ' ').split()
    matches = set(spLine).intersection(queries)
    if len(matches) > 0:
        return matches, line.split('\t')
    return matches, False",def,check,(,line,",",queries,),:,line,=,line,.,strip,(,),spLine,=,line,.,replace,(,'.',",",' ',),.,split,(,),matches,=,set,(,spLine,),.,intersection,(,queries,),if,len,(,"check that at least one of
    queries is in list, l",check,that,at,least,one,of,queries,is,in,list,l,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L99-L109,train,matches,),>,0,:,return,matches,",",line,.,split,(,'\t',),return,matches,",",False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,entrez,"def entrez(db, acc):
    """"""
    search entrez using specified database
    and accession
    """"""
    c1 = ['esearch', '-db', db, '-query', acc]
    c2 = ['efetch', '-db', 'BioSample', '-format', 'docsum']
    p1 = Popen(c1, stdout = PIPE, stderr = PIPE)
    p2 = Popen(c2, stdin = p1.stdout, stdout = PIPE, stderr = PIPE)
    return p2.communicate()",python,"def entrez(db, acc):
    """"""
    search entrez using specified database
    and accession
    """"""
    c1 = ['esearch', '-db', db, '-query', acc]
    c2 = ['efetch', '-db', 'BioSample', '-format', 'docsum']
    p1 = Popen(c1, stdout = PIPE, stderr = PIPE)
    p2 = Popen(c2, stdin = p1.stdout, stdout = PIPE, stderr = PIPE)
    return p2.communicate()",def,entrez,(,db,",",acc,),:,c1,=,[,'esearch',",",'-db',",",db,",",'-query',",",acc,],c2,=,[,'efetch',",",'-db',",",'BioSample',",",'-format',",",'docsum',],p1,=,Popen,(,c1,",",stdout,=,PIPE,"search entrez using specified database
    and accession",search,entrez,using,specified,database,and,accession,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L111-L120,train,",",stderr,=,PIPE,),p2,=,Popen,(,c2,",",stdin,=,p1,.,stdout,",",stdout,=,PIPE,",",stderr,=,PIPE,),return,p2,.,communicate,(,,,,,,,,,,,,,,,,,,,,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,searchAccession,"def searchAccession(acc):
    """"""
    attempt to use NCBI Entrez to get
    BioSample ID
    """"""
    # try genbank file
    # genome database
    out, error = entrez('genome', acc)
    for line in out.splitlines():
        line = line.decode('ascii').strip()
        if 'Assembly_Accession' in line or 'BioSample' in line:
            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]
            if len(newAcc) > 0:
                return (True, acc, newAcc)
    # nucleotide database
    out, error = entrez('nucleotide', acc)
    for line in out.splitlines():
        line = line.decode('ascii').strip()
        if 'Assembly_Accession' in line or 'BioSample' in line:
            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]
            if len(newAcc) > 0:
                return (True, acc, newAcc)
    # assembly database
    out, error = entrez('assembly', acc)
    for line in out.splitlines():
        line = line.decode('ascii').strip()
        if 'Assembly_Accession' in line or 'BioSample' in line:
            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]
            if len(newAcc) > 0:
                return (True, acc, newAcc)
    for error in error.splitlines():
        error = error.decode('ascii').strip()
        if '500 Can' in error:
            return (False, acc, 'no network')
    return (False, acc, 'efetch failed')",python,"def searchAccession(acc):
    """"""
    attempt to use NCBI Entrez to get
    BioSample ID
    """"""
    # try genbank file
    # genome database
    out, error = entrez('genome', acc)
    for line in out.splitlines():
        line = line.decode('ascii').strip()
        if 'Assembly_Accession' in line or 'BioSample' in line:
            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]
            if len(newAcc) > 0:
                return (True, acc, newAcc)
    # nucleotide database
    out, error = entrez('nucleotide', acc)
    for line in out.splitlines():
        line = line.decode('ascii').strip()
        if 'Assembly_Accession' in line or 'BioSample' in line:
            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]
            if len(newAcc) > 0:
                return (True, acc, newAcc)
    # assembly database
    out, error = entrez('assembly', acc)
    for line in out.splitlines():
        line = line.decode('ascii').strip()
        if 'Assembly_Accession' in line or 'BioSample' in line:
            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]
            if len(newAcc) > 0:
                return (True, acc, newAcc)
    for error in error.splitlines():
        error = error.decode('ascii').strip()
        if '500 Can' in error:
            return (False, acc, 'no network')
    return (False, acc, 'efetch failed')",def,searchAccession,(,acc,),:,# try genbank file,# genome database,out,",",error,=,entrez,(,'genome',",",acc,),for,line,in,out,.,splitlines,(,),:,line,=,line,.,decode,(,'ascii',),.,strip,(,),if,'Assembly_Accession',in,line,"attempt to use NCBI Entrez to get
    BioSample ID",attempt,to,use,NCBI,Entrez,to,get,BioSample,ID,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L122-L156,train,or,'BioSample',in,line,:,newAcc,=,line,.,split,(,'>',),[,1,],.,split,(,'<',),[,0,],.,split,(,'.',),[,,,,,,,,,,,,,,,,,,,,0,],.,split,(,"','",),[,0,],if,len,(,newAcc,),>,0,:,return,(,True,",",acc,",",newAcc,),# nucleotide database,out,",",error,=,entrez,(,'nucleotide',",",acc,),for,line,in,out,.,splitlines,(,),:,line,=,line,.,decode,(,'ascii',),.,strip,(,),if,'Assembly_Accession',in,line,or,'BioSample',in,line,:,newAcc,=,line,.,split,(,'>',),[,1,],.,split,(,'<',),[,0,],.,split,(,'.',),[,0,],.,split,(,"','",),[,0,],if,len,(,newAcc,),>,0,:,return,(,True,",",acc,",",newAcc,),# assembly database,out,",",error,=,entrez,(,'assembly',",",acc,),for,line,in,out,.,splitlines,(,),:,line,=,line,.,decode,(,'ascii',),.,strip,(,),if,'Assembly_Accession',in,line,or,'BioSample',in,line,:,newAcc,=,line,.,split,(,'>',),[,1,],.,split,(,'<',),[,0,],.,split,(,'.',),[,0,],.,split,(,"','",),[,0,],if,len,(,newAcc,),>,0,:,return,(,True,",",acc,",",newAcc,),for,error,in,error,.,splitlines,(,),:,error,=,error,.,decode,(,'ascii',),.,strip,(,),if,'500 Can',in,error,:,return,(,False,",",acc,",",'no network',),return,(,False,",",acc,",",'efetch failed',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,getFTPs,"def getFTPs(accessions, ftp, search, exclude, convert = False, threads = 1, attempt = 1,
            max_attempts = 2):
    """"""
    download genome info from NCBI
    """"""
    info = wget(ftp)[0]
    allMatches = []
    for genome in open(info, encoding = 'utf8'):
        genome = str(genome)
        matches, genomeInfo = check(genome, accessions)
        if genomeInfo is not False:
            f = genomeInfo[0] + search
            Gftp = genomeInfo[19]
            Gftp = Gftp + '/' + search
            allMatches.extend(matches)
            yield (Gftp, f, exclude, matches)
    # print accessions that could not be matched
    # and whether or not they could be converted (optional)
    newAccs = []
    missing = accessions.difference(set(allMatches))
    if convert is True:
        pool = Pool(threads)
        pool = pool.imap_unordered(searchAccession, missing)
        for newAcc in tqdm(pool, total = len(missing)):
            status, accession, newAcc = newAcc
            if status is True:
                newAccs.append(newAcc)
            print('not found:', accession, '->', newAcc)
    else:
        for accession in missing:
            print('not found:', accession)
    # re-try after converting accessions (optional)
    if len(newAccs) > 0 and attempt <= max_attempts:
        print('convert accession attempt', attempt)
        attempt += 1
        for hit in getFTPs(set(newAccs), ftp, search, exclude, convert,
                threads = 1, attempt = attempt):
            yield hit",python,"def getFTPs(accessions, ftp, search, exclude, convert = False, threads = 1, attempt = 1,
            max_attempts = 2):
    """"""
    download genome info from NCBI
    """"""
    info = wget(ftp)[0]
    allMatches = []
    for genome in open(info, encoding = 'utf8'):
        genome = str(genome)
        matches, genomeInfo = check(genome, accessions)
        if genomeInfo is not False:
            f = genomeInfo[0] + search
            Gftp = genomeInfo[19]
            Gftp = Gftp + '/' + search
            allMatches.extend(matches)
            yield (Gftp, f, exclude, matches)
    # print accessions that could not be matched
    # and whether or not they could be converted (optional)
    newAccs = []
    missing = accessions.difference(set(allMatches))
    if convert is True:
        pool = Pool(threads)
        pool = pool.imap_unordered(searchAccession, missing)
        for newAcc in tqdm(pool, total = len(missing)):
            status, accession, newAcc = newAcc
            if status is True:
                newAccs.append(newAcc)
            print('not found:', accession, '->', newAcc)
    else:
        for accession in missing:
            print('not found:', accession)
    # re-try after converting accessions (optional)
    if len(newAccs) > 0 and attempt <= max_attempts:
        print('convert accession attempt', attempt)
        attempt += 1
        for hit in getFTPs(set(newAccs), ftp, search, exclude, convert,
                threads = 1, attempt = attempt):
            yield hit",def,getFTPs,(,accessions,",",ftp,",",search,",",exclude,",",convert,=,False,",",threads,=,1,",",attempt,=,1,",",max_attempts,=,2,),:,info,=,wget,(,ftp,),[,0,],allMatches,=,[,],for,genome,download genome info from NCBI,download,genome,info,from,NCBI,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L158-L195,train,in,open,(,info,",",encoding,=,'utf8',),:,genome,=,str,(,genome,),matches,",",genomeInfo,=,check,(,genome,",",accessions,),if,genomeInfo,is,not,,,,,,,,,,,,,,,,,,,,False,:,f,=,genomeInfo,[,0,],+,search,Gftp,=,genomeInfo,[,19,],Gftp,=,Gftp,+,'/',+,search,allMatches,.,extend,(,matches,),yield,(,Gftp,",",f,",",exclude,",",matches,),# print accessions that could not be matched,# and whether or not they could be converted (optional),newAccs,=,[,],missing,=,accessions,.,difference,(,set,(,allMatches,),),if,convert,is,True,:,pool,=,Pool,(,threads,),pool,=,pool,.,imap_unordered,(,searchAccession,",",missing,),for,newAcc,in,tqdm,(,pool,",",total,=,len,(,missing,),),:,status,",",accession,",",newAcc,=,newAcc,if,status,is,True,:,newAccs,.,append,(,newAcc,),print,(,'not found:',",",accession,",",'->',",",newAcc,),else,:,for,accession,in,missing,:,print,(,'not found:',",",accession,),# re-try after converting accessions (optional),if,len,(,newAccs,),>,0,and,attempt,<=,max_attempts,:,print,(,'convert accession attempt',",",attempt,),attempt,+=,1,for,hit,in,getFTPs,(,set,(,newAccs,),",",ftp,",",search,",",exclude,",",convert,",",threads,=,1,",",attempt,=,attempt,),:,yield,hit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/ncbi_download.py,download,"def download(args):
    """"""
    download genomes from NCBI
    """"""
    accessions, infoFTP = set(args['g']), args['i']
    search, exclude = args['s'], args['e']
    FTPs = getFTPs(accessions, infoFTP, search, exclude, threads = args['t'],
            convert = args['convert'])
    if args['test'] is True:
        for genome in FTPs:
            print('found:', ';'.join(genome[-1]), genome[0])
        return FTPs
    pool = Pool(args['t'])
    pool = pool.imap_unordered(wgetGenome, FTPs)
    files = []
    for f in tqdm(pool, total = len(accessions)):
        files.append(f)
    return files",python,"def download(args):
    """"""
    download genomes from NCBI
    """"""
    accessions, infoFTP = set(args['g']), args['i']
    search, exclude = args['s'], args['e']
    FTPs = getFTPs(accessions, infoFTP, search, exclude, threads = args['t'],
            convert = args['convert'])
    if args['test'] is True:
        for genome in FTPs:
            print('found:', ';'.join(genome[-1]), genome[0])
        return FTPs
    pool = Pool(args['t'])
    pool = pool.imap_unordered(wgetGenome, FTPs)
    files = []
    for f in tqdm(pool, total = len(accessions)):
        files.append(f)
    return files",def,download,(,args,),:,accessions,",",infoFTP,=,set,(,args,[,'g',],),",",args,[,'i',],search,",",exclude,=,args,[,'s',],",",args,[,'e',],FTPs,=,getFTPs,(,accessions,",",infoFTP,",",download genomes from NCBI,download,genomes,from,NCBI,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/ncbi_download.py#L204-L221,train,search,",",exclude,",",threads,=,args,[,'t',],",",convert,=,args,[,'convert',],),if,args,[,'test',],is,True,:,for,genome,in,FTPs,,,,,,,,,,,,,,,,,,,,:,print,(,'found:',",",';',.,join,(,genome,[,-,1,],),",",genome,[,0,],),return,FTPs,pool,=,Pool,(,args,[,'t',],),pool,=,pool,.,imap_unordered,(,wgetGenome,",",FTPs,),files,=,[,],for,f,in,tqdm,(,pool,",",total,=,len,(,accessions,),),:,files,.,append,(,f,),return,files,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/fix_fasta.py,fix_fasta,"def fix_fasta(fasta):
    """"""
    remove pesky characters from fasta file header
    """"""
    for seq in parse_fasta(fasta):
        seq[0] = remove_char(seq[0])
        if len(seq[1]) > 0:
            yield seq",python,"def fix_fasta(fasta):
    """"""
    remove pesky characters from fasta file header
    """"""
    for seq in parse_fasta(fasta):
        seq[0] = remove_char(seq[0])
        if len(seq[1]) > 0:
            yield seq",def,fix_fasta,(,fasta,),:,for,seq,in,parse_fasta,(,fasta,),:,seq,[,0,],=,remove_char,(,seq,[,0,],),if,len,(,seq,[,1,],),>,0,:,yield,seq,,,,,remove pesky characters from fasta file header,remove,pesky,characters,from,fasta,file,header,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/fix_fasta.py#L18-L25,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ssanderson/pstats-view,pstatsviewer/viewer.py,_calc_frames,"def _calc_frames(stats):
    """"""
    Compute a DataFrame summary of a Stats object.
    """"""
    timings = []
    callers = []
    for key, values in iteritems(stats.stats):
        timings.append(
            pd.Series(
                key + values[:-1],
                index=timing_colnames,
            )
        )
        for caller_key, caller_values in iteritems(values[-1]):
            callers.append(
                pd.Series(
                    key + caller_key + caller_values,
                    index=caller_columns,
                )
            )

    timings_df = pd.DataFrame(timings)
    callers_df = pd.DataFrame(callers)
    timings_df['filename:funcname'] = \
        (timings_df['filename'] + ':' + timings_df['funcname'])
    timings_df = timings_df.groupby('filename:funcname').sum()
    return timings_df, callers_df",python,"def _calc_frames(stats):
    """"""
    Compute a DataFrame summary of a Stats object.
    """"""
    timings = []
    callers = []
    for key, values in iteritems(stats.stats):
        timings.append(
            pd.Series(
                key + values[:-1],
                index=timing_colnames,
            )
        )
        for caller_key, caller_values in iteritems(values[-1]):
            callers.append(
                pd.Series(
                    key + caller_key + caller_values,
                    index=caller_columns,
                )
            )

    timings_df = pd.DataFrame(timings)
    callers_df = pd.DataFrame(callers)
    timings_df['filename:funcname'] = \
        (timings_df['filename'] + ':' + timings_df['funcname'])
    timings_df = timings_df.groupby('filename:funcname').sum()
    return timings_df, callers_df",def,_calc_frames,(,stats,),:,timings,=,[,],callers,=,[,],for,key,",",values,in,iteritems,(,stats,.,stats,),:,timings,.,append,(,pd,.,Series,(,key,+,values,[,:,-,1,],",",Compute a DataFrame summary of a Stats object.,Compute,a,DataFrame,summary,of,a,Stats,object,.,,,,,,,62148d4e01765806bc5e6bb40628cdb186482c05,https://github.com/ssanderson/pstats-view/blob/62148d4e01765806bc5e6bb40628cdb186482c05/pstatsviewer/viewer.py#L40-L66,train,index,=,timing_colnames,",",),),for,caller_key,",",caller_values,in,iteritems,(,values,[,-,1,],),:,callers,.,append,(,pd,.,Series,(,key,+,,,,,,,,,,,,,,,,,,,,caller_key,+,caller_values,",",index,=,caller_columns,",",),),timings_df,=,pd,.,DataFrame,(,timings,),callers_df,=,pd,.,DataFrame,(,callers,),timings_df,[,'filename:funcname',],=,(,timings_df,[,'filename',],+,':',+,timings_df,[,'funcname',],),timings_df,=,timings_df,.,groupby,(,'filename:funcname',),.,sum,(,),return,timings_df,",",callers_df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/unmapped.py,unmapped,"def unmapped(sam, mates):
    """"""
    get unmapped reads
    """"""
    for read in sam:
        if read.startswith('@') is True:
            continue
        read = read.strip().split()
        if read[2] == '*' and read[6] == '*':
                yield read
        elif mates is True:
            if read[2] == '*' or read[6] == '*':
                yield read
            for i in read:
                if i == 'YT:Z:UP':
                    yield read",python,"def unmapped(sam, mates):
    """"""
    get unmapped reads
    """"""
    for read in sam:
        if read.startswith('@') is True:
            continue
        read = read.strip().split()
        if read[2] == '*' and read[6] == '*':
                yield read
        elif mates is True:
            if read[2] == '*' or read[6] == '*':
                yield read
            for i in read:
                if i == 'YT:Z:UP':
                    yield read",def,unmapped,(,sam,",",mates,),:,for,read,in,sam,:,if,read,.,startswith,(,'@',),is,True,:,continue,read,=,read,.,strip,(,),.,split,(,),if,read,[,2,],==,'*',and,get unmapped reads,get,unmapped,reads,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/unmapped.py#L11-L26,train,read,[,6,],==,'*',:,yield,read,elif,mates,is,True,:,if,read,[,2,],==,'*',or,read,[,6,],==,'*',:,yield,,,,,,,,,,,,,,,,,,,,read,for,i,in,read,:,if,i,==,'YT:Z:UP',:,yield,read,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/parallel.py,parallel,"def parallel(processes, threads):
    """"""
    execute jobs in processes using N threads
    """"""
    pool = multithread(threads)
    pool.map(run_process, processes)
    pool.close()
    pool.join()",python,"def parallel(processes, threads):
    """"""
    execute jobs in processes using N threads
    """"""
    pool = multithread(threads)
    pool.map(run_process, processes)
    pool.close()
    pool.join()",def,parallel,(,processes,",",threads,),:,pool,=,multithread,(,threads,),pool,.,map,(,run_process,",",processes,),pool,.,close,(,),pool,.,join,(,),,,,,,,,,,,,execute jobs in processes using N threads,execute,jobs,in,processes,using,N,threads,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/parallel.py#L19-L26,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/log.py,define_log_renderer,"def define_log_renderer(fmt, fpath, quiet):
    """"""
    the final log processor that structlog requires to render.
    """"""
    # it must accept a logger, method_name and event_dict (just like processors)
    # but must return the rendered string, not a dictionary.
    # TODO tty logic

    if fmt:
        return structlog.processors.JSONRenderer()

    if fpath is not None:
        return structlog.processors.JSONRenderer()

    if sys.stderr.isatty() and not quiet:
        return structlog.dev.ConsoleRenderer()

    return structlog.processors.JSONRenderer()",python,"def define_log_renderer(fmt, fpath, quiet):
    """"""
    the final log processor that structlog requires to render.
    """"""
    # it must accept a logger, method_name and event_dict (just like processors)
    # but must return the rendered string, not a dictionary.
    # TODO tty logic

    if fmt:
        return structlog.processors.JSONRenderer()

    if fpath is not None:
        return structlog.processors.JSONRenderer()

    if sys.stderr.isatty() and not quiet:
        return structlog.dev.ConsoleRenderer()

    return structlog.processors.JSONRenderer()",def,define_log_renderer,(,fmt,",",fpath,",",quiet,),:,"# it must accept a logger, method_name and event_dict (just like processors)","# but must return the rendered string, not a dictionary.",# TODO tty logic,if,fmt,:,return,structlog,.,processors,.,JSONRenderer,(,),if,fpath,is,not,None,:,return,structlog,.,processors,.,JSONRenderer,(,),if,sys,.,stderr,.,the final log processor that structlog requires to render.,the,final,log,processor,that,structlog,requires,to,render,.,,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/log.py#L239-L256,train,isatty,(,),and,not,quiet,:,return,structlog,.,dev,.,ConsoleRenderer,(,),return,structlog,.,processors,.,JSONRenderer,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/log.py,_structlog_default_keys_processor,"def _structlog_default_keys_processor(logger_class, log_method, event):
    ''' Add unique id, type and hostname '''
    global HOSTNAME

    if 'id' not in event:
        event['id'] = '%s_%s' % (
            datetime.utcnow().strftime('%Y%m%dT%H%M%S'),
            uuid.uuid1().hex
        )

    if 'type' not in event:
        event['type'] = 'log'

    event['host'] = HOSTNAME

    return event",python,"def _structlog_default_keys_processor(logger_class, log_method, event):
    ''' Add unique id, type and hostname '''
    global HOSTNAME

    if 'id' not in event:
        event['id'] = '%s_%s' % (
            datetime.utcnow().strftime('%Y%m%dT%H%M%S'),
            uuid.uuid1().hex
        )

    if 'type' not in event:
        event['type'] = 'log'

    event['host'] = HOSTNAME

    return event",def,_structlog_default_keys_processor,(,logger_class,",",log_method,",",event,),:,global,HOSTNAME,if,'id',not,in,event,:,event,[,'id',],=,'%s_%s',%,(,datetime,.,utcnow,(,),.,strftime,(,'%Y%m%dT%H%M%S',),",",uuid,.,uuid1,(,),.,"Add unique id, type and hostname",Add,unique,id,type,and,hostname,,,,,,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/log.py#L258-L273,train,hex,),if,'type',not,in,event,:,event,[,'type',],=,'log',event,[,'host',],=,HOSTNAME,return,event,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/log.py,define_log_processors,"def define_log_processors():
    """"""
    log processors that structlog executes before final rendering
    """"""
    # these processors should accept logger, method_name and event_dict
    # and return a new dictionary which will be passed as event_dict to the next one.
    return [
        structlog.processors.TimeStamper(fmt=""iso""),
        _structlog_default_keys_processor,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
    ]",python,"def define_log_processors():
    """"""
    log processors that structlog executes before final rendering
    """"""
    # these processors should accept logger, method_name and event_dict
    # and return a new dictionary which will be passed as event_dict to the next one.
    return [
        structlog.processors.TimeStamper(fmt=""iso""),
        _structlog_default_keys_processor,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
    ]",def,define_log_processors,(,),:,"# these processors should accept logger, method_name and event_dict",# and return a new dictionary which will be passed as event_dict to the next one.,return,[,structlog,.,processors,.,TimeStamper,(,fmt,=,"""iso""",),",",_structlog_default_keys_processor,",",structlog,.,stdlib,.,PositionalArgumentsFormatter,(,),",",structlog,.,processors,.,StackInfoRenderer,(,),",",structlog,.,processors,.,format_exc_info,log processors that structlog executes before final rendering,log,processors,that,structlog,executes,before,final,rendering,,,,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/log.py#L352-L364,train,",",],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/log.py,_configure_logger,"def _configure_logger(fmt, quiet, level, fpath,
    pre_hooks, post_hooks, metric_grouping_interval):
    """"""
    configures a logger when required write to stderr or a file
    """"""

    # NOTE not thread safe. Multiple BaseScripts cannot be instantiated concurrently.
    level = getattr(logging, level.upper())

    global _GLOBAL_LOG_CONFIGURED
    if _GLOBAL_LOG_CONFIGURED:
        return

    # since the hooks need to run through structlog, need to wrap them like processors
    def wrap_hook(fn):
        @wraps(fn)
        def processor(logger, method_name, event_dict):
            fn(event_dict)
            return event_dict

        return processor

    processors = define_log_processors()
    processors.extend(
        [ wrap_hook(h) for h in pre_hooks ]
    )
    if metric_grouping_interval:
        processors.append(metrics_grouping_processor)

    log_renderer = define_log_renderer(fmt, fpath, quiet)
    stderr_required = (not quiet)
    pretty_to_stderr = (
        stderr_required
        and (
            fmt == ""pretty""
            or (fmt is None and sys.stderr.isatty())
        )
    )

    should_inject_pretty_renderer = (
        pretty_to_stderr
        and not isinstance(log_renderer, structlog.dev.ConsoleRenderer)
    )
    if should_inject_pretty_renderer:
        stderr_required = False
        processors.append(StderrConsoleRenderer())

    processors.append(log_renderer)
    processors.extend(
        [ wrap_hook(h) for h in post_hooks ]
    )

    streams = []
    # we need to use a stream if we are writing to both file and stderr, and both are json
    if stderr_required:
        streams.append(sys.stderr)

    if fpath is not None:
        # TODO handle creating a directory for this log file ?
        # TODO set mode and encoding appropriately
        streams.append(open(fpath, 'a'))

    assert len(streams) != 0, ""cannot configure logger for 0 streams""

    stream = streams[0] if len(streams) == 1 else Stream(*streams)
    atexit.register(stream.close)

    # a global level struct log config unless otherwise specified.
    structlog.configure(
        processors=processors,
        context_class=dict,
        logger_factory=LevelLoggerFactory(stream, level=level),
        wrapper_class=BoundLevelLogger,
        cache_logger_on_first_use=True,
    )

    # TODO take care of removing other handlers
    stdlib_root_log = logging.getLogger()
    stdlib_root_log.addHandler(StdlibStructlogHandler())
    stdlib_root_log.setLevel(level)

    _GLOBAL_LOG_CONFIGURED = True",python,"def _configure_logger(fmt, quiet, level, fpath,
    pre_hooks, post_hooks, metric_grouping_interval):
    """"""
    configures a logger when required write to stderr or a file
    """"""

    # NOTE not thread safe. Multiple BaseScripts cannot be instantiated concurrently.
    level = getattr(logging, level.upper())

    global _GLOBAL_LOG_CONFIGURED
    if _GLOBAL_LOG_CONFIGURED:
        return

    # since the hooks need to run through structlog, need to wrap them like processors
    def wrap_hook(fn):
        @wraps(fn)
        def processor(logger, method_name, event_dict):
            fn(event_dict)
            return event_dict

        return processor

    processors = define_log_processors()
    processors.extend(
        [ wrap_hook(h) for h in pre_hooks ]
    )
    if metric_grouping_interval:
        processors.append(metrics_grouping_processor)

    log_renderer = define_log_renderer(fmt, fpath, quiet)
    stderr_required = (not quiet)
    pretty_to_stderr = (
        stderr_required
        and (
            fmt == ""pretty""
            or (fmt is None and sys.stderr.isatty())
        )
    )

    should_inject_pretty_renderer = (
        pretty_to_stderr
        and not isinstance(log_renderer, structlog.dev.ConsoleRenderer)
    )
    if should_inject_pretty_renderer:
        stderr_required = False
        processors.append(StderrConsoleRenderer())

    processors.append(log_renderer)
    processors.extend(
        [ wrap_hook(h) for h in post_hooks ]
    )

    streams = []
    # we need to use a stream if we are writing to both file and stderr, and both are json
    if stderr_required:
        streams.append(sys.stderr)

    if fpath is not None:
        # TODO handle creating a directory for this log file ?
        # TODO set mode and encoding appropriately
        streams.append(open(fpath, 'a'))

    assert len(streams) != 0, ""cannot configure logger for 0 streams""

    stream = streams[0] if len(streams) == 1 else Stream(*streams)
    atexit.register(stream.close)

    # a global level struct log config unless otherwise specified.
    structlog.configure(
        processors=processors,
        context_class=dict,
        logger_factory=LevelLoggerFactory(stream, level=level),
        wrapper_class=BoundLevelLogger,
        cache_logger_on_first_use=True,
    )

    # TODO take care of removing other handlers
    stdlib_root_log = logging.getLogger()
    stdlib_root_log.addHandler(StdlibStructlogHandler())
    stdlib_root_log.setLevel(level)

    _GLOBAL_LOG_CONFIGURED = True",def,_configure_logger,(,fmt,",",quiet,",",level,",",fpath,",",pre_hooks,",",post_hooks,",",metric_grouping_interval,),:,# NOTE not thread safe. Multiple BaseScripts cannot be instantiated concurrently.,level,=,getattr,(,logging,",",level,.,upper,(,),),global,_GLOBAL_LOG_CONFIGURED,if,_GLOBAL_LOG_CONFIGURED,:,return,"# since the hooks need to run through structlog, need to wrap them like processors",def,wrap_hook,(,fn,),configures a logger when required write to stderr or a file,configures,a,logger,when,required,write,to,stderr,or,a,file,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/log.py#L366-L447,train,:,@,wraps,(,fn,),def,processor,(,logger,",",method_name,",",event_dict,),:,fn,(,event_dict,),return,event_dict,return,processor,processors,=,define_log_processors,(,),processors,,,,,,,,,,,,,,,,,,,,.,extend,(,[,wrap_hook,(,h,),for,h,in,pre_hooks,],),if,metric_grouping_interval,:,processors,.,append,(,metrics_grouping_processor,),log_renderer,=,define_log_renderer,(,fmt,",",fpath,",",quiet,),stderr_required,=,(,not,quiet,),pretty_to_stderr,=,(,stderr_required,and,(,fmt,==,"""pretty""",or,(,fmt,is,None,and,sys,.,stderr,.,isatty,(,),),),),should_inject_pretty_renderer,=,(,pretty_to_stderr,and,not,isinstance,(,log_renderer,",",structlog,.,dev,.,ConsoleRenderer,),),if,should_inject_pretty_renderer,:,stderr_required,=,False,processors,.,append,(,StderrConsoleRenderer,(,),),processors,.,append,(,log_renderer,),processors,.,extend,(,[,wrap_hook,(,h,),for,h,in,post_hooks,],),streams,=,[,],"# we need to use a stream if we are writing to both file and stderr, and both are json",if,stderr_required,:,streams,.,append,(,sys,.,stderr,),if,fpath,is,not,None,:,# TODO handle creating a directory for this log file ?,# TODO set mode and encoding appropriately,streams,.,append,(,open,(,fpath,",",'a',),),assert,len,(,streams,),!=,0,",","""cannot configure logger for 0 streams""",stream,=,streams,[,0,],if,len,(,streams,),==,1,else,Stream,(,*,streams,),atexit,.,register,(,stream,.,close,),# a global level struct log config unless otherwise specified.,structlog,.,configure,(,processors,=,processors,",",context_class,=,dict,",",logger_factory,=,LevelLoggerFactory,(,stream,",",level,=,level,),",",wrapper_class,=,BoundLevelLogger,",",cache_logger_on_first_use,=,True,",",),# TODO take care of removing other handlers,stdlib_root_log,=,logging,.,getLogger,(,),stdlib_root_log,.,addHandler,(,StdlibStructlogHandler,(,),),stdlib_root_log,.,setLevel,(,level,),_GLOBAL_LOG_CONFIGURED,=,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/log.py,BoundLevelLogger._add_base_info,"def _add_base_info(self, event_dict):
        """"""
        Instead of using a processor, adding basic information like caller, filename etc
        here.
        """"""
        f = sys._getframe()
        level_method_frame = f.f_back
        caller_frame = level_method_frame.f_back
        return event_dict",python,"def _add_base_info(self, event_dict):
        """"""
        Instead of using a processor, adding basic information like caller, filename etc
        here.
        """"""
        f = sys._getframe()
        level_method_frame = f.f_back
        caller_frame = level_method_frame.f_back
        return event_dict",def,_add_base_info,(,self,",",event_dict,),:,f,=,sys,.,_getframe,(,),level_method_frame,=,f,.,f_back,caller_frame,=,level_method_frame,.,f_back,return,event_dict,,,,,,,,,,,,,,,,,"Instead of using a processor, adding basic information like caller, filename etc
        here.",Instead,of,using,a,processor,adding,basic,information,like,caller,filename,etc,here,.,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/log.py#L121-L129,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/log.py,BoundLevelLogger._proxy_to_logger,"def _proxy_to_logger(self, method_name, event, *event_args,
                         **event_kw):
        """"""
        Propagate a method call to the wrapped logger.

        This is the same as the superclass implementation, except that
        it also preserves positional arguments in the `event_dict` so
        that the stdblib's support for format strings can be used.
        """"""

        if isinstance(event, bytes):
            event = event.decode('utf-8')

        if event_args:
            event_kw['positional_args'] = event_args

        return super(BoundLevelLogger, self)._proxy_to_logger(method_name,
                                                         event=event,
                                                         **event_kw)",python,"def _proxy_to_logger(self, method_name, event, *event_args,
                         **event_kw):
        """"""
        Propagate a method call to the wrapped logger.

        This is the same as the superclass implementation, except that
        it also preserves positional arguments in the `event_dict` so
        that the stdblib's support for format strings can be used.
        """"""

        if isinstance(event, bytes):
            event = event.decode('utf-8')

        if event_args:
            event_kw['positional_args'] = event_args

        return super(BoundLevelLogger, self)._proxy_to_logger(method_name,
                                                         event=event,
                                                         **event_kw)",def,_proxy_to_logger,(,self,",",method_name,",",event,",",*,event_args,",",*,*,event_kw,),:,if,isinstance,(,event,",",bytes,),:,event,=,event,.,decode,(,'utf-8',),if,event_args,:,event_kw,[,'positional_args',],=,event_args,return,"Propagate a method call to the wrapped logger.

        This is the same as the superclass implementation, except that
        it also preserves positional arguments in the `event_dict` so
        that the stdblib's support for format strings can be used.",Propagate,a,method,call,to,the,wrapped,logger,.,,,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/log.py#L211-L229,train,super,(,BoundLevelLogger,",",self,),.,_proxy_to_logger,(,method_name,",",event,=,event,",",*,*,event_kw,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/core_overlap_plot.py,translate,"def translate(rect, x, y, width=1):
    """"""
    Given four points of a rectangle, translate the
    rectangle to the specified x and y coordinates and,
    optionally, change the width.

    :type rect: list of tuples
    :param rect: Four points describing a rectangle.
    :type x: float
    :param x: The amount to shift the rectangle along the x-axis.
    :type y: float
    :param y: The amount to shift the rectangle along the y-axis.
    :type width: float
    :param width: The amount by which to change the width of the
                  rectangle.
    """"""
    return ((rect[0][0]+x, rect[0][1]+y), (rect[1][0]+x, rect[1][1]+y),
            (rect[2][0]+x+width, rect[2][1]+y), (rect[3][0]+x+width, rect[3][1]+y))",python,"def translate(rect, x, y, width=1):
    """"""
    Given four points of a rectangle, translate the
    rectangle to the specified x and y coordinates and,
    optionally, change the width.

    :type rect: list of tuples
    :param rect: Four points describing a rectangle.
    :type x: float
    :param x: The amount to shift the rectangle along the x-axis.
    :type y: float
    :param y: The amount to shift the rectangle along the y-axis.
    :type width: float
    :param width: The amount by which to change the width of the
                  rectangle.
    """"""
    return ((rect[0][0]+x, rect[0][1]+y), (rect[1][0]+x, rect[1][1]+y),
            (rect[2][0]+x+width, rect[2][1]+y), (rect[3][0]+x+width, rect[3][1]+y))",def,translate,(,rect,",",x,",",y,",",width,=,1,),:,return,(,(,rect,[,0,],[,0,],+,x,",",rect,[,0,],[,1,],+,y,),",",(,rect,[,1,],"Given four points of a rectangle, translate the
    rectangle to the specified x and y coordinates and,
    optionally, change the width.

    :type rect: list of tuples
    :param rect: Four points describing a rectangle.
    :type x: float
    :param x: The amount to shift the rectangle along the x-axis.
    :type y: float
    :param y: The amount to shift the rectangle along the y-axis.
    :type width: float
    :param width: The amount by which to change the width of the
                  rectangle.",Given,four,points,of,a,rectangle,translate,the,rectangle,to,the,specified,x,and,y,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/core_overlap_plot.py#L57-L74,train,[,0,],+,x,",",rect,[,1,],[,1,],+,y,),",",(,rect,[,2,],[,0,],+,x,+,width,",",coordinates,and,optionally,change,the,width,.,,,,,,,,,,,,,rect,[,2,],[,1,],+,y,),",",(,rect,[,3,],[,0,],+,x,+,width,",",rect,[,3,],[,1,],+,y,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rax.py,remove_bad,"def remove_bad(string):
    """"""
    remove problem characters from string
    """"""
    remove = [':', ',', '(', ')', ' ', '|', ';', '\'']
    for c in remove:
        string = string.replace(c, '_')
    return string",python,"def remove_bad(string):
    """"""
    remove problem characters from string
    """"""
    remove = [':', ',', '(', ')', ' ', '|', ';', '\'']
    for c in remove:
        string = string.replace(c, '_')
    return string",def,remove_bad,(,string,),:,remove,=,[,':',",","','",",",'(',",",')',",",' ',",",'|',",",';',",",'\'',],for,c,in,remove,:,string,=,string,.,replace,(,c,",",'_',),return,string,,remove problem characters from string,remove,problem,characters,from,string,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rax.py#L43-L50,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rax.py,get_ids,"def get_ids(a):
    """"""
    make copy of sequences with short identifier
    """"""
    a_id = '%s.id.fa' % (a.rsplit('.', 1)[0])
    a_id_lookup = '%s.id.lookup' % (a.rsplit('.', 1)[0])
    if check(a_id) is True:
        return a_id, a_id_lookup
    a_id_f = open(a_id, 'w')
    a_id_lookup_f = open(a_id_lookup, 'w')
    ids = []
    for seq in parse_fasta(open(a)):
        id = id_generator() 
        while id in ids:
            id = id_generator() 
        ids.append(id)
        header = seq[0].split('>')[1]
        name = remove_bad(header)
        seq[0] = '>%s %s' % (id, header)
        print('\n'.join(seq), file=a_id_f)
        print('%s\t%s\t%s' % (id, name, header), file=a_id_lookup_f)
    return a_id, a_id_lookup",python,"def get_ids(a):
    """"""
    make copy of sequences with short identifier
    """"""
    a_id = '%s.id.fa' % (a.rsplit('.', 1)[0])
    a_id_lookup = '%s.id.lookup' % (a.rsplit('.', 1)[0])
    if check(a_id) is True:
        return a_id, a_id_lookup
    a_id_f = open(a_id, 'w')
    a_id_lookup_f = open(a_id_lookup, 'w')
    ids = []
    for seq in parse_fasta(open(a)):
        id = id_generator() 
        while id in ids:
            id = id_generator() 
        ids.append(id)
        header = seq[0].split('>')[1]
        name = remove_bad(header)
        seq[0] = '>%s %s' % (id, header)
        print('\n'.join(seq), file=a_id_f)
        print('%s\t%s\t%s' % (id, name, header), file=a_id_lookup_f)
    return a_id, a_id_lookup",def,get_ids,(,a,),:,a_id,=,'%s.id.fa',%,(,a,.,rsplit,(,'.',",",1,),[,0,],),a_id_lookup,=,'%s.id.lookup',%,(,a,.,rsplit,(,'.',",",1,),[,0,],),if,check,(,make copy of sequences with short identifier,make,copy,of,sequences,with,short,identifier,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rax.py#L55-L76,train,a_id,),is,True,:,return,a_id,",",a_id_lookup,a_id_f,=,open,(,a_id,",",'w',),a_id_lookup_f,=,open,(,a_id_lookup,",",'w',),ids,=,[,],for,,,,,,,,,,,,,,,,,,,,seq,in,parse_fasta,(,open,(,a,),),:,id,=,id_generator,(,),while,id,in,ids,:,id,=,id_generator,(,),ids,.,append,(,id,),header,=,seq,[,0,],.,split,(,'>',),[,1,],name,=,remove_bad,(,header,),seq,[,0,],=,'>%s %s',%,(,id,",",header,),print,(,'\n',.,join,(,seq,),",",file,=,a_id_f,),print,(,'%s\t%s\t%s',%,(,id,",",name,",",header,),",",file,=,a_id_lookup_f,),return,a_id,",",a_id_lookup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rax.py,convert2phylip,"def convert2phylip(convert):
    """"""
    convert fasta to phylip because RAxML is ridiculous
    """"""
    out = '%s.phy' % (convert.rsplit('.', 1)[0])
    if check(out) is False:
        convert = open(convert, 'rU')
        out_f = open(out, 'w')
        alignments = AlignIO.parse(convert, ""fasta"")
        AlignIO.write(alignments, out, ""phylip"")
    return out",python,"def convert2phylip(convert):
    """"""
    convert fasta to phylip because RAxML is ridiculous
    """"""
    out = '%s.phy' % (convert.rsplit('.', 1)[0])
    if check(out) is False:
        convert = open(convert, 'rU')
        out_f = open(out, 'w')
        alignments = AlignIO.parse(convert, ""fasta"")
        AlignIO.write(alignments, out, ""phylip"")
    return out",def,convert2phylip,(,convert,),:,out,=,'%s.phy',%,(,convert,.,rsplit,(,'.',",",1,),[,0,],),if,check,(,out,),is,False,:,convert,=,open,(,convert,",",'rU',),out_f,=,open,(,convert fasta to phylip because RAxML is ridiculous,convert,fasta,to,phylip,because,RAxML,is,ridiculous,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rax.py#L78-L88,train,out,",",'w',),alignments,=,AlignIO,.,parse,(,convert,",","""fasta""",),AlignIO,.,write,(,alignments,",",out,",","""phylip""",),return,out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rax.py,run_iqtree,"def run_iqtree(phy, model, threads, cluster, node):
    """"""
    run IQ-Tree
    """"""
    # set ppn based on threads
    if threads > 24:
        ppn = 24
    else:
        ppn = threads
    tree = '%s.treefile' % (phy)
    if check(tree) is False:
        if model is False:
            model = 'TEST'
        dir = os.getcwd()
        command = 'iqtree-omp -s %s -m %s -nt %s -quiet' % \
                    (phy, model, threads)
        if cluster is False:
            p = Popen(command, shell = True)
        else:
            if node is False:
               node = '1'
            qsub = 'qsub -l nodes=%s:ppn=%s -m e -N iqtree' % (node, ppn)
            command = 'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree' \
                    % (dir, phy, command, dir)
            re_call = 'cd %s; %s --no-fast --iq' % (dir.rsplit('/', 1)[0], ' '.join(sys.argv))
            p = Popen('echo ""%s;%s"" | %s' % (command, re_call, qsub), shell = True)
        p.communicate()
    return tree",python,"def run_iqtree(phy, model, threads, cluster, node):
    """"""
    run IQ-Tree
    """"""
    # set ppn based on threads
    if threads > 24:
        ppn = 24
    else:
        ppn = threads
    tree = '%s.treefile' % (phy)
    if check(tree) is False:
        if model is False:
            model = 'TEST'
        dir = os.getcwd()
        command = 'iqtree-omp -s %s -m %s -nt %s -quiet' % \
                    (phy, model, threads)
        if cluster is False:
            p = Popen(command, shell = True)
        else:
            if node is False:
               node = '1'
            qsub = 'qsub -l nodes=%s:ppn=%s -m e -N iqtree' % (node, ppn)
            command = 'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree' \
                    % (dir, phy, command, dir)
            re_call = 'cd %s; %s --no-fast --iq' % (dir.rsplit('/', 1)[0], ' '.join(sys.argv))
            p = Popen('echo ""%s;%s"" | %s' % (command, re_call, qsub), shell = True)
        p.communicate()
    return tree",def,run_iqtree,(,phy,",",model,",",threads,",",cluster,",",node,),:,# set ppn based on threads,if,threads,>,24,:,ppn,=,24,else,:,ppn,=,threads,tree,=,'%s.treefile',%,(,phy,),if,check,(,tree,),is,False,:,run IQ-Tree,run,IQ,-,Tree,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rax.py#L163-L190,train,if,model,is,False,:,model,=,'TEST',dir,=,os,.,getcwd,(,),command,=,'iqtree-omp -s %s -m %s -nt %s -quiet',%,(,phy,",",model,",",threads,),if,cluster,is,False,,,,,,,,,,,,,,,,,,,,:,p,=,Popen,(,command,",",shell,=,True,),else,:,if,node,is,False,:,node,=,'1',qsub,=,'qsub -l nodes=%s:ppn=%s -m e -N iqtree',%,(,node,",",ppn,),command,=,'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree',%,(,dir,",",phy,",",command,",",dir,),re_call,=,'cd %s; %s --no-fast --iq',%,(,dir,.,rsplit,(,'/',",",1,),[,0,],",",' ',.,join,(,sys,.,argv,),),p,=,Popen,(,"'echo ""%s;%s"" | %s'",%,(,command,",",re_call,",",qsub,),",",shell,=,True,),p,.,communicate,(,),return,tree,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rax.py,fix_tree,"def fix_tree(tree, a_id_lookup, out):
    """"""
    get the names for sequences in the raxml tree
    """"""
    if check(out) is False and check(tree) is True:
        tree = open(tree).read()
        for line in open(a_id_lookup):
            id, name, header = line.strip().split('\t')
            tree = tree.replace(id+':', name+':')
        out_f = open(out, 'w')
        print(tree.strip(), file=out_f)
    return out",python,"def fix_tree(tree, a_id_lookup, out):
    """"""
    get the names for sequences in the raxml tree
    """"""
    if check(out) is False and check(tree) is True:
        tree = open(tree).read()
        for line in open(a_id_lookup):
            id, name, header = line.strip().split('\t')
            tree = tree.replace(id+':', name+':')
        out_f = open(out, 'w')
        print(tree.strip(), file=out_f)
    return out",def,fix_tree,(,tree,",",a_id_lookup,",",out,),:,if,check,(,out,),is,False,and,check,(,tree,),is,True,:,tree,=,open,(,tree,),.,read,(,),for,line,in,open,(,a_id_lookup,),:,get the names for sequences in the raxml tree,get,the,names,for,sequences,in,the,raxml,tree,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rax.py#L192-L203,train,id,",",name,",",header,=,line,.,strip,(,),.,split,(,'\t',),tree,=,tree,.,replace,(,id,+,':',",",name,+,':',),,,,,,,,,,,,,,,,,,,,out_f,=,open,(,out,",",'w',),print,(,tree,.,strip,(,),",",file,=,out_f,),return,out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/__init__.py,create_cluster,"def create_cluster(settings):
    """"""
    Creates a new Nydus cluster from the given settings.

    :param settings: Dictionary of the cluster settings.
    :returns: Configured instance of ``nydus.db.base.Cluster``.

    >>> redis = create_cluster({
    >>>     'backend': 'nydus.db.backends.redis.Redis',
    >>>     'router': 'nydus.db.routers.redis.PartitionRouter',
    >>>     'defaults': {
    >>>         'host': 'localhost',
    >>>         'port': 6379,
    >>>     },
    >>>     'hosts': {
    >>>         0: {'db': 0},
    >>>         1: {'db': 1},
    >>>         2: {'db': 2},
    >>>     }
    >>> })
    """"""
    # Pull in our client
    settings = copy.deepcopy(settings)
    backend = settings.pop('engine', settings.pop('backend', None))
    if isinstance(backend, basestring):
        Conn = import_string(backend)
    elif backend:
        Conn = backend
    else:
        raise KeyError('backend')

    # Pull in our cluster
    cluster = settings.pop('cluster', None)
    if not cluster:
        Cluster = Conn.get_cluster()
    elif isinstance(cluster, basestring):
        Cluster = import_string(cluster)
    else:
        Cluster = cluster

    # Pull in our router
    router = settings.pop('router', None)
    if not router:
        Router = BaseRouter
    elif isinstance(router, basestring):
        Router = import_string(router)
    else:
        Router = router

    # Build the connection cluster
    return Cluster(
        router=Router,
        backend=Conn,
        **settings
    )",python,"def create_cluster(settings):
    """"""
    Creates a new Nydus cluster from the given settings.

    :param settings: Dictionary of the cluster settings.
    :returns: Configured instance of ``nydus.db.base.Cluster``.

    >>> redis = create_cluster({
    >>>     'backend': 'nydus.db.backends.redis.Redis',
    >>>     'router': 'nydus.db.routers.redis.PartitionRouter',
    >>>     'defaults': {
    >>>         'host': 'localhost',
    >>>         'port': 6379,
    >>>     },
    >>>     'hosts': {
    >>>         0: {'db': 0},
    >>>         1: {'db': 1},
    >>>         2: {'db': 2},
    >>>     }
    >>> })
    """"""
    # Pull in our client
    settings = copy.deepcopy(settings)
    backend = settings.pop('engine', settings.pop('backend', None))
    if isinstance(backend, basestring):
        Conn = import_string(backend)
    elif backend:
        Conn = backend
    else:
        raise KeyError('backend')

    # Pull in our cluster
    cluster = settings.pop('cluster', None)
    if not cluster:
        Cluster = Conn.get_cluster()
    elif isinstance(cluster, basestring):
        Cluster = import_string(cluster)
    else:
        Cluster = cluster

    # Pull in our router
    router = settings.pop('router', None)
    if not router:
        Router = BaseRouter
    elif isinstance(router, basestring):
        Router = import_string(router)
    else:
        Router = router

    # Build the connection cluster
    return Cluster(
        router=Router,
        backend=Conn,
        **settings
    )",def,create_cluster,(,settings,),:,# Pull in our client,settings,=,copy,.,deepcopy,(,settings,),backend,=,settings,.,pop,(,'engine',",",settings,.,pop,(,'backend',",",None,),),if,isinstance,(,backend,",",basestring,),:,Conn,=,import_string,"Creates a new Nydus cluster from the given settings.

    :param settings: Dictionary of the cluster settings.
    :returns: Configured instance of ``nydus.db.base.Cluster``.

    >>> redis = create_cluster({
    >>>     'backend': 'nydus.db.backends.redis.Redis',
    >>>     'router': 'nydus.db.routers.redis.PartitionRouter',
    >>>     'defaults': {
    >>>         'host': 'localhost',
    >>>         'port': 6379,
    >>>     },
    >>>     'hosts': {
    >>>         0: {'db': 0},
    >>>         1: {'db': 1},
    >>>         2: {'db': 2},
    >>>     }
    >>> })",Creates,a,new,Nydus,cluster,from,the,given,settings,.,,,,,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/__init__.py#L28-L82,train,(,backend,),elif,backend,:,Conn,=,backend,else,:,raise,KeyError,(,'backend',),# Pull in our cluster,cluster,=,settings,.,pop,(,'cluster',",",None,),if,not,cluster,,,,,,,,,,,,,,,,,,,,:,Cluster,=,Conn,.,get_cluster,(,),elif,isinstance,(,cluster,",",basestring,),:,Cluster,=,import_string,(,cluster,),else,:,Cluster,=,cluster,# Pull in our router,router,=,settings,.,pop,(,'router',",",None,),if,not,router,:,Router,=,BaseRouter,elif,isinstance,(,router,",",basestring,),:,Router,=,import_string,(,router,),else,:,Router,=,router,# Build the connection cluster,return,Cluster,(,router,=,Router,",",backend,=,Conn,",",*,*,settings,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dokterbob/django-multilingual-model,multilingual_model/models.py,MultilingualModel._get_translation,"def _get_translation(self, field, code):
        """"""
        Gets the translation of a specific field for a specific language code.

        This raises ObjectDoesNotExist if the lookup was unsuccesful. As of
        today, this stuff is cached. As the cache is rather aggressive it
        might cause rather strange effects. However, we would see the same
        effects when an ordinary object is changed which is already in memory:
        the old state would remain.
        """"""

        if not code in self._translation_cache:
            translations = self.translations.select_related()

            logger.debug(
                u'Matched with field %s for language %s. Attempting lookup.',
                field, code
            )

            try:
                translation_obj = translations.get(language_code=code)

            except ObjectDoesNotExist:
                translation_obj = None

            self._translation_cache[code] = translation_obj

            logger.debug(u'Translation not found in cache.')

        else:
            logger.debug(u'Translation found in cache.')
            # Get the translation from the cache
            translation_obj = self._translation_cache.get(code)

        # If this is none, it means that a translation does not exist
        # It is important to cache this one as well
        if not translation_obj:
            raise ObjectDoesNotExist

        field_value = getattr(translation_obj, field)

        logger.debug(
            u'Found translation object %s, returning value %s.',
            translation_obj, field_value
        )

        return field_value",python,"def _get_translation(self, field, code):
        """"""
        Gets the translation of a specific field for a specific language code.

        This raises ObjectDoesNotExist if the lookup was unsuccesful. As of
        today, this stuff is cached. As the cache is rather aggressive it
        might cause rather strange effects. However, we would see the same
        effects when an ordinary object is changed which is already in memory:
        the old state would remain.
        """"""

        if not code in self._translation_cache:
            translations = self.translations.select_related()

            logger.debug(
                u'Matched with field %s for language %s. Attempting lookup.',
                field, code
            )

            try:
                translation_obj = translations.get(language_code=code)

            except ObjectDoesNotExist:
                translation_obj = None

            self._translation_cache[code] = translation_obj

            logger.debug(u'Translation not found in cache.')

        else:
            logger.debug(u'Translation found in cache.')
            # Get the translation from the cache
            translation_obj = self._translation_cache.get(code)

        # If this is none, it means that a translation does not exist
        # It is important to cache this one as well
        if not translation_obj:
            raise ObjectDoesNotExist

        field_value = getattr(translation_obj, field)

        logger.debug(
            u'Found translation object %s, returning value %s.',
            translation_obj, field_value
        )

        return field_value",def,_get_translation,(,self,",",field,",",code,),:,if,not,code,in,self,.,_translation_cache,:,translations,=,self,.,translations,.,select_related,(,),logger,.,debug,(,u'Matched with field %s for language %s. Attempting lookup.',",",field,",",code,),try,:,translation_obj,=,translations,.,"Gets the translation of a specific field for a specific language code.

        This raises ObjectDoesNotExist if the lookup was unsuccesful. As of
        today, this stuff is cached. As the cache is rather aggressive it
        might cause rather strange effects. However, we would see the same
        effects when an ordinary object is changed which is already in memory:
        the old state would remain.",Gets,the,translation,of,a,specific,field,for,a,specific,language,code,.,,,2479b2c3d6f7b697e95aa1e082c8bc8699f1f638,https://github.com/dokterbob/django-multilingual-model/blob/2479b2c3d6f7b697e95aa1e082c8bc8699f1f638/multilingual_model/models.py#L44-L90,train,get,(,language_code,=,code,),except,ObjectDoesNotExist,:,translation_obj,=,None,self,.,_translation_cache,[,code,],=,translation_obj,logger,.,debug,(,u'Translation not found in cache.',),else,:,logger,.,,,,,,,,,,,,,,,,,,,,debug,(,u'Translation found in cache.',),# Get the translation from the cache,translation_obj,=,self,.,_translation_cache,.,get,(,code,),"# If this is none, it means that a translation does not exist",# It is important to cache this one as well,if,not,translation_obj,:,raise,ObjectDoesNotExist,field_value,=,getattr,(,translation_obj,",",field,),logger,.,debug,(,"u'Found translation object %s, returning value %s.'",",",translation_obj,",",field_value,),return,field_value,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dokterbob/django-multilingual-model,multilingual_model/models.py,MultilingualModel.unicode_wrapper,"def unicode_wrapper(self, property, default=ugettext('Untitled')):
        """"""
        Wrapper to allow for easy unicode representation of an object by
        the specified property. If this wrapper is not able to find the
        right translation of the specified property, it will return the
        default value instead.

        Example::
            def __unicode__(self):
                return unicode_wrapper('name', default='Unnamed')

        """"""
        # TODO: Test coverage!
        try:
            value = getattr(self, property)
        except ValueError:
            logger.warn(
                u'ValueError rendering unicode for %s object.',
                self._meta.object_name
            )

            value = None

        if not value:
            value = default

        return value",python,"def unicode_wrapper(self, property, default=ugettext('Untitled')):
        """"""
        Wrapper to allow for easy unicode representation of an object by
        the specified property. If this wrapper is not able to find the
        right translation of the specified property, it will return the
        default value instead.

        Example::
            def __unicode__(self):
                return unicode_wrapper('name', default='Unnamed')

        """"""
        # TODO: Test coverage!
        try:
            value = getattr(self, property)
        except ValueError:
            logger.warn(
                u'ValueError rendering unicode for %s object.',
                self._meta.object_name
            )

            value = None

        if not value:
            value = default

        return value",def,unicode_wrapper,(,self,",",property,",",default,=,ugettext,(,'Untitled',),),:,# TODO: Test coverage!,try,:,value,=,getattr,(,self,",",property,),except,ValueError,:,logger,.,warn,(,u'ValueError rendering unicode for %s object.',",",self,.,_meta,.,object_name,),value,=,"Wrapper to allow for easy unicode representation of an object by
        the specified property. If this wrapper is not able to find the
        right translation of the specified property, it will return the
        default value instead.

        Example::
            def __unicode__(self):
                return unicode_wrapper('name', default='Unnamed')",Wrapper,to,allow,for,easy,unicode,representation,of,an,object,by,the,specified,property,.,2479b2c3d6f7b697e95aa1e082c8bc8699f1f638,https://github.com/dokterbob/django-multilingual-model/blob/2479b2c3d6f7b697e95aa1e082c8bc8699f1f638/multilingual_model/models.py#L202-L228,train,None,if,not,value,:,value,=,default,return,value,,,,,,,,,,,,,,,,,,,,,If,this,wrapper,is,not,able,to,find,the,right,translation,of,the,specified,property,it,will,return,the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,default,value,instead,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/strip_align_inserts.py,strip_inserts,"def strip_inserts(fasta):
    """"""
    remove insertion columns from aligned fasta file
    """"""
    for seq in parse_fasta(fasta):
        seq[1] = ''.join([b for b in seq[1] if b == '-' or b.isupper()])
        yield seq",python,"def strip_inserts(fasta):
    """"""
    remove insertion columns from aligned fasta file
    """"""
    for seq in parse_fasta(fasta):
        seq[1] = ''.join([b for b in seq[1] if b == '-' or b.isupper()])
        yield seq",def,strip_inserts,(,fasta,),:,for,seq,in,parse_fasta,(,fasta,),:,seq,[,1,],=,'',.,join,(,[,b,for,b,in,seq,[,1,],if,b,==,'-',or,b,.,isupper,(,),],remove insertion columns from aligned fasta file,remove,insertion,columns,from,aligned,fasta,file,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_align_inserts.py#L12-L18,train,),yield,seq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cldf/segments,src/segments/tokenizer.py,Tokenizer.transform,"def transform(self, word, column=Profile.GRAPHEME_COL, error=errors.replace):
        """"""
        Transform a string's graphemes into the mappings given in a different column
        in the orthography profile.

        Parameters
        ----------
        word : str
            The input string to be tokenized.

        column : str (default = ""Grapheme"")
            The label of the column to transform to. Default it to tokenize with
            orthography profile.

        Returns
        -------
        result : list of lists
            Result of the transformation.

        """"""
        assert self.op, 'method can only be called with orthography profile.'

        if column != Profile.GRAPHEME_COL and column not in self.op.column_labels:
            raise ValueError(""Column {0} not found in profile."".format(column))

        word = self.op.tree.parse(word, error)
        if column == Profile.GRAPHEME_COL:
            return word
        out = []
        for token in word:
            try:
                target = self.op.graphemes[token][column]
            except KeyError:
                target = self._errors['replace'](token)
            if target is not None:
                if isinstance(target, (tuple, list)):
                    out.extend(target)
                else:
                    out.append(target)
        return out",python,"def transform(self, word, column=Profile.GRAPHEME_COL, error=errors.replace):
        """"""
        Transform a string's graphemes into the mappings given in a different column
        in the orthography profile.

        Parameters
        ----------
        word : str
            The input string to be tokenized.

        column : str (default = ""Grapheme"")
            The label of the column to transform to. Default it to tokenize with
            orthography profile.

        Returns
        -------
        result : list of lists
            Result of the transformation.

        """"""
        assert self.op, 'method can only be called with orthography profile.'

        if column != Profile.GRAPHEME_COL and column not in self.op.column_labels:
            raise ValueError(""Column {0} not found in profile."".format(column))

        word = self.op.tree.parse(word, error)
        if column == Profile.GRAPHEME_COL:
            return word
        out = []
        for token in word:
            try:
                target = self.op.graphemes[token][column]
            except KeyError:
                target = self._errors['replace'](token)
            if target is not None:
                if isinstance(target, (tuple, list)):
                    out.extend(target)
                else:
                    out.append(target)
        return out",def,transform,(,self,",",word,",",column,=,Profile,.,GRAPHEME_COL,",",error,=,errors,.,replace,),:,assert,self,.,op,",",'method can only be called with orthography profile.',if,column,!=,Profile,.,GRAPHEME_COL,and,column,not,in,self,.,op,.,column_labels,:,raise,"Transform a string's graphemes into the mappings given in a different column
        in the orthography profile.

        Parameters
        ----------
        word : str
            The input string to be tokenized.

        column : str (default = ""Grapheme"")
            The label of the column to transform to. Default it to tokenize with
            orthography profile.

        Returns
        -------
        result : list of lists
            Result of the transformation.",Transform,a,string,s,graphemes,into,the,mappings,given,in,a,different,column,in,the,9136a4ec89555bf9b574399ffbb07f3cc9a9f45f,https://github.com/cldf/segments/blob/9136a4ec89555bf9b574399ffbb07f3cc9a9f45f/src/segments/tokenizer.py#L231-L270,train,ValueError,(,"""Column {0} not found in profile.""",.,format,(,column,),),word,=,self,.,op,.,tree,.,parse,(,word,",",error,),if,column,==,Profile,.,GRAPHEME_COL,:,orthography,profile,.,,,,,,,,,,,,,,,,,return,word,out,=,[,],for,token,in,word,:,try,:,target,=,self,.,op,.,graphemes,[,token,],[,column,],except,KeyError,:,target,=,self,.,_errors,[,'replace',],(,token,),if,target,is,not,None,:,if,isinstance,(,target,",",(,tuple,",",list,),),:,out,.,extend,(,target,),else,:,out,.,append,(,target,),return,out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cldf/segments,src/segments/tokenizer.py,Tokenizer.rules,"def rules(self, word):
        """"""
        Function to tokenize input string and return output of str with ortho rules
        applied.

        Parameters
        ----------
        word : str
            The input string to be tokenized.

        Returns
        -------
        result : str
            Result of the orthography rules applied to the input str.

        """"""
        return self._rules.apply(word) if self._rules else word",python,"def rules(self, word):
        """"""
        Function to tokenize input string and return output of str with ortho rules
        applied.

        Parameters
        ----------
        word : str
            The input string to be tokenized.

        Returns
        -------
        result : str
            Result of the orthography rules applied to the input str.

        """"""
        return self._rules.apply(word) if self._rules else word",def,rules,(,self,",",word,),:,return,self,.,_rules,.,apply,(,word,),if,self,.,_rules,else,word,,,,,,,,,,,,,,,,,,,,,"Function to tokenize input string and return output of str with ortho rules
        applied.

        Parameters
        ----------
        word : str
            The input string to be tokenized.

        Returns
        -------
        result : str
            Result of the orthography rules applied to the input str.",Function,to,tokenize,input,string,and,return,output,of,str,with,ortho,rules,applied,.,9136a4ec89555bf9b574399ffbb07f3cc9a9f45f,https://github.com/cldf/segments/blob/9136a4ec89555bf9b574399ffbb07f3cc9a9f45f/src/segments/tokenizer.py#L272-L288,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cldf/segments,src/segments/tokenizer.py,Tokenizer.combine_modifiers,"def combine_modifiers(self, graphemes):
        """"""
        Given a string that is space-delimited on Unicode grapheme clusters,
        group Unicode modifier letters with their preceding base characters,
        deal with tie bars, etc.

        Parameters
        ----------
        string : str
            A Unicode string tokenized into grapheme clusters to be tokenized into simple
            IPA.

        """"""
        result = []
        temp = """"
        count = len(graphemes)
        for grapheme in reversed(graphemes):
            count -= 1
            if len(grapheme) == 1 and unicodedata.category(grapheme) == ""Lm"" \
                    and not ord(grapheme) in [712, 716]:
                temp = grapheme + temp
                # hack for the cases where a space modifier is the first character in the
                # string
                if count == 0:
                    result[-1] = temp + result[-1]
                continue  # pragma: no cover
            # catch and repair stress marks
            if len(grapheme) == 1 and ord(grapheme) in [712, 716]:
                result[-1] = grapheme + result[-1]
                temp = """"
                continue

            # combine contour tone marks (non-accents)
            if len(grapheme) == 1 and unicodedata.category(grapheme) == ""Sk"":
                if len(result) == 0:
                    result.append(grapheme)
                    temp = """"
                    continue
                else:
                    if unicodedata.category(result[-1][0]) == ""Sk"":
                        result[-1] = grapheme + result[-1]
                        temp = """"
                        continue

            result.append(grapheme + temp)
            temp = """"

        # last check for tie bars
        segments = result[::-1]
        i = 0
        r = []
        while i < len(segments):
            # tie bars
            if ord(segments[i][-1]) in [865, 860]:
                r.append(segments[i] + segments[i + 1])
                i += 2
            else:
                r.append(segments[i])
                i += 1
        return r",python,"def combine_modifiers(self, graphemes):
        """"""
        Given a string that is space-delimited on Unicode grapheme clusters,
        group Unicode modifier letters with their preceding base characters,
        deal with tie bars, etc.

        Parameters
        ----------
        string : str
            A Unicode string tokenized into grapheme clusters to be tokenized into simple
            IPA.

        """"""
        result = []
        temp = """"
        count = len(graphemes)
        for grapheme in reversed(graphemes):
            count -= 1
            if len(grapheme) == 1 and unicodedata.category(grapheme) == ""Lm"" \
                    and not ord(grapheme) in [712, 716]:
                temp = grapheme + temp
                # hack for the cases where a space modifier is the first character in the
                # string
                if count == 0:
                    result[-1] = temp + result[-1]
                continue  # pragma: no cover
            # catch and repair stress marks
            if len(grapheme) == 1 and ord(grapheme) in [712, 716]:
                result[-1] = grapheme + result[-1]
                temp = """"
                continue

            # combine contour tone marks (non-accents)
            if len(grapheme) == 1 and unicodedata.category(grapheme) == ""Sk"":
                if len(result) == 0:
                    result.append(grapheme)
                    temp = """"
                    continue
                else:
                    if unicodedata.category(result[-1][0]) == ""Sk"":
                        result[-1] = grapheme + result[-1]
                        temp = """"
                        continue

            result.append(grapheme + temp)
            temp = """"

        # last check for tie bars
        segments = result[::-1]
        i = 0
        r = []
        while i < len(segments):
            # tie bars
            if ord(segments[i][-1]) in [865, 860]:
                r.append(segments[i] + segments[i + 1])
                i += 2
            else:
                r.append(segments[i])
                i += 1
        return r",def,combine_modifiers,(,self,",",graphemes,),:,result,=,[,],temp,=,"""""",count,=,len,(,graphemes,),for,grapheme,in,reversed,(,graphemes,),:,count,-=,1,if,len,(,grapheme,),==,1,and,unicodedata,.,category,"Given a string that is space-delimited on Unicode grapheme clusters,
        group Unicode modifier letters with their preceding base characters,
        deal with tie bars, etc.

        Parameters
        ----------
        string : str
            A Unicode string tokenized into grapheme clusters to be tokenized into simple
            IPA.",Given,a,string,that,is,space,-,delimited,on,Unicode,grapheme,clusters,group,Unicode,modifier,9136a4ec89555bf9b574399ffbb07f3cc9a9f45f,https://github.com/cldf/segments/blob/9136a4ec89555bf9b574399ffbb07f3cc9a9f45f/src/segments/tokenizer.py#L290-L349,train,(,grapheme,),==,"""Lm""",and,not,ord,(,grapheme,),in,[,712,",",716,],:,temp,=,grapheme,+,temp,# hack for the cases where a space modifier is the first character in the,# string,if,count,==,0,:,letters,with,their,preceding,base,characters,deal,with,tie,bars,etc,.,,,,,,,,result,[,-,1,],=,temp,+,result,[,-,1,],continue,# pragma: no cover,# catch and repair stress marks,if,len,(,grapheme,),==,1,and,ord,(,grapheme,),in,[,712,",",716,],:,result,[,-,1,],=,grapheme,+,result,[,-,1,],temp,=,"""""",continue,# combine contour tone marks (non-accents),if,len,(,grapheme,),==,1,and,unicodedata,.,category,(,grapheme,),==,"""Sk""",:,if,len,(,result,),==,0,:,result,.,append,(,grapheme,),temp,=,"""""",continue,else,:,if,unicodedata,.,category,(,result,[,-,1,],[,0,],),==,"""Sk""",:,result,[,-,1,],=,grapheme,+,result,[,-,1,],temp,=,"""""",continue,result,.,append,(,grapheme,+,temp,),temp,=,"""""",# last check for tie bars,segments,=,result,[,:,:,-,1,],i,=,0,r,=,[,],while,i,<,len,(,segments,),:,# tie bars,if,ord,(,segments,[,i,],[,-,1,],),in,[,865,",",860,],:,r,.,append,(,segments,[,i,],+,segments,[,i,+,1,],),i,+=,2,else,:,r,.,append,(,segments,[,i,],),i,+=,1,return,r,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions_gff.py,parse_catalytic,"def parse_catalytic(insertion, gff):
    """"""
    parse catalytic RNAs to gff format
    """"""
    offset = insertion['offset']
    GeneStrand = insertion['strand']
    if type(insertion['intron']) is not str:
        return gff
    for intron in parse_fasta(insertion['intron'].split('|')):
        ID, annot, strand, pos = intron[0].split('>')[1].split()
        Start, End = [int(i) for i in pos.split('-')]
        if strand != GeneStrand:
            if strand == '+':
                strand = '-'
            else:
                strand = '+'
            Start, End = End - 2, Start - 2
        Start, End = abs(Start + offset) - 1, abs(End + offset) - 1
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append('Rfam')
        gff['feature'].append('Catalytic RNA')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s; Name=%s' % (ID, annot))
    return gff",python,"def parse_catalytic(insertion, gff):
    """"""
    parse catalytic RNAs to gff format
    """"""
    offset = insertion['offset']
    GeneStrand = insertion['strand']
    if type(insertion['intron']) is not str:
        return gff
    for intron in parse_fasta(insertion['intron'].split('|')):
        ID, annot, strand, pos = intron[0].split('>')[1].split()
        Start, End = [int(i) for i in pos.split('-')]
        if strand != GeneStrand:
            if strand == '+':
                strand = '-'
            else:
                strand = '+'
            Start, End = End - 2, Start - 2
        Start, End = abs(Start + offset) - 1, abs(End + offset) - 1
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append('Rfam')
        gff['feature'].append('Catalytic RNA')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s; Name=%s' % (ID, annot))
    return gff",def,parse_catalytic,(,insertion,",",gff,),:,offset,=,insertion,[,'offset',],GeneStrand,=,insertion,[,'strand',],if,type,(,insertion,[,'intron',],),is,not,str,:,return,gff,for,intron,in,parse_fasta,(,insertion,[,'intron',],parse catalytic RNAs to gff format,parse,catalytic,RNAs,to,gff,format,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions_gff.py#L13-L40,train,.,split,(,'|',),),:,ID,",",annot,",",strand,",",pos,=,intron,[,0,],.,split,(,'>',),[,1,],.,split,(,,,,,,,,,,,,,,,,,,,,),Start,",",End,=,[,int,(,i,),for,i,in,pos,.,split,(,'-',),],if,strand,!=,GeneStrand,:,if,strand,==,'+',:,strand,=,'-',else,:,strand,=,'+',Start,",",End,=,End,-,2,",",Start,-,2,Start,",",End,=,abs,(,Start,+,offset,),-,1,",",abs,(,End,+,offset,),-,1,gff,[,'#seqname',],.,append,(,insertion,[,'ID',],),gff,[,'source',],.,append,(,'Rfam',),gff,[,'feature',],.,append,(,'Catalytic RNA',),gff,[,'start',],.,append,(,Start,),gff,[,'end',],.,append,(,End,),gff,[,'score',],.,append,(,'.',),gff,[,'strand',],.,append,(,strand,),gff,[,'frame',],.,append,(,'.',),gff,[,'attribute',],.,append,(,'ID=%s; Name=%s',%,(,ID,",",annot,),),return,gff,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions_gff.py,parse_orf,"def parse_orf(insertion, gff):
    """"""
    parse ORF to gff format
    """"""
    offset = insertion['offset']
    if type(insertion['orf']) is not str:
        return gff
    for orf in parse_fasta(insertion['orf'].split('|')):
        ID = orf[0].split('>')[1].split()[0]
        Start, End, strand = [int(i) for i in orf[0].split(' # ')[1:4]]
        if strand == 1:
            strand = '+'
        else:
            strand = '-'
        GeneStrand = insertion['strand']
        if strand != GeneStrand:
            if strand == '+':
                strand = '-'
            else:
                strand = '+'
            Start, End = End - 2, Start - 2
        Start, End = abs(Start + offset) - 1, abs(End + offset) - 1
        annot = orf[0].split()[1]
        if annot == 'n/a':
            annot = 'unknown'
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append('Prodigal and Pfam')
        gff['feature'].append('CDS')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s; Name=%s' % (ID, annot))
    return gff",python,"def parse_orf(insertion, gff):
    """"""
    parse ORF to gff format
    """"""
    offset = insertion['offset']
    if type(insertion['orf']) is not str:
        return gff
    for orf in parse_fasta(insertion['orf'].split('|')):
        ID = orf[0].split('>')[1].split()[0]
        Start, End, strand = [int(i) for i in orf[0].split(' # ')[1:4]]
        if strand == 1:
            strand = '+'
        else:
            strand = '-'
        GeneStrand = insertion['strand']
        if strand != GeneStrand:
            if strand == '+':
                strand = '-'
            else:
                strand = '+'
            Start, End = End - 2, Start - 2
        Start, End = abs(Start + offset) - 1, abs(End + offset) - 1
        annot = orf[0].split()[1]
        if annot == 'n/a':
            annot = 'unknown'
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append('Prodigal and Pfam')
        gff['feature'].append('CDS')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s; Name=%s' % (ID, annot))
    return gff",def,parse_orf,(,insertion,",",gff,),:,offset,=,insertion,[,'offset',],if,type,(,insertion,[,'orf',],),is,not,str,:,return,gff,for,orf,in,parse_fasta,(,insertion,[,'orf',],.,split,(,'|',),),parse ORF to gff format,parse,ORF,to,gff,format,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions_gff.py#L42-L76,train,:,ID,=,orf,[,0,],.,split,(,'>',),[,1,],.,split,(,),[,0,],Start,",",End,",",strand,=,[,int,,,,,,,,,,,,,,,,,,,,(,i,),for,i,in,orf,[,0,],.,split,(,' # ',),[,1,:,4,],],if,strand,==,1,:,strand,=,'+',else,:,strand,=,'-',GeneStrand,=,insertion,[,'strand',],if,strand,!=,GeneStrand,:,if,strand,==,'+',:,strand,=,'-',else,:,strand,=,'+',Start,",",End,=,End,-,2,",",Start,-,2,Start,",",End,=,abs,(,Start,+,offset,),-,1,",",abs,(,End,+,offset,),-,1,annot,=,orf,[,0,],.,split,(,),[,1,],if,annot,==,'n/a',:,annot,=,'unknown',gff,[,'#seqname',],.,append,(,insertion,[,'ID',],),gff,[,'source',],.,append,(,'Prodigal and Pfam',),gff,[,'feature',],.,append,(,'CDS',),gff,[,'start',],.,append,(,Start,),gff,[,'end',],.,append,(,End,),gff,[,'score',],.,append,(,'.',),gff,[,'strand',],.,append,(,strand,),gff,[,'frame',],.,append,(,'.',),gff,[,'attribute',],.,append,(,'ID=%s; Name=%s',%,(,ID,",",annot,),),return,gff,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions_gff.py,parse_insertion,"def parse_insertion(insertion, gff):
    """"""
    parse insertion to gff format
    """"""
    offset = insertion['offset']
    for ins in parse_fasta(insertion['insertion sequence'].split('|')):
        strand = insertion['strand']
        ID = ins[0].split('>')[1].split()[0]
        Start, End = [int(i) for i in ins[0].split('gene-pos=', 1)[1].split()[0].split('-')]
        Start, End = abs(Start + offset), abs(End + offset)
        if strand == '-':
            Start, End = End, Start
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append(insertion['source'])
        gff['feature'].append('IVS')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand) # same as rRNA
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s' % (ID))
    return gff",python,"def parse_insertion(insertion, gff):
    """"""
    parse insertion to gff format
    """"""
    offset = insertion['offset']
    for ins in parse_fasta(insertion['insertion sequence'].split('|')):
        strand = insertion['strand']
        ID = ins[0].split('>')[1].split()[0]
        Start, End = [int(i) for i in ins[0].split('gene-pos=', 1)[1].split()[0].split('-')]
        Start, End = abs(Start + offset), abs(End + offset)
        if strand == '-':
            Start, End = End, Start
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append(insertion['source'])
        gff['feature'].append('IVS')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand) # same as rRNA
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s' % (ID))
    return gff",def,parse_insertion,(,insertion,",",gff,),:,offset,=,insertion,[,'offset',],for,ins,in,parse_fasta,(,insertion,[,'insertion sequence',],.,split,(,'|',),),:,strand,=,insertion,[,'strand',],ID,=,ins,[,0,],.,parse insertion to gff format,parse,insertion,to,gff,format,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions_gff.py#L78-L99,train,split,(,'>',),[,1,],.,split,(,),[,0,],Start,",",End,=,[,int,(,i,),for,i,in,ins,[,0,],,,,,,,,,,,,,,,,,,,,.,split,(,'gene-pos=',",",1,),[,1,],.,split,(,),[,0,],.,split,(,'-',),],Start,",",End,=,abs,(,Start,+,offset,),",",abs,(,End,+,offset,),if,strand,==,'-',:,Start,",",End,=,End,",",Start,gff,[,'#seqname',],.,append,(,insertion,[,'ID',],),gff,[,'source',],.,append,(,insertion,[,'source',],),gff,[,'feature',],.,append,(,'IVS',),gff,[,'start',],.,append,(,Start,),gff,[,'end',],.,append,(,End,),gff,[,'score',],.,append,(,'.',),gff,[,'strand',],.,append,(,strand,),# same as rRNA,gff,[,'frame',],.,append,(,'.',),gff,[,'attribute',],.,append,(,'ID=%s',%,(,ID,),),return,gff,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions_gff.py,parse_rRNA,"def parse_rRNA(insertion, seq, gff):
    """"""
    parse rRNA to gff format
    """"""
    offset = insertion['offset']
    strand = insertion['strand']
    for rRNA in parse_masked(seq, 0)[0]:
        rRNA = ''.join(rRNA)
        Start = seq[1].find(rRNA) + 1
        End = Start + len(rRNA) - 1
        if strand == '-':
            Start, End = End - 2, Start - 2
        pos = (abs(Start + offset) - 1, abs(End + offset) - 1)
        Start, End = min(pos), max(pos)
        source = insertion['source']
        annot = '%s rRNA' % (source.split('from', 1)[0])
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append(source)
        gff['feature'].append('rRNA')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('Name=%s' % (annot))
    return gff",python,"def parse_rRNA(insertion, seq, gff):
    """"""
    parse rRNA to gff format
    """"""
    offset = insertion['offset']
    strand = insertion['strand']
    for rRNA in parse_masked(seq, 0)[0]:
        rRNA = ''.join(rRNA)
        Start = seq[1].find(rRNA) + 1
        End = Start + len(rRNA) - 1
        if strand == '-':
            Start, End = End - 2, Start - 2
        pos = (abs(Start + offset) - 1, abs(End + offset) - 1)
        Start, End = min(pos), max(pos)
        source = insertion['source']
        annot = '%s rRNA' % (source.split('from', 1)[0])
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append(source)
        gff['feature'].append('rRNA')
        gff['start'].append(Start)
        gff['end'].append(End)
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('Name=%s' % (annot))
    return gff",def,parse_rRNA,(,insertion,",",seq,",",gff,),:,offset,=,insertion,[,'offset',],strand,=,insertion,[,'strand',],for,rRNA,in,parse_masked,(,seq,",",0,),[,0,],:,rRNA,=,'',.,join,(,rRNA,),parse rRNA to gff format,parse,rRNA,to,gff,format,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions_gff.py#L122-L147,train,Start,=,seq,[,1,],.,find,(,rRNA,),+,1,End,=,Start,+,len,(,rRNA,),-,1,if,strand,==,'-',:,Start,",",,,,,,,,,,,,,,,,,,,,End,=,End,-,2,",",Start,-,2,pos,=,(,abs,(,Start,+,offset,),-,1,",",abs,(,End,+,offset,),-,1,),Start,",",End,=,min,(,pos,),",",max,(,pos,),source,=,insertion,[,'source',],annot,=,'%s rRNA',%,(,source,.,split,(,'from',",",1,),[,0,],),gff,[,'#seqname',],.,append,(,insertion,[,'ID',],),gff,[,'source',],.,append,(,source,),gff,[,'feature',],.,append,(,'rRNA',),gff,[,'start',],.,append,(,Start,),gff,[,'end',],.,append,(,End,),gff,[,'score',],.,append,(,'.',),gff,[,'strand',],.,append,(,strand,),gff,[,'frame',],.,append,(,'.',),gff,[,'attribute',],.,append,(,'Name=%s',%,(,annot,),),return,gff,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions_gff.py,iTable2GFF,"def iTable2GFF(iTable, fa, contig = False):
    """"""
    convert iTable to gff file
    """"""
    columns = ['#seqname', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']
    gff = {c:[] for c in columns}
    for insertion in iTable.iterrows():
        insertion = insertion[1]
        if insertion['ID'] not in fa:
            continue
        # rRNA strand
        strand = insertion['sequence'].split('strand=', 1)[1].split()[0]
        # set rRNA positions for reporting features on contig or extracted sequence
        if contig is True:
            gene = [int(i) for i in insertion['sequence'].split('pos=', 1)[1].split()[0].split('-')]
            if strand == '-':
                offset = -1 * (gene[1])
            else:
                offset = gene[0]
        else:
            strand = '+'
            gene = [1, int(insertion['sequence'].split('total-len=', 1)[1].split()[0])]
            offset = gene[0]
        insertion['strand'] = strand
        insertion['offset'] = offset
        # source for prediction
        source = insertion['sequence'].split('::model', 1)[0].rsplit(' ', 1)[-1]
        insertion['source'] = source
        # rRNA gene
        geneAnnot = '%s rRNA gene' % (source.split('from', 1)[0])
        geneNum = insertion['sequence'].split('seq=', 1)[1].split()[0]
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append(source)
        gff['feature'].append('Gene')
        gff['start'].append(gene[0])
        gff['end'].append(gene[1])
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s; Name=%s' % (geneNum, geneAnnot))
        # rRNA
        gff = parse_rRNA(insertion, fa[insertion['ID']], gff)
        # insertions
        gff = parse_insertion(insertion, gff)
        # orfs
        gff = parse_orf(insertion, gff)
        # catalytic RNAs
        gff = parse_catalytic(insertion, gff)
    return pd.DataFrame(gff)[columns].drop_duplicates()",python,"def iTable2GFF(iTable, fa, contig = False):
    """"""
    convert iTable to gff file
    """"""
    columns = ['#seqname', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']
    gff = {c:[] for c in columns}
    for insertion in iTable.iterrows():
        insertion = insertion[1]
        if insertion['ID'] not in fa:
            continue
        # rRNA strand
        strand = insertion['sequence'].split('strand=', 1)[1].split()[0]
        # set rRNA positions for reporting features on contig or extracted sequence
        if contig is True:
            gene = [int(i) for i in insertion['sequence'].split('pos=', 1)[1].split()[0].split('-')]
            if strand == '-':
                offset = -1 * (gene[1])
            else:
                offset = gene[0]
        else:
            strand = '+'
            gene = [1, int(insertion['sequence'].split('total-len=', 1)[1].split()[0])]
            offset = gene[0]
        insertion['strand'] = strand
        insertion['offset'] = offset
        # source for prediction
        source = insertion['sequence'].split('::model', 1)[0].rsplit(' ', 1)[-1]
        insertion['source'] = source
        # rRNA gene
        geneAnnot = '%s rRNA gene' % (source.split('from', 1)[0])
        geneNum = insertion['sequence'].split('seq=', 1)[1].split()[0]
        gff['#seqname'].append(insertion['ID'])
        gff['source'].append(source)
        gff['feature'].append('Gene')
        gff['start'].append(gene[0])
        gff['end'].append(gene[1])
        gff['score'].append('.')
        gff['strand'].append(strand)
        gff['frame'].append('.')
        gff['attribute'].append('ID=%s; Name=%s' % (geneNum, geneAnnot))
        # rRNA
        gff = parse_rRNA(insertion, fa[insertion['ID']], gff)
        # insertions
        gff = parse_insertion(insertion, gff)
        # orfs
        gff = parse_orf(insertion, gff)
        # catalytic RNAs
        gff = parse_catalytic(insertion, gff)
    return pd.DataFrame(gff)[columns].drop_duplicates()",def,iTable2GFF,(,iTable,",",fa,",",contig,=,False,),:,columns,=,[,'#seqname',",",'source',",",'feature',",",'start',",",'end',",",'score',",",'strand',",",'frame',",",'attribute',],gff,=,{,c,:,[,],for,c,in,convert iTable to gff file,convert,iTable,to,gff,file,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions_gff.py#L149-L197,train,columns,},for,insertion,in,iTable,.,iterrows,(,),:,insertion,=,insertion,[,1,],if,insertion,[,'ID',],not,in,fa,:,continue,# rRNA strand,strand,=,,,,,,,,,,,,,,,,,,,,insertion,[,'sequence',],.,split,(,'strand=',",",1,),[,1,],.,split,(,),[,0,],# set rRNA positions for reporting features on contig or extracted sequence,if,contig,is,True,:,gene,=,[,int,(,i,),for,i,in,insertion,[,'sequence',],.,split,(,'pos=',",",1,),[,1,],.,split,(,),[,0,],.,split,(,'-',),],if,strand,==,'-',:,offset,=,-,1,*,(,gene,[,1,],),else,:,offset,=,gene,[,0,],else,:,strand,=,'+',gene,=,[,1,",",int,(,insertion,[,'sequence',],.,split,(,'total-len=',",",1,),[,1,],.,split,(,),[,0,],),],offset,=,gene,[,0,],insertion,[,'strand',],=,strand,insertion,[,'offset',],=,offset,# source for prediction,source,=,insertion,[,'sequence',],.,split,(,'::model',",",1,),[,0,],.,rsplit,(,' ',",",1,),[,-,1,],insertion,[,'source',],=,source,# rRNA gene,geneAnnot,=,'%s rRNA gene',%,(,source,.,split,(,'from',",",1,),[,0,],),geneNum,=,insertion,[,'sequence',],.,split,(,'seq=',",",1,),[,1,],.,split,(,),[,0,],gff,[,'#seqname',],.,append,(,insertion,[,'ID',],),gff,[,'source',],.,append,(,source,),gff,[,'feature',],.,append,(,'Gene',),gff,[,'start',],.,append,(,gene,[,0,],),gff,[,'end',],.,append,(,gene,[,1,],),gff,[,'score',],.,append,(,'.',),gff,[,'strand',],.,append,(,strand,),gff,[,'frame',],.,append,(,'.',),gff,[,'attribute',],.,append,(,'ID=%s; Name=%s',%,(,geneNum,",",geneAnnot,),),# rRNA,gff,=,parse_rRNA,(,insertion,",",fa,[,insertion,[,,,,,,,,,,,,'ID',],],",",gff,),# insertions,gff,=,parse_insertion,(,insertion,",",gff,),# orfs,gff,=,parse_orf,(,insertion,",",gff,),# catalytic RNAs,gff,=,parse_catalytic,(,insertion,",",gff,),return,pd,.,DataFrame,(,gff,),[,columns,],.,drop_duplicates,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/biom_phyla_summary.py,summarize_taxa,"def summarize_taxa(biom):
    """"""
    Given an abundance table, group the counts by every
    taxonomic level.
    """"""
    tamtcounts = defaultdict(int)
    tot_seqs = 0.0

    for row, col, amt in biom['data']:
        tot_seqs += amt
        rtax = biom['rows'][row]['metadata']['taxonomy']
        for i, t in enumerate(rtax):
            t = t.strip()
            if i == len(rtax)-1 and len(t) > 3 and len(rtax[-1]) > 3:
                t = 's__'+rtax[i-1].strip().split('_')[-1]+'_'+t.split('_')[-1]
            tamtcounts[t] += amt

    lvlData = {lvl: levelData(tamtcounts, tot_seqs, lvl) for lvl in ['k', 'p', 'c', 'o', 'f', 'g', 's']}

    return tot_seqs, lvlData",python,"def summarize_taxa(biom):
    """"""
    Given an abundance table, group the counts by every
    taxonomic level.
    """"""
    tamtcounts = defaultdict(int)
    tot_seqs = 0.0

    for row, col, amt in biom['data']:
        tot_seqs += amt
        rtax = biom['rows'][row]['metadata']['taxonomy']
        for i, t in enumerate(rtax):
            t = t.strip()
            if i == len(rtax)-1 and len(t) > 3 and len(rtax[-1]) > 3:
                t = 's__'+rtax[i-1].strip().split('_')[-1]+'_'+t.split('_')[-1]
            tamtcounts[t] += amt

    lvlData = {lvl: levelData(tamtcounts, tot_seqs, lvl) for lvl in ['k', 'p', 'c', 'o', 'f', 'g', 's']}

    return tot_seqs, lvlData",def,summarize_taxa,(,biom,),:,tamtcounts,=,defaultdict,(,int,),tot_seqs,=,0.0,for,row,",",col,",",amt,in,biom,[,'data',],:,tot_seqs,+=,amt,rtax,=,biom,[,'rows',],[,row,],[,'metadata',],[,"Given an abundance table, group the counts by every
    taxonomic level.",Given,an,abundance,table,group,the,counts,by,every,taxonomic,level,.,,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/biom_phyla_summary.py#L27-L46,train,'taxonomy',],for,i,",",t,in,enumerate,(,rtax,),:,t,=,t,.,strip,(,),if,i,==,len,(,rtax,),-,1,and,len,,,,,,,,,,,,,,,,,,,,(,t,),>,3,and,len,(,rtax,[,-,1,],),>,3,:,t,=,'s__',+,rtax,[,i,-,1,],.,strip,(,),.,split,(,'_',),[,-,1,],+,'_',+,t,.,split,(,'_',),[,-,1,],tamtcounts,[,t,],+=,amt,lvlData,=,{,lvl,:,levelData,(,tamtcounts,",",tot_seqs,",",lvl,),for,lvl,in,[,'k',",",'p',",",'c',",",'o',",",'f',",",'g',",",'s',],},return,tot_seqs,",",lvlData,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/legacy/game.py,Game.custom_image,"def custom_image(self, user):
        """"""Returns the path to the custom image set for this game, or None if
        no image is set""""""
        for ext in self.valid_custom_image_extensions():
            image_location = self._custom_image_path(user, ext)
            if os.path.isfile(image_location):
                return image_location
        return None",python,"def custom_image(self, user):
        """"""Returns the path to the custom image set for this game, or None if
        no image is set""""""
        for ext in self.valid_custom_image_extensions():
            image_location = self._custom_image_path(user, ext)
            if os.path.isfile(image_location):
                return image_location
        return None",def,custom_image,(,self,",",user,),:,for,ext,in,self,.,valid_custom_image_extensions,(,),:,image_location,=,self,.,_custom_image_path,(,user,",",ext,),if,os,.,path,.,isfile,(,image_location,),:,return,image_location,return,None,,,"Returns the path to the custom image set for this game, or None if
        no image is set",Returns,the,path,to,the,custom,image,set,for,this,game,or,None,if,no,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/legacy/game.py#L41-L48,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,image,is,set,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/legacy/game.py,Game.set_image,"def set_image(self, user, image_path):
        """"""Sets a custom image for the game. `image_path` should refer to
        an image file on disk""""""
        _, ext = os.path.splitext(image_path)
        shutil.copy(image_path, self._custom_image_path(user, ext))",python,"def set_image(self, user, image_path):
        """"""Sets a custom image for the game. `image_path` should refer to
        an image file on disk""""""
        _, ext = os.path.splitext(image_path)
        shutil.copy(image_path, self._custom_image_path(user, ext))",def,set_image,(,self,",",user,",",image_path,),:,_,",",ext,=,os,.,path,.,splitext,(,image_path,),shutil,.,copy,(,image_path,",",self,.,_custom_image_path,(,user,",",ext,),),,,,,,,"Sets a custom image for the game. `image_path` should refer to
        an image file on disk",Sets,a,custom,image,for,the,game,.,image_path,should,refer,to,an,image,file,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/legacy/game.py#L50-L54,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on,disk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/filter_fastq_sam.py,sam_list,"def sam_list(sam):
	""""""
	get a list of mapped reads
	""""""
	list = []
	for file in sam:
		for line in file:
			if line.startswith('@') is False:
				line = line.strip().split()
				id, map = line[0], int(line[1])
				if map != 4 and map != 8:
					list.append(id)
	return set(list)",python,"def sam_list(sam):
	""""""
	get a list of mapped reads
	""""""
	list = []
	for file in sam:
		for line in file:
			if line.startswith('@') is False:
				line = line.strip().split()
				id, map = line[0], int(line[1])
				if map != 4 and map != 8:
					list.append(id)
	return set(list)",def,sam_list,(,sam,),:,list,=,[,],for,file,in,sam,:,for,line,in,file,:,if,line,.,startswith,(,'@',),is,False,:,line,=,line,.,strip,(,),.,split,(,),id,",",get a list of mapped reads,get,a,list,of,mapped,reads,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/filter_fastq_sam.py#L7-L19,train,map,=,line,[,0,],",",int,(,line,[,1,],),if,map,!=,4,and,map,!=,8,:,list,.,append,(,id,),return,,,,,,,,,,,,,,,,,,,,set,(,list,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/filter_fastq_sam.py,sam_list_paired,"def sam_list_paired(sam):
	""""""
	get a list of mapped reads
	require that both pairs are mapped in the sam file in order to remove the reads
	""""""
	list = []
	pair = ['1', '2']
	prev = ''
	for file in sam:
		for line in file:
			if line.startswith('@') is False:
				line = line.strip().split()
				id, map = line[0], int(line[1])
				if map != 4 and map != 8:
					read = id.rsplit('/')[0]
					if read == prev:
						list.append(read)
					prev = read
	return set(list)",python,"def sam_list_paired(sam):
	""""""
	get a list of mapped reads
	require that both pairs are mapped in the sam file in order to remove the reads
	""""""
	list = []
	pair = ['1', '2']
	prev = ''
	for file in sam:
		for line in file:
			if line.startswith('@') is False:
				line = line.strip().split()
				id, map = line[0], int(line[1])
				if map != 4 and map != 8:
					read = id.rsplit('/')[0]
					if read == prev:
						list.append(read)
					prev = read
	return set(list)",def,sam_list_paired,(,sam,),:,list,=,[,],pair,=,[,'1',",",'2',],prev,=,'',for,file,in,sam,:,for,line,in,file,:,if,line,.,startswith,(,'@',),is,False,:,line,=,line,"get a list of mapped reads
	require that both pairs are mapped in the sam file in order to remove the reads",get,a,list,of,mapped,reads,require,that,both,pairs,are,mapped,in,the,sam,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/filter_fastq_sam.py#L21-L39,train,.,strip,(,),.,split,(,),id,",",map,=,line,[,0,],",",int,(,line,[,1,],),if,map,!=,4,and,map,file,in,order,to,remove,the,reads,,,,,,,,,,,,,!=,8,:,read,=,id,.,rsplit,(,'/',),[,0,],if,read,==,prev,:,list,.,append,(,read,),prev,=,read,return,set,(,list,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/filter_fastq_sam.py,filter_paired,"def filter_paired(list):
	""""""
	require that both pairs are mapped in the sam file in order to remove the reads
	""""""
	pairs = {}
	filtered = []
	for id in list:
		read = id.rsplit('/')[0]
		if read not in pairs:
			pairs[read] = []
		pairs[read].append(id)
	for read in pairs:
		ids = pairs[read]
		if len(ids) == 2:
			filtered.extend(ids)
	return set(filtered)",python,"def filter_paired(list):
	""""""
	require that both pairs are mapped in the sam file in order to remove the reads
	""""""
	pairs = {}
	filtered = []
	for id in list:
		read = id.rsplit('/')[0]
		if read not in pairs:
			pairs[read] = []
		pairs[read].append(id)
	for read in pairs:
		ids = pairs[read]
		if len(ids) == 2:
			filtered.extend(ids)
	return set(filtered)",def,filter_paired,(,list,),:,pairs,=,{,},filtered,=,[,],for,id,in,list,:,read,=,id,.,rsplit,(,'/',),[,0,],if,read,not,in,pairs,:,pairs,[,read,],=,[,],require that both pairs are mapped in the sam file in order to remove the reads,require,that,both,pairs,are,mapped,in,the,sam,file,in,order,to,remove,the,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/filter_fastq_sam.py#L41-L56,train,pairs,[,read,],.,append,(,id,),for,read,in,pairs,:,ids,=,pairs,[,read,],if,len,(,ids,),==,2,:,filtered,.,reads,,,,,,,,,,,,,,,,,,,extend,(,ids,),return,set,(,filtered,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/mapped.py,sam2fastq,"def sam2fastq(line):
    """"""
    print fastq from sam
    """"""
    fastq = []
    fastq.append('@%s' % line[0])
    fastq.append(line[9])
    fastq.append('+%s' % line[0])
    fastq.append(line[10])
    return fastq",python,"def sam2fastq(line):
    """"""
    print fastq from sam
    """"""
    fastq = []
    fastq.append('@%s' % line[0])
    fastq.append(line[9])
    fastq.append('+%s' % line[0])
    fastq.append(line[10])
    return fastq",def,sam2fastq,(,line,),:,fastq,=,[,],fastq,.,append,(,'@%s',%,line,[,0,],),fastq,.,append,(,line,[,9,],),fastq,.,append,(,'+%s',%,line,[,0,],),fastq,.,print fastq from sam,print,fastq,from,sam,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/mapped.py#L13-L22,train,append,(,line,[,10,],),return,fastq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/mapped.py,check_mismatches,"def check_mismatches(read, pair, mismatches, mm_option, req_map):
    """"""
    - check to see if the read maps with <= threshold number of mismatches
    - mm_option = 'one' or 'both' depending on whether or not one or both reads
       in a pair need to pass the mismatch threshold
    - pair can be False if read does not have a pair
    - make sure alignment score is not 0, which would indicate that the read was not aligned to the reference
    """"""
    # if read is not paired, make sure it is mapped and that mm <= thresh
    if pair is False:
        mm = count_mismatches(read)
        if mm is False:
            return False
        # if no threshold is supplied, return True
        if mismatches is False:
            return True
        # passes threshold?
        if mm <= mismatches:
            return True
    # paired reads
    r_mm = count_mismatches(read)
    p_mm = count_mismatches(pair)
    # if neither read is mapped, return False
    if r_mm is False and p_mm is False:
        return False
    # if no threshold, return True
    if mismatches is False:
        return True
    # if req_map is True, both reads have to map
    if req_map is True:
        if r_mm is False or p_mm is False:
            return False
    ## if option is 'one,' only one read has to pass threshold
    if mm_option == 'one':
        if (r_mm is not False and r_mm <= mismatches) or (p_mm is not False and p_mm <= mismatches):
            return True
    ## if option is 'both,' both reads have to pass threshold
    if mm_option == 'both':
        ## if one read in pair does not map to the scaffold,
        ## make sure the other read passes threshold
        if r_mm is False:
            if p_mm <= mismatches:
                return True
        elif p_mm is False:
            if r_mm <= mismatches:
                return True
        elif (r_mm is not False and r_mm <= mismatches) and (p_mm is not False and p_mm <= mismatches):
            return True
    return False",python,"def check_mismatches(read, pair, mismatches, mm_option, req_map):
    """"""
    - check to see if the read maps with <= threshold number of mismatches
    - mm_option = 'one' or 'both' depending on whether or not one or both reads
       in a pair need to pass the mismatch threshold
    - pair can be False if read does not have a pair
    - make sure alignment score is not 0, which would indicate that the read was not aligned to the reference
    """"""
    # if read is not paired, make sure it is mapped and that mm <= thresh
    if pair is False:
        mm = count_mismatches(read)
        if mm is False:
            return False
        # if no threshold is supplied, return True
        if mismatches is False:
            return True
        # passes threshold?
        if mm <= mismatches:
            return True
    # paired reads
    r_mm = count_mismatches(read)
    p_mm = count_mismatches(pair)
    # if neither read is mapped, return False
    if r_mm is False and p_mm is False:
        return False
    # if no threshold, return True
    if mismatches is False:
        return True
    # if req_map is True, both reads have to map
    if req_map is True:
        if r_mm is False or p_mm is False:
            return False
    ## if option is 'one,' only one read has to pass threshold
    if mm_option == 'one':
        if (r_mm is not False and r_mm <= mismatches) or (p_mm is not False and p_mm <= mismatches):
            return True
    ## if option is 'both,' both reads have to pass threshold
    if mm_option == 'both':
        ## if one read in pair does not map to the scaffold,
        ## make sure the other read passes threshold
        if r_mm is False:
            if p_mm <= mismatches:
                return True
        elif p_mm is False:
            if r_mm <= mismatches:
                return True
        elif (r_mm is not False and r_mm <= mismatches) and (p_mm is not False and p_mm <= mismatches):
            return True
    return False",def,check_mismatches,(,read,",",pair,",",mismatches,",",mm_option,",",req_map,),:,"# if read is not paired, make sure it is mapped and that mm <= thresh",if,pair,is,False,:,mm,=,count_mismatches,(,read,),if,mm,is,False,:,return,False,"# if no threshold is supplied, return True",if,mismatches,is,False,:,return,True,# passes threshold?,if,"- check to see if the read maps with <= threshold number of mismatches
    - mm_option = 'one' or 'both' depending on whether or not one or both reads
       in a pair need to pass the mismatch threshold
    - pair can be False if read does not have a pair
    - make sure alignment score is not 0, which would indicate that the read was not aligned to the reference",-,check,to,see,if,the,read,maps,with,<,=,threshold,number,of,mismatches,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/mapped.py#L36-L84,train,mm,<=,mismatches,:,return,True,# paired reads,r_mm,=,count_mismatches,(,read,),p_mm,=,count_mismatches,(,pair,),"# if neither read is mapped, return False",if,r_mm,is,False,and,p_mm,is,False,:,return,-,mm_option,=,one,or,both,depending,on,whether,or,not,one,or,both,reads,in,a,pair,need,False,"# if no threshold, return True",if,mismatches,is,False,:,return,True,"# if req_map is True, both reads have to map",if,req_map,is,True,:,if,r_mm,is,False,or,p_mm,is,False,:,return,False,"## if option is 'one,' only one read has to pass threshold",if,mm_option,==,'one',:,if,(,r_mm,is,not,False,and,r_mm,<=,mismatches,),or,(,p_mm,is,not,False,and,p_mm,<=,mismatches,),:,return,True,"## if option is 'both,' both reads have to pass threshold",if,mm_option,==,'both',:,"## if one read in pair does not map to the scaffold,",## make sure the other read passes threshold,if,r_mm,is,False,:,if,p_mm,<=,mismatches,:,return,True,elif,p_mm,is,False,:,if,r_mm,<=,mismatches,:,return,True,elif,(,r_mm,is,not,False,and,r_mm,<=,mismatches,),and,(,p_mm,is,not,False,and,p_mm,<=,mismatches,),:,return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,to,pass,the,mismatch,threshold,-,pair,can,be,False,if,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,read,does,not,have,a,pair,-,make,sure,alignment,score,is,not,0,which,would,indicate,that,the,read,was,not,aligned,to,the,reference,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/mapped.py,check_region,"def check_region(read, pair, region):
    """"""
    determine whether or not reads map to specific region of scaffold
    """"""
    if region is False:
        return True
    for mapping in read, pair:
        if mapping is False:
            continue
        start, length = int(mapping[3]), len(mapping[9])
        r = [start, start + length - 1]
        if get_overlap(r, region) > 0:
            return True
    return False",python,"def check_region(read, pair, region):
    """"""
    determine whether or not reads map to specific region of scaffold
    """"""
    if region is False:
        return True
    for mapping in read, pair:
        if mapping is False:
            continue
        start, length = int(mapping[3]), len(mapping[9])
        r = [start, start + length - 1]
        if get_overlap(r, region) > 0:
            return True
    return False",def,check_region,(,read,",",pair,",",region,),:,if,region,is,False,:,return,True,for,mapping,in,read,",",pair,:,if,mapping,is,False,:,continue,start,",",length,=,int,(,mapping,[,3,],),",",len,determine whether or not reads map to specific region of scaffold,determine,whether,or,not,reads,map,to,specific,region,of,scaffold,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/mapped.py#L92-L105,train,(,mapping,[,9,],),r,=,[,start,",",start,+,length,-,1,],if,get_overlap,(,r,",",region,),>,0,:,return,True,return,,,,,,,,,,,,,,,,,,,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/steam.py,get_steam,"def get_steam():
  """"""
  Returns a Steam object representing the current Steam installation on the
  users computer. If the user doesn't have Steam installed, returns None.
  """"""
  # Helper function which checks if the potential userdata directory exists
  # and returns a new Steam instance with that userdata directory if it does.
  # If the directory doesnt exist it returns None instead
  helper = lambda udd: Steam(udd) if os.path.exists(udd) else None

  # For both OS X and Linux, Steam stores it's userdata in a consistent
  # location.
  plat = platform.system()
  if plat == 'Darwin':
    return helper(paths.default_osx_userdata_path())
  if plat == 'Linux':
    return helper(paths.default_linux_userdata_path())

  # Windows is a bit trickier. The userdata directory is stored in the Steam
  # installation directory, meaning that theoretically it could be anywhere.
  # Luckily, Valve stores the installation directory in the registry, so its
  # still possible for us to figure out automatically
  if plat == 'Windows':
    possible_dir = winutils.find_userdata_directory()
    # Unlike the others, `possible_dir` might be None (if something odd
    # happened with the registry)
    return helper(possible_dir) if possible_dir is not None else None
  # This should never be hit. Windows, OS X, and Linux should be the only
  # supported platforms.
  # TODO: Add logging here so that the user (developer) knows that something
  # odd happened.
  return None",python,"def get_steam():
  """"""
  Returns a Steam object representing the current Steam installation on the
  users computer. If the user doesn't have Steam installed, returns None.
  """"""
  # Helper function which checks if the potential userdata directory exists
  # and returns a new Steam instance with that userdata directory if it does.
  # If the directory doesnt exist it returns None instead
  helper = lambda udd: Steam(udd) if os.path.exists(udd) else None

  # For both OS X and Linux, Steam stores it's userdata in a consistent
  # location.
  plat = platform.system()
  if plat == 'Darwin':
    return helper(paths.default_osx_userdata_path())
  if plat == 'Linux':
    return helper(paths.default_linux_userdata_path())

  # Windows is a bit trickier. The userdata directory is stored in the Steam
  # installation directory, meaning that theoretically it could be anywhere.
  # Luckily, Valve stores the installation directory in the registry, so its
  # still possible for us to figure out automatically
  if plat == 'Windows':
    possible_dir = winutils.find_userdata_directory()
    # Unlike the others, `possible_dir` might be None (if something odd
    # happened with the registry)
    return helper(possible_dir) if possible_dir is not None else None
  # This should never be hit. Windows, OS X, and Linux should be the only
  # supported platforms.
  # TODO: Add logging here so that the user (developer) knows that something
  # odd happened.
  return None",def,get_steam,(,),:,# Helper function which checks if the potential userdata directory exists,# and returns a new Steam instance with that userdata directory if it does.,# If the directory doesnt exist it returns None instead,helper,=,lambda,udd,:,Steam,(,udd,),if,os,.,path,.,exists,(,udd,),else,None,"# For both OS X and Linux, Steam stores it's userdata in a consistent",# location.,plat,=,platform,.,system,(,),if,plat,==,'Darwin',:,return,"Returns a Steam object representing the current Steam installation on the
  users computer. If the user doesn't have Steam installed, returns None.",Returns,a,Steam,object,representing,the,current,Steam,installation,on,the,users,computer,.,If,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/steam.py#L12-L43,train,helper,(,paths,.,default_osx_userdata_path,(,),),if,plat,==,'Linux',:,return,helper,(,paths,.,default_linux_userdata_path,(,),),# Windows is a bit trickier. The userdata directory is stored in the Steam,"# installation directory, meaning that theoretically it could be anywhere.","# Luckily, Valve stores the installation directory in the registry, so its",# still possible for us to figure out automatically,if,plat,==,'Windows',the,user,doesn,t,have,Steam,installed,returns,None,.,,,,,,,,,,:,possible_dir,=,winutils,.,find_userdata_directory,(,),"# Unlike the others, `possible_dir` might be None (if something odd",# happened with the registry),return,helper,(,possible_dir,),if,possible_dir,is,not,None,else,None,"# This should never be hit. Windows, OS X, and Linux should be the only",# supported platforms.,# TODO: Add logging here so that the user (developer) knows that something,# odd happened.,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,zero_to_one,"def zero_to_one(table, option):
    """"""
    normalize from zero to one for row or table
    """"""
    if option == 'table':
        m = min(min(table))
        ma = max(max(table))
    t = []
    for row in table:
        t_row = []
        if option != 'table':
            m, ma = min(row), max(row)
        for i in row:
            if ma == m:
                t_row.append(0)
            else:
                t_row.append((i - m)/(ma - m))
        t.append(t_row)
    return t",python,"def zero_to_one(table, option):
    """"""
    normalize from zero to one for row or table
    """"""
    if option == 'table':
        m = min(min(table))
        ma = max(max(table))
    t = []
    for row in table:
        t_row = []
        if option != 'table':
            m, ma = min(row), max(row)
        for i in row:
            if ma == m:
                t_row.append(0)
            else:
                t_row.append((i - m)/(ma - m))
        t.append(t_row)
    return t",def,zero_to_one,(,table,",",option,),:,if,option,==,'table',:,m,=,min,(,min,(,table,),),ma,=,max,(,max,(,table,),),t,=,[,],for,row,in,table,:,t_row,=,[,normalize from zero to one for row or table,normalize,from,zero,to,one,for,row,or,table,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L18-L36,train,],if,option,!=,'table',:,m,",",ma,=,min,(,row,),",",max,(,row,),for,i,in,row,:,if,ma,==,m,:,t_row,,,,,,,,,,,,,,,,,,,,.,append,(,0,),else,:,t_row,.,append,(,(,i,-,m,),/,(,ma,-,m,),),t,.,append,(,t_row,),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,pertotal,"def pertotal(table, option):
    """"""
    calculate percent of total
    """"""
    if option == 'table':
        total = sum([i for line in table for i in line])
    t = []
    for row in table:
        t_row = []
        if option != 'table':
            total = sum(row)
        for i in row:
            if total == 0:
                t_row.append(0)
            else:
                t_row.append(i/total*100)
        t.append(t_row)
    return t",python,"def pertotal(table, option):
    """"""
    calculate percent of total
    """"""
    if option == 'table':
        total = sum([i for line in table for i in line])
    t = []
    for row in table:
        t_row = []
        if option != 'table':
            total = sum(row)
        for i in row:
            if total == 0:
                t_row.append(0)
            else:
                t_row.append(i/total*100)
        t.append(t_row)
    return t",def,pertotal,(,table,",",option,),:,if,option,==,'table',:,total,=,sum,(,[,i,for,line,in,table,for,i,in,line,],),t,=,[,],for,row,in,table,:,t_row,=,[,],if,calculate percent of total,calculate,percent,of,total,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L38-L55,train,option,!=,'table',:,total,=,sum,(,row,),for,i,in,row,:,if,total,==,0,:,t_row,.,append,(,0,),else,:,t_row,.,,,,,,,,,,,,,,,,,,,,append,(,i,/,total,*,100,),t,.,append,(,t_row,),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,scale,"def scale(table):
    """"""
    scale table based on the column with the largest sum
    """"""
    t = []
    columns = [[] for i in table[0]]
    for row in table:
        for i, v in enumerate(row):
            columns[i].append(v)
    sums = [float(sum(i)) for i in columns]
    scale_to = float(max(sums))
    scale_factor = [scale_to/i for i in sums if i != 0]
    for row in table:
        t.append([a * b for a,b in zip(row, scale_factor)])
    return t",python,"def scale(table):
    """"""
    scale table based on the column with the largest sum
    """"""
    t = []
    columns = [[] for i in table[0]]
    for row in table:
        for i, v in enumerate(row):
            columns[i].append(v)
    sums = [float(sum(i)) for i in columns]
    scale_to = float(max(sums))
    scale_factor = [scale_to/i for i in sums if i != 0]
    for row in table:
        t.append([a * b for a,b in zip(row, scale_factor)])
    return t",def,scale,(,table,),:,t,=,[,],columns,=,[,[,],for,i,in,table,[,0,],],for,row,in,table,:,for,i,",",v,in,enumerate,(,row,),:,columns,[,i,],.,scale table based on the column with the largest sum,scale,table,based,on,the,column,with,the,largest,sum,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L79-L93,train,append,(,v,),sums,=,[,float,(,sum,(,i,),),for,i,in,columns,],scale_to,=,float,(,max,(,sums,),),scale_factor,=,,,,,,,,,,,,,,,,,,,,[,scale_to,/,i,for,i,in,sums,if,i,!=,0,],for,row,in,table,:,t,.,append,(,[,a,*,b,for,a,",",b,in,zip,(,row,",",scale_factor,),],),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,norm,"def norm(table):
    """"""
    fit to normal distribution
    """"""
    print('# norm dist is broken', file=sys.stderr)
    exit()
    from matplotlib.pyplot import hist as hist
    t = []
    for i in table:
        t.append(np.ndarray.tolist(hist(i, bins = len(i), normed = True)[0]))
    return t",python,"def norm(table):
    """"""
    fit to normal distribution
    """"""
    print('# norm dist is broken', file=sys.stderr)
    exit()
    from matplotlib.pyplot import hist as hist
    t = []
    for i in table:
        t.append(np.ndarray.tolist(hist(i, bins = len(i), normed = True)[0]))
    return t",def,norm,(,table,),:,print,(,'# norm dist is broken',",",file,=,sys,.,stderr,),exit,(,),from,matplotlib,.,pyplot,import,hist,as,hist,t,=,[,],for,i,in,table,:,t,.,append,(,np,.,ndarray,fit to normal distribution,fit,to,normal,distribution,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L95-L105,train,.,tolist,(,hist,(,i,",",bins,=,len,(,i,),",",normed,=,True,),[,0,],),),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,log_trans,"def log_trans(table):
    """"""
    log transform each value in table
    """"""
    t = []
    all = [item for sublist in table for item in sublist]
    if min(all) == 0:
        scale = min([i for i in all if i != 0]) * 10e-10
    else:
        scale = 0
    for i in table:
        t.append(np.ndarray.tolist(np.log10([j + scale for j in i])))
    return t",python,"def log_trans(table):
    """"""
    log transform each value in table
    """"""
    t = []
    all = [item for sublist in table for item in sublist]
    if min(all) == 0:
        scale = min([i for i in all if i != 0]) * 10e-10
    else:
        scale = 0
    for i in table:
        t.append(np.ndarray.tolist(np.log10([j + scale for j in i])))
    return t",def,log_trans,(,table,),:,t,=,[,],all,=,[,item,for,sublist,in,table,for,item,in,sublist,],if,min,(,all,),==,0,:,scale,=,min,(,[,i,for,i,in,all,if,i,log transform each value in table,log,transform,each,value,in,table,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L107-L119,train,!=,0,],),*,10e-10,else,:,scale,=,0,for,i,in,table,:,t,.,append,(,np,.,ndarray,.,tolist,(,np,.,log10,(,,,,,,,,,,,,,,,,,,,,[,j,+,scale,for,j,in,i,],),),),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,box_cox,"def box_cox(table):
    """"""
    box-cox transform table
    """"""
    from scipy.stats import boxcox as bc
    t = []
    for i in table:
        if min(i) == 0:
            scale = min([j for j in i if j != 0]) * 10e-10
        else:
            scale = 0
        t.append(np.ndarray.tolist(bc(np.array([j + scale for j in i]))[0]))
    return t",python,"def box_cox(table):
    """"""
    box-cox transform table
    """"""
    from scipy.stats import boxcox as bc
    t = []
    for i in table:
        if min(i) == 0:
            scale = min([j for j in i if j != 0]) * 10e-10
        else:
            scale = 0
        t.append(np.ndarray.tolist(bc(np.array([j + scale for j in i]))[0]))
    return t",def,box_cox,(,table,),:,from,scipy,.,stats,import,boxcox,as,bc,t,=,[,],for,i,in,table,:,if,min,(,i,),==,0,:,scale,=,min,(,[,j,for,j,in,i,if,j,box-cox transform table,box,-,cox,transform,table,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L121-L133,train,!=,0,],),*,10e-10,else,:,scale,=,0,t,.,append,(,np,.,ndarray,.,tolist,(,bc,(,np,.,array,(,[,j,+,,,,,,,,,,,,,,,,,,,,scale,for,j,in,i,],),),[,0,],),),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,inh,"def inh(table):
    """"""
    inverse hyperbolic sine transformation
    """"""
    t = []
    for i in table:
        t.append(np.ndarray.tolist(np.arcsinh(i)))
    return t",python,"def inh(table):
    """"""
    inverse hyperbolic sine transformation
    """"""
    t = []
    for i in table:
        t.append(np.ndarray.tolist(np.arcsinh(i)))
    return t",def,inh,(,table,),:,t,=,[,],for,i,in,table,:,t,.,append,(,np,.,ndarray,.,tolist,(,np,.,arcsinh,(,i,),),),return,t,,,,,,,,,inverse hyperbolic sine transformation,inverse,hyperbolic,sine,transformation,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L135-L142,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/transform.py,diri,"def diri(table):
    """"""
    from SparCC - ""randomly draw from the corresponding posterior
    Dirichlet distribution with a uniform prior""
    """"""
    t = []
    for i in table:
        a = [j + 1 for j in i]
        t.append(np.ndarray.tolist(np.random.mtrand.dirichlet(a)))
    return t",python,"def diri(table):
    """"""
    from SparCC - ""randomly draw from the corresponding posterior
    Dirichlet distribution with a uniform prior""
    """"""
    t = []
    for i in table:
        a = [j + 1 for j in i]
        t.append(np.ndarray.tolist(np.random.mtrand.dirichlet(a)))
    return t",def,diri,(,table,),:,t,=,[,],for,i,in,table,:,a,=,[,j,+,1,for,j,in,i,],t,.,append,(,np,.,ndarray,.,tolist,(,np,.,random,.,mtrand,.,dirichlet,"from SparCC - ""randomly draw from the corresponding posterior
    Dirichlet distribution with a uniform prior""",from,SparCC,-,randomly,draw,from,the,corresponding,posterior,Dirichlet,distribution,with,a,uniform,prior,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/transform.py#L144-L153,train,(,a,),),),return,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/sanger_qiimify.py,generate_barcodes,"def generate_barcodes(nIds, codeLen=12):
    """"""
    Given a list of sample IDs generate unique n-base barcodes for each.
    Note that only 4^n unique barcodes are possible.
    """"""
    def next_code(b, c, i):
        return c[:i] + b + (c[i+1:] if i < -1 else '')

    def rand_base():
        return random.choice(['A', 'T', 'C', 'G'])

    def rand_seq(n):
        return ''.join([rand_base() for _ in range(n)])

    # homopolymer filter regex: match if 4 identical bases in a row
    hpf = re.compile('aaaa|cccc|gggg|tttt', re.IGNORECASE)

    while True:
        codes = [rand_seq(codeLen)]
        if (hpf.search(codes[0]) is None):
            break
    idx = 0

    while len(codes) < nIds:
        idx -= 1
        if idx < -codeLen:
            idx = -1
            codes.append(rand_seq(codeLen))
        else:
            nc = next_code(rand_base(), codes[-1], idx)
            if hpf.search(nc) is None:
                codes.append(nc)
        codes = list(set(codes))

    return codes",python,"def generate_barcodes(nIds, codeLen=12):
    """"""
    Given a list of sample IDs generate unique n-base barcodes for each.
    Note that only 4^n unique barcodes are possible.
    """"""
    def next_code(b, c, i):
        return c[:i] + b + (c[i+1:] if i < -1 else '')

    def rand_base():
        return random.choice(['A', 'T', 'C', 'G'])

    def rand_seq(n):
        return ''.join([rand_base() for _ in range(n)])

    # homopolymer filter regex: match if 4 identical bases in a row
    hpf = re.compile('aaaa|cccc|gggg|tttt', re.IGNORECASE)

    while True:
        codes = [rand_seq(codeLen)]
        if (hpf.search(codes[0]) is None):
            break
    idx = 0

    while len(codes) < nIds:
        idx -= 1
        if idx < -codeLen:
            idx = -1
            codes.append(rand_seq(codeLen))
        else:
            nc = next_code(rand_base(), codes[-1], idx)
            if hpf.search(nc) is None:
                codes.append(nc)
        codes = list(set(codes))

    return codes",def,generate_barcodes,(,nIds,",",codeLen,=,12,),:,def,next_code,(,b,",",c,",",i,),:,return,c,[,:,i,],+,b,+,(,c,[,i,+,1,:,],if,i,<,-,1,else,"Given a list of sample IDs generate unique n-base barcodes for each.
    Note that only 4^n unique barcodes are possible.",Given,a,list,of,sample,IDs,generate,unique,n,-,base,barcodes,for,each,.,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/sanger_qiimify.py#L94-L128,train,'',),def,rand_base,(,),:,return,random,.,choice,(,[,'A',",",'T',",",'C',",",'G',],),def,rand_seq,(,n,),:,return,'',Note,that,only,4^n,unique,barcodes,are,possible,.,,,,,,,,,,,.,join,(,[,rand_base,(,),for,_,in,range,(,n,),],),# homopolymer filter regex: match if 4 identical bases in a row,hpf,=,re,.,compile,(,'aaaa|cccc|gggg|tttt',",",re,.,IGNORECASE,),while,True,:,codes,=,[,rand_seq,(,codeLen,),],if,(,hpf,.,search,(,codes,[,0,],),is,None,),:,break,idx,=,0,while,len,(,codes,),<,nIds,:,idx,-=,1,if,idx,<,-,codeLen,:,idx,=,-,1,codes,.,append,(,rand_seq,(,codeLen,),),else,:,nc,=,next_code,(,rand_base,(,),",",codes,[,-,1,],",",idx,),if,hpf,.,search,(,nc,),is,None,:,codes,.,append,(,nc,),codes,=,list,(,set,(,codes,),),return,codes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/sanger_qiimify.py,scrobble_data_dir,"def scrobble_data_dir(dataDir, sampleMap, outF, qualF=None, idopt=None,
                      utf16=False):
    """"""
    Given a sample ID and a mapping, modify a Sanger FASTA file
    to include the barcode and 'primer' in the sequence data
    and change the description line as needed.
    """"""
    seqcount = 0
    outfiles = [osp.split(outF.name)[1]]
    if qualF:
        outfiles.append(osp.split(qualF.name)[1])

    for item in os.listdir(dataDir):
        if item in outfiles or not osp.isfile(os.path.join(dataDir, item)):
            continue
        # FASTA files
        if osp.splitext(item)[1] in file_types['fasta']:
            fh = open_enc(os.path.join(dataDir, item), utf16)
            records = SeqIO.parse(fh, 'fasta')
            for record in records:
                if isinstance(idopt, tuple):
                    sep, field = idopt
                    sampleID = record.id.split(sep)[field - 1]
                else:
                    sampleID = osp.splitext(item)[0]
                record.seq = (sampleMap[sampleID].barcode +
                              sampleMap[sampleID].primer +
                              record.seq)
                SeqIO.write(record, outF, 'fasta')
                seqcount += 1
            fh.close()
        # QUAL files
        elif qualF and osp.splitext(item)[1] in file_types['qual']:
            fh = open_enc(os.path.join(dataDir, item), utf16)
            records = SeqIO.parse(fh, 'qual')
            for record in records:
                mi = sampleMap[sampleMap.keys()[0]]
                quals = [40 for _ in range(len(mi.barcode) + len(mi.primer))]
                record.letter_annotations['phred_quality'][0:0] = quals
                SeqIO.write(record, qualF, 'qual')
            fh.close()
    return seqcount",python,"def scrobble_data_dir(dataDir, sampleMap, outF, qualF=None, idopt=None,
                      utf16=False):
    """"""
    Given a sample ID and a mapping, modify a Sanger FASTA file
    to include the barcode and 'primer' in the sequence data
    and change the description line as needed.
    """"""
    seqcount = 0
    outfiles = [osp.split(outF.name)[1]]
    if qualF:
        outfiles.append(osp.split(qualF.name)[1])

    for item in os.listdir(dataDir):
        if item in outfiles or not osp.isfile(os.path.join(dataDir, item)):
            continue
        # FASTA files
        if osp.splitext(item)[1] in file_types['fasta']:
            fh = open_enc(os.path.join(dataDir, item), utf16)
            records = SeqIO.parse(fh, 'fasta')
            for record in records:
                if isinstance(idopt, tuple):
                    sep, field = idopt
                    sampleID = record.id.split(sep)[field - 1]
                else:
                    sampleID = osp.splitext(item)[0]
                record.seq = (sampleMap[sampleID].barcode +
                              sampleMap[sampleID].primer +
                              record.seq)
                SeqIO.write(record, outF, 'fasta')
                seqcount += 1
            fh.close()
        # QUAL files
        elif qualF and osp.splitext(item)[1] in file_types['qual']:
            fh = open_enc(os.path.join(dataDir, item), utf16)
            records = SeqIO.parse(fh, 'qual')
            for record in records:
                mi = sampleMap[sampleMap.keys()[0]]
                quals = [40 for _ in range(len(mi.barcode) + len(mi.primer))]
                record.letter_annotations['phred_quality'][0:0] = quals
                SeqIO.write(record, qualF, 'qual')
            fh.close()
    return seqcount",def,scrobble_data_dir,(,dataDir,",",sampleMap,",",outF,",",qualF,=,None,",",idopt,=,None,",",utf16,=,False,),:,seqcount,=,0,outfiles,=,[,osp,.,split,(,outF,.,name,),[,1,],],if,qualF,:,"Given a sample ID and a mapping, modify a Sanger FASTA file
    to include the barcode and 'primer' in the sequence data
    and change the description line as needed.",Given,a,sample,ID,and,a,mapping,modify,a,Sanger,FASTA,file,to,include,the,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/sanger_qiimify.py#L158-L199,train,outfiles,.,append,(,osp,.,split,(,qualF,.,name,),[,1,],),for,item,in,os,.,listdir,(,dataDir,),:,if,item,in,outfiles,barcode,and,primer,in,the,sequence,data,and,change,the,description,line,as,needed,.,,,,,or,not,osp,.,isfile,(,os,.,path,.,join,(,dataDir,",",item,),),:,continue,# FASTA files,if,osp,.,splitext,(,item,),[,1,],in,file_types,[,'fasta',],:,fh,=,open_enc,(,os,.,path,.,join,(,dataDir,",",item,),",",utf16,),records,=,SeqIO,.,parse,(,fh,",",'fasta',),for,record,in,records,:,if,isinstance,(,idopt,",",tuple,),:,sep,",",field,=,idopt,sampleID,=,record,.,id,.,split,(,sep,),[,field,-,1,],else,:,sampleID,=,osp,.,splitext,(,item,),[,0,],record,.,seq,=,(,sampleMap,[,sampleID,],.,barcode,+,sampleMap,[,sampleID,],.,primer,+,record,.,seq,),SeqIO,.,write,(,record,",",outF,",",'fasta',),seqcount,+=,1,fh,.,close,(,),# QUAL files,elif,qualF,and,osp,.,splitext,(,item,),[,1,],in,file_types,[,'qual',],:,fh,=,open_enc,(,os,.,path,.,join,(,dataDir,",",item,),",",utf16,),records,=,SeqIO,.,parse,(,fh,",",'qual',),for,record,in,records,:,mi,=,sampleMap,[,sampleMap,.,keys,(,),[,0,],],quals,=,[,40,for,_,in,range,(,len,(,mi,.,barcode,),+,len,(,mi,.,primer,),),],record,.,letter_annotations,[,'phred_quality',],[,0,:,0,],=,quals,SeqIO,.,write,(,record,",",qualF,",",'qual',),fh,.,close,(,),return,seqcount,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/sanger_qiimify.py,handle_program_options,"def handle_program_options():
    """"""
    Uses the built-in argparse module to handle command-line options for the
    program.

    :return: The gathered command-line options specified by the user
    :rtype: argparse.ArgumentParser
    """"""
    parser = argparse.ArgumentParser(description=""Convert Sanger-sequencing \
                                     derived data files for use with the \
                                     metagenomics analysis program QIIME, by \
                                     extracting Sample ID information, adding\
                                     barcodes and primers to the sequence \
                                     data, and outputting a mapping file and\
                                     single FASTA-formatted sequence file \
                                     formed by concatenating all input data."")
    parser.add_argument('-i', '--input_dir', required=True,
                        help=""The directory containing sequence data files. \
                              Assumes all data files are placed in this \
                              directory. For files organized within folders by\
                              sample, use -s in addition."")
    parser.add_argument('-m', '--map_file', default='map.txt',
                        help=""QIIME-formatted mapping file linking Sample IDs \
                              with barcodes and primers."")
    parser.add_argument('-o', '--output', default='output.fasta',
                        metavar='OUTPUT_FILE',
                        help=""Single file containing all sequence data found \
                              in input_dir, FASTA-formatted with barcode and \
                              primer preprended to sequence. If the -q option \
                              is passed, any quality data will also be output \
                              to a single file of the same name with a .qual \
                              extension."")
    parser.add_argument('-b', '--barcode_length', type=int, default=12,
                        help=""Length of the generated barcode sequences. \
                              Default is 12 (QIIME default), minimum is 8."")

    parser.add_argument('-q', '--qual', action='store_true', default=False,
                        help=""Instruct the program to look for quality \
                              input files"")
    parser.add_argument('-u', '--utf16', action='store_true', default=False,
                        help=""UTF-16 encoded input files"")

    parser.add_argument('-t', '--treatment',
                        help=""Inserts an additional column into the mapping \
                              file specifying some treatment or other variable\
                              that separates the current set of sequences \
                              from any other set of seqeunces. For example:\
                              -t DiseaseState=healthy"")

    # data input options
    sidGroup = parser.add_mutually_exclusive_group(required=True)
    sidGroup.add_argument('-d', '--identifier_pattern',
                          action=ValidateIDPattern,
                          nargs=2, metavar=('SEPARATOR', 'FIELD_NUMBER'),
                          help=""Indicates how to extract the Sample ID from \
                               the description line. Specify two things: \
                               1. Field separator, 2. Field number of Sample \
                               ID (1 or greater). If the separator is a space \
                               or tab, use \s or \\t respectively. \
                               Example: >ka-SampleID-2091, use -i - 2, \
                               indicating - is the separator and the Sample ID\
                               is field #2."")
    sidGroup.add_argument('-f', '--filename_sample_id', action='store_true',
                          default=False, help='Specify that the program should\
                          the name of each fasta file as the Sample ID for use\
                          in the mapping file. This is meant to be used when \
                          all sequence data for a sample is stored in a single\
                          file.')

    return parser.parse_args()",python,"def handle_program_options():
    """"""
    Uses the built-in argparse module to handle command-line options for the
    program.

    :return: The gathered command-line options specified by the user
    :rtype: argparse.ArgumentParser
    """"""
    parser = argparse.ArgumentParser(description=""Convert Sanger-sequencing \
                                     derived data files for use with the \
                                     metagenomics analysis program QIIME, by \
                                     extracting Sample ID information, adding\
                                     barcodes and primers to the sequence \
                                     data, and outputting a mapping file and\
                                     single FASTA-formatted sequence file \
                                     formed by concatenating all input data."")
    parser.add_argument('-i', '--input_dir', required=True,
                        help=""The directory containing sequence data files. \
                              Assumes all data files are placed in this \
                              directory. For files organized within folders by\
                              sample, use -s in addition."")
    parser.add_argument('-m', '--map_file', default='map.txt',
                        help=""QIIME-formatted mapping file linking Sample IDs \
                              with barcodes and primers."")
    parser.add_argument('-o', '--output', default='output.fasta',
                        metavar='OUTPUT_FILE',
                        help=""Single file containing all sequence data found \
                              in input_dir, FASTA-formatted with barcode and \
                              primer preprended to sequence. If the -q option \
                              is passed, any quality data will also be output \
                              to a single file of the same name with a .qual \
                              extension."")
    parser.add_argument('-b', '--barcode_length', type=int, default=12,
                        help=""Length of the generated barcode sequences. \
                              Default is 12 (QIIME default), minimum is 8."")

    parser.add_argument('-q', '--qual', action='store_true', default=False,
                        help=""Instruct the program to look for quality \
                              input files"")
    parser.add_argument('-u', '--utf16', action='store_true', default=False,
                        help=""UTF-16 encoded input files"")

    parser.add_argument('-t', '--treatment',
                        help=""Inserts an additional column into the mapping \
                              file specifying some treatment or other variable\
                              that separates the current set of sequences \
                              from any other set of seqeunces. For example:\
                              -t DiseaseState=healthy"")

    # data input options
    sidGroup = parser.add_mutually_exclusive_group(required=True)
    sidGroup.add_argument('-d', '--identifier_pattern',
                          action=ValidateIDPattern,
                          nargs=2, metavar=('SEPARATOR', 'FIELD_NUMBER'),
                          help=""Indicates how to extract the Sample ID from \
                               the description line. Specify two things: \
                               1. Field separator, 2. Field number of Sample \
                               ID (1 or greater). If the separator is a space \
                               or tab, use \s or \\t respectively. \
                               Example: >ka-SampleID-2091, use -i - 2, \
                               indicating - is the separator and the Sample ID\
                               is field #2."")
    sidGroup.add_argument('-f', '--filename_sample_id', action='store_true',
                          default=False, help='Specify that the program should\
                          the name of each fasta file as the Sample ID for use\
                          in the mapping file. This is meant to be used when \
                          all sequence data for a sample is stored in a single\
                          file.')

    return parser.parse_args()",def,handle_program_options,(,),:,parser,=,argparse,.,ArgumentParser,(,description,=,"""Convert Sanger-sequencing \
                                     derived data files for use with the \
                                     metagenomics analysis program QIIME, by \
                                     extracting Sample ID information, adding\
                                     barcodes and primers to the sequence \
                                     data, and outputting a mapping file and\
                                     single FASTA-formatted sequence file \
                                     formed by concatenating all input data.""",),parser,.,add_argument,(,'-i',",",'--input_dir',",",required,=,True,",",help,=,"""The directory containing sequence data files. \
                              Assumes all data files are placed in this \
                              directory. For files organized within folders by\
                              sample, use -s in addition.""",),parser,.,add_argument,(,'-m',",",'--map_file',",",default,=,'map.txt',",","Uses the built-in argparse module to handle command-line options for the
    program.

    :return: The gathered command-line options specified by the user
    :rtype: argparse.ArgumentParser",Uses,the,built,-,in,argparse,module,to,handle,command,-,line,options,for,the,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/sanger_qiimify.py#L202-L271,train,help,=,"""QIIME-formatted mapping file linking Sample IDs \
                              with barcodes and primers.""",),parser,.,add_argument,(,'-o',",",'--output',",",default,=,'output.fasta',",",metavar,=,'OUTPUT_FILE',",",help,=,"""Single file containing all sequence data found \
                              in input_dir, FASTA-formatted with barcode and \
                              primer preprended to sequence. If the -q option \
                              is passed, any quality data will also be output \
                              to a single file of the same name with a .qual \
                              extension.""",),parser,.,add_argument,(,'-b',",",program,.,,,,,,,,,,,,,,,,,,'--barcode_length',",",type,=,int,",",default,=,12,",",help,=,"""Length of the generated barcode sequences. \
                              Default is 12 (QIIME default), minimum is 8.""",),parser,.,add_argument,(,'-q',",",'--qual',",",action,=,'store_true',",",default,=,False,",",help,=,"""Instruct the program to look for quality \
                              input files""",),parser,.,add_argument,(,'-u',",",'--utf16',",",action,=,'store_true',",",default,=,False,",",help,=,"""UTF-16 encoded input files""",),parser,.,add_argument,(,'-t',",",'--treatment',",",help,=,"""Inserts an additional column into the mapping \
                              file specifying some treatment or other variable\
                              that separates the current set of sequences \
                              from any other set of seqeunces. For example:\
                              -t DiseaseState=healthy""",),# data input options,sidGroup,=,parser,.,add_mutually_exclusive_group,(,required,=,True,),sidGroup,.,add_argument,(,'-d',",",'--identifier_pattern',",",action,=,ValidateIDPattern,",",nargs,=,2,",",metavar,=,(,'SEPARATOR',",",'FIELD_NUMBER',),",",help,=,"""Indicates how to extract the Sample ID from \
                               the description line. Specify two things: \
                               1. Field separator, 2. Field number of Sample \
                               ID (1 or greater). If the separator is a space \
                               or tab, use \s or \\t respectively. \
                               Example: >ka-SampleID-2091, use -i - 2, \
                               indicating - is the separator and the Sample ID\
                               is field #2.""",),sidGroup,.,add_argument,(,'-f',",",'--filename_sample_id',",",action,=,'store_true',",",default,=,False,",",help,=,"'Specify that the program should\
                          the name of each fasta file as the Sample ID for use\
                          in the mapping file. This is meant to be used when \
                          all sequence data for a sample is stored in a single\
                          file.'",),return,parser,.,parse_args,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/transform_biom.py,arcsin_sqrt,"def arcsin_sqrt(biom_tbl):
    """"""
    Applies the arcsine square root transform to the
    given BIOM-format table
    """"""
    arcsint = lambda data, id_, md: np.arcsin(np.sqrt(data))

    tbl_relabd = relative_abd(biom_tbl)
    tbl_asin = tbl_relabd.transform(arcsint, inplace=False)

    return tbl_asin",python,"def arcsin_sqrt(biom_tbl):
    """"""
    Applies the arcsine square root transform to the
    given BIOM-format table
    """"""
    arcsint = lambda data, id_, md: np.arcsin(np.sqrt(data))

    tbl_relabd = relative_abd(biom_tbl)
    tbl_asin = tbl_relabd.transform(arcsint, inplace=False)

    return tbl_asin",def,arcsin_sqrt,(,biom_tbl,),:,arcsint,=,lambda,data,",",id_,",",md,:,np,.,arcsin,(,np,.,sqrt,(,data,),),tbl_relabd,=,relative_abd,(,biom_tbl,),tbl_asin,=,tbl_relabd,.,transform,(,arcsint,",",inplace,=,False,"Applies the arcsine square root transform to the
    given BIOM-format table",Applies,the,arcsine,square,root,transform,to,the,given,BIOM,-,format,table,,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/transform_biom.py#L78-L88,train,),return,tbl_asin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,parse_sam,"def parse_sam(sam, qual):
    """"""
    parse sam file and check mapping quality
    """"""
    for line in sam:
        if line.startswith('@'):
            continue
        line = line.strip().split()
        if int(line[4]) == 0 or int(line[4]) < qual:
            continue
        yield line",python,"def parse_sam(sam, qual):
    """"""
    parse sam file and check mapping quality
    """"""
    for line in sam:
        if line.startswith('@'):
            continue
        line = line.strip().split()
        if int(line[4]) == 0 or int(line[4]) < qual:
            continue
        yield line",def,parse_sam,(,sam,",",qual,),:,for,line,in,sam,:,if,line,.,startswith,(,'@',),:,continue,line,=,line,.,strip,(,),.,split,(,),if,int,(,line,[,4,],),==,0,parse sam file and check mapping quality,parse,sam,file,and,check,mapping,quality,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L23-L33,train,or,int,(,line,[,4,],),<,qual,:,continue,yield,line,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,rc_stats,"def rc_stats(stats):
    """"""
    reverse completement stats
    """"""
    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}
    rcs = []
    for pos in reversed(stats):
        rc = {}
        rc['reference frequencey'] = pos['reference frequency']
        rc['consensus frequencey'] = pos['consensus frequency']
        rc['In'] = pos['In']
        rc['Del'] = pos['Del']
        rc['ref'] = rc_nucs[pos['ref']]
        rc['consensus'] = (rc_nucs[pos['consensus'][0]], pos['consensus'][1])
        for base, stat in list(pos.items()):
            if base in rc_nucs:
                rc[rc_nucs[base]] = stat
        rcs.append(rc)
    return rcs",python,"def rc_stats(stats):
    """"""
    reverse completement stats
    """"""
    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}
    rcs = []
    for pos in reversed(stats):
        rc = {}
        rc['reference frequencey'] = pos['reference frequency']
        rc['consensus frequencey'] = pos['consensus frequency']
        rc['In'] = pos['In']
        rc['Del'] = pos['Del']
        rc['ref'] = rc_nucs[pos['ref']]
        rc['consensus'] = (rc_nucs[pos['consensus'][0]], pos['consensus'][1])
        for base, stat in list(pos.items()):
            if base in rc_nucs:
                rc[rc_nucs[base]] = stat
        rcs.append(rc)
    return rcs",def,rc_stats,(,stats,),:,rc_nucs,=,{,'A',:,'T',",",'T',:,'A',",",'G',:,'C',",",'C',:,'G',",",'N',:,'N',},rcs,=,[,],for,pos,in,reversed,(,stats,),:,rc,=,reverse completement stats,reverse,completement,stats,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L138-L156,train,{,},rc,[,'reference frequencey',],=,pos,[,'reference frequency',],rc,[,'consensus frequencey',],=,pos,[,'consensus frequency',],rc,[,'In',],=,pos,[,'In',],rc,,,,,,,,,,,,,,,,,,,,[,'Del',],=,pos,[,'Del',],rc,[,'ref',],=,rc_nucs,[,pos,[,'ref',],],rc,[,'consensus',],=,(,rc_nucs,[,pos,[,'consensus',],[,0,],],",",pos,[,'consensus',],[,1,],),for,base,",",stat,in,list,(,pos,.,items,(,),),:,if,base,in,rc_nucs,:,rc,[,rc_nucs,[,base,],],=,stat,rcs,.,append,(,rc,),return,rcs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,parse_codons,"def parse_codons(ref, start, end, strand):
    """"""
    parse codon nucleotide positions in range start -> end, wrt strand
    """"""
    codon = []
    c = cycle([1, 2, 3])
    ref = ref[start - 1:end]
    if strand == -1:
        ref = rc_stats(ref)
    for pos in ref:
        n = next(c)
        codon.append(pos)
        if n == 3:
            yield codon
            codon = []",python,"def parse_codons(ref, start, end, strand):
    """"""
    parse codon nucleotide positions in range start -> end, wrt strand
    """"""
    codon = []
    c = cycle([1, 2, 3])
    ref = ref[start - 1:end]
    if strand == -1:
        ref = rc_stats(ref)
    for pos in ref:
        n = next(c)
        codon.append(pos)
        if n == 3:
            yield codon
            codon = []",def,parse_codons,(,ref,",",start,",",end,",",strand,),:,codon,=,[,],c,=,cycle,(,[,1,",",2,",",3,],),ref,=,ref,[,start,-,1,:,end,],if,strand,==,-,1,"parse codon nucleotide positions in range start -> end, wrt strand",parse,codon,nucleotide,positions,in,range,start,-,>,end,wrt,strand,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L158-L172,train,:,ref,=,rc_stats,(,ref,),for,pos,in,ref,:,n,=,next,(,c,),codon,.,append,(,pos,),if,n,==,3,:,yield,,,,,,,,,,,,,,,,,,,,codon,codon,=,[,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,calc_coverage,"def calc_coverage(ref, start, end, length, nucs):
    """"""
    calculate coverage for positions in range start -> end
    """"""
    ref = ref[start - 1:end]
    bases = 0
    for pos in ref:
        for base, count in list(pos.items()):
            if base in nucs:
                bases += count
    return float(bases)/float(length)",python,"def calc_coverage(ref, start, end, length, nucs):
    """"""
    calculate coverage for positions in range start -> end
    """"""
    ref = ref[start - 1:end]
    bases = 0
    for pos in ref:
        for base, count in list(pos.items()):
            if base in nucs:
                bases += count
    return float(bases)/float(length)",def,calc_coverage,(,ref,",",start,",",end,",",length,",",nucs,),:,ref,=,ref,[,start,-,1,:,end,],bases,=,0,for,pos,in,ref,:,for,base,",",count,in,list,(,pos,.,items,(,calculate coverage for positions in range start -> end,calculate,coverage,for,positions,in,range,start,-,>,end,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L174-L184,train,),),:,if,base,in,nucs,:,bases,+=,count,return,float,(,bases,),/,float,(,length,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,parse_gbk,"def parse_gbk(gbks):
    """"""
    parse gbk file
    """"""
    for gbk in gbks:
        for record in SeqIO.parse(open(gbk), 'genbank'):
            for feature in record.features:
                if feature.type == 'gene':
                    try:
                        locus = feature.qualifiers['locus_tag'][0]
                    except:
                        continue
                if feature.type == 'CDS':
                    try:
                        locus = feature.qualifiers['locus_tag'][0]
                    except:
                        pass
                    start = int(feature.location.start) + int(feature.qualifiers['codon_start'][0])
                    end, strand = int(feature.location.end), feature.location.strand
                    if strand is None:
                        strand = 1
                    else:
                        strand = -1
                    contig = record.id
#                    contig = record.id.rsplit('.', 1)[0]
                    yield contig, [locus, \
                            [start, end, strand], \
                            feature.qualifiers]",python,"def parse_gbk(gbks):
    """"""
    parse gbk file
    """"""
    for gbk in gbks:
        for record in SeqIO.parse(open(gbk), 'genbank'):
            for feature in record.features:
                if feature.type == 'gene':
                    try:
                        locus = feature.qualifiers['locus_tag'][0]
                    except:
                        continue
                if feature.type == 'CDS':
                    try:
                        locus = feature.qualifiers['locus_tag'][0]
                    except:
                        pass
                    start = int(feature.location.start) + int(feature.qualifiers['codon_start'][0])
                    end, strand = int(feature.location.end), feature.location.strand
                    if strand is None:
                        strand = 1
                    else:
                        strand = -1
                    contig = record.id
#                    contig = record.id.rsplit('.', 1)[0]
                    yield contig, [locus, \
                            [start, end, strand], \
                            feature.qualifiers]",def,parse_gbk,(,gbks,),:,for,gbk,in,gbks,:,for,record,in,SeqIO,.,parse,(,open,(,gbk,),",",'genbank',),:,for,feature,in,record,.,features,:,if,feature,.,type,==,'gene',:,try,:,locus,parse gbk file,parse,gbk,file,,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L186-L213,train,=,feature,.,qualifiers,[,'locus_tag',],[,0,],except,:,continue,if,feature,.,type,==,'CDS',:,try,:,locus,=,feature,.,qualifiers,[,'locus_tag',],,,,,,,,,,,,,,,,,,,,[,0,],except,:,pass,start,=,int,(,feature,.,location,.,start,),+,int,(,feature,.,qualifiers,[,'codon_start',],[,0,],),end,",",strand,=,int,(,feature,.,location,.,end,),",",feature,.,location,.,strand,if,strand,is,None,:,strand,=,1,else,:,strand,=,-,1,contig,=,record,.,id,"#                    contig = record.id.rsplit('.', 1)[0]",yield,contig,",",[,locus,",",[,start,",",end,",",strand,],",",feature,.,qualifiers,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,parse_fasta_annotations,"def parse_fasta_annotations(fastas, annot_tables, trans_table):
    """"""
    parse gene call information from Prodigal fasta output
    """"""
    if annot_tables is not False:
        annots = {}
        for table in annot_tables:
            for cds in open(table):
                ID, start, end, strand = cds.strip().split()
                annots[ID] = [start, end, int(strand)]
    for fasta in fastas:
        for seq in parse_fasta(fasta):
            if ('# ;gc_cont' not in seq[0] and '# ID=' not in seq[0]) and annot_tables is False:
                print('# specify fasta from Prodigal or annotations table (-t)', file=sys.stderr)
                exit()
            if 'ID=' in seq[0]:
                ID = seq[0].rsplit('ID=', 1)[1].split(';', 1)[0]
                contig = seq[0].split()[0].split('>')[1].rsplit('_%s' % (ID), 1)[0]
            else:
                contig = seq[0].split()[0].split('>')[1].rsplit('_', 1)[0]
            locus = seq[0].split()[0].split('>')[1]
            # annotation info from Prodigal
            if ('# ;gc_cont' in seq[0] or '# ID=' in seq[0]):
                info = seq[0].split(' # ')
                start, end, strand = int(info[1]), int(info[2]), info[3]
                if strand == '1':
                    strand = 1
                else:
                    strand = -1
                product = [''.join(info[4].split()[1:])]
            # annotation info from table
            else:
                start, end, strand = annots[locus]
                product = seq[0].split(' ', 1)[1]
            info = {'transl_table':[trans_table], \
                    'translation':[seq[1]], \
                    'product':product}
            yield contig, [locus, [start, end, strand], info]",python,"def parse_fasta_annotations(fastas, annot_tables, trans_table):
    """"""
    parse gene call information from Prodigal fasta output
    """"""
    if annot_tables is not False:
        annots = {}
        for table in annot_tables:
            for cds in open(table):
                ID, start, end, strand = cds.strip().split()
                annots[ID] = [start, end, int(strand)]
    for fasta in fastas:
        for seq in parse_fasta(fasta):
            if ('# ;gc_cont' not in seq[0] and '# ID=' not in seq[0]) and annot_tables is False:
                print('# specify fasta from Prodigal or annotations table (-t)', file=sys.stderr)
                exit()
            if 'ID=' in seq[0]:
                ID = seq[0].rsplit('ID=', 1)[1].split(';', 1)[0]
                contig = seq[0].split()[0].split('>')[1].rsplit('_%s' % (ID), 1)[0]
            else:
                contig = seq[0].split()[0].split('>')[1].rsplit('_', 1)[0]
            locus = seq[0].split()[0].split('>')[1]
            # annotation info from Prodigal
            if ('# ;gc_cont' in seq[0] or '# ID=' in seq[0]):
                info = seq[0].split(' # ')
                start, end, strand = int(info[1]), int(info[2]), info[3]
                if strand == '1':
                    strand = 1
                else:
                    strand = -1
                product = [''.join(info[4].split()[1:])]
            # annotation info from table
            else:
                start, end, strand = annots[locus]
                product = seq[0].split(' ', 1)[1]
            info = {'transl_table':[trans_table], \
                    'translation':[seq[1]], \
                    'product':product}
            yield contig, [locus, [start, end, strand], info]",def,parse_fasta_annotations,(,fastas,",",annot_tables,",",trans_table,),:,if,annot_tables,is,not,False,:,annots,=,{,},for,table,in,annot_tables,:,for,cds,in,open,(,table,),:,ID,",",start,",",end,",",strand,=,cds,.,parse gene call information from Prodigal fasta output,parse,gene,call,information,from,Prodigal,fasta,output,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L215-L252,train,strip,(,),.,split,(,),annots,[,ID,],=,[,start,",",end,",",int,(,strand,),],for,fasta,in,fastas,:,for,seq,in,,,,,,,,,,,,,,,,,,,,parse_fasta,(,fasta,),:,if,(,'# ;gc_cont',not,in,seq,[,0,],and,'# ID=',not,in,seq,[,0,],),and,annot_tables,is,False,:,print,(,'# specify fasta from Prodigal or annotations table (-t)',",",file,=,sys,.,stderr,),exit,(,),if,'ID=',in,seq,[,0,],:,ID,=,seq,[,0,],.,rsplit,(,'ID=',",",1,),[,1,],.,split,(,';',",",1,),[,0,],contig,=,seq,[,0,],.,split,(,),[,0,],.,split,(,'>',),[,1,],.,rsplit,(,'_%s',%,(,ID,),",",1,),[,0,],else,:,contig,=,seq,[,0,],.,split,(,),[,0,],.,split,(,'>',),[,1,],.,rsplit,(,'_',",",1,),[,0,],locus,=,seq,[,0,],.,split,(,),[,0,],.,split,(,'>',),[,1,],# annotation info from Prodigal,if,(,'# ;gc_cont',in,seq,[,0,],or,'# ID=',in,seq,[,0,],),:,info,=,seq,[,0,],.,split,(,' # ',),start,",",end,",",strand,=,int,(,info,[,1,],),",",int,(,info,[,2,],),",",info,[,3,],if,strand,==,'1',:,strand,=,1,else,:,strand,=,-,1,product,=,[,'',.,join,(,info,[,4,],.,split,(,),[,1,:,],),],# annotation info from table,else,:,start,",",end,",",strand,=,annots,[,locus,],product,=,seq,[,0,],.,split,(,' ',",",1,),[,1,],info,=,{,'transl_table',:,[,trans_table,],",",'translation',:,[,seq,[,1,],],",",'product',:,product,},yield,contig,",",[,locus,",",[,start,",",end,",",strand,],",",info,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,parse_annotations,"def parse_annotations(annots, fmt, annot_tables, trans_table):
    """"""
    parse annotations in either gbk or Prodigal fasta format
    """"""
    annotations = {} # annotations[contig] = [features]
    # gbk format
    if fmt is False:
        for contig, feature in parse_gbk(annots):
            if contig not in annotations:
                annotations[contig] = []
            annotations[contig].append(feature)
    # fasta format
    else:
        for contig, feature in parse_fasta_annotations(annots, annot_tables, trans_table):
            if contig not in annotations:
                annotations[contig] = []
            annotations[contig].append(feature)
    return annotations",python,"def parse_annotations(annots, fmt, annot_tables, trans_table):
    """"""
    parse annotations in either gbk or Prodigal fasta format
    """"""
    annotations = {} # annotations[contig] = [features]
    # gbk format
    if fmt is False:
        for contig, feature in parse_gbk(annots):
            if contig not in annotations:
                annotations[contig] = []
            annotations[contig].append(feature)
    # fasta format
    else:
        for contig, feature in parse_fasta_annotations(annots, annot_tables, trans_table):
            if contig not in annotations:
                annotations[contig] = []
            annotations[contig].append(feature)
    return annotations",def,parse_annotations,(,annots,",",fmt,",",annot_tables,",",trans_table,),:,annotations,=,{,},# annotations[contig] = [features],# gbk format,if,fmt,is,False,:,for,contig,",",feature,in,parse_gbk,(,annots,),:,if,contig,not,in,annotations,:,annotations,[,contig,],parse annotations in either gbk or Prodigal fasta format,parse,annotations,in,either,gbk,or,Prodigal,fasta,format,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L254-L271,train,=,[,],annotations,[,contig,],.,append,(,feature,),# fasta format,else,:,for,contig,",",feature,in,parse_fasta_annotations,(,annots,",",annot_tables,",",trans_table,),:,if,,,,,,,,,,,,,,,,,,,,contig,not,in,annotations,:,annotations,[,contig,],=,[,],annotations,[,contig,],.,append,(,feature,),return,annotations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,codon2aa,"def codon2aa(codon, trans_table):
    """"""
    convert codon to amino acid
    """"""
    return Seq(''.join(codon), IUPAC.ambiguous_dna).translate(table = trans_table)[0]",python,"def codon2aa(codon, trans_table):
    """"""
    convert codon to amino acid
    """"""
    return Seq(''.join(codon), IUPAC.ambiguous_dna).translate(table = trans_table)[0]",def,codon2aa,(,codon,",",trans_table,),:,return,Seq,(,'',.,join,(,codon,),",",IUPAC,.,ambiguous_dna,),.,translate,(,table,=,trans_table,),[,0,],,,,,,,,,,,,convert codon to amino acid,convert,codon,to,amino,acid,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L311-L315,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,find_consensus,"def find_consensus(bases):
    """"""
    find consensus base based on nucleotide
    frequencies
    """"""
    nucs = ['A', 'T', 'G', 'C', 'N']
    total = sum([bases[nuc] for nuc in nucs if nuc in bases])
    # save most common base as consensus (random nuc if there is a tie)
    try:
        top = max([bases[nuc] for nuc in nucs if nuc in bases])
    except:
        bases['consensus'] = ('N', 'n/a')
        bases['consensus frequency'] = 'n/a'
        bases['reference frequency'] = 'n/a'
        return bases
    top = [(nuc, bases[nuc]) for nuc in bases if bases[nuc] == top]
    if top[0][1] == 0:
        bases['consensus'] = ('n/a', 0)
    else:
        bases['consensus'] = random.choice(top)
    if total == 0:
        c_freq = 'n/a'
        ref_freq = 'n/a'
    else:
        c_freq = float(bases['consensus'][1]) / float(total)
        if bases['ref'] not in bases:
            ref_freq = 0
        else:
            ref_freq = float(bases[bases['ref']]) / float(total)
    bases['consensus frequency'] = c_freq
    bases['reference frequency'] = ref_freq
    return bases",python,"def find_consensus(bases):
    """"""
    find consensus base based on nucleotide
    frequencies
    """"""
    nucs = ['A', 'T', 'G', 'C', 'N']
    total = sum([bases[nuc] for nuc in nucs if nuc in bases])
    # save most common base as consensus (random nuc if there is a tie)
    try:
        top = max([bases[nuc] for nuc in nucs if nuc in bases])
    except:
        bases['consensus'] = ('N', 'n/a')
        bases['consensus frequency'] = 'n/a'
        bases['reference frequency'] = 'n/a'
        return bases
    top = [(nuc, bases[nuc]) for nuc in bases if bases[nuc] == top]
    if top[0][1] == 0:
        bases['consensus'] = ('n/a', 0)
    else:
        bases['consensus'] = random.choice(top)
    if total == 0:
        c_freq = 'n/a'
        ref_freq = 'n/a'
    else:
        c_freq = float(bases['consensus'][1]) / float(total)
        if bases['ref'] not in bases:
            ref_freq = 0
        else:
            ref_freq = float(bases[bases['ref']]) / float(total)
    bases['consensus frequency'] = c_freq
    bases['reference frequency'] = ref_freq
    return bases",def,find_consensus,(,bases,),:,nucs,=,[,'A',",",'T',",",'G',",",'C',",",'N',],total,=,sum,(,[,bases,[,nuc,],for,nuc,in,nucs,if,nuc,in,bases,],),# save most common base as consensus (random nuc if there is a tie),try,:,top,=,"find consensus base based on nucleotide
    frequencies",find,consensus,base,based,on,nucleotide,frequencies,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L371-L402,train,max,(,[,bases,[,nuc,],for,nuc,in,nucs,if,nuc,in,bases,],),except,:,bases,[,'consensus',],=,(,'N',",",'n/a',),bases,,,,,,,,,,,,,,,,,,,,[,'consensus frequency',],=,'n/a',bases,[,'reference frequency',],=,'n/a',return,bases,top,=,[,(,nuc,",",bases,[,nuc,],),for,nuc,in,bases,if,bases,[,nuc,],==,top,],if,top,[,0,],[,1,],==,0,:,bases,[,'consensus',],=,(,'n/a',",",0,),else,:,bases,[,'consensus',],=,random,.,choice,(,top,),if,total,==,0,:,c_freq,=,'n/a',ref_freq,=,'n/a',else,:,c_freq,=,float,(,bases,[,'consensus',],[,1,],),/,float,(,total,),if,bases,[,'ref',],not,in,bases,:,ref_freq,=,0,else,:,ref_freq,=,float,(,bases,[,bases,[,'ref',],],),/,float,(,total,),bases,[,'consensus frequency',],=,c_freq,bases,[,'reference frequency',],=,ref_freq,return,bases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_variation.py,print_consensus,"def print_consensus(genomes):
    """"""
    print consensensus sequences for each genome and sample
    """"""
    # generate consensus sequences
    cons = {} # cons[genome][sample][contig] = consensus
    for genome, contigs in list(genomes.items()):
        cons[genome] = {}
        for contig, samples in list(contigs.items()):
            for sample, stats in list(samples.items()):
                if sample not in cons[genome]:
                    cons[genome][sample] = {}
                seq = cons[genome][sample][contig] = []
                for pos, ps in enumerate(stats['bp_stats'], 1):
                    ref, consensus = ps['ref'], ps['consensus'][0]
                    if consensus == 'n/a':
                        consensus = ref.lower()
                    seq.append(consensus)
    # print consensus sequences
    for genome, samples in cons.items():
        for sample, contigs in samples.items():
            fn = '%s.%s.consensus.fa' % (genome, sample)
            f = open(fn, 'w')
            for contig, seq in contigs.items():
                print('>%s' % (contig), file = f)
                print(''.join(seq), file = f)
            f.close()
    return cons",python,"def print_consensus(genomes):
    """"""
    print consensensus sequences for each genome and sample
    """"""
    # generate consensus sequences
    cons = {} # cons[genome][sample][contig] = consensus
    for genome, contigs in list(genomes.items()):
        cons[genome] = {}
        for contig, samples in list(contigs.items()):
            for sample, stats in list(samples.items()):
                if sample not in cons[genome]:
                    cons[genome][sample] = {}
                seq = cons[genome][sample][contig] = []
                for pos, ps in enumerate(stats['bp_stats'], 1):
                    ref, consensus = ps['ref'], ps['consensus'][0]
                    if consensus == 'n/a':
                        consensus = ref.lower()
                    seq.append(consensus)
    # print consensus sequences
    for genome, samples in cons.items():
        for sample, contigs in samples.items():
            fn = '%s.%s.consensus.fa' % (genome, sample)
            f = open(fn, 'w')
            for contig, seq in contigs.items():
                print('>%s' % (contig), file = f)
                print(''.join(seq), file = f)
            f.close()
    return cons",def,print_consensus,(,genomes,),:,# generate consensus sequences,cons,=,{,},# cons[genome][sample][contig] = consensus,for,genome,",",contigs,in,list,(,genomes,.,items,(,),),:,cons,[,genome,],=,{,},for,contig,",",samples,in,list,(,contigs,.,items,print consensensus sequences for each genome and sample,print,consensensus,sequences,for,each,genome,and,sample,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_variation.py#L451-L478,train,(,),),:,for,sample,",",stats,in,list,(,samples,.,items,(,),),:,if,sample,not,in,cons,[,genome,],:,cons,[,genome,,,,,,,,,,,,,,,,,,,,],[,sample,],=,{,},seq,=,cons,[,genome,],[,sample,],[,contig,],=,[,],for,pos,",",ps,in,enumerate,(,stats,[,'bp_stats',],",",1,),:,ref,",",consensus,=,ps,[,'ref',],",",ps,[,'consensus',],[,0,],if,consensus,==,'n/a',:,consensus,=,ref,.,lower,(,),seq,.,append,(,consensus,),# print consensus sequences,for,genome,",",samples,in,cons,.,items,(,),:,for,sample,",",contigs,in,samples,.,items,(,),:,fn,=,'%s.%s.consensus.fa',%,(,genome,",",sample,),f,=,open,(,fn,",",'w',),for,contig,",",seq,in,contigs,.,items,(,),:,print,(,'>%s',%,(,contig,),",",file,=,f,),print,(,'',.,join,(,seq,),",",file,=,f,),f,.,close,(,),return,cons,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_coverage.py,parse_cov,"def parse_cov(cov_table, scaffold2genome):
    """"""
    calculate genome coverage from scaffold coverage table
    """"""
    size   = {} # size[genome] = genome size
    mapped = {} # mapped[genome][sample] = mapped bases
    # parse coverage files
    for line in open(cov_table):
        line = line.strip().split('\t')
        if line[0].startswith('#'):
            samples = line[1:]
            samples = [i.rsplit('/', 1)[-1].split('.', 1)[0] for i in samples]
            continue
        scaffold, length = line[0].split(': ')
        length = float(length)
        covs  = [float(i) for i in line[1:]]
        bases = [c * length for c in covs]
        if scaffold not in scaffold2genome:
            continue
        genome = scaffold2genome[scaffold]
        if genome not in size:
            size[genome] = 0
            mapped[genome] = {sample:0 for sample in samples}
        # keep track of genome size
        size[genome] += length
        # keep track of number of mapped bases
        for sample, count in zip(samples, bases):
            mapped[genome][sample] += count
    # calculate coverage from base counts and genome size
    coverage = {'genome':[], 'genome size (bp)':[], 'sample':[], 'coverage':[]}
    for genome, length in size.items():
        for sample in samples:
            cov = mapped[genome][sample] / length
            coverage['genome'].append(genome)
            coverage['genome size (bp)'].append(length)
            coverage['sample'].append(sample)
            coverage['coverage'].append(cov)
    return pd.DataFrame(coverage)",python,"def parse_cov(cov_table, scaffold2genome):
    """"""
    calculate genome coverage from scaffold coverage table
    """"""
    size   = {} # size[genome] = genome size
    mapped = {} # mapped[genome][sample] = mapped bases
    # parse coverage files
    for line in open(cov_table):
        line = line.strip().split('\t')
        if line[0].startswith('#'):
            samples = line[1:]
            samples = [i.rsplit('/', 1)[-1].split('.', 1)[0] for i in samples]
            continue
        scaffold, length = line[0].split(': ')
        length = float(length)
        covs  = [float(i) for i in line[1:]]
        bases = [c * length for c in covs]
        if scaffold not in scaffold2genome:
            continue
        genome = scaffold2genome[scaffold]
        if genome not in size:
            size[genome] = 0
            mapped[genome] = {sample:0 for sample in samples}
        # keep track of genome size
        size[genome] += length
        # keep track of number of mapped bases
        for sample, count in zip(samples, bases):
            mapped[genome][sample] += count
    # calculate coverage from base counts and genome size
    coverage = {'genome':[], 'genome size (bp)':[], 'sample':[], 'coverage':[]}
    for genome, length in size.items():
        for sample in samples:
            cov = mapped[genome][sample] / length
            coverage['genome'].append(genome)
            coverage['genome size (bp)'].append(length)
            coverage['sample'].append(sample)
            coverage['coverage'].append(cov)
    return pd.DataFrame(coverage)",def,parse_cov,(,cov_table,",",scaffold2genome,),:,size,=,{,},# size[genome] = genome size,mapped,=,{,},# mapped[genome][sample] = mapped bases,# parse coverage files,for,line,in,open,(,cov_table,),:,line,=,line,.,strip,(,),.,split,(,'\t',),if,line,[,0,calculate genome coverage from scaffold coverage table,calculate,genome,coverage,from,scaffold,coverage,table,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_coverage.py#L13-L50,train,],.,startswith,(,'#',),:,samples,=,line,[,1,:,],samples,=,[,i,.,rsplit,(,'/',",",1,),[,-,1,],.,,,,,,,,,,,,,,,,,,,,split,(,'.',",",1,),[,0,],for,i,in,samples,],continue,scaffold,",",length,=,line,[,0,],.,split,(,': ',),length,=,float,(,length,),covs,=,[,float,(,i,),for,i,in,line,[,1,:,],],bases,=,[,c,*,length,for,c,in,covs,],if,scaffold,not,in,scaffold2genome,:,continue,genome,=,scaffold2genome,[,scaffold,],if,genome,not,in,size,:,size,[,genome,],=,0,mapped,[,genome,],=,{,sample,:,0,for,sample,in,samples,},# keep track of genome size,size,[,genome,],+=,length,# keep track of number of mapped bases,for,sample,",",count,in,zip,(,samples,",",bases,),:,mapped,[,genome,],[,sample,],+=,count,# calculate coverage from base counts and genome size,coverage,=,{,'genome',:,[,],",",'genome size (bp)',:,[,],",",'sample',:,[,],",",'coverage',:,[,],},for,genome,",",length,in,size,.,items,(,),:,for,sample,in,samples,:,cov,=,mapped,[,genome,],[,sample,],/,length,coverage,[,'genome',],.,append,(,genome,),coverage,[,'genome size (bp)',],.,append,(,length,),coverage,[,'sample',],.,append,(,sample,),coverage,[,'coverage',],.,append,(,cov,),return,pd,.,DataFrame,(,coverage,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_coverage.py,genome_coverage,"def genome_coverage(covs, s2b):
    """"""
    calculate genome coverage from scaffold coverage
    """"""
    COV = []
    for cov in covs:
        COV.append(parse_cov(cov, s2b))
    return pd.concat(COV)",python,"def genome_coverage(covs, s2b):
    """"""
    calculate genome coverage from scaffold coverage
    """"""
    COV = []
    for cov in covs:
        COV.append(parse_cov(cov, s2b))
    return pd.concat(COV)",def,genome_coverage,(,covs,",",s2b,),:,COV,=,[,],for,cov,in,covs,:,COV,.,append,(,parse_cov,(,cov,",",s2b,),),return,pd,.,concat,(,COV,),,,,,,,,,calculate genome coverage from scaffold coverage,calculate,genome,coverage,from,scaffold,coverage,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_coverage.py#L52-L59,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_coverage.py,parse_s2bs,"def parse_s2bs(s2bs):
    """"""
    convert s2b files to dictionary
    """"""
    s2b = {}
    for s in s2bs:
        for line in open(s):
            line = line.strip().split('\t')
            s, b = line[0], line[1]
            s2b[s] = b
    return s2b",python,"def parse_s2bs(s2bs):
    """"""
    convert s2b files to dictionary
    """"""
    s2b = {}
    for s in s2bs:
        for line in open(s):
            line = line.strip().split('\t')
            s, b = line[0], line[1]
            s2b[s] = b
    return s2b",def,parse_s2bs,(,s2bs,),:,s2b,=,{,},for,s,in,s2bs,:,for,line,in,open,(,s,),:,line,=,line,.,strip,(,),.,split,(,'\t',),s,",",b,=,line,[,0,],convert s2b files to dictionary,convert,s2b,files,to,dictionary,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_coverage.py#L61-L71,train,",",line,[,1,],s2b,[,s,],=,b,return,s2b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/genome_coverage.py,fa2s2b,"def fa2s2b(fastas):
    """"""
    convert fastas to s2b dictionary
    """"""
    s2b = {}
    for fa in fastas:
        for seq in parse_fasta(fa):
            s = seq[0].split('>', 1)[1].split()[0]
            s2b[s] = fa.rsplit('/', 1)[-1].rsplit('.', 1)[0]
    return s2b",python,"def fa2s2b(fastas):
    """"""
    convert fastas to s2b dictionary
    """"""
    s2b = {}
    for fa in fastas:
        for seq in parse_fasta(fa):
            s = seq[0].split('>', 1)[1].split()[0]
            s2b[s] = fa.rsplit('/', 1)[-1].rsplit('.', 1)[0]
    return s2b",def,fa2s2b,(,fastas,),:,s2b,=,{,},for,fa,in,fastas,:,for,seq,in,parse_fasta,(,fa,),:,s,=,seq,[,0,],.,split,(,'>',",",1,),[,1,],.,split,(,),convert fastas to s2b dictionary,convert,fastas,to,s2b,dictionary,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/genome_coverage.py#L73-L82,train,[,0,],s2b,[,s,],=,fa,.,rsplit,(,'/',",",1,),[,-,1,],.,rsplit,(,'.',",",1,),[,0,],,,,,,,,,,,,,,,,,,,,return,s2b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/filter_ambiguity.py,filter_ambiguity,"def filter_ambiguity(records, percent=0.5):  # , repeats=6)
    """"""
    Filters out sequences with too much ambiguity as defined by the method
    parameters.

    :type records: list
    :param records: A list of sequences
    :type repeats: int
    :param repeats: Defines the number of repeated N that trigger truncating a
                    sequence.
    :type percent: float
    :param percent: Defines the overall percentage of N in a sequence that
                     will cause the sequence to be filtered out.
    """"""
    seqs = []
    # Ns = ''.join(['N' for _ in range(repeats)])
    count = 0
    for record in records:
        if record.seq.count('N')/float(len(record)) < percent:
#            pos = record.seq.find(Ns)
#            if pos >= 0:
#                record.seq = Seq(str(record.seq)[:pos])
            seqs.append(record)
        count += 1

    return seqs, count",python,"def filter_ambiguity(records, percent=0.5):  # , repeats=6)
    """"""
    Filters out sequences with too much ambiguity as defined by the method
    parameters.

    :type records: list
    :param records: A list of sequences
    :type repeats: int
    :param repeats: Defines the number of repeated N that trigger truncating a
                    sequence.
    :type percent: float
    :param percent: Defines the overall percentage of N in a sequence that
                     will cause the sequence to be filtered out.
    """"""
    seqs = []
    # Ns = ''.join(['N' for _ in range(repeats)])
    count = 0
    for record in records:
        if record.seq.count('N')/float(len(record)) < percent:
#            pos = record.seq.find(Ns)
#            if pos >= 0:
#                record.seq = Seq(str(record.seq)[:pos])
            seqs.append(record)
        count += 1

    return seqs, count",def,filter_ambiguity,(,records,",",percent,=,0.5,),:,"# , repeats=6)",seqs,=,[,],# Ns = ''.join(['N' for _ in range(repeats)]),count,=,0,for,record,in,records,:,if,record,.,seq,.,count,(,'N',),/,float,(,len,(,record,),),<,percent,"Filters out sequences with too much ambiguity as defined by the method
    parameters.

    :type records: list
    :param records: A list of sequences
    :type repeats: int
    :param repeats: Defines the number of repeated N that trigger truncating a
                    sequence.
    :type percent: float
    :param percent: Defines the overall percentage of N in a sequence that
                     will cause the sequence to be filtered out.",Filters,out,sequences,with,too,much,ambiguity,as,defined,by,the,method,parameters,.,,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/filter_ambiguity.py#L16-L41,train,:,#            pos = record.seq.find(Ns),#            if pos >= 0:,#                record.seq = Seq(str(record.seq)[:pos]),seqs,.,append,(,record,),count,+=,1,return,seqs,",",count,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/pypi.py,package_existent,"def package_existent(name):
    """"""Search package.

    * :class:`bootstrap_py.exceptions.Conflict` exception occurs
      when user specified name has already existed.

    * :class:`bootstrap_py.exceptions.BackendFailure` exception occurs
      when PyPI service is down.

    :param str name: package name
    """"""
    try:
        response = requests.get(PYPI_URL.format(name))
        if response.ok:
            msg = ('[error] ""{0}"" is registered already in PyPI.\n'
                   '\tSpecify another package name.').format(name)
            raise Conflict(msg)
    except (socket.gaierror,
            Timeout,
            ConnectionError,
            HTTPError) as exc:
        raise BackendFailure(exc)",python,"def package_existent(name):
    """"""Search package.

    * :class:`bootstrap_py.exceptions.Conflict` exception occurs
      when user specified name has already existed.

    * :class:`bootstrap_py.exceptions.BackendFailure` exception occurs
      when PyPI service is down.

    :param str name: package name
    """"""
    try:
        response = requests.get(PYPI_URL.format(name))
        if response.ok:
            msg = ('[error] ""{0}"" is registered already in PyPI.\n'
                   '\tSpecify another package name.').format(name)
            raise Conflict(msg)
    except (socket.gaierror,
            Timeout,
            ConnectionError,
            HTTPError) as exc:
        raise BackendFailure(exc)",def,package_existent,(,name,),:,try,:,response,=,requests,.,get,(,PYPI_URL,.,format,(,name,),),if,response,.,ok,:,msg,=,(,"'[error] ""{0}"" is registered already in PyPI.\n'",'\tSpecify another package name.',),.,format,(,name,),raise,Conflict,(,msg,),except,"Search package.

    * :class:`bootstrap_py.exceptions.Conflict` exception occurs
      when user specified name has already existed.

    * :class:`bootstrap_py.exceptions.BackendFailure` exception occurs
      when PyPI service is down.

    :param str name: package name",Search,package,.,,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/pypi.py#L12-L33,train,(,socket,.,gaierror,",",Timeout,",",ConnectionError,",",HTTPError,),as,exc,:,raise,BackendFailure,(,exc,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/nr_fasta.py,append_index_id,"def append_index_id(id, ids):
    """"""
    add index to id to make it unique wrt ids
    """"""
    index = 1
    mod = '%s_%s' % (id, index)
    while mod in ids:
        index += 1
        mod = '%s_%s' % (id, index)
    ids.append(mod)
    return mod, ids",python,"def append_index_id(id, ids):
    """"""
    add index to id to make it unique wrt ids
    """"""
    index = 1
    mod = '%s_%s' % (id, index)
    while mod in ids:
        index += 1
        mod = '%s_%s' % (id, index)
    ids.append(mod)
    return mod, ids",def,append_index_id,(,id,",",ids,),:,index,=,1,mod,=,'%s_%s',%,(,id,",",index,),while,mod,in,ids,:,index,+=,1,mod,=,'%s_%s',%,(,id,",",index,),ids,.,append,(,mod,),add index to id to make it unique wrt ids,add,index,to,id,to,make,it,unique,wrt,ids,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/nr_fasta.py#L11-L21,train,return,mod,",",ids,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/nr_fasta.py,de_rep,"def de_rep(fastas, append_index, return_original = False):
    """"""
    de-replicate fastas based on sequence names
    """"""
    ids = []
    for fasta in fastas:
        for seq in parse_fasta(fasta):
            header = seq[0].split('>')[1].split()
            id = header[0]
            if id not in ids:
                ids.append(id)
                if return_original is True:
                    yield [header, seq]
                else:
                    yield seq
            elif append_index == True:
                new, ids = append_index_id(id, ids) 
                if return_original is True:
                    yield [header, ['>%s %s' % (new, ' '.join(header[1::])), seq[1]]]
                else:
                    yield ['>%s %s' % (new, ' '.join(header[1::])), seq[1]]",python,"def de_rep(fastas, append_index, return_original = False):
    """"""
    de-replicate fastas based on sequence names
    """"""
    ids = []
    for fasta in fastas:
        for seq in parse_fasta(fasta):
            header = seq[0].split('>')[1].split()
            id = header[0]
            if id not in ids:
                ids.append(id)
                if return_original is True:
                    yield [header, seq]
                else:
                    yield seq
            elif append_index == True:
                new, ids = append_index_id(id, ids) 
                if return_original is True:
                    yield [header, ['>%s %s' % (new, ' '.join(header[1::])), seq[1]]]
                else:
                    yield ['>%s %s' % (new, ' '.join(header[1::])), seq[1]]",def,de_rep,(,fastas,",",append_index,",",return_original,=,False,),:,ids,=,[,],for,fasta,in,fastas,:,for,seq,in,parse_fasta,(,fasta,),:,header,=,seq,[,0,],.,split,(,'>',),[,1,],de-replicate fastas based on sequence names,de,-,replicate,fastas,based,on,sequence,names,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/nr_fasta.py#L23-L43,train,.,split,(,),id,=,header,[,0,],if,id,not,in,ids,:,ids,.,append,(,id,),if,return_original,is,True,:,yield,[,header,,,,,,,,,,,,,,,,,,,,",",seq,],else,:,yield,seq,elif,append_index,==,True,:,new,",",ids,=,append_index_id,(,id,",",ids,),if,return_original,is,True,:,yield,[,header,",",[,'>%s %s',%,(,new,",",' ',.,join,(,header,[,1,:,:,],),),",",seq,[,1,],],],else,:,yield,[,'>%s %s',%,(,new,",",' ',.,join,(,header,[,1,:,:,],),),",",seq,[,1,],],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,get,"def get(postcode):
    """"""
    Request data associated with `postcode`.

    :param postcode: the postcode to search for. The postcode may 
                     contain spaces (they will be removed).

    :returns: a dict of the nearest postcode's data or None if no 
              postcode data is found.
    """"""
    postcode = quote(postcode.replace(' ', ''))
    url = '%s/postcode/%s.json' % (END_POINT, postcode)
    return _get_json_resp(url)",python,"def get(postcode):
    """"""
    Request data associated with `postcode`.

    :param postcode: the postcode to search for. The postcode may 
                     contain spaces (they will be removed).

    :returns: a dict of the nearest postcode's data or None if no 
              postcode data is found.
    """"""
    postcode = quote(postcode.replace(' ', ''))
    url = '%s/postcode/%s.json' % (END_POINT, postcode)
    return _get_json_resp(url)",def,get,(,postcode,),:,postcode,=,quote,(,postcode,.,replace,(,' ',",",'',),),url,=,'%s/postcode/%s.json',%,(,END_POINT,",",postcode,),return,_get_json_resp,(,url,),,,,,,,,,,,"Request data associated with `postcode`.

    :param postcode: the postcode to search for. The postcode may 
                     contain spaces (they will be removed).

    :returns: a dict of the nearest postcode's data or None if no 
              postcode data is found.",Request,data,associated,with,postcode,.,,,,,,,,,,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L22-L34,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,get_from_postcode,"def get_from_postcode(postcode, distance):
    """"""
    Request all postcode data within `distance` miles of `postcode`.

    :param postcode: the postcode to search for. The postcode may 
                     contain spaces (they will be removed).

    :param distance: distance in miles to `postcode`.

    :returns: a list of dicts containing postcode data within the 
              specified distance or `None` if `postcode` is not valid.
    """"""
    postcode = quote(postcode.replace(' ', ''))
    return _get_from(distance, 'postcode=%s' % postcode)",python,"def get_from_postcode(postcode, distance):
    """"""
    Request all postcode data within `distance` miles of `postcode`.

    :param postcode: the postcode to search for. The postcode may 
                     contain spaces (they will be removed).

    :param distance: distance in miles to `postcode`.

    :returns: a list of dicts containing postcode data within the 
              specified distance or `None` if `postcode` is not valid.
    """"""
    postcode = quote(postcode.replace(' ', ''))
    return _get_from(distance, 'postcode=%s' % postcode)",def,get_from_postcode,(,postcode,",",distance,),:,postcode,=,quote,(,postcode,.,replace,(,' ',",",'',),),return,_get_from,(,distance,",",'postcode=%s',%,postcode,),,,,,,,,,,,,,,"Request all postcode data within `distance` miles of `postcode`.

    :param postcode: the postcode to search for. The postcode may 
                     contain spaces (they will be removed).

    :param distance: distance in miles to `postcode`.

    :returns: a list of dicts containing postcode data within the 
              specified distance or `None` if `postcode` is not valid.",Request,all,postcode,data,within,distance,miles,of,postcode,.,,,,,,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L56-L69,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,PostCoder._check_point,"def _check_point(self, lat, lng):
        """""" Checks if latitude and longitude correct """"""
        if abs(lat) > 90 or abs(lng) > 180:
            msg = ""Illegal lat and/or lng, (%s, %s) provided."" % (lat, lng)
            raise IllegalPointException(msg)",python,"def _check_point(self, lat, lng):
        """""" Checks if latitude and longitude correct """"""
        if abs(lat) > 90 or abs(lng) > 180:
            msg = ""Illegal lat and/or lng, (%s, %s) provided."" % (lat, lng)
            raise IllegalPointException(msg)",def,_check_point,(,self,",",lat,",",lng,),:,if,abs,(,lat,),>,90,or,abs,(,lng,),>,180,:,msg,=,"""Illegal lat and/or lng, (%s, %s) provided.""",%,(,lat,",",lng,),raise,IllegalPointException,(,msg,),,,,,Checks if latitude and longitude correct,Checks,if,latitude,and,longitude,correct,,,,,,,,,,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L123-L127,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,PostCoder._lookup,"def _lookup(self, skip_cache, fun, *args, **kwargs):
        """""" 
        Checks for cached responses, before requesting from 
        web-service
        """"""
        if args not in self.cache or skip_cache:
            self.cache[args] = fun(*args, **kwargs)
        return self.cache[args]",python,"def _lookup(self, skip_cache, fun, *args, **kwargs):
        """""" 
        Checks for cached responses, before requesting from 
        web-service
        """"""
        if args not in self.cache or skip_cache:
            self.cache[args] = fun(*args, **kwargs)
        return self.cache[args]",def,_lookup,(,self,",",skip_cache,",",fun,",",*,args,",",*,*,kwargs,),:,if,args,not,in,self,.,cache,or,skip_cache,:,self,.,cache,[,args,],=,fun,(,*,args,",",*,*,kwargs,),"Checks for cached responses, before requesting from 
        web-service",Checks,for,cached,responses,before,requesting,from,web,-,service,,,,,,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L129-L136,train,return,self,.,cache,[,args,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,PostCoder.get_nearest,"def get_nearest(self, lat, lng, skip_cache=False): 
        """"""
        Calls `postcodes.get_nearest` but checks correctness of `lat` 
        and `long`, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a dict of the nearest postcode's data.
        """"""
        lat, lng = float(lat), float(lng)
        self._check_point(lat, lng)
        return self._lookup(skip_cache, get_nearest, lat, lng)",python,"def get_nearest(self, lat, lng, skip_cache=False): 
        """"""
        Calls `postcodes.get_nearest` but checks correctness of `lat` 
        and `long`, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a dict of the nearest postcode's data.
        """"""
        lat, lng = float(lat), float(lng)
        self._check_point(lat, lng)
        return self._lookup(skip_cache, get_nearest, lat, lng)",def,get_nearest,(,self,",",lat,",",lng,",",skip_cache,=,False,),:,lat,",",lng,=,float,(,lat,),",",float,(,lng,),self,.,_check_point,(,lat,",",lng,),return,self,.,_lookup,(,skip_cache,",",get_nearest,"Calls `postcodes.get_nearest` but checks correctness of `lat` 
        and `long`, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a dict of the nearest postcode's data.",Calls,postcodes,.,get_nearest,but,checks,correctness,of,lat,and,long,and,by,default,utilises,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L152-L167,train,",",lat,",",lng,),,,,,,,,,,,,,,,,,,,,,,,,,,a,local,cache,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,PostCoder.get_from_postcode,"def get_from_postcode(self, postcode, distance, skip_cache=False):
        """"""
        Calls `postcodes.get_from_postcode` but checks correctness of 
        `distance`, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a list of dicts containing postcode data within the 
                  specified distance.
        """"""
        distance = float(distance)
        if distance < 0:
            raise IllegalDistanceException(""Distance must not be negative"")
        # remove spaces and change case here due to caching
        postcode = postcode.lower().replace(' ', '')
        return self._lookup(skip_cache, get_from_postcode, postcode, 
                            float(distance))",python,"def get_from_postcode(self, postcode, distance, skip_cache=False):
        """"""
        Calls `postcodes.get_from_postcode` but checks correctness of 
        `distance`, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a list of dicts containing postcode data within the 
                  specified distance.
        """"""
        distance = float(distance)
        if distance < 0:
            raise IllegalDistanceException(""Distance must not be negative"")
        # remove spaces and change case here due to caching
        postcode = postcode.lower().replace(' ', '')
        return self._lookup(skip_cache, get_from_postcode, postcode, 
                            float(distance))",def,get_from_postcode,(,self,",",postcode,",",distance,",",skip_cache,=,False,),:,distance,=,float,(,distance,),if,distance,<,0,:,raise,IllegalDistanceException,(,"""Distance must not be negative""",),# remove spaces and change case here due to caching,postcode,=,postcode,.,lower,(,),.,replace,(,' ',",","Calls `postcodes.get_from_postcode` but checks correctness of 
        `distance`, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a list of dicts containing postcode data within the 
                  specified distance.",Calls,postcodes,.,get_from_postcode,but,checks,correctness,of,distance,and,by,default,utilises,a,local,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L169-L189,train,'',),return,self,.,_lookup,(,skip_cache,",",get_from_postcode,",",postcode,",",float,(,distance,),),,,,,,,,,,,,,cache,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e-dard/postcodes,postcodes.py,PostCoder.get_from_geo,"def get_from_geo(self, lat, lng, distance, skip_cache=False):
        """"""
        Calls `postcodes.get_from_geo` but checks the correctness of 
        all arguments, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a list of dicts containing postcode data within the 
                  specified distance.
        """"""
        # remove spaces and change case here due to caching
        lat, lng, distance = float(lat), float(lng), float(distance)
        if distance < 0:
            raise IllegalDistanceException(""Distance must not be negative"")
        self._check_point(lat, lng)
        return self._lookup(skip_cache, get_from_geo, lat, lng, distance)",python,"def get_from_geo(self, lat, lng, distance, skip_cache=False):
        """"""
        Calls `postcodes.get_from_geo` but checks the correctness of 
        all arguments, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a list of dicts containing postcode data within the 
                  specified distance.
        """"""
        # remove spaces and change case here due to caching
        lat, lng, distance = float(lat), float(lng), float(distance)
        if distance < 0:
            raise IllegalDistanceException(""Distance must not be negative"")
        self._check_point(lat, lng)
        return self._lookup(skip_cache, get_from_geo, lat, lng, distance)",def,get_from_geo,(,self,",",lat,",",lng,",",distance,",",skip_cache,=,False,),:,# remove spaces and change case here due to caching,lat,",",lng,",",distance,=,float,(,lat,),",",float,(,lng,),",",float,(,distance,),if,distance,<,0,:,raise,"Calls `postcodes.get_from_geo` but checks the correctness of 
        all arguments, and by default utilises a local cache.

        :param skip_cache: optional argument specifying whether to skip 
                           the cache and make an explicit request.

        :raises IllegalPointException: if the latitude or longitude 
                                       are out of bounds.

        :returns: a list of dicts containing postcode data within the 
                  specified distance.",Calls,postcodes,.,get_from_geo,but,checks,the,correctness,of,all,arguments,and,by,default,utilises,d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005,https://github.com/e-dard/postcodes/blob/d63c47b4ecd765bc2e4e6ba34bc0b8a796f44005/postcodes.py#L191-L210,train,IllegalDistanceException,(,"""Distance must not be negative""",),self,.,_check_point,(,lat,",",lng,),return,self,.,_lookup,(,skip_cache,",",get_from_geo,",",lat,",",lng,",",distance,),,,,a,local,cache,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,insertions_from_masked,"def insertions_from_masked(seq):
    """"""
    get coordinates of insertions from insertion-masked sequence
    """"""
    insertions = []
    prev = True
    for i, base in enumerate(seq):
        if base.isupper() and prev is True:
            insertions.append([])
            prev = False
        elif base.islower():
            insertions[-1].append(i)
            prev = True
    return [[min(i), max(i)] for i in insertions if i != []]",python,"def insertions_from_masked(seq):
    """"""
    get coordinates of insertions from insertion-masked sequence
    """"""
    insertions = []
    prev = True
    for i, base in enumerate(seq):
        if base.isupper() and prev is True:
            insertions.append([])
            prev = False
        elif base.islower():
            insertions[-1].append(i)
            prev = True
    return [[min(i), max(i)] for i in insertions if i != []]",def,insertions_from_masked,(,seq,),:,insertions,=,[,],prev,=,True,for,i,",",base,in,enumerate,(,seq,),:,if,base,.,isupper,(,),and,prev,is,True,:,insertions,.,append,(,[,],),prev,=,get coordinates of insertions from insertion-masked sequence,get,coordinates,of,insertions,from,insertion,-,masked,sequence,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L15-L28,train,False,elif,base,.,islower,(,),:,insertions,[,-,1,],.,append,(,i,),prev,=,True,return,[,[,min,(,i,),",",max,,,,,,,,,,,,,,,,,,,,(,i,),],for,i,in,insertions,if,i,!=,[,],],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,seq_info,"def seq_info(names, id2names, insertions, sequences):
    """"""
    get insertion information from header
    """"""
    seqs = {} # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns]], ...]]
    for name in names:
        id = id2names[name]
        gene = name.split('fromHMM::', 1)[0].rsplit(' ', 1)[1]
        model = name.split('fromHMM::', 1)[1].split('=', 1)[1].split()[0]
        i_gene_pos = insertions[id] # coordinates of each insertion wrt gene
        i_model_pos = name.split('fromHMM::', 1)[1].split('model-pos(ins-len)=')[1].split()[0].split(';') # model overlap
        i_info = []
        for i, ins in enumerate(i_gene_pos):
            model_pos = i_model_pos[i].split('-')[1].split('(')[0]
            length = i_model_pos[i].split('(')[1].split(')')[0]
            iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s'\
                    % (id, (i + 1), (i + 1), ins[0], ins[1], model_pos)
            iseq = sequences[id][1][ins[0]:(ins[1] + 1)]
            iseq = [iheader, iseq]
            info = [ins, model_pos, length, iseq, [], []]
            i_info.append(info)
        seqs[id] = [gene, model, i_info]
    return seqs",python,"def seq_info(names, id2names, insertions, sequences):
    """"""
    get insertion information from header
    """"""
    seqs = {} # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns]], ...]]
    for name in names:
        id = id2names[name]
        gene = name.split('fromHMM::', 1)[0].rsplit(' ', 1)[1]
        model = name.split('fromHMM::', 1)[1].split('=', 1)[1].split()[0]
        i_gene_pos = insertions[id] # coordinates of each insertion wrt gene
        i_model_pos = name.split('fromHMM::', 1)[1].split('model-pos(ins-len)=')[1].split()[0].split(';') # model overlap
        i_info = []
        for i, ins in enumerate(i_gene_pos):
            model_pos = i_model_pos[i].split('-')[1].split('(')[0]
            length = i_model_pos[i].split('(')[1].split(')')[0]
            iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s'\
                    % (id, (i + 1), (i + 1), ins[0], ins[1], model_pos)
            iseq = sequences[id][1][ins[0]:(ins[1] + 1)]
            iseq = [iheader, iseq]
            info = [ins, model_pos, length, iseq, [], []]
            i_info.append(info)
        seqs[id] = [gene, model, i_info]
    return seqs",def,seq_info,(,names,",",id2names,",",insertions,",",sequences,),:,seqs,=,{,},"# seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns]], ...]]",for,name,in,names,:,id,=,id2names,[,name,],gene,=,name,.,split,(,'fromHMM::',",",1,),[,0,],.,rsplit,get insertion information from header,get,insertion,information,from,header,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L64-L86,train,(,' ',",",1,),[,1,],model,=,name,.,split,(,'fromHMM::',",",1,),[,1,],.,split,(,'=',",",1,),[,1,,,,,,,,,,,,,,,,,,,,],.,split,(,),[,0,],i_gene_pos,=,insertions,[,id,],# coordinates of each insertion wrt gene,i_model_pos,=,name,.,split,(,'fromHMM::',",",1,),[,1,],.,split,(,'model-pos(ins-len)=',),[,1,],.,split,(,),[,0,],.,split,(,';',),# model overlap,i_info,=,[,],for,i,",",ins,in,enumerate,(,i_gene_pos,),:,model_pos,=,i_model_pos,[,i,],.,split,(,'-',),[,1,],.,split,(,'(',),[,0,],length,=,i_model_pos,[,i,],.,split,(,'(',),[,1,],.,split,(,')',),[,0,],iheader,=,'>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s',%,(,id,",",(,i,+,1,),",",(,i,+,1,),",",ins,[,0,],",",ins,[,1,],",",model_pos,),iseq,=,sequences,[,id,],[,1,],[,ins,[,0,],:,(,ins,[,1,],+,1,),],iseq,=,[,iheader,",",iseq,],info,=,[,ins,",",model_pos,",",length,",",iseq,",",[,],",",[,],],i_info,.,append,(,info,),seqs,[,id,],=,[,gene,",",model,",",i_info,],return,seqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,check_overlap,"def check_overlap(pos, ins, thresh):
    """"""
    make sure thresh % feature is contained within insertion
    """"""
    ins_pos = ins[0]
    ins_len = ins[2]
    ol = overlap(ins_pos, pos)
    feat_len = pos[1] - pos[0] + 1
#    print float(ol) / float(feat_len)
    if float(ol) / float(feat_len) >= thresh:
        return True
    return False",python,"def check_overlap(pos, ins, thresh):
    """"""
    make sure thresh % feature is contained within insertion
    """"""
    ins_pos = ins[0]
    ins_len = ins[2]
    ol = overlap(ins_pos, pos)
    feat_len = pos[1] - pos[0] + 1
#    print float(ol) / float(feat_len)
    if float(ol) / float(feat_len) >= thresh:
        return True
    return False",def,check_overlap,(,pos,",",ins,",",thresh,),:,ins_pos,=,ins,[,0,],ins_len,=,ins,[,2,],ol,=,overlap,(,ins_pos,",",pos,),feat_len,=,pos,[,1,],-,pos,[,0,],+,1,make sure thresh % feature is contained within insertion,make,sure,thresh,%,feature,is,contained,within,insertion,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L91-L102,train,#    print float(ol) / float(feat_len),if,float,(,ol,),/,float,(,feat_len,),>=,thresh,:,return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,max_insertion,"def max_insertion(seqs, gene, domain):
    """"""
    length of largest insertion
    """"""
    seqs = [i[2] for i in list(seqs.values()) if i[2] != [] and i[0] == gene and i[1] == domain]
    lengths = []
    for seq in seqs:
        for ins in seq:
            lengths.append(int(ins[2]))
    if lengths == []:
        return 100 
    return max(lengths)",python,"def max_insertion(seqs, gene, domain):
    """"""
    length of largest insertion
    """"""
    seqs = [i[2] for i in list(seqs.values()) if i[2] != [] and i[0] == gene and i[1] == domain]
    lengths = []
    for seq in seqs:
        for ins in seq:
            lengths.append(int(ins[2]))
    if lengths == []:
        return 100 
    return max(lengths)",def,max_insertion,(,seqs,",",gene,",",domain,),:,seqs,=,[,i,[,2,],for,i,in,list,(,seqs,.,values,(,),),if,i,[,2,],!=,[,],and,i,[,0,],==,gene,length of largest insertion,length,of,largest,insertion,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L305-L316,train,and,i,[,1,],==,domain,],lengths,=,[,],for,seq,in,seqs,:,for,ins,in,seq,:,lengths,.,append,(,int,(,ins,[,,,,,,,,,,,,,,,,,,,,2,],),),if,lengths,==,[,],:,return,100,return,max,(,lengths,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,model_length,"def model_length(gene, domain):
    """"""
    get length of model
    """"""
    if gene == '16S':
        domain2max = {'E_coli_K12': int(1538), 'bacteria': int(1689), 'archaea': int(1563), 'eukarya': int(2652)}
        return domain2max[domain]
    elif gene == '23S':
        domain2max = {'E_coli_K12': int(2903), 'bacteria': int(3146), 'archaea': int(3774), 'eukarya': int(9079)}
        return domain2max[domain]
    else:
        print(sys.stderr, '# length unknown for gene: %s, domain: %s' % (gene, domain))
        exit()",python,"def model_length(gene, domain):
    """"""
    get length of model
    """"""
    if gene == '16S':
        domain2max = {'E_coli_K12': int(1538), 'bacteria': int(1689), 'archaea': int(1563), 'eukarya': int(2652)}
        return domain2max[domain]
    elif gene == '23S':
        domain2max = {'E_coli_K12': int(2903), 'bacteria': int(3146), 'archaea': int(3774), 'eukarya': int(9079)}
        return domain2max[domain]
    else:
        print(sys.stderr, '# length unknown for gene: %s, domain: %s' % (gene, domain))
        exit()",def,model_length,(,gene,",",domain,),:,if,gene,==,'16S',:,domain2max,=,{,'E_coli_K12',:,int,(,1538,),",",'bacteria',:,int,(,1689,),",",'archaea',:,int,(,1563,),",",'eukarya',:,int,(,2652,),get length of model,get,length,of,model,,,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L318-L330,train,},return,domain2max,[,domain,],elif,gene,==,'23S',:,domain2max,=,{,'E_coli_K12',:,int,(,2903,),",",'bacteria',:,int,(,3146,),",",'archaea',:,,,,,,,,,,,,,,,,,,,,int,(,3774,),",",'eukarya',:,int,(,9079,),},return,domain2max,[,domain,],else,:,print,(,sys,.,stderr,",","'# length unknown for gene: %s, domain: %s'",%,(,gene,",",domain,),),exit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,setup_markers,"def setup_markers(seqs):
    """"""
    setup unique marker for every orf annotation
    - change size if necessary
    """"""
    family2marker = {} # family2marker[family] = [marker, size]
    markers = cycle(['^', 'p', '*', '+', 'x', 'd', '|', 'v', '>', '<', '8'])
    size = 60
    families = []
    for seq in list(seqs.values()):
        for insertion in seq[2]:
            for family in list(insertion[-1].values()):
                if family not in families:
                    families.append(family)
    for family in families:
        marker = next(markers) 
        if marker == '^':
            size = size * 0.5
        family2marker[family] = [marker, size]
    return family2marker",python,"def setup_markers(seqs):
    """"""
    setup unique marker for every orf annotation
    - change size if necessary
    """"""
    family2marker = {} # family2marker[family] = [marker, size]
    markers = cycle(['^', 'p', '*', '+', 'x', 'd', '|', 'v', '>', '<', '8'])
    size = 60
    families = []
    for seq in list(seqs.values()):
        for insertion in seq[2]:
            for family in list(insertion[-1].values()):
                if family not in families:
                    families.append(family)
    for family in families:
        marker = next(markers) 
        if marker == '^':
            size = size * 0.5
        family2marker[family] = [marker, size]
    return family2marker",def,setup_markers,(,seqs,),:,family2marker,=,{,},"# family2marker[family] = [marker, size]",markers,=,cycle,(,[,'^',",",'p',",",'*',",",'+',",",'x',",",'d',",",'|',",",'v',",",'>',",",'<',",",'8',],),size,=,60,families,"setup unique marker for every orf annotation
    - change size if necessary",setup,unique,marker,for,every,orf,annotation,-,change,size,if,necessary,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L332-L351,train,=,[,],for,seq,in,list,(,seqs,.,values,(,),),:,for,insertion,in,seq,[,2,],:,for,family,in,list,(,insertion,[,,,,,,,,,,,,,,,,,,,,-,1,],.,values,(,),),:,if,family,not,in,families,:,families,.,append,(,family,),for,family,in,families,:,marker,=,next,(,markers,),if,marker,==,'^',:,size,=,size,*,0.5,family2marker,[,family,],=,[,marker,",",size,],return,family2marker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rRNA_insertions.py,plot_by_gene_and_domain,"def plot_by_gene_and_domain(name, seqs, tax, id2name):
    """"""
    plot insertions for each gene and domain
    """"""
    for gene in set([seq[0] for seq in list(seqs.values())]):
        for domain in set([seq[1] for seq in list(seqs.values())]):
            plot_insertions(name, seqs, gene, domain, tax, id2name)",python,"def plot_by_gene_and_domain(name, seqs, tax, id2name):
    """"""
    plot insertions for each gene and domain
    """"""
    for gene in set([seq[0] for seq in list(seqs.values())]):
        for domain in set([seq[1] for seq in list(seqs.values())]):
            plot_insertions(name, seqs, gene, domain, tax, id2name)",def,plot_by_gene_and_domain,(,name,",",seqs,",",tax,",",id2name,),:,for,gene,in,set,(,[,seq,[,0,],for,seq,in,list,(,seqs,.,values,(,),),],),:,for,domain,in,set,(,[,seq,plot insertions for each gene and domain,plot,insertions,for,each,gene,and,domain,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rRNA_insertions.py#L551-L557,train,[,1,],for,seq,in,list,(,seqs,.,values,(,),),],),:,plot_insertions,(,name,",",seqs,",",gene,",",domain,",",tax,",",id2name,,,,,,,,,,,,,,,,,,,,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/neto.py,get_descriptions,"def get_descriptions(fastas):
    """"""
    get the description for each ORF 
    """"""
    id2desc = {}
    for fasta in fastas:
        for seq in parse_fasta(fasta):
            header = seq[0].split('>')[1].split(' ')
            id = header[0]
            if len(header) > 1:
                desc = ' '.join(header[1:])
            else:
                desc = 'n/a'
            length = float(len([i for i in seq[1].strip() if i != '*']))
            id2desc[id] = [fasta, desc, length]
    return id2desc",python,"def get_descriptions(fastas):
    """"""
    get the description for each ORF 
    """"""
    id2desc = {}
    for fasta in fastas:
        for seq in parse_fasta(fasta):
            header = seq[0].split('>')[1].split(' ')
            id = header[0]
            if len(header) > 1:
                desc = ' '.join(header[1:])
            else:
                desc = 'n/a'
            length = float(len([i for i in seq[1].strip() if i != '*']))
            id2desc[id] = [fasta, desc, length]
    return id2desc",def,get_descriptions,(,fastas,),:,id2desc,=,{,},for,fasta,in,fastas,:,for,seq,in,parse_fasta,(,fasta,),:,header,=,seq,[,0,],.,split,(,'>',),[,1,],.,split,(,' ',),id,get the description for each ORF,get,the,description,for,each,ORF,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/neto.py#L37-L52,train,=,header,[,0,],if,len,(,header,),>,1,:,desc,=,' ',.,join,(,header,[,1,:,],),else,:,desc,=,'n/a',,,,,,,,,,,,,,,,,,,,length,=,float,(,len,(,[,i,for,i,in,seq,[,1,],.,strip,(,),if,i,!=,'*',],),),id2desc,[,id,],=,[,fasta,",",desc,",",length,],return,id2desc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/neto.py,print_genome_matrix,"def print_genome_matrix(hits, fastas, id2desc, file_name):
    """"""
    optimize later? slow ...
    should combine with calculate_threshold module
    """"""
    out = open(file_name, 'w')
    fastas = sorted(fastas)
    print('## percent identity between genomes', file=out)
    print('# - \t %s' % ('\t'.join(fastas)), file=out)
    for fasta in fastas:
        line = [fasta]
        for other in fastas:
            if other == fasta:
                average = '-'
            else:
                average = numpy.average([hits[fasta][other][i][3] for i in hits[fasta][other]])
            line.append(str(average))
        print('\t'.join(line), file=out)
    print('', file=out)
    print('## percent of orfs that are orthologous between genomes', file=out)
    print('# - \t %s' % ('\t'.join(fastas)), file=out)
    for fasta in fastas:
        line = [fasta]
        for other in fastas:
            if other == fasta:
                percent = '-'
            else:
                orthologs = float(len(hits[fasta][other]))
                orfs = float(len([i for i in id2desc if id2desc[i][0] == fasta]))
                percent = float(orthologs / orfs) * 100
            line.append(str(percent))
        print('\t'.join(line), file=out)",python,"def print_genome_matrix(hits, fastas, id2desc, file_name):
    """"""
    optimize later? slow ...
    should combine with calculate_threshold module
    """"""
    out = open(file_name, 'w')
    fastas = sorted(fastas)
    print('## percent identity between genomes', file=out)
    print('# - \t %s' % ('\t'.join(fastas)), file=out)
    for fasta in fastas:
        line = [fasta]
        for other in fastas:
            if other == fasta:
                average = '-'
            else:
                average = numpy.average([hits[fasta][other][i][3] for i in hits[fasta][other]])
            line.append(str(average))
        print('\t'.join(line), file=out)
    print('', file=out)
    print('## percent of orfs that are orthologous between genomes', file=out)
    print('# - \t %s' % ('\t'.join(fastas)), file=out)
    for fasta in fastas:
        line = [fasta]
        for other in fastas:
            if other == fasta:
                percent = '-'
            else:
                orthologs = float(len(hits[fasta][other]))
                orfs = float(len([i for i in id2desc if id2desc[i][0] == fasta]))
                percent = float(orthologs / orfs) * 100
            line.append(str(percent))
        print('\t'.join(line), file=out)",def,print_genome_matrix,(,hits,",",fastas,",",id2desc,",",file_name,),:,out,=,open,(,file_name,",",'w',),fastas,=,sorted,(,fastas,),print,(,'## percent identity between genomes',",",file,=,out,),print,(,'# - \t %s',%,(,'\t',.,join,(,"optimize later? slow ...
    should combine with calculate_threshold module",optimize,later?,slow,...,should,combine,with,calculate_threshold,module,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/neto.py#L140-L171,train,fastas,),),",",file,=,out,),for,fasta,in,fastas,:,line,=,[,fasta,],for,other,in,fastas,:,if,other,==,fasta,:,average,=,,,,,,,,,,,,,,,,,,,,'-',else,:,average,=,numpy,.,average,(,[,hits,[,fasta,],[,other,],[,i,],[,3,],for,i,in,hits,[,fasta,],[,other,],],),line,.,append,(,str,(,average,),),print,(,'\t',.,join,(,line,),",",file,=,out,),print,(,'',",",file,=,out,),print,(,'## percent of orfs that are orthologous between genomes',",",file,=,out,),print,(,'# - \t %s',%,(,'\t',.,join,(,fastas,),),",",file,=,out,),for,fasta,in,fastas,:,line,=,[,fasta,],for,other,in,fastas,:,if,other,==,fasta,:,percent,=,'-',else,:,orthologs,=,float,(,len,(,hits,[,fasta,],[,other,],),),orfs,=,float,(,len,(,[,i,for,i,in,id2desc,if,id2desc,[,i,],[,0,],==,fasta,],),),percent,=,float,(,orthologs,/,orfs,),*,100,line,.,append,(,str,(,percent,),),print,(,'\t',.,join,(,line,),",",file,=,out,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/neto.py,self_compare,"def self_compare(fastas, id2desc, algorithm):
    """"""
    compare genome to self to get the best possible bit score for each ORF
    """"""
    for fasta in fastas:
        blast = open(search(fasta, fasta, method = algorithm, alignment = 'local'))
        for hit in best_blast(blast, 1):
            id, bit = hit[0].split()[0], float(hit[-1])
            id2desc[id].append(bit)
    return id2desc",python,"def self_compare(fastas, id2desc, algorithm):
    """"""
    compare genome to self to get the best possible bit score for each ORF
    """"""
    for fasta in fastas:
        blast = open(search(fasta, fasta, method = algorithm, alignment = 'local'))
        for hit in best_blast(blast, 1):
            id, bit = hit[0].split()[0], float(hit[-1])
            id2desc[id].append(bit)
    return id2desc",def,self_compare,(,fastas,",",id2desc,",",algorithm,),:,for,fasta,in,fastas,:,blast,=,open,(,search,(,fasta,",",fasta,",",method,=,algorithm,",",alignment,=,'local',),),for,hit,in,best_blast,(,blast,",",1,),compare genome to self to get the best possible bit score for each ORF,compare,genome,to,self,to,get,the,best,possible,bit,score,for,each,ORF,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/neto.py#L240-L249,train,:,id,",",bit,=,hit,[,0,],.,split,(,),[,0,],",",float,(,hit,[,-,1,],),id2desc,[,id,],.,,,,,,,,,,,,,,,,,,,,append,(,bit,),return,id2desc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/neto.py,calc_thresholds,"def calc_thresholds(rbh, file_name, thresholds = [False, False, False, False], stdevs = 2):
    """"""
    if thresholds are not specififed, calculate based on the distribution of normalized bit scores
    """"""
    calc_threshold = thresholds[-1]
    norm_threshold = {}
    for pair in itertools.permutations([i for i in rbh], 2):
        if pair[0] not in norm_threshold:
            norm_threshold[pair[0]] = {}
        norm_threshold[pair[0]][pair[1]] = {}
    out = open(file_name, 'w')
    print('#### summary of rbh comparisons\n', file=out)
    comparisons = []
    for genome in rbh:
        for compare in rbh[genome]:
            pair = ''.join(sorted([genome, compare]))
            if pair in comparisons:
                continue
            comparisons.append(pair)
            scores = {'percent identity': [], 'e-value': [], 'bit score': [], 'normalized bit score': [], 'alignment length fraction': []}
            print('### blast between %s and %s\n' % (genome, compare), file=out)
            for id in rbh[genome][compare]:
                pident, length_fraction, e, bit, norm_bit = rbh[genome][compare][id][3:]
                scores['percent identity'].append(pident)
                scores['alignment length fraction'].append(length_fraction)
                scores['e-value'].append(e)
                scores['bit score'].append(bit)
                scores['normalized bit score'].append(norm_bit)
            if calc_threshold is True:
                norms = scores['normalized bit score']
                average = numpy.average(norms) 
                std = numpy.std(norms)
                normal_thresh = average - (std * stdevs)
                print('## average normalized bit score: %s' % average, file=out)
                print('## standard deviation of normalized bit scores: %s' % std, file=out)
                print('## normalized bit score threshold set to: %s\n' % (normal_thresh), file=out)
                norm_threshold[genome][compare], norm_threshold[compare][genome] = normal_thresh, normal_thresh
            for score in scores:
                print('## %s' % (score), file=out)
                if len(scores[score]) > 0:
                    print('## average: %s' % numpy.average(scores[score]), file=out)
#                    hist = histogram(scores[score], [])
#                    for line in hist:
#                        print >> out, line
                print('', file=out)
    out.close()
    if calc_threshold is True:
        return thresholds[0:-1] + [norm_threshold]
    else:
        return thresholds",python,"def calc_thresholds(rbh, file_name, thresholds = [False, False, False, False], stdevs = 2):
    """"""
    if thresholds are not specififed, calculate based on the distribution of normalized bit scores
    """"""
    calc_threshold = thresholds[-1]
    norm_threshold = {}
    for pair in itertools.permutations([i for i in rbh], 2):
        if pair[0] not in norm_threshold:
            norm_threshold[pair[0]] = {}
        norm_threshold[pair[0]][pair[1]] = {}
    out = open(file_name, 'w')
    print('#### summary of rbh comparisons\n', file=out)
    comparisons = []
    for genome in rbh:
        for compare in rbh[genome]:
            pair = ''.join(sorted([genome, compare]))
            if pair in comparisons:
                continue
            comparisons.append(pair)
            scores = {'percent identity': [], 'e-value': [], 'bit score': [], 'normalized bit score': [], 'alignment length fraction': []}
            print('### blast between %s and %s\n' % (genome, compare), file=out)
            for id in rbh[genome][compare]:
                pident, length_fraction, e, bit, norm_bit = rbh[genome][compare][id][3:]
                scores['percent identity'].append(pident)
                scores['alignment length fraction'].append(length_fraction)
                scores['e-value'].append(e)
                scores['bit score'].append(bit)
                scores['normalized bit score'].append(norm_bit)
            if calc_threshold is True:
                norms = scores['normalized bit score']
                average = numpy.average(norms) 
                std = numpy.std(norms)
                normal_thresh = average - (std * stdevs)
                print('## average normalized bit score: %s' % average, file=out)
                print('## standard deviation of normalized bit scores: %s' % std, file=out)
                print('## normalized bit score threshold set to: %s\n' % (normal_thresh), file=out)
                norm_threshold[genome][compare], norm_threshold[compare][genome] = normal_thresh, normal_thresh
            for score in scores:
                print('## %s' % (score), file=out)
                if len(scores[score]) > 0:
                    print('## average: %s' % numpy.average(scores[score]), file=out)
#                    hist = histogram(scores[score], [])
#                    for line in hist:
#                        print >> out, line
                print('', file=out)
    out.close()
    if calc_threshold is True:
        return thresholds[0:-1] + [norm_threshold]
    else:
        return thresholds",def,calc_thresholds,(,rbh,",",file_name,",",thresholds,=,[,False,",",False,",",False,",",False,],",",stdevs,=,2,),:,calc_threshold,=,thresholds,[,-,1,],norm_threshold,=,{,},for,pair,in,itertools,.,permutations,(,[,"if thresholds are not specififed, calculate based on the distribution of normalized bit scores",if,thresholds,are,not,specififed,calculate,based,on,the,distribution,of,normalized,bit,scores,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/neto.py#L303-L352,train,i,for,i,in,rbh,],",",2,),:,if,pair,[,0,],not,in,norm_threshold,:,norm_threshold,[,pair,[,0,],],=,{,},norm_threshold,,,,,,,,,,,,,,,,,,,,[,pair,[,0,],],[,pair,[,1,],],=,{,},out,=,open,(,file_name,",",'w',),print,(,'#### summary of rbh comparisons\n',",",file,=,out,),comparisons,=,[,],for,genome,in,rbh,:,for,compare,in,rbh,[,genome,],:,pair,=,'',.,join,(,sorted,(,[,genome,",",compare,],),),if,pair,in,comparisons,:,continue,comparisons,.,append,(,pair,),scores,=,{,'percent identity',:,[,],",",'e-value',:,[,],",",'bit score',:,[,],",",'normalized bit score',:,[,],",",'alignment length fraction',:,[,],},print,(,'### blast between %s and %s\n',%,(,genome,",",compare,),",",file,=,out,),for,id,in,rbh,[,genome,],[,compare,],:,pident,",",length_fraction,",",e,",",bit,",",norm_bit,=,rbh,[,genome,],[,compare,],[,id,],[,3,:,],scores,[,'percent identity',],.,append,(,pident,),scores,[,'alignment length fraction',],.,append,(,length_fraction,),scores,[,'e-value',],.,append,(,e,),scores,[,'bit score',],.,append,(,bit,),scores,[,'normalized bit score',],.,append,(,norm_bit,),if,calc_threshold,is,True,:,norms,=,scores,[,'normalized bit score',],average,=,numpy,.,average,(,norms,),std,=,numpy,.,std,(,norms,),normal_thresh,=,average,-,(,std,*,stdevs,),print,(,'## average normalized bit score: %s',%,average,",",file,=,out,),print,(,'## standard deviation of normalized bit scores: %s',%,std,",",file,=,out,),print,(,'## normalized bit score threshold set to: %s\n',%,(,normal_thresh,),",",file,=,out,),norm_threshold,[,genome,],[,compare,],",",norm_threshold,[,compare,],[,genome,],=,normal_thresh,",",normal_thresh,for,score,in,scores,:,print,(,'## %s',%,(,score,),",",file,=,out,),if,len,(,scores,[,score,],),>,0,:,print,(,'## average: %s',%,numpy,.,average,(,scores,[,score,,,,,,,,,,,,],),",",file,=,out,),"#                    hist = histogram(scores[score], [])",#                    for line in hist:,"#                        print >> out, line",print,(,'',",",file,=,out,),out,.,close,(,),if,calc_threshold,is,True,:,return,thresholds,[,0,:,-,1,],+,[,norm_threshold,],else,:,return,thresholds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/neto.py,neto,"def neto(fastas, algorithm = 'usearch', e = 0.01, bit = 40, length = .65, norm_bit = False):
    """"""
    make and split a rbh network
    """"""
    thresholds = [e, bit, length, norm_bit]
    id2desc = get_descriptions(fastas)
            # get [fasta, description, length] for ORF id
    id2desc = self_compare(fastas, id2desc, algorithm)
            # get best possible bit score for each ORF 
            # (comparing with itself) [fasta, description, length, bestbit]
    hits = compare_genomes(fastas, id2desc, algorithm)
            # pair wise genome comparisons {genome: {id: [match_type = 'rbh' or 'fbh', scores]}}
    calc_thresholds(hits, file_name = 'fbh.scores.summary.txt')
    rbh_network(id2desc, hits, file_name = 'fbh.network.edges.txt')
    hits, rbh = find_rbh(hits, id2desc)
            # remove hits that are not reciprocal best blast hits
    thresholds = calc_thresholds(rbh, 'rbh.scores.summary.txt', thresholds)
            # print rbh score summary to rbh_score_summary.txt and
            # calculate normalized bit score cutoff for each pair of
            # genomes, if desired
    g = rbh_network(id2desc, rbh, file_name = 'rbh.network.edges.txt')
    filtered_g, filtered_rbh = rbh_network(id2desc, rbh, 'rbh.filtered.network.edges.txt', thresholds)
    calc_thresholds(filtered_rbh, file_name = 'rbh.filtered.scores.summary.txt')
    print_summary(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.nodes.txt')
    print_network_matrix(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.matrix.txt')
    print_genome_matrix(filtered_rbh, fastas, id2desc, file_name = 'rbh.filtered.network.genome_matrix.txt')
    split_g = split_network(filtered_g, id2desc, file_name = 'rbh.filtered.split.network.edges.txt')
    print_summary(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.nodes.txt')
    print_network_matrix(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.matrix.txt')
    return split_g",python,"def neto(fastas, algorithm = 'usearch', e = 0.01, bit = 40, length = .65, norm_bit = False):
    """"""
    make and split a rbh network
    """"""
    thresholds = [e, bit, length, norm_bit]
    id2desc = get_descriptions(fastas)
            # get [fasta, description, length] for ORF id
    id2desc = self_compare(fastas, id2desc, algorithm)
            # get best possible bit score for each ORF 
            # (comparing with itself) [fasta, description, length, bestbit]
    hits = compare_genomes(fastas, id2desc, algorithm)
            # pair wise genome comparisons {genome: {id: [match_type = 'rbh' or 'fbh', scores]}}
    calc_thresholds(hits, file_name = 'fbh.scores.summary.txt')
    rbh_network(id2desc, hits, file_name = 'fbh.network.edges.txt')
    hits, rbh = find_rbh(hits, id2desc)
            # remove hits that are not reciprocal best blast hits
    thresholds = calc_thresholds(rbh, 'rbh.scores.summary.txt', thresholds)
            # print rbh score summary to rbh_score_summary.txt and
            # calculate normalized bit score cutoff for each pair of
            # genomes, if desired
    g = rbh_network(id2desc, rbh, file_name = 'rbh.network.edges.txt')
    filtered_g, filtered_rbh = rbh_network(id2desc, rbh, 'rbh.filtered.network.edges.txt', thresholds)
    calc_thresholds(filtered_rbh, file_name = 'rbh.filtered.scores.summary.txt')
    print_summary(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.nodes.txt')
    print_network_matrix(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.matrix.txt')
    print_genome_matrix(filtered_rbh, fastas, id2desc, file_name = 'rbh.filtered.network.genome_matrix.txt')
    split_g = split_network(filtered_g, id2desc, file_name = 'rbh.filtered.split.network.edges.txt')
    print_summary(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.nodes.txt')
    print_network_matrix(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.matrix.txt')
    return split_g",def,neto,(,fastas,",",algorithm,=,'usearch',",",e,=,0.01,",",bit,=,40,",",length,=,.65,",",norm_bit,=,False,),:,thresholds,=,[,e,",",bit,",",length,",",norm_bit,],id2desc,=,get_descriptions,(,fastas,),make and split a rbh network,make,and,split,a,rbh,network,,,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/neto.py#L416-L445,train,"# get [fasta, description, length] for ORF id",id2desc,=,self_compare,(,fastas,",",id2desc,",",algorithm,),# get best possible bit score for each ORF ,"# (comparing with itself) [fasta, description, length, bestbit]",hits,=,compare_genomes,(,fastas,",",id2desc,",",algorithm,),"# pair wise genome comparisons {genome: {id: [match_type = 'rbh' or 'fbh', scores]}}",calc_thresholds,(,hits,",",file_name,=,,,,,,,,,,,,,,,,,,,,'fbh.scores.summary.txt',),rbh_network,(,id2desc,",",hits,",",file_name,=,'fbh.network.edges.txt',),hits,",",rbh,=,find_rbh,(,hits,",",id2desc,),# remove hits that are not reciprocal best blast hits,thresholds,=,calc_thresholds,(,rbh,",",'rbh.scores.summary.txt',",",thresholds,),# print rbh score summary to rbh_score_summary.txt and,# calculate normalized bit score cutoff for each pair of,"# genomes, if desired",g,=,rbh_network,(,id2desc,",",rbh,",",file_name,=,'rbh.network.edges.txt',),filtered_g,",",filtered_rbh,=,rbh_network,(,id2desc,",",rbh,",",'rbh.filtered.network.edges.txt',",",thresholds,),calc_thresholds,(,filtered_rbh,",",file_name,=,'rbh.filtered.scores.summary.txt',),print_summary,(,filtered_g,",",fastas,",",id2desc,",",file_name,=,'rbh.filtered.network.nodes.txt',),print_network_matrix,(,filtered_g,",",fastas,",",id2desc,",",file_name,=,'rbh.filtered.network.matrix.txt',),print_genome_matrix,(,filtered_rbh,",",fastas,",",id2desc,",",file_name,=,'rbh.filtered.network.genome_matrix.txt',),split_g,=,split_network,(,filtered_g,",",id2desc,",",file_name,=,'rbh.filtered.split.network.edges.txt',),print_summary,(,split_g,",",fastas,",",id2desc,",",file_name,=,'rbh.filtered.split.network.nodes.txt',),print_network_matrix,(,split_g,",",fastas,",",id2desc,",",file_name,=,'rbh.filtered.split.network.matrix.txt',),return,split_g,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/iso_metadata_parser.py,IsoParser._parse_raster_info,"def _parse_raster_info(self, prop=RASTER_INFO):
        """""" Collapses multiple dimensions into a single raster_info complex struct """"""

        raster_info = {}.fromkeys(_iso_definitions[prop], u'')

        # Ensure conversion of lists to newlines is in place
        raster_info['dimensions'] = get_default_for_complex_sub(
            prop=prop,
            subprop='dimensions',
            value=parse_property(self._xml_tree, None, self._data_map, '_ri_num_dims'),
            xpath=self._data_map['_ri_num_dims']
        )

        xpath_root = self._get_xroot_for(prop)
        xpath_map = self._data_structures[prop]

        for dimension in parse_complex_list(self._xml_tree, xpath_root, xpath_map, RASTER_DIMS):
            dimension_type = dimension['type'].lower()

            if dimension_type == 'vertical':
                raster_info['vertical_count'] = dimension['size']

            elif dimension_type == 'column':
                raster_info['column_count'] = dimension['size']
                raster_info['x_resolution'] = u' '.join(dimension[k] for k in ['value', 'units']).strip()

            elif dimension_type == 'row':
                raster_info['row_count'] = dimension['size']
                raster_info['y_resolution'] = u' '.join(dimension[k] for k in ['value', 'units']).strip()

        return raster_info if any(raster_info[k] for k in raster_info) else {}",python,"def _parse_raster_info(self, prop=RASTER_INFO):
        """""" Collapses multiple dimensions into a single raster_info complex struct """"""

        raster_info = {}.fromkeys(_iso_definitions[prop], u'')

        # Ensure conversion of lists to newlines is in place
        raster_info['dimensions'] = get_default_for_complex_sub(
            prop=prop,
            subprop='dimensions',
            value=parse_property(self._xml_tree, None, self._data_map, '_ri_num_dims'),
            xpath=self._data_map['_ri_num_dims']
        )

        xpath_root = self._get_xroot_for(prop)
        xpath_map = self._data_structures[prop]

        for dimension in parse_complex_list(self._xml_tree, xpath_root, xpath_map, RASTER_DIMS):
            dimension_type = dimension['type'].lower()

            if dimension_type == 'vertical':
                raster_info['vertical_count'] = dimension['size']

            elif dimension_type == 'column':
                raster_info['column_count'] = dimension['size']
                raster_info['x_resolution'] = u' '.join(dimension[k] for k in ['value', 'units']).strip()

            elif dimension_type == 'row':
                raster_info['row_count'] = dimension['size']
                raster_info['y_resolution'] = u' '.join(dimension[k] for k in ['value', 'units']).strip()

        return raster_info if any(raster_info[k] for k in raster_info) else {}",def,_parse_raster_info,(,self,",",prop,=,RASTER_INFO,),:,raster_info,=,{,},.,fromkeys,(,_iso_definitions,[,prop,],",",u'',),# Ensure conversion of lists to newlines is in place,raster_info,[,'dimensions',],=,get_default_for_complex_sub,(,prop,=,prop,",",subprop,=,'dimensions',",",value,=,parse_property,Collapses multiple dimensions into a single raster_info complex struct,Collapses,multiple,dimensions,into,a,single,raster_info,complex,struct,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/iso_metadata_parser.py#L472-L502,train,(,self,.,_xml_tree,",",None,",",self,.,_data_map,",",'_ri_num_dims',),",",xpath,=,self,.,_data_map,[,'_ri_num_dims',],),xpath_root,=,self,.,_get_xroot_for,(,prop,,,,,,,,,,,,,,,,,,,,),xpath_map,=,self,.,_data_structures,[,prop,],for,dimension,in,parse_complex_list,(,self,.,_xml_tree,",",xpath_root,",",xpath_map,",",RASTER_DIMS,),:,dimension_type,=,dimension,[,'type',],.,lower,(,),if,dimension_type,==,'vertical',:,raster_info,[,'vertical_count',],=,dimension,[,'size',],elif,dimension_type,==,'column',:,raster_info,[,'column_count',],=,dimension,[,'size',],raster_info,[,'x_resolution',],=,u' ',.,join,(,dimension,[,k,],for,k,in,[,'value',",",'units',],),.,strip,(,),elif,dimension_type,==,'row',:,raster_info,[,'row_count',],=,dimension,[,'size',],raster_info,[,'y_resolution',],=,u' ',.,join,(,dimension,[,k,],for,k,in,[,'value',",",'units',],),.,strip,(,),return,raster_info,if,any,(,raster_info,[,k,],for,k,in,raster_info,),else,{,},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/iso_metadata_parser.py,IsoParser._update_raster_info,"def _update_raster_info(self, **update_props):
        """""" Derives multiple dimensions from a single raster_info complex struct """"""

        tree_to_update = update_props['tree_to_update']
        prop = update_props['prop']
        values = update_props.pop('values')

        # Update number of dimensions at raster_info root (applies to all dimensions below)

        xroot, xpath = None, self._data_map['_ri_num_dims']
        raster_info = [update_property(tree_to_update, xroot, xpath, prop, values.get('dimensions', u''))]

        # Derive vertical, longitude, and latitude dimensions from raster_info

        xpath_root = self._get_xroot_for(prop)
        xpath_map = self._data_structures[prop]

        v_dimension = {}
        if values.get('vertical_count'):
            v_dimension = v_dimension.fromkeys(xpath_map, u'')
            v_dimension['type'] = 'vertical'
            v_dimension['size'] = values.get('vertical_count', u'')

        x_dimension = {}
        if values.get('column_count') or values.get('x_resolution'):
            x_dimension = x_dimension.fromkeys(xpath_map, u'')
            x_dimension['type'] = 'column'
            x_dimension['size'] = values.get('column_count', u'')
            x_dimension['value'] = values.get('x_resolution', u'')

        y_dimension = {}
        if values.get('row_count') or values.get('y_resolution'):
            y_dimension = y_dimension.fromkeys(xpath_map, u'')
            y_dimension['type'] = 'row'
            y_dimension['size'] = values.get('row_count', u'')
            y_dimension['value'] = values.get('y_resolution', u'')

        # Update derived dimensions as complex list, and append affected elements for return

        update_props['prop'] = RASTER_DIMS
        update_props['values'] = [v_dimension, x_dimension, y_dimension]

        raster_info += update_complex_list(xpath_root=xpath_root, xpath_map=xpath_map, **update_props)

        return raster_info",python,"def _update_raster_info(self, **update_props):
        """""" Derives multiple dimensions from a single raster_info complex struct """"""

        tree_to_update = update_props['tree_to_update']
        prop = update_props['prop']
        values = update_props.pop('values')

        # Update number of dimensions at raster_info root (applies to all dimensions below)

        xroot, xpath = None, self._data_map['_ri_num_dims']
        raster_info = [update_property(tree_to_update, xroot, xpath, prop, values.get('dimensions', u''))]

        # Derive vertical, longitude, and latitude dimensions from raster_info

        xpath_root = self._get_xroot_for(prop)
        xpath_map = self._data_structures[prop]

        v_dimension = {}
        if values.get('vertical_count'):
            v_dimension = v_dimension.fromkeys(xpath_map, u'')
            v_dimension['type'] = 'vertical'
            v_dimension['size'] = values.get('vertical_count', u'')

        x_dimension = {}
        if values.get('column_count') or values.get('x_resolution'):
            x_dimension = x_dimension.fromkeys(xpath_map, u'')
            x_dimension['type'] = 'column'
            x_dimension['size'] = values.get('column_count', u'')
            x_dimension['value'] = values.get('x_resolution', u'')

        y_dimension = {}
        if values.get('row_count') or values.get('y_resolution'):
            y_dimension = y_dimension.fromkeys(xpath_map, u'')
            y_dimension['type'] = 'row'
            y_dimension['size'] = values.get('row_count', u'')
            y_dimension['value'] = values.get('y_resolution', u'')

        # Update derived dimensions as complex list, and append affected elements for return

        update_props['prop'] = RASTER_DIMS
        update_props['values'] = [v_dimension, x_dimension, y_dimension]

        raster_info += update_complex_list(xpath_root=xpath_root, xpath_map=xpath_map, **update_props)

        return raster_info",def,_update_raster_info,(,self,",",*,*,update_props,),:,tree_to_update,=,update_props,[,'tree_to_update',],prop,=,update_props,[,'prop',],values,=,update_props,.,pop,(,'values',),# Update number of dimensions at raster_info root (applies to all dimensions below),xroot,",",xpath,=,None,",",self,.,_data_map,[,'_ri_num_dims',],Derives multiple dimensions from a single raster_info complex struct,Derives,multiple,dimensions,from,a,single,raster_info,complex,struct,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/iso_metadata_parser.py#L622-L666,train,raster_info,=,[,update_property,(,tree_to_update,",",xroot,",",xpath,",",prop,",",values,.,get,(,'dimensions',",",u'',),),],"# Derive vertical, longitude, and latitude dimensions from raster_info",xpath_root,=,self,.,_get_xroot_for,(,,,,,,,,,,,,,,,,,,,,prop,),xpath_map,=,self,.,_data_structures,[,prop,],v_dimension,=,{,},if,values,.,get,(,'vertical_count',),:,v_dimension,=,v_dimension,.,fromkeys,(,xpath_map,",",u'',),v_dimension,[,'type',],=,'vertical',v_dimension,[,'size',],=,values,.,get,(,'vertical_count',",",u'',),x_dimension,=,{,},if,values,.,get,(,'column_count',),or,values,.,get,(,'x_resolution',),:,x_dimension,=,x_dimension,.,fromkeys,(,xpath_map,",",u'',),x_dimension,[,'type',],=,'column',x_dimension,[,'size',],=,values,.,get,(,'column_count',",",u'',),x_dimension,[,'value',],=,values,.,get,(,'x_resolution',",",u'',),y_dimension,=,{,},if,values,.,get,(,'row_count',),or,values,.,get,(,'y_resolution',),:,y_dimension,=,y_dimension,.,fromkeys,(,xpath_map,",",u'',),y_dimension,[,'type',],=,'row',y_dimension,[,'size',],=,values,.,get,(,'row_count',",",u'',),y_dimension,[,'value',],=,values,.,get,(,'y_resolution',",",u'',),"# Update derived dimensions as complex list, and append affected elements for return",update_props,[,'prop',],=,RASTER_DIMS,update_props,[,'values',],=,[,v_dimension,",",x_dimension,",",y_dimension,],raster_info,+=,update_complex_list,(,xpath_root,=,xpath_root,",",xpath_map,=,xpath_map,",",*,*,update_props,),return,raster_info,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/iso_metadata_parser.py,IsoParser._trim_xpath,"def _trim_xpath(self, xpath, prop):
        """""" Removes primitive type tags from an XPATH """"""

        xroot = self._get_xroot_for(prop)

        if xroot is None and isinstance(xpath, string_types):
            xtags = xpath.split(XPATH_DELIM)

            if xtags[-1] in _iso_tag_primitives:
                xroot = XPATH_DELIM.join(xtags[:-1])

        return xroot",python,"def _trim_xpath(self, xpath, prop):
        """""" Removes primitive type tags from an XPATH """"""

        xroot = self._get_xroot_for(prop)

        if xroot is None and isinstance(xpath, string_types):
            xtags = xpath.split(XPATH_DELIM)

            if xtags[-1] in _iso_tag_primitives:
                xroot = XPATH_DELIM.join(xtags[:-1])

        return xroot",def,_trim_xpath,(,self,",",xpath,",",prop,),:,xroot,=,self,.,_get_xroot_for,(,prop,),if,xroot,is,None,and,isinstance,(,xpath,",",string_types,),:,xtags,=,xpath,.,split,(,XPATH_DELIM,),if,xtags,[,-,1,Removes primitive type tags from an XPATH,Removes,primitive,type,tags,from,an,XPATH,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/iso_metadata_parser.py#L691-L702,train,],in,_iso_tag_primitives,:,xroot,=,XPATH_DELIM,.,join,(,xtags,[,:,-,1,],),return,xroot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scottrice/pysteam,pysteam/shortcuts.py,shortcut_app_id,"def shortcut_app_id(shortcut):
  """"""
  Generates the app id for a given shortcut. Steam uses app ids as a unique
  identifier for games, but since shortcuts dont have a canonical serverside
  representation they need to be generated on the fly. The important part
  about this function is that it will generate the same app id as Steam does
  for a given shortcut
  """"""
  algorithm = Crc(width = 32, poly = 0x04C11DB7, reflect_in = True, xor_in = 0xffffffff, reflect_out = True, xor_out = 0xffffffff)
  crc_input = ''.join([shortcut.exe,shortcut.name])
  high_32 = algorithm.bit_by_bit(crc_input) | 0x80000000
  full_64 = (high_32 << 32) | 0x02000000
  return str(full_64)",python,"def shortcut_app_id(shortcut):
  """"""
  Generates the app id for a given shortcut. Steam uses app ids as a unique
  identifier for games, but since shortcuts dont have a canonical serverside
  representation they need to be generated on the fly. The important part
  about this function is that it will generate the same app id as Steam does
  for a given shortcut
  """"""
  algorithm = Crc(width = 32, poly = 0x04C11DB7, reflect_in = True, xor_in = 0xffffffff, reflect_out = True, xor_out = 0xffffffff)
  crc_input = ''.join([shortcut.exe,shortcut.name])
  high_32 = algorithm.bit_by_bit(crc_input) | 0x80000000
  full_64 = (high_32 << 32) | 0x02000000
  return str(full_64)",def,shortcut_app_id,(,shortcut,),:,algorithm,=,Crc,(,width,=,32,",",poly,=,0x04C11DB7,",",reflect_in,=,True,",",xor_in,=,0xffffffff,",",reflect_out,=,True,",",xor_out,=,0xffffffff,),crc_input,=,'',.,join,(,[,shortcut,.,"Generates the app id for a given shortcut. Steam uses app ids as a unique
  identifier for games, but since shortcuts dont have a canonical serverside
  representation they need to be generated on the fly. The important part
  about this function is that it will generate the same app id as Steam does
  for a given shortcut",Generates,the,app,id,for,a,given,shortcut,.,Steam,uses,app,ids,as,a,1eb2254b5235a053a953e596fa7602d0b110245d,https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/shortcuts.py#L9-L21,train,exe,",",shortcut,.,name,],),high_32,=,algorithm,.,bit_by_bit,(,crc_input,),|,0x80000000,full_64,=,(,high_32,<<,32,),|,0x02000000,return,str,(,full_64,unique,identifier,for,games,but,since,shortcuts,dont,have,a,canonical,serverside,representation,they,need,to,be,generated,on,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the,fly,.,The,important,part,about,this,function,is,that,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,it,will,generate,the,same,app,id,as,Steam,does,for,a,given,shortcut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/vcs.py,VCS._config,"def _config(self):
        """"""Execute git config.""""""
        cfg_wr = self.repo.config_writer()
        cfg_wr.add_section('user')
        cfg_wr.set_value('user', 'name', self.metadata.author)
        cfg_wr.set_value('user', 'email', self.metadata.email)
        cfg_wr.release()",python,"def _config(self):
        """"""Execute git config.""""""
        cfg_wr = self.repo.config_writer()
        cfg_wr.add_section('user')
        cfg_wr.set_value('user', 'name', self.metadata.author)
        cfg_wr.set_value('user', 'email', self.metadata.email)
        cfg_wr.release()",def,_config,(,self,),:,cfg_wr,=,self,.,repo,.,config_writer,(,),cfg_wr,.,add_section,(,'user',),cfg_wr,.,set_value,(,'user',",",'name',",",self,.,metadata,.,author,),cfg_wr,.,set_value,(,'user',",",'email',",",Execute git config.,Execute,git,config,.,,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/vcs.py#L35-L41,train,self,.,metadata,.,email,),cfg_wr,.,release,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mkouhei/bootstrap-py,bootstrap_py/vcs.py,VCS._remote_add,"def _remote_add(self):
        """"""Execute git remote add.""""""
        self.repo.create_remote(
            'origin',
            'git@github.com:{username}/{repo}.git'.format(
                username=self.metadata.username,
                repo=self.metadata.name))",python,"def _remote_add(self):
        """"""Execute git remote add.""""""
        self.repo.create_remote(
            'origin',
            'git@github.com:{username}/{repo}.git'.format(
                username=self.metadata.username,
                repo=self.metadata.name))",def,_remote_add,(,self,),:,self,.,repo,.,create_remote,(,'origin',",",'git@github.com:{username}/{repo}.git',.,format,(,username,=,self,.,metadata,.,username,",",repo,=,self,.,metadata,.,name,),),,,,,,,,,Execute git remote add.,Execute,git,remote,add,.,,,,,,,,,,,95d56ed98ef409fd9f019dc352fd1c3711533275,https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/vcs.py#L47-L53,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/basescript.py,BaseScript.start,"def start(self):
        '''
        Starts execution of the script
        '''
        # invoke the appropriate sub-command as requested from command-line
        try:
            self.args.func()
        except SystemExit as e:
            if e.code != 0:
                raise
        except KeyboardInterrupt:
            self.log.warning(""exited via keyboard interrupt"")
        except:
            self.log.exception(""exited start function"")
            # set exit code so we know it did not end successfully
            # TODO different exit codes based on signals ?
        finally:
            self._flush_metrics_q.put(None, block=True)
            self._flush_metrics_q.put(None, block=True, timeout=1)

        self.log.debug(""exited_successfully"")",python,"def start(self):
        '''
        Starts execution of the script
        '''
        # invoke the appropriate sub-command as requested from command-line
        try:
            self.args.func()
        except SystemExit as e:
            if e.code != 0:
                raise
        except KeyboardInterrupt:
            self.log.warning(""exited via keyboard interrupt"")
        except:
            self.log.exception(""exited start function"")
            # set exit code so we know it did not end successfully
            # TODO different exit codes based on signals ?
        finally:
            self._flush_metrics_q.put(None, block=True)
            self._flush_metrics_q.put(None, block=True, timeout=1)

        self.log.debug(""exited_successfully"")",def,start,(,self,),:,# invoke the appropriate sub-command as requested from command-line,try,:,self,.,args,.,func,(,),except,SystemExit,as,e,:,if,e,.,code,!=,0,:,raise,except,KeyboardInterrupt,:,self,.,log,.,warning,(,"""exited via keyboard interrupt""",),except,:,self,Starts execution of the script,Starts,execution,of,the,script,,,,,,,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/basescript.py#L67-L87,train,.,log,.,exception,(,"""exited start function""",),# set exit code so we know it did not end successfully,# TODO different exit codes based on signals ?,finally,:,self,.,_flush_metrics_q,.,put,(,None,",",block,=,True,),self,.,_flush_metrics_q,.,put,(,None,,,,,,,,,,,,,,,,,,,,",",block,=,True,",",timeout,=,1,),self,.,log,.,debug,(,"""exited_successfully""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deep-compute/basescript,basescript/basescript.py,BaseScript.define_baseargs,"def define_baseargs(self, parser):
        '''
        Define basic command-line arguments required by the script.
        @parser is a parser object created using the `argparse` module.
        returns: None
        '''
        parser.add_argument('--name', default=sys.argv[0],
            help='Name to identify this instance')
        parser.add_argument('--log-level', default=None,
            help='Logging level as picked from the logging module')
        parser.add_argument('--log-format', default=None,
            # TODO add more formats
            choices=(""json"", ""pretty"",),
            help=(""Force the format of the logs. By default, if the ""
                  ""command is from a terminal, print colorful logs. ""
                  ""Otherwise print json.""),
        )
        parser.add_argument('--log-file', default=None,
            help='Writes logs to log file if specified, default: %(default)s',
        )
        parser.add_argument('--quiet', default=False, action=""store_true"",
            help='if true, does not print logs to stderr, default: %(default)s',
        )
        parser.add_argument('--metric-grouping-interval', default=None, type=int,
            help='To group metrics based on time interval ex:10 i.e;(10 sec)',
        )
        parser.add_argument('--debug', default=False, action=""store_true"",
            help='To run the code in debug mode',
        )",python,"def define_baseargs(self, parser):
        '''
        Define basic command-line arguments required by the script.
        @parser is a parser object created using the `argparse` module.
        returns: None
        '''
        parser.add_argument('--name', default=sys.argv[0],
            help='Name to identify this instance')
        parser.add_argument('--log-level', default=None,
            help='Logging level as picked from the logging module')
        parser.add_argument('--log-format', default=None,
            # TODO add more formats
            choices=(""json"", ""pretty"",),
            help=(""Force the format of the logs. By default, if the ""
                  ""command is from a terminal, print colorful logs. ""
                  ""Otherwise print json.""),
        )
        parser.add_argument('--log-file', default=None,
            help='Writes logs to log file if specified, default: %(default)s',
        )
        parser.add_argument('--quiet', default=False, action=""store_true"",
            help='if true, does not print logs to stderr, default: %(default)s',
        )
        parser.add_argument('--metric-grouping-interval', default=None, type=int,
            help='To group metrics based on time interval ex:10 i.e;(10 sec)',
        )
        parser.add_argument('--debug', default=False, action=""store_true"",
            help='To run the code in debug mode',
        )",def,define_baseargs,(,self,",",parser,),:,parser,.,add_argument,(,'--name',",",default,=,sys,.,argv,[,0,],",",help,=,'Name to identify this instance',),parser,.,add_argument,(,'--log-level',",",default,=,None,",",help,=,'Logging level as picked from the logging module',),parser,.,"Define basic command-line arguments required by the script.
        @parser is a parser object created using the `argparse` module.
        returns: None",Define,basic,command,-,line,arguments,required,by,the,script,.,,,,,f7233963c5291530fcb2444a7f45b556e6407b90,https://github.com/deep-compute/basescript/blob/f7233963c5291530fcb2444a7f45b556e6407b90/basescript/basescript.py#L123-L151,train,add_argument,(,'--log-format',",",default,=,None,",",# TODO add more formats,choices,=,(,"""json""",",","""pretty""",",",),",",help,=,(,"""Force the format of the logs. By default, if the ""","""command is from a terminal, print colorful logs. ""","""Otherwise print json.""",),",",),parser,.,add_argument,,,,,,,,,,,,,,,,,,,,(,'--log-file',",",default,=,None,",",help,=,"'Writes logs to log file if specified, default: %(default)s'",",",),parser,.,add_argument,(,'--quiet',",",default,=,False,",",action,=,"""store_true""",",",help,=,"'if true, does not print logs to stderr, default: %(default)s'",",",),parser,.,add_argument,(,'--metric-grouping-interval',",",default,=,None,",",type,=,int,",",help,=,'To group metrics based on time interval ex:10 i.e;(10 sec)',",",),parser,.,add_argument,(,'--debug',",",default,=,False,",",action,=,"""store_true""",",",help,=,'To run the code in debug mode',",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
elbow-jason/Uno-deprecated,uno/parser/source_coder.py,SourceCoder.cleanup_payload,"def cleanup_payload(self, payload):
        """"""
        Basically, turns payload that looks like '   \\n  ' to ''. In the 
        calling function, if this function returns '' no object is added 
        for that payload.
        """"""
        p = payload.replace('\n', '')
        p = p.rstrip()
        p = p.lstrip()
        return p",python,"def cleanup_payload(self, payload):
        """"""
        Basically, turns payload that looks like '   \\n  ' to ''. In the 
        calling function, if this function returns '' no object is added 
        for that payload.
        """"""
        p = payload.replace('\n', '')
        p = p.rstrip()
        p = p.lstrip()
        return p",def,cleanup_payload,(,self,",",payload,),:,p,=,payload,.,replace,(,'\n',",",'',),p,=,p,.,rstrip,(,),p,=,p,.,lstrip,(,),return,p,,,,,,,,,,"Basically, turns payload that looks like '   \\n  ' to ''. In the 
        calling function, if this function returns '' no object is added 
        for that payload.",Basically,turns,payload,that,looks,like,\\,n,to,.,In,the,calling,function,if,4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4,https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/parser/source_coder.py#L73-L82,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,this,function,returns,no,object,is,added,for,that,payload,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,get_default_for,"def get_default_for(prop, value):
    """""" Ensures complex property types have the correct default values """"""

    prop = prop.strip('_')     # Handle alternate props (leading underscores)
    val = reduce_value(value)  # Filtering of value happens here

    if prop in _COMPLEX_LISTS:
        return wrap_value(val)
    elif prop in _COMPLEX_STRUCTS:
        return val or {}
    else:
        return u'' if val is None else val",python,"def get_default_for(prop, value):
    """""" Ensures complex property types have the correct default values """"""

    prop = prop.strip('_')     # Handle alternate props (leading underscores)
    val = reduce_value(value)  # Filtering of value happens here

    if prop in _COMPLEX_LISTS:
        return wrap_value(val)
    elif prop in _COMPLEX_STRUCTS:
        return val or {}
    else:
        return u'' if val is None else val",def,get_default_for,(,prop,",",value,),:,prop,=,prop,.,strip,(,'_',),# Handle alternate props (leading underscores),val,=,reduce_value,(,value,),# Filtering of value happens here,if,prop,in,_COMPLEX_LISTS,:,return,wrap_value,(,val,),elif,prop,in,_COMPLEX_STRUCTS,:,return,val,or,{,Ensures complex property types have the correct default values,Ensures,complex,property,types,have,the,correct,default,values,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L223-L234,train,},else,:,return,u'',if,val,is,None,else,val,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,update_property,"def update_property(tree_to_update, xpath_root, xpaths, prop, values, supported=None):
    """"""
    Either update the tree the default way, or call the custom updater

    Default Way: Existing values in the tree are overwritten. If xpaths contains a single path,
    then each value is written to the tree at that path. If xpaths contains a list of xpaths,
    then the values corresponding to each xpath index are written to their respective locations.
    In either case, empty values are ignored.

    :param tree_to_update: the XML tree compatible with element_utils to be updated
    :param xpath_root: the XPATH location shared by all the xpaths passed in
    :param xpaths: a string or a list of strings representing the XPATH location(s) to which to write values
    :param prop: the name of the property of the parser containing the value(s) with which to update the tree
    :param values: a single value, or a list of values to write to the specified XPATHs

    :see: ParserProperty for more on custom updaters

    :return: a list of all elements updated by this operation
    """"""

    if supported and prop.startswith('_') and prop.strip('_') in supported:
        values = u''  # Remove alternate elements: write values only to primary location
    else:
        values = get_default_for(prop, values)  # Enforce defaults as required per property

    if not xpaths:
        return []
    elif not isinstance(xpaths, ParserProperty):
        return _update_property(tree_to_update, xpath_root, xpaths, values)
    else:
        # Call ParserProperty.set_prop without xpath_root (managed internally)
        return xpaths.set_prop(tree_to_update=tree_to_update, prop=prop, values=values)",python,"def update_property(tree_to_update, xpath_root, xpaths, prop, values, supported=None):
    """"""
    Either update the tree the default way, or call the custom updater

    Default Way: Existing values in the tree are overwritten. If xpaths contains a single path,
    then each value is written to the tree at that path. If xpaths contains a list of xpaths,
    then the values corresponding to each xpath index are written to their respective locations.
    In either case, empty values are ignored.

    :param tree_to_update: the XML tree compatible with element_utils to be updated
    :param xpath_root: the XPATH location shared by all the xpaths passed in
    :param xpaths: a string or a list of strings representing the XPATH location(s) to which to write values
    :param prop: the name of the property of the parser containing the value(s) with which to update the tree
    :param values: a single value, or a list of values to write to the specified XPATHs

    :see: ParserProperty for more on custom updaters

    :return: a list of all elements updated by this operation
    """"""

    if supported and prop.startswith('_') and prop.strip('_') in supported:
        values = u''  # Remove alternate elements: write values only to primary location
    else:
        values = get_default_for(prop, values)  # Enforce defaults as required per property

    if not xpaths:
        return []
    elif not isinstance(xpaths, ParserProperty):
        return _update_property(tree_to_update, xpath_root, xpaths, values)
    else:
        # Call ParserProperty.set_prop without xpath_root (managed internally)
        return xpaths.set_prop(tree_to_update=tree_to_update, prop=prop, values=values)",def,update_property,(,tree_to_update,",",xpath_root,",",xpaths,",",prop,",",values,",",supported,=,None,),:,if,supported,and,prop,.,startswith,(,'_',),and,prop,.,strip,(,'_',),in,supported,:,values,=,u'',# Remove alternate elements: write values only to primary location,else,:,"Either update the tree the default way, or call the custom updater

    Default Way: Existing values in the tree are overwritten. If xpaths contains a single path,
    then each value is written to the tree at that path. If xpaths contains a list of xpaths,
    then the values corresponding to each xpath index are written to their respective locations.
    In either case, empty values are ignored.

    :param tree_to_update: the XML tree compatible with element_utils to be updated
    :param xpath_root: the XPATH location shared by all the xpaths passed in
    :param xpaths: a string or a list of strings representing the XPATH location(s) to which to write values
    :param prop: the name of the property of the parser containing the value(s) with which to update the tree
    :param values: a single value, or a list of values to write to the specified XPATHs

    :see: ParserProperty for more on custom updaters

    :return: a list of all elements updated by this operation",Either,update,the,tree,the,default,way,or,call,the,custom,updater,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L392-L423,train,values,=,get_default_for,(,prop,",",values,),# Enforce defaults as required per property,if,not,xpaths,:,return,[,],elif,not,isinstance,(,xpaths,",",ParserProperty,),:,return,_update_property,(,tree_to_update,",",,,,,,,,,,,,,,,,,,,,xpath_root,",",xpaths,",",values,),else,:,# Call ParserProperty.set_prop without xpath_root (managed internally),return,xpaths,.,set_prop,(,tree_to_update,=,tree_to_update,",",prop,=,prop,",",values,=,values,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,_update_property,"def _update_property(tree_to_update, xpath_root, xpaths, values):
    """"""
    Default update operation for a single parser property. If xpaths contains one xpath,
    then one element per value will be inserted at that location in the tree_to_update;
    otherwise, the number of values must match the number of xpaths.
    """"""

    # Inner function to update a specific XPATH with the values provided

    def update_element(elem, idx, root, path, vals):
        """""" Internal helper function to encapsulate single item update """"""

        has_root = bool(root and len(path) > len(root) and path.startswith(root))
        path, attr = get_xpath_tuple(path)  # 'path/@attr' to ('path', 'attr')

        if attr:
            removed = [get_element(elem, path)]
            remove_element_attributes(removed[0], attr)
        elif not has_root:
            removed = wrap_value(remove_element(elem, path))
        else:
            path = get_xpath_branch(root, path)
            removed = [] if idx != 0 else [remove_element(e, path, True) for e in get_elements(elem, root)]

        if not vals:
            return removed

        items = []

        for i, val in enumerate(wrap_value(vals)):
            elem_to_update = elem

            if has_root:
                elem_to_update = insert_element(elem, (i + idx), root)

            val = val.decode('utf-8') if not isinstance(val, string_types) else val
            if not attr:
                items.append(insert_element(elem_to_update, i, path, val))
            elif path:
                items.append(insert_element(elem_to_update, i, path, **{attr: val}))
            else:
                set_element_attributes(elem_to_update, **{attr: val})
                items.append(elem_to_update)

        return items

    # Code to update each of the XPATHs with each of the values

    xpaths = reduce_value(xpaths)
    values = filter_empty(values)

    if isinstance(xpaths, string_types):
        return update_element(tree_to_update, 0, xpath_root, xpaths, values)
    else:
        each = []

        for index, xpath in enumerate(xpaths):
            value = values[index] if values else None
            each.extend(update_element(tree_to_update, index, xpath_root, xpath, value))

        return each",python,"def _update_property(tree_to_update, xpath_root, xpaths, values):
    """"""
    Default update operation for a single parser property. If xpaths contains one xpath,
    then one element per value will be inserted at that location in the tree_to_update;
    otherwise, the number of values must match the number of xpaths.
    """"""

    # Inner function to update a specific XPATH with the values provided

    def update_element(elem, idx, root, path, vals):
        """""" Internal helper function to encapsulate single item update """"""

        has_root = bool(root and len(path) > len(root) and path.startswith(root))
        path, attr = get_xpath_tuple(path)  # 'path/@attr' to ('path', 'attr')

        if attr:
            removed = [get_element(elem, path)]
            remove_element_attributes(removed[0], attr)
        elif not has_root:
            removed = wrap_value(remove_element(elem, path))
        else:
            path = get_xpath_branch(root, path)
            removed = [] if idx != 0 else [remove_element(e, path, True) for e in get_elements(elem, root)]

        if not vals:
            return removed

        items = []

        for i, val in enumerate(wrap_value(vals)):
            elem_to_update = elem

            if has_root:
                elem_to_update = insert_element(elem, (i + idx), root)

            val = val.decode('utf-8') if not isinstance(val, string_types) else val
            if not attr:
                items.append(insert_element(elem_to_update, i, path, val))
            elif path:
                items.append(insert_element(elem_to_update, i, path, **{attr: val}))
            else:
                set_element_attributes(elem_to_update, **{attr: val})
                items.append(elem_to_update)

        return items

    # Code to update each of the XPATHs with each of the values

    xpaths = reduce_value(xpaths)
    values = filter_empty(values)

    if isinstance(xpaths, string_types):
        return update_element(tree_to_update, 0, xpath_root, xpaths, values)
    else:
        each = []

        for index, xpath in enumerate(xpaths):
            value = values[index] if values else None
            each.extend(update_element(tree_to_update, index, xpath_root, xpath, value))

        return each",def,_update_property,(,tree_to_update,",",xpath_root,",",xpaths,",",values,),:,# Inner function to update a specific XPATH with the values provided,def,update_element,(,elem,",",idx,",",root,",",path,",",vals,),:,""""""" Internal helper function to encapsulate single item update """"""",has_root,=,bool,(,root,and,len,(,path,),>,len,(,root,),"Default update operation for a single parser property. If xpaths contains one xpath,
    then one element per value will be inserted at that location in the tree_to_update;
    otherwise, the number of values must match the number of xpaths.",Default,update,operation,for,a,single,parser,property,.,If,xpaths,contains,one,xpath,then,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L426-L486,train,and,path,.,startswith,(,root,),),path,",",attr,=,get_xpath_tuple,(,path,),"# 'path/@attr' to ('path', 'attr')",if,attr,:,removed,=,[,get_element,(,elem,",",path,),],one,element,per,value,will,be,inserted,at,that,location,in,the,tree_to_update,;,otherwise,the,number,of,values,remove_element_attributes,(,removed,[,0,],",",attr,),elif,not,has_root,:,removed,=,wrap_value,(,remove_element,(,elem,",",path,),),else,:,path,=,get_xpath_branch,(,root,",",path,),removed,=,[,],if,idx,!=,0,else,[,remove_element,(,e,",",path,",",True,),for,e,in,get_elements,(,elem,",",root,),],if,not,vals,:,return,removed,items,=,[,],for,i,",",val,in,enumerate,(,wrap_value,(,vals,),),:,elem_to_update,=,elem,if,has_root,:,elem_to_update,=,insert_element,(,elem,",",(,i,+,idx,),",",root,),val,=,val,.,decode,(,'utf-8',),if,not,isinstance,(,val,",",string_types,),else,val,if,not,attr,:,items,.,append,(,insert_element,(,elem_to_update,",",i,",",path,",",val,),),elif,path,:,items,.,append,(,insert_element,(,elem_to_update,",",i,",",path,",",*,*,{,attr,:,val,},),),else,:,set_element_attributes,(,elem_to_update,",",*,*,{,attr,:,val,},),items,.,append,(,elem_to_update,),return,items,# Code to update each of the XPATHs with each of the values,xpaths,=,reduce_value,(,xpaths,),values,=,filter_empty,(,values,),if,isinstance,(,xpaths,",",string_types,),:,return,update_element,(,tree_to_update,",",0,",",xpath_root,",",xpaths,",",values,),else,:,each,=,[,],for,index,",",xpath,in,enumerate,(,xpaths,),:,value,=,values,[,index,],if,values,else,None,each,.,extend,(,update_element,(,tree_to_update,",",index,",",xpath_root,",",xpath,",",value,),),return,each,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,must,match,the,number,of,xpaths,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,validate_complex,"def validate_complex(prop, value, xpath_map=None):
    """""" Default validation for single complex data structure """"""

    if value is not None:
        validate_type(prop, value, dict)

        if prop in _complex_definitions:
            complex_keys = _complex_definitions[prop]
        else:
            complex_keys = {} if xpath_map is None else xpath_map

        for complex_prop, complex_val in iteritems(value):
            complex_key = '.'.join((prop, complex_prop))

            if complex_prop not in complex_keys:
                _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))

            validate_type(complex_key, complex_val, (string_types, list))",python,"def validate_complex(prop, value, xpath_map=None):
    """""" Default validation for single complex data structure """"""

    if value is not None:
        validate_type(prop, value, dict)

        if prop in _complex_definitions:
            complex_keys = _complex_definitions[prop]
        else:
            complex_keys = {} if xpath_map is None else xpath_map

        for complex_prop, complex_val in iteritems(value):
            complex_key = '.'.join((prop, complex_prop))

            if complex_prop not in complex_keys:
                _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))

            validate_type(complex_key, complex_val, (string_types, list))",def,validate_complex,(,prop,",",value,",",xpath_map,=,None,),:,if,value,is,not,None,:,validate_type,(,prop,",",value,",",dict,),if,prop,in,_complex_definitions,:,complex_keys,=,_complex_definitions,[,prop,],else,:,complex_keys,=,{,},Default validation for single complex data structure,Default,validation,for,single,complex,data,structure,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L572-L589,train,if,xpath_map,is,None,else,xpath_map,for,complex_prop,",",complex_val,in,iteritems,(,value,),:,complex_key,=,'.',.,join,(,(,prop,",",complex_prop,),),if,complex_prop,,,,,,,,,,,,,,,,,,,,not,in,complex_keys,:,_validation_error,(,prop,",",None,",",value,",",(,'keys: {0}',.,format,(,"','",.,join,(,complex_keys,),),),),validate_type,(,complex_key,",",complex_val,",",(,string_types,",",list,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,validate_complex_list,"def validate_complex_list(prop, value, xpath_map=None):
    """""" Default validation for Attribute Details data structure """"""

    if value is not None:
        validate_type(prop, value, (dict, list))

        if prop in _complex_definitions:
            complex_keys = _complex_definitions[prop]
        else:
            complex_keys = {} if xpath_map is None else xpath_map

        for idx, complex_struct in enumerate(wrap_value(value)):
            cs_idx = prop + '[' + str(idx) + ']'
            validate_type(cs_idx, complex_struct, dict)

            for cs_prop, cs_val in iteritems(complex_struct):
                cs_key = '.'.join((cs_idx, cs_prop))

                if cs_prop not in complex_keys:
                    _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))

                if not isinstance(cs_val, list):
                    validate_type(cs_key, cs_val, (string_types, list))
                else:
                    for list_idx, list_val in enumerate(cs_val):
                        list_prop = cs_key + '[' + str(list_idx) + ']'
                        validate_type(list_prop, list_val, string_types)",python,"def validate_complex_list(prop, value, xpath_map=None):
    """""" Default validation for Attribute Details data structure """"""

    if value is not None:
        validate_type(prop, value, (dict, list))

        if prop in _complex_definitions:
            complex_keys = _complex_definitions[prop]
        else:
            complex_keys = {} if xpath_map is None else xpath_map

        for idx, complex_struct in enumerate(wrap_value(value)):
            cs_idx = prop + '[' + str(idx) + ']'
            validate_type(cs_idx, complex_struct, dict)

            for cs_prop, cs_val in iteritems(complex_struct):
                cs_key = '.'.join((cs_idx, cs_prop))

                if cs_prop not in complex_keys:
                    _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))

                if not isinstance(cs_val, list):
                    validate_type(cs_key, cs_val, (string_types, list))
                else:
                    for list_idx, list_val in enumerate(cs_val):
                        list_prop = cs_key + '[' + str(list_idx) + ']'
                        validate_type(list_prop, list_val, string_types)",def,validate_complex_list,(,prop,",",value,",",xpath_map,=,None,),:,if,value,is,not,None,:,validate_type,(,prop,",",value,",",(,dict,",",list,),),if,prop,in,_complex_definitions,:,complex_keys,=,_complex_definitions,[,prop,],else,:,Default validation for Attribute Details data structure,Default,validation,for,Attribute,Details,data,structure,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L592-L618,train,complex_keys,=,{,},if,xpath_map,is,None,else,xpath_map,for,idx,",",complex_struct,in,enumerate,(,wrap_value,(,value,),),:,cs_idx,=,prop,+,'[',+,str,,,,,,,,,,,,,,,,,,,,(,idx,),+,']',validate_type,(,cs_idx,",",complex_struct,",",dict,),for,cs_prop,",",cs_val,in,iteritems,(,complex_struct,),:,cs_key,=,'.',.,join,(,(,cs_idx,",",cs_prop,),),if,cs_prop,not,in,complex_keys,:,_validation_error,(,prop,",",None,",",value,",",(,'keys: {0}',.,format,(,"','",.,join,(,complex_keys,),),),),if,not,isinstance,(,cs_val,",",list,),:,validate_type,(,cs_key,",",cs_val,",",(,string_types,",",list,),),else,:,for,list_idx,",",list_val,in,enumerate,(,cs_val,),:,list_prop,=,cs_key,+,'[',+,str,(,list_idx,),+,']',validate_type,(,list_prop,",",list_val,",",string_types,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,validate_dates,"def validate_dates(prop, value, xpath_map=None):
    """""" Default validation for Date Types data structure """"""

    if value is not None:
        validate_type(prop, value, dict)

        date_keys = set(value)

        if date_keys:
            if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys:
                if prop in _complex_definitions:
                    complex_keys = _complex_definitions[prop]
                else:
                    complex_keys = _complex_definitions[DATES] if xpath_map is None else xpath_map

                _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))

            date_type = value[DATE_TYPE]

            if date_type not in DATE_TYPES:
                _validation_error('dates.type', None, date_type, DATE_TYPES)

            date_vals = value[DATE_VALUES]

            validate_type('dates.values', date_vals, list)

            dates_len = len(date_vals)

            if date_type == DATE_TYPE_MISSING and dates_len != 0:
                _validation_error('len(dates.values)', None, dates_len, 0)

            if date_type == DATE_TYPE_SINGLE and dates_len != 1:
                _validation_error('len(dates.values)', None, dates_len, 1)

            if date_type == DATE_TYPE_RANGE and dates_len != 2:
                _validation_error('len(dates.values)', None, dates_len, 2)

            if date_type == DATE_TYPE_MULTIPLE and dates_len < 2:
                _validation_error('len(dates.values)', None, dates_len, 'at least two')

            for idx, date in enumerate(date_vals):
                date_key = 'dates.value[' + str(idx) + ']'
                validate_type(date_key, date, string_types)",python,"def validate_dates(prop, value, xpath_map=None):
    """""" Default validation for Date Types data structure """"""

    if value is not None:
        validate_type(prop, value, dict)

        date_keys = set(value)

        if date_keys:
            if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys:
                if prop in _complex_definitions:
                    complex_keys = _complex_definitions[prop]
                else:
                    complex_keys = _complex_definitions[DATES] if xpath_map is None else xpath_map

                _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))

            date_type = value[DATE_TYPE]

            if date_type not in DATE_TYPES:
                _validation_error('dates.type', None, date_type, DATE_TYPES)

            date_vals = value[DATE_VALUES]

            validate_type('dates.values', date_vals, list)

            dates_len = len(date_vals)

            if date_type == DATE_TYPE_MISSING and dates_len != 0:
                _validation_error('len(dates.values)', None, dates_len, 0)

            if date_type == DATE_TYPE_SINGLE and dates_len != 1:
                _validation_error('len(dates.values)', None, dates_len, 1)

            if date_type == DATE_TYPE_RANGE and dates_len != 2:
                _validation_error('len(dates.values)', None, dates_len, 2)

            if date_type == DATE_TYPE_MULTIPLE and dates_len < 2:
                _validation_error('len(dates.values)', None, dates_len, 'at least two')

            for idx, date in enumerate(date_vals):
                date_key = 'dates.value[' + str(idx) + ']'
                validate_type(date_key, date, string_types)",def,validate_dates,(,prop,",",value,",",xpath_map,=,None,),:,if,value,is,not,None,:,validate_type,(,prop,",",value,",",dict,),date_keys,=,set,(,value,),if,date_keys,:,if,DATE_TYPE,not,in,date_keys,or,DATE_VALUES,not,Default validation for Date Types data structure,Default,validation,for,Date,Types,data,structure,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L621-L663,train,in,date_keys,:,if,prop,in,_complex_definitions,:,complex_keys,=,_complex_definitions,[,prop,],else,:,complex_keys,=,_complex_definitions,[,DATES,],if,xpath_map,is,None,else,xpath_map,_validation_error,(,,,,,,,,,,,,,,,,,,,,prop,",",None,",",value,",",(,'keys: {0}',.,format,(,"','",.,join,(,complex_keys,),),),),date_type,=,value,[,DATE_TYPE,],if,date_type,not,in,DATE_TYPES,:,_validation_error,(,'dates.type',",",None,",",date_type,",",DATE_TYPES,),date_vals,=,value,[,DATE_VALUES,],validate_type,(,'dates.values',",",date_vals,",",list,),dates_len,=,len,(,date_vals,),if,date_type,==,DATE_TYPE_MISSING,and,dates_len,!=,0,:,_validation_error,(,'len(dates.values)',",",None,",",dates_len,",",0,),if,date_type,==,DATE_TYPE_SINGLE,and,dates_len,!=,1,:,_validation_error,(,'len(dates.values)',",",None,",",dates_len,",",1,),if,date_type,==,DATE_TYPE_RANGE,and,dates_len,!=,2,:,_validation_error,(,'len(dates.values)',",",None,",",dates_len,",",2,),if,date_type,==,DATE_TYPE_MULTIPLE,and,dates_len,<,2,:,_validation_error,(,'len(dates.values)',",",None,",",dates_len,",",'at least two',),for,idx,",",date,in,enumerate,(,date_vals,),:,date_key,=,'dates.value[',+,str,(,idx,),+,']',validate_type,(,date_key,",",date,",",string_types,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,validate_process_steps,"def validate_process_steps(prop, value):
    """""" Default validation for Process Steps data structure """"""

    if value is not None:
        validate_type(prop, value, (dict, list))

        procstep_keys = set(_complex_definitions[prop])

        for idx, procstep in enumerate(wrap_value(value)):
            ps_idx = prop + '[' + str(idx) + ']'
            validate_type(ps_idx, procstep, dict)

            for ps_prop, ps_val in iteritems(procstep):
                ps_key = '.'.join((ps_idx, ps_prop))

                if ps_prop not in procstep_keys:
                    _validation_error(prop, None, value, ('keys: {0}'.format(','.join(procstep_keys))))

                if ps_prop != 'sources':
                    validate_type(ps_key, ps_val, string_types)
                else:
                    validate_type(ps_key, ps_val, (string_types, list))

                    for src_idx, src_val in enumerate(wrap_value(ps_val)):
                        src_key = ps_key + '[' + str(src_idx) + ']'
                        validate_type(src_key, src_val, string_types)",python,"def validate_process_steps(prop, value):
    """""" Default validation for Process Steps data structure """"""

    if value is not None:
        validate_type(prop, value, (dict, list))

        procstep_keys = set(_complex_definitions[prop])

        for idx, procstep in enumerate(wrap_value(value)):
            ps_idx = prop + '[' + str(idx) + ']'
            validate_type(ps_idx, procstep, dict)

            for ps_prop, ps_val in iteritems(procstep):
                ps_key = '.'.join((ps_idx, ps_prop))

                if ps_prop not in procstep_keys:
                    _validation_error(prop, None, value, ('keys: {0}'.format(','.join(procstep_keys))))

                if ps_prop != 'sources':
                    validate_type(ps_key, ps_val, string_types)
                else:
                    validate_type(ps_key, ps_val, (string_types, list))

                    for src_idx, src_val in enumerate(wrap_value(ps_val)):
                        src_key = ps_key + '[' + str(src_idx) + ']'
                        validate_type(src_key, src_val, string_types)",def,validate_process_steps,(,prop,",",value,),:,if,value,is,not,None,:,validate_type,(,prop,",",value,",",(,dict,",",list,),),procstep_keys,=,set,(,_complex_definitions,[,prop,],),for,idx,",",procstep,in,enumerate,(,wrap_value,Default validation for Process Steps data structure,Default,validation,for,Process,Steps,data,structure,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L666-L691,train,(,value,),),:,ps_idx,=,prop,+,'[',+,str,(,idx,),+,']',validate_type,(,ps_idx,",",procstep,",",dict,),for,ps_prop,",",ps_val,in,,,,,,,,,,,,,,,,,,,,iteritems,(,procstep,),:,ps_key,=,'.',.,join,(,(,ps_idx,",",ps_prop,),),if,ps_prop,not,in,procstep_keys,:,_validation_error,(,prop,",",None,",",value,",",(,'keys: {0}',.,format,(,"','",.,join,(,procstep_keys,),),),),if,ps_prop,!=,'sources',:,validate_type,(,ps_key,",",ps_val,",",string_types,),else,:,validate_type,(,ps_key,",",ps_val,",",(,string_types,",",list,),),for,src_idx,",",src_val,in,enumerate,(,wrap_value,(,ps_val,),),:,src_key,=,ps_key,+,'[',+,str,(,src_idx,),+,']',validate_type,(,src_key,",",src_val,",",string_types,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,validate_type,"def validate_type(prop, value, expected):
    """""" Default validation for all types """"""

    # Validate on expected type(s), but ignore None: defaults handled elsewhere
    if value is not None and not isinstance(value, expected):
        _validation_error(prop, type(value).__name__, None, expected)",python,"def validate_type(prop, value, expected):
    """""" Default validation for all types """"""

    # Validate on expected type(s), but ignore None: defaults handled elsewhere
    if value is not None and not isinstance(value, expected):
        _validation_error(prop, type(value).__name__, None, expected)",def,validate_type,(,prop,",",value,",",expected,),:,"# Validate on expected type(s), but ignore None: defaults handled elsewhere",if,value,is,not,None,and,not,isinstance,(,value,",",expected,),:,_validation_error,(,prop,",",type,(,value,),.,__name__,",",None,",",expected,),,,,Default validation for all types,Default,validation,for,all,types,,,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L710-L715,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,_validation_error,"def _validation_error(prop, prop_type, prop_value, expected):
    """""" Default validation for updated properties """"""

    if prop_type is None:
        attrib = 'value'
        assigned = prop_value
    else:
        attrib = 'type'
        assigned = prop_type

    raise ValidationError(
        'Invalid property {attrib} for {prop}:\n\t{attrib}: {assigned}\n\texpected: {expected}',
        attrib=attrib, prop=prop, assigned=assigned, expected=expected,
        invalid={prop: prop_value} if attrib == 'value' else {}
    )",python,"def _validation_error(prop, prop_type, prop_value, expected):
    """""" Default validation for updated properties """"""

    if prop_type is None:
        attrib = 'value'
        assigned = prop_value
    else:
        attrib = 'type'
        assigned = prop_type

    raise ValidationError(
        'Invalid property {attrib} for {prop}:\n\t{attrib}: {assigned}\n\texpected: {expected}',
        attrib=attrib, prop=prop, assigned=assigned, expected=expected,
        invalid={prop: prop_value} if attrib == 'value' else {}
    )",def,_validation_error,(,prop,",",prop_type,",",prop_value,",",expected,),:,if,prop_type,is,None,:,attrib,=,'value',assigned,=,prop_value,else,:,attrib,=,'type',assigned,=,prop_type,raise,ValidationError,(,'Invalid property {attrib} for {prop}:\n\t{attrib}: {assigned}\n\texpected: {expected}',",",attrib,=,attrib,",",prop,=,prop,Default validation for updated properties,Default,validation,for,updated,properties,,,,,,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L718-L732,train,",",assigned,=,assigned,",",expected,=,expected,",",invalid,=,{,prop,:,prop_value,},if,attrib,==,'value',else,{,},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/utils.py,ParserProperty.get_prop,"def get_prop(self, prop):
        """""" Calls the getter with no arguments and returns its value """"""

        if self._parser is None:
            raise ConfigurationError('Cannot call ParserProperty.""get_prop"" with no parser configured')

        return self._parser(prop) if prop else self._parser()",python,"def get_prop(self, prop):
        """""" Calls the getter with no arguments and returns its value """"""

        if self._parser is None:
            raise ConfigurationError('Cannot call ParserProperty.""get_prop"" with no parser configured')

        return self._parser(prop) if prop else self._parser()",def,get_prop,(,self,",",prop,),:,if,self,.,_parser,is,None,:,raise,ConfigurationError,(,"'Cannot call ParserProperty.""get_prop"" with no parser configured'",),return,self,.,_parser,(,prop,),if,prop,else,self,.,_parser,(,),,,,,,,,,Calls the getter with no arguments and returns its value,Calls,the,getter,with,no,arguments,and,returns,its,value,,,,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/utils.py#L765-L771,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disqus/nydus,nydus/db/backends/memcache.py,can_group_commands,"def can_group_commands(command, next_command):
    """"""
    Returns a boolean representing whether these commands can be
    grouped together or not.

    A few things are taken into account for this decision:

    For ``set`` commands:

    - Are all arguments other than the key/value the same?

    For ``delete`` and ``get`` commands:

    - Are all arguments other than the key the same?
    """"""
    multi_capable_commands = ('get', 'set', 'delete')

    if next_command is None:
        return False

    name = command.get_name()

    # TODO: support multi commands
    if name not in multi_capable_commands:
        return False

    if name != next_command.get_name():
        return False

    # if the shared args (key, or key/value) do not match, we cannot group
    if grouped_args_for_command(command) != grouped_args_for_command(next_command):
        return False

    # If the keyword arguments do not much (e.g. key_prefix, or timeout on set)
    # then we cannot group
    if command.get_kwargs() != next_command.get_kwargs():
        return False

    return True",python,"def can_group_commands(command, next_command):
    """"""
    Returns a boolean representing whether these commands can be
    grouped together or not.

    A few things are taken into account for this decision:

    For ``set`` commands:

    - Are all arguments other than the key/value the same?

    For ``delete`` and ``get`` commands:

    - Are all arguments other than the key the same?
    """"""
    multi_capable_commands = ('get', 'set', 'delete')

    if next_command is None:
        return False

    name = command.get_name()

    # TODO: support multi commands
    if name not in multi_capable_commands:
        return False

    if name != next_command.get_name():
        return False

    # if the shared args (key, or key/value) do not match, we cannot group
    if grouped_args_for_command(command) != grouped_args_for_command(next_command):
        return False

    # If the keyword arguments do not much (e.g. key_prefix, or timeout on set)
    # then we cannot group
    if command.get_kwargs() != next_command.get_kwargs():
        return False

    return True",def,can_group_commands,(,command,",",next_command,),:,multi_capable_commands,=,(,'get',",",'set',",",'delete',),if,next_command,is,None,:,return,False,name,=,command,.,get_name,(,),# TODO: support multi commands,if,name,not,in,multi_capable_commands,:,return,False,if,name,!=,"Returns a boolean representing whether these commands can be
    grouped together or not.

    A few things are taken into account for this decision:

    For ``set`` commands:

    - Are all arguments other than the key/value the same?

    For ``delete`` and ``get`` commands:

    - Are all arguments other than the key the same?",Returns,a,boolean,representing,whether,these,commands,can,be,grouped,together,or,not,.,,9b505840da47a34f758a830c3992fa5dcb7bb7ad,https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/backends/memcache.py#L97-L135,train,next_command,.,get_name,(,),:,return,False,"# if the shared args (key, or key/value) do not match, we cannot group",if,grouped_args_for_command,(,command,),!=,grouped_args_for_command,(,next_command,),:,return,False,"# If the keyword arguments do not much (e.g. key_prefix, or timeout on set)",# then we cannot group,if,command,.,get_kwargs,(,),,,,,,,,,,,,,,,,,,,,!=,next_command,.,get_kwargs,(,),:,return,False,return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rp16.py,find_databases,"def find_databases(databases):
    """"""
    define ribosomal proteins and location of curated databases
    """"""
    # 16 ribosomal proteins in their expected order
    proteins = ['L15', 'L18', 'L6', 'S8', 'L5', 'L24', 'L14',
            'S17', 'L16', 'S3', 'L22', 'S19', 'L2', 'L4', 'L3', 'S10']
    # curated databases
    protein_databases = {
                'L14': 'rpL14_JGI_MDM.filtered.faa',
                'L15': 'rpL15_JGI_MDM.filtered.faa',
                'L16': 'rpL16_JGI_MDM.filtered.faa',
                'L18': 'rpL18_JGI_MDM.filtered.faa',
                'L22': 'rpL22_JGI_MDM.filtered.faa',
                'L24': 'rpL24_JGI_MDM.filtered.faa',
                'L2': 'rpL2_JGI_MDM.filtered.faa',
                'L3': 'rpL3_JGI_MDM.filtered.faa',
                'L4': 'rpL4_JGI_MDM.filtered.faa',
                'L5': 'rpL5_JGI_MDM.filtered.faa',
                'L6': 'rpL6_JGI_MDM.filtered.faa',
                'S10': 'rpS10_JGI_MDM.filtered.faa',
                'S17': 'rpS17_JGI_MDM.filtered.faa',
                'S19': 'rpS19_JGI_MDM.filtered.faa',
                'S3': 'rpS3_JGI_MDM.filtered.faa',
                'S8': 'rpS8_JGI_MDM.filtered.faa'}
    protein_databases = {key: '%s/%s' % (databases, database) \
            for key, database in list(protein_databases.items())}
    return proteins, protein_databases",python,"def find_databases(databases):
    """"""
    define ribosomal proteins and location of curated databases
    """"""
    # 16 ribosomal proteins in their expected order
    proteins = ['L15', 'L18', 'L6', 'S8', 'L5', 'L24', 'L14',
            'S17', 'L16', 'S3', 'L22', 'S19', 'L2', 'L4', 'L3', 'S10']
    # curated databases
    protein_databases = {
                'L14': 'rpL14_JGI_MDM.filtered.faa',
                'L15': 'rpL15_JGI_MDM.filtered.faa',
                'L16': 'rpL16_JGI_MDM.filtered.faa',
                'L18': 'rpL18_JGI_MDM.filtered.faa',
                'L22': 'rpL22_JGI_MDM.filtered.faa',
                'L24': 'rpL24_JGI_MDM.filtered.faa',
                'L2': 'rpL2_JGI_MDM.filtered.faa',
                'L3': 'rpL3_JGI_MDM.filtered.faa',
                'L4': 'rpL4_JGI_MDM.filtered.faa',
                'L5': 'rpL5_JGI_MDM.filtered.faa',
                'L6': 'rpL6_JGI_MDM.filtered.faa',
                'S10': 'rpS10_JGI_MDM.filtered.faa',
                'S17': 'rpS17_JGI_MDM.filtered.faa',
                'S19': 'rpS19_JGI_MDM.filtered.faa',
                'S3': 'rpS3_JGI_MDM.filtered.faa',
                'S8': 'rpS8_JGI_MDM.filtered.faa'}
    protein_databases = {key: '%s/%s' % (databases, database) \
            for key, database in list(protein_databases.items())}
    return proteins, protein_databases",def,find_databases,(,databases,),:,# 16 ribosomal proteins in their expected order,proteins,=,[,'L15',",",'L18',",",'L6',",",'S8',",",'L5',",",'L24',",",'L14',",",'S17',",",'L16',",",'S3',",",'L22',",",'S19',",",'L2',",",'L4',",",'L3',",",'S10',],# curated databases,define ribosomal proteins and location of curated databases,define,ribosomal,proteins,and,location,of,curated,databases,,,,,,,,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rp16.py#L21-L48,train,protein_databases,=,{,'L14',:,'rpL14_JGI_MDM.filtered.faa',",",'L15',:,'rpL15_JGI_MDM.filtered.faa',",",'L16',:,'rpL16_JGI_MDM.filtered.faa',",",'L18',:,'rpL18_JGI_MDM.filtered.faa',",",'L22',:,'rpL22_JGI_MDM.filtered.faa',",",'L24',:,'rpL24_JGI_MDM.filtered.faa',",",'L2',:,'rpL2_JGI_MDM.filtered.faa',,,,,,,,,,,,,,,,,,,,",",'L3',:,'rpL3_JGI_MDM.filtered.faa',",",'L4',:,'rpL4_JGI_MDM.filtered.faa',",",'L5',:,'rpL5_JGI_MDM.filtered.faa',",",'L6',:,'rpL6_JGI_MDM.filtered.faa',",",'S10',:,'rpS10_JGI_MDM.filtered.faa',",",'S17',:,'rpS17_JGI_MDM.filtered.faa',",",'S19',:,'rpS19_JGI_MDM.filtered.faa',",",'S3',:,'rpS3_JGI_MDM.filtered.faa',",",'S8',:,'rpS8_JGI_MDM.filtered.faa',},protein_databases,=,{,key,:,'%s/%s',%,(,databases,",",database,),for,key,",",database,in,list,(,protein_databases,.,items,(,),),},return,proteins,",",protein_databases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rp16.py,find_next,"def find_next(start, stop, i2hits):
    """"""
    which protein has the best hit, the one to the 'right' or to the 'left?'
    """"""
    if start not in i2hits and stop in i2hits:
        index = stop
    elif stop not in i2hits and start in i2hits:
        index = start
    elif start not in i2hits and stop not in i2hits:
        index = choice([start, stop])
        i2hits[index] = [[False]]
    else:
        A, B = i2hits[start][0], i2hits[stop][0]
        if B[10] <= A[10]:
            index = stop
        else:
            index = start
    if index == start:
        nstart = start - 1
        nstop = stop
    else:
        nstop = stop + 1
        nstart = start
    match = i2hits[index][0]
    rp = match[-1]
    return index, nstart, nstop, rp, match",python,"def find_next(start, stop, i2hits):
    """"""
    which protein has the best hit, the one to the 'right' or to the 'left?'
    """"""
    if start not in i2hits and stop in i2hits:
        index = stop
    elif stop not in i2hits and start in i2hits:
        index = start
    elif start not in i2hits and stop not in i2hits:
        index = choice([start, stop])
        i2hits[index] = [[False]]
    else:
        A, B = i2hits[start][0], i2hits[stop][0]
        if B[10] <= A[10]:
            index = stop
        else:
            index = start
    if index == start:
        nstart = start - 1
        nstop = stop
    else:
        nstop = stop + 1
        nstart = start
    match = i2hits[index][0]
    rp = match[-1]
    return index, nstart, nstop, rp, match",def,find_next,(,start,",",stop,",",i2hits,),:,if,start,not,in,i2hits,and,stop,in,i2hits,:,index,=,stop,elif,stop,not,in,i2hits,and,start,in,i2hits,:,index,=,start,elif,start,not,in,i2hits,and,stop,"which protein has the best hit, the one to the 'right' or to the 'left?'",which,protein,has,the,best,hit,the,one,to,the,right,or,to,the,left?,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rp16.py#L77-L102,train,not,in,i2hits,:,index,=,choice,(,[,start,",",stop,],),i2hits,[,index,],=,[,[,False,],],else,:,A,",",B,=,,,,,,,,,,,,,,,,,,,,i2hits,[,start,],[,0,],",",i2hits,[,stop,],[,0,],if,B,[,10,],<=,A,[,10,],:,index,=,stop,else,:,index,=,start,if,index,==,start,:,nstart,=,start,-,1,nstop,=,stop,else,:,nstop,=,stop,+,1,nstart,=,start,match,=,i2hits,[,index,],[,0,],rp,=,match,[,-,1,],return,index,",",nstart,",",nstop,",",rp,",",match,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
christophertbrown/bioscripts,ctbBio/rp16.py,find_ribosomal,"def find_ribosomal(rps, scaffolds, s2rp, min_hits, max_hits_rp, max_errors):
    """"""
    determine which hits represent real ribosomal proteins, identify each in syntenic block
    max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold
    """"""
    for scaffold, proteins in list(s2rp.items()):
        # for each scaffold, get best hits for each rp
        hits = {p: [i for i in sorted(hits, key = itemgetter(10))][0:max_hits_rp]
            for p, hits in list(proteins.items()) if len(hits) > 0}
        # skip if fewer than min_hits RPs are identified
        if len(hits) < min_hits:
            continue
        best = sorted([hit[0] + [p]
            for p, hit in list(hits.items())], key = itemgetter(10))[0]
        block = find_block(rps, scaffolds[scaffold], hits, best, max_errors)
        if (len(block) - 1) >= min_hits:
            yield scaffold, block",python,"def find_ribosomal(rps, scaffolds, s2rp, min_hits, max_hits_rp, max_errors):
    """"""
    determine which hits represent real ribosomal proteins, identify each in syntenic block
    max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold
    """"""
    for scaffold, proteins in list(s2rp.items()):
        # for each scaffold, get best hits for each rp
        hits = {p: [i for i in sorted(hits, key = itemgetter(10))][0:max_hits_rp]
            for p, hits in list(proteins.items()) if len(hits) > 0}
        # skip if fewer than min_hits RPs are identified
        if len(hits) < min_hits:
            continue
        best = sorted([hit[0] + [p]
            for p, hit in list(hits.items())], key = itemgetter(10))[0]
        block = find_block(rps, scaffolds[scaffold], hits, best, max_errors)
        if (len(block) - 1) >= min_hits:
            yield scaffold, block",def,find_ribosomal,(,rps,",",scaffolds,",",s2rp,",",min_hits,",",max_hits_rp,",",max_errors,),:,for,scaffold,",",proteins,in,list,(,s2rp,.,items,(,),),:,"# for each scaffold, get best hits for each rp",hits,=,{,p,:,[,i,for,i,in,sorted,(,"determine which hits represent real ribosomal proteins, identify each in syntenic block
    max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold",determine,which,hits,represent,real,ribosomal,proteins,identify,each,in,syntenic,block,max_hits_rp,=,maximum,83b2566b3a5745437ec651cd6cafddd056846240,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/rp16.py#L134-L150,train,hits,",",key,=,itemgetter,(,10,),),],[,0,:,max_hits_rp,],for,p,",",hits,in,list,(,proteins,.,items,(,),),if,len,number,of,hits,to,consider,per,ribosomal,protein,per,scaffold,,,,,,,,,,(,hits,),>,0,},# skip if fewer than min_hits RPs are identified,if,len,(,hits,),<,min_hits,:,continue,best,=,sorted,(,[,hit,[,0,],+,[,p,],for,p,",",hit,in,list,(,hits,.,items,(,),),],",",key,=,itemgetter,(,10,),),[,0,],block,=,find_block,(,rps,",",scaffolds,[,scaffold,],",",hits,",",best,",",max_errors,),if,(,len,(,block,),-,1,),>=,min_hits,:,yield,scaffold,",",block,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smdabdoub/phylotoast,bin/filter_rep_set.py,filter_rep_set,"def filter_rep_set(inF, otuSet):
    """"""
    Parse the rep set file and remove all sequences not associated with unique
    OTUs.

    :@type inF: file
    :@param inF: The representative sequence set

    :@rtype: list
    :@return: The set of sequences associated with unique OTUs
    """"""
    seqs = []
    for record in SeqIO.parse(inF, ""fasta""):
        if record.id in otuSet:
            seqs.append(record)
    return seqs",python,"def filter_rep_set(inF, otuSet):
    """"""
    Parse the rep set file and remove all sequences not associated with unique
    OTUs.

    :@type inF: file
    :@param inF: The representative sequence set

    :@rtype: list
    :@return: The set of sequences associated with unique OTUs
    """"""
    seqs = []
    for record in SeqIO.parse(inF, ""fasta""):
        if record.id in otuSet:
            seqs.append(record)
    return seqs",def,filter_rep_set,(,inF,",",otuSet,),:,seqs,=,[,],for,record,in,SeqIO,.,parse,(,inF,",","""fasta""",),:,if,record,.,id,in,otuSet,:,seqs,.,append,(,record,),return,seqs,,,,,"Parse the rep set file and remove all sequences not associated with unique
    OTUs.

    :@type inF: file
    :@param inF: The representative sequence set

    :@rtype: list
    :@return: The set of sequences associated with unique OTUs",Parse,the,rep,set,file,and,remove,all,sequences,not,associated,with,unique,OTUs,.,0b74ef171e6a84761710548501dfac71285a58a3,https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/filter_rep_set.py#L32-L47,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consbio/gis-metadata-parser,gis_metadata/arcgis_metadata_parser.py,ArcGISParser._update_report_item,"def _update_report_item(self, **update_props):
        """""" Update the text for each element at the configured path if attribute matches """"""

        tree_to_update = update_props['tree_to_update']
        prop = update_props['prop']
        values = wrap_value(update_props['values'])
        xroot = self._get_xroot_for(prop)

        attr_key = 'type'
        attr_val = u''

        if prop == 'attribute_accuracy':
            attr_val = 'DQQuanAttAcc'
        elif prop == 'dataset_completeness':
            attr_val = 'DQCompOm'

        # Clear (make empty) all elements of the appropriate type
        for elem in get_elements(tree_to_update, xroot):
            if get_element_attributes(elem).get(attr_key) == attr_val:
                clear_element(elem)

        # Remove all empty elements, including those previously cleared
        remove_empty_element(tree_to_update, xroot)

        # Insert elements with correct attributes for each new value

        attrs = {attr_key: attr_val}
        updated = []

        for idx, value in enumerate(values):
            elem = insert_element(tree_to_update, idx, xroot, **attrs)
            updated.append(insert_element(elem, idx, 'measDesc', value))

        return updated",python,"def _update_report_item(self, **update_props):
        """""" Update the text for each element at the configured path if attribute matches """"""

        tree_to_update = update_props['tree_to_update']
        prop = update_props['prop']
        values = wrap_value(update_props['values'])
        xroot = self._get_xroot_for(prop)

        attr_key = 'type'
        attr_val = u''

        if prop == 'attribute_accuracy':
            attr_val = 'DQQuanAttAcc'
        elif prop == 'dataset_completeness':
            attr_val = 'DQCompOm'

        # Clear (make empty) all elements of the appropriate type
        for elem in get_elements(tree_to_update, xroot):
            if get_element_attributes(elem).get(attr_key) == attr_val:
                clear_element(elem)

        # Remove all empty elements, including those previously cleared
        remove_empty_element(tree_to_update, xroot)

        # Insert elements with correct attributes for each new value

        attrs = {attr_key: attr_val}
        updated = []

        for idx, value in enumerate(values):
            elem = insert_element(tree_to_update, idx, xroot, **attrs)
            updated.append(insert_element(elem, idx, 'measDesc', value))

        return updated",def,_update_report_item,(,self,",",*,*,update_props,),:,tree_to_update,=,update_props,[,'tree_to_update',],prop,=,update_props,[,'prop',],values,=,wrap_value,(,update_props,[,'values',],),xroot,=,self,.,_get_xroot_for,(,prop,),attr_key,=,'type',attr_val,Update the text for each element at the configured path if attribute matches,Update,the,text,for,each,element,at,the,configured,path,if,attribute,matches,,,59eefb2e51cd4d8cc3e94623a2167499ca9ef70f,https://github.com/consbio/gis-metadata-parser/blob/59eefb2e51cd4d8cc3e94623a2167499ca9ef70f/gis_metadata/arcgis_metadata_parser.py#L407-L440,train,=,u'',if,prop,==,'attribute_accuracy',:,attr_val,=,'DQQuanAttAcc',elif,prop,==,'dataset_completeness',:,attr_val,=,'DQCompOm',# Clear (make empty) all elements of the appropriate type,for,elem,in,get_elements,(,tree_to_update,",",xroot,),:,if,,,,,,,,,,,,,,,,,,,,get_element_attributes,(,elem,),.,get,(,attr_key,),==,attr_val,:,clear_element,(,elem,),"# Remove all empty elements, including those previously cleared",remove_empty_element,(,tree_to_update,",",xroot,),# Insert elements with correct attributes for each new value,attrs,=,{,attr_key,:,attr_val,},updated,=,[,],for,idx,",",value,in,enumerate,(,values,),:,elem,=,insert_element,(,tree_to_update,",",idx,",",xroot,",",*,*,attrs,),updated,.,append,(,insert_element,(,elem,",",idx,",",'measDesc',",",value,),),return,updated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
adafruit/Adafruit_Python_VCNL40xx,Adafruit_VCNL40xx/VCNL40xx.py,VCNL4010._clear_interrupt,"def _clear_interrupt(self, intbit):
        """"""Clear the specified interrupt bit in the interrupt status register.
        """"""
        int_status = self._device.readU8(VCNL4010_INTSTAT);
        int_status &= ~intbit;
        self._device.write8(VCNL4010_INTSTAT, int_status);",python,"def _clear_interrupt(self, intbit):
        """"""Clear the specified interrupt bit in the interrupt status register.
        """"""
        int_status = self._device.readU8(VCNL4010_INTSTAT);
        int_status &= ~intbit;
        self._device.write8(VCNL4010_INTSTAT, int_status);",def,_clear_interrupt,(,self,",",intbit,),:,int_status,=,self,.,_device,.,readU8,(,VCNL4010_INTSTAT,),int_status,&=,~,intbit,self,.,_device,.,write8,(,VCNL4010_INTSTAT,",",int_status,),,,,,,,,,,,,Clear the specified interrupt bit in the interrupt status register.,Clear,the,specified,interrupt,bit,in,the,interrupt,status,register,.,,,,,f88ec755fd23017028b6dec1be0607ff4a018e10,https://github.com/adafruit/Adafruit_Python_VCNL40xx/blob/f88ec755fd23017028b6dec1be0607ff4a018e10/Adafruit_VCNL40xx/VCNL40xx.py#L123-L128,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
skojaku/core-periphery-detection,cpalgorithm/Rombach.py,SimAlg.move,"def move(self):
		""""""Swaps two nodes""""""
		a = random.randint(0, len(self.state) - 1)
		b = random.randint(0, len(self.state) - 1)
		self.state[[a,b]] = self.state[[b,a]]",python,"def move(self):
		""""""Swaps two nodes""""""
		a = random.randint(0, len(self.state) - 1)
		b = random.randint(0, len(self.state) - 1)
		self.state[[a,b]] = self.state[[b,a]]",def,move,(,self,),:,a,=,random,.,randint,(,0,",",len,(,self,.,state,),-,1,),b,=,random,.,randint,(,0,",",len,(,self,.,state,),-,1,),self,.,state,Swaps two nodes,Swaps,two,nodes,,,,,,,,,,,,,d724e6441066622506ddb54d81ee9a1cfd15f766,https://github.com/skojaku/core-periphery-detection/blob/d724e6441066622506ddb54d81ee9a1cfd15f766/cpalgorithm/Rombach.py#L19-L23,train,[,[,a,",",b,],],=,self,.,state,[,[,b,",",a,],],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wbond/certbuilder,certbuilder/__init__.py,CertificateBuilder.self_signed,"def self_signed(self, value):
        """"""
        A bool - if the certificate should be self-signed.
        """"""

        self._self_signed = bool(value)

        if self._self_signed:
            self._issuer = None",python,"def self_signed(self, value):
        """"""
        A bool - if the certificate should be self-signed.
        """"""

        self._self_signed = bool(value)

        if self._self_signed:
            self._issuer = None",def,self_signed,(,self,",",value,),:,self,.,_self_signed,=,bool,(,value,),if,self,.,_self_signed,:,self,.,_issuer,=,None,,,,,,,,,,,,,,,,,,A bool - if the certificate should be self-signed.,A,bool,-,if,the,certificate,should,be,self,-,signed,.,,,,969dae884fa7f73988bbf1dcbec4fb51e234a3c5,https://github.com/wbond/certbuilder/blob/969dae884fa7f73988bbf1dcbec4fb51e234a3c5/certbuilder/__init__.py#L122-L130,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wbond/certbuilder,certbuilder/__init__.py,CertificateBuilder._get_crl_url,"def _get_crl_url(self, distribution_points):
        """"""
        Grabs the first URL out of a asn1crypto.x509.CRLDistributionPoints
        object

        :param distribution_points:
            The x509.CRLDistributionPoints object to pull the URL out of

        :return:
            A unicode string or None
        """"""

        if distribution_points is None:
            return None

        for distribution_point in distribution_points:
            name = distribution_point['distribution_point']
            if name.name == 'full_name' and name.chosen[0].name == 'uniform_resource_identifier':
                return name.chosen[0].chosen.native

        return None",python,"def _get_crl_url(self, distribution_points):
        """"""
        Grabs the first URL out of a asn1crypto.x509.CRLDistributionPoints
        object

        :param distribution_points:
            The x509.CRLDistributionPoints object to pull the URL out of

        :return:
            A unicode string or None
        """"""

        if distribution_points is None:
            return None

        for distribution_point in distribution_points:
            name = distribution_point['distribution_point']
            if name.name == 'full_name' and name.chosen[0].name == 'uniform_resource_identifier':
                return name.chosen[0].chosen.native

        return None",def,_get_crl_url,(,self,",",distribution_points,),:,if,distribution_points,is,None,:,return,None,for,distribution_point,in,distribution_points,:,name,=,distribution_point,[,'distribution_point',],if,name,.,name,==,'full_name',and,name,.,chosen,[,0,],.,name,==,'uniform_resource_identifier',"Grabs the first URL out of a asn1crypto.x509.CRLDistributionPoints
        object

        :param distribution_points:
            The x509.CRLDistributionPoints object to pull the URL out of

        :return:
            A unicode string or None",Grabs,the,first,URL,out,of,a,asn1crypto,.,x509,.,CRLDistributionPoints,object,,,969dae884fa7f73988bbf1dcbec4fb51e234a3c5,https://github.com/wbond/certbuilder/blob/969dae884fa7f73988bbf1dcbec4fb51e234a3c5/certbuilder/__init__.py#L544-L564,train,:,return,name,.,chosen,[,0,],.,chosen,.,native,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wbond/certbuilder,certbuilder/__init__.py,CertificateBuilder.ocsp_no_check,"def ocsp_no_check(self, value):
        """"""
        A bool - if the certificate should have the OCSP no check extension.
        Only applicable to certificates created for signing OCSP responses.
        Such certificates should normally be issued for a very short period of
        time since they are effectively whitelisted by clients.
        """"""

        if value is None:
            self._ocsp_no_check = None
        else:
            self._ocsp_no_check = bool(value)",python,"def ocsp_no_check(self, value):
        """"""
        A bool - if the certificate should have the OCSP no check extension.
        Only applicable to certificates created for signing OCSP responses.
        Such certificates should normally be issued for a very short period of
        time since they are effectively whitelisted by clients.
        """"""

        if value is None:
            self._ocsp_no_check = None
        else:
            self._ocsp_no_check = bool(value)",def,ocsp_no_check,(,self,",",value,),:,if,value,is,None,:,self,.,_ocsp_no_check,=,None,else,:,self,.,_ocsp_no_check,=,bool,(,value,),,,,,,,,,,,,,,,,"A bool - if the certificate should have the OCSP no check extension.
        Only applicable to certificates created for signing OCSP responses.
        Such certificates should normally be issued for a very short period of
        time since they are effectively whitelisted by clients.",A,bool,-,if,the,certificate,should,have,the,OCSP,no,check,extension,.,Only,969dae884fa7f73988bbf1dcbec4fb51e234a3c5,https://github.com/wbond/certbuilder/blob/969dae884fa7f73988bbf1dcbec4fb51e234a3c5/certbuilder/__init__.py#L688-L699,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,applicable,to,certificates,created,for,signing,OCSP,responses,.,Such,certificates,should,normally,be,issued,for,a,very,short,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,period,of,time,since,they,are,effectively,whitelisted,by,clients,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tell-k/django-modelsdoc,modelsdoc/templatetags/modelsdoc_tags.py,emptylineless,"def emptylineless(parser, token):
    """"""
    Removes empty line.

    Example usage::

        {% emptylineless %}
            test1

            test2

            test3
        {% endemptylineless %}

    This example would return this HTML::

            test1
            test2
            test3

    """"""
    nodelist = parser.parse(('endemptylineless',))
    parser.delete_first_token()
    return EmptylinelessNode(nodelist)",python,"def emptylineless(parser, token):
    """"""
    Removes empty line.

    Example usage::

        {% emptylineless %}
            test1

            test2

            test3
        {% endemptylineless %}

    This example would return this HTML::

            test1
            test2
            test3

    """"""
    nodelist = parser.parse(('endemptylineless',))
    parser.delete_first_token()
    return EmptylinelessNode(nodelist)",def,emptylineless,(,parser,",",token,),:,nodelist,=,parser,.,parse,(,(,'endemptylineless',",",),),parser,.,delete_first_token,(,),return,EmptylinelessNode,(,nodelist,),,,,,,,,,,,,,,,"Removes empty line.

    Example usage::

        {% emptylineless %}
            test1

            test2

            test3
        {% endemptylineless %}

    This example would return this HTML::

            test1
            test2
            test3",Removes,empty,line,.,,,,,,,,,,,,c9d336e76251feb142347b3a41365430d3365436,https://github.com/tell-k/django-modelsdoc/blob/c9d336e76251feb142347b3a41365430d3365436/modelsdoc/templatetags/modelsdoc_tags.py#L31-L54,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
justquick/python-varnish,varnish.py,http_purge_url,"def http_purge_url(url):
    """"""
    Do an HTTP PURGE of the given asset.
    The URL is run through urlparse and must point to the varnish instance not the varnishadm
    """"""
    url = urlparse(url)
    connection = HTTPConnection(url.hostname, url.port or 80)
    path = url.path or '/'
    connection.request('PURGE', '%s?%s' % (path, url.query) if url.query else path, '',
                       {'Host': '%s:%s' % (url.hostname, url.port) if url.port else url.hostname})
    response = connection.getresponse()
    if response.status != 200:
        logging.error('Purge failed with status: %s' % response.status)
    return response",python,"def http_purge_url(url):
    """"""
    Do an HTTP PURGE of the given asset.
    The URL is run through urlparse and must point to the varnish instance not the varnishadm
    """"""
    url = urlparse(url)
    connection = HTTPConnection(url.hostname, url.port or 80)
    path = url.path or '/'
    connection.request('PURGE', '%s?%s' % (path, url.query) if url.query else path, '',
                       {'Host': '%s:%s' % (url.hostname, url.port) if url.port else url.hostname})
    response = connection.getresponse()
    if response.status != 200:
        logging.error('Purge failed with status: %s' % response.status)
    return response",def,http_purge_url,(,url,),:,url,=,urlparse,(,url,),connection,=,HTTPConnection,(,url,.,hostname,",",url,.,port,or,80,),path,=,url,.,path,or,'/',connection,.,request,(,'PURGE',",",'%s?%s',%,(,path,"Do an HTTP PURGE of the given asset.
    The URL is run through urlparse and must point to the varnish instance not the varnishadm",Do,an,HTTP,PURGE,of,the,given,asset,.,The,URL,is,run,through,urlparse,8f114c74898e6c5ade2ce49c8b595040bd150465,https://github.com/justquick/python-varnish/blob/8f114c74898e6c5ade2ce49c8b595040bd150465/varnish.py#L47-L60,train,",",url,.,query,),if,url,.,query,else,path,",",'',",",{,'Host',:,'%s:%s',%,(,url,.,hostname,",",url,.,port,),if,url,and,must,point,to,the,varnish,instance,not,the,varnishadm,,,,,,,,,,.,port,else,url,.,hostname,},),response,=,connection,.,getresponse,(,),if,response,.,status,!=,200,:,logging,.,error,(,'Purge failed with status: %s',%,response,.,status,),return,response,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
justquick/python-varnish,varnish.py,run,"def run(addr, *commands, **kwargs):
    """"""
    Non-threaded batch command runner returning output results
    """"""
    results = []
    handler = VarnishHandler(addr, **kwargs)
    for cmd in commands:
        if isinstance(cmd, tuple) and len(cmd)>1:
            results.extend([getattr(handler, c[0].replace('.','_'))(*c[1:]) for c in cmd])
        else:
            results.append(getattr(handler, cmd.replace('.','_'))(*commands[1:]))
            break
    handler.close()
    return results",python,"def run(addr, *commands, **kwargs):
    """"""
    Non-threaded batch command runner returning output results
    """"""
    results = []
    handler = VarnishHandler(addr, **kwargs)
    for cmd in commands:
        if isinstance(cmd, tuple) and len(cmd)>1:
            results.extend([getattr(handler, c[0].replace('.','_'))(*c[1:]) for c in cmd])
        else:
            results.append(getattr(handler, cmd.replace('.','_'))(*commands[1:]))
            break
    handler.close()
    return results",def,run,(,addr,",",*,commands,",",*,*,kwargs,),:,results,=,[,],handler,=,VarnishHandler,(,addr,",",*,*,kwargs,),for,cmd,in,commands,:,if,isinstance,(,cmd,",",tuple,),and,len,(,cmd,Non-threaded batch command runner returning output results,Non,-,threaded,batch,command,runner,returning,output,results,,,,,,,8f114c74898e6c5ade2ce49c8b595040bd150465,https://github.com/justquick/python-varnish/blob/8f114c74898e6c5ade2ce49c8b595040bd150465/varnish.py#L289-L302,train,),>,1,:,results,.,extend,(,[,getattr,(,handler,",",c,[,0,],.,replace,(,'.',",",'_',),),(,*,c,[,1,,,,,,,,,,,,,,,,,,,,:,],),for,c,in,cmd,],),else,:,results,.,append,(,getattr,(,handler,",",cmd,.,replace,(,'.',",",'_',),),(,*,commands,[,1,:,],),),break,handler,.,close,(,),return,results,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown.add_stylesheets,"def add_stylesheets(self, *css_files):
        """"""add stylesheet files in HTML head""""""
        for css_file in css_files:
            self.main_soup.style.append(self._text_file(css_file))",python,"def add_stylesheets(self, *css_files):
        """"""add stylesheet files in HTML head""""""
        for css_file in css_files:
            self.main_soup.style.append(self._text_file(css_file))",def,add_stylesheets,(,self,",",*,css_files,),:,for,css_file,in,css_files,:,self,.,main_soup,.,style,.,append,(,self,.,_text_file,(,css_file,),),,,,,,,,,,,,,,,add stylesheet files in HTML head,add,stylesheet,files,in,HTML,head,,,,,,,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L43-L46,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown.add_javascripts,"def add_javascripts(self, *js_files):
        """"""add javascripts files in HTML body""""""
        # create the script tag if don't exists
        if self.main_soup.script is None:
            script_tag = self.main_soup.new_tag('script')
            self.main_soup.body.append(script_tag)

        for js_file in js_files:
            self.main_soup.script.append(self._text_file(js_file))",python,"def add_javascripts(self, *js_files):
        """"""add javascripts files in HTML body""""""
        # create the script tag if don't exists
        if self.main_soup.script is None:
            script_tag = self.main_soup.new_tag('script')
            self.main_soup.body.append(script_tag)

        for js_file in js_files:
            self.main_soup.script.append(self._text_file(js_file))",def,add_javascripts,(,self,",",*,js_files,),:,# create the script tag if don't exists,if,self,.,main_soup,.,script,is,None,:,script_tag,=,self,.,main_soup,.,new_tag,(,'script',),self,.,main_soup,.,body,.,append,(,script_tag,),for,js_file,in,js_files,add javascripts files in HTML body,add,javascripts,files,in,HTML,body,,,,,,,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L48-L56,train,:,self,.,main_soup,.,script,.,append,(,self,.,_text_file,(,js_file,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown.export,"def export(self):
        """"""return the object in a file""""""

        with open(self.export_url, 'w', encoding='utf-8') as file:
            file.write(self.build())
            if self.open_browser:
                webbrowser.open_new_tab(self.export_url)",python,"def export(self):
        """"""return the object in a file""""""

        with open(self.export_url, 'w', encoding='utf-8') as file:
            file.write(self.build())
            if self.open_browser:
                webbrowser.open_new_tab(self.export_url)",def,export,(,self,),:,with,open,(,self,.,export_url,",",'w',",",encoding,=,'utf-8',),as,file,:,file,.,write,(,self,.,build,(,),),if,self,.,open_browser,:,webbrowser,.,open_new_tab,(,self,.,return the object in a file,return,the,object,in,a,file,,,,,,,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L58-L64,train,export_url,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown.build,"def build(self):
        """"""convert Markdown text as html. return the html file as string""""""
        markdown_html = markdown.markdown(self.markdown_text, extensions=[
                TocExtension(), 'fenced_code', 'markdown_checklist.extension',
                'markdown.extensions.tables'])
        markdown_soup = BeautifulSoup(markdown_html, 'html.parser')

        # include jquery & mermaid.js only if there are Mermaid graph
        if markdown_soup.find('code', attrs={'class': 'mermaid'}):
            self._add_mermaid_js()

        # search in markdown html if there are Dot Graph & replace it with .svg result
        for dot_tag in markdown_soup.find_all('code', attrs={'class': 'dotgraph'}):
            grap_svg = self._text_to_graphiz(dot_tag.string)
            graph_soup = BeautifulSoup(grap_svg, 'html.parser')
            dot_tag.parent.replaceWith(graph_soup)

        self.main_soup.body.append(markdown_soup)
        return self.main_soup.prettify()",python,"def build(self):
        """"""convert Markdown text as html. return the html file as string""""""
        markdown_html = markdown.markdown(self.markdown_text, extensions=[
                TocExtension(), 'fenced_code', 'markdown_checklist.extension',
                'markdown.extensions.tables'])
        markdown_soup = BeautifulSoup(markdown_html, 'html.parser')

        # include jquery & mermaid.js only if there are Mermaid graph
        if markdown_soup.find('code', attrs={'class': 'mermaid'}):
            self._add_mermaid_js()

        # search in markdown html if there are Dot Graph & replace it with .svg result
        for dot_tag in markdown_soup.find_all('code', attrs={'class': 'dotgraph'}):
            grap_svg = self._text_to_graphiz(dot_tag.string)
            graph_soup = BeautifulSoup(grap_svg, 'html.parser')
            dot_tag.parent.replaceWith(graph_soup)

        self.main_soup.body.append(markdown_soup)
        return self.main_soup.prettify()",def,build,(,self,),:,markdown_html,=,markdown,.,markdown,(,self,.,markdown_text,",",extensions,=,[,TocExtension,(,),",",'fenced_code',",",'markdown_checklist.extension',",",'markdown.extensions.tables',],),markdown_soup,=,BeautifulSoup,(,markdown_html,",",'html.parser',),# include jquery & mermaid.js only if there are Mermaid graph,if,markdown_soup,.,find,convert Markdown text as html. return the html file as string,convert,Markdown,text,as,html,.,return,the,html,file,as,string,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L66-L84,train,(,'code',",",attrs,=,{,'class',:,'mermaid',},),:,self,.,_add_mermaid_js,(,),# search in markdown html if there are Dot Graph & replace it with .svg result,for,dot_tag,in,markdown_soup,.,find_all,(,'code',",",attrs,=,{,,,,,,,,,,,,,,,,,,,,'class',:,'dotgraph',},),:,grap_svg,=,self,.,_text_to_graphiz,(,dot_tag,.,string,),graph_soup,=,BeautifulSoup,(,grap_svg,",",'html.parser',),dot_tag,.,parent,.,replaceWith,(,graph_soup,),self,.,main_soup,.,body,.,append,(,markdown_soup,),return,self,.,main_soup,.,prettify,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown._text_file,"def _text_file(self, url):
        """"""return the content of a file""""""
        try:
            with open(url, 'r', encoding='utf-8') as file:
                return file.read()
        except FileNotFoundError:
            print('File `{}` not found'.format(url))
            sys.exit(0)",python,"def _text_file(self, url):
        """"""return the content of a file""""""
        try:
            with open(url, 'r', encoding='utf-8') as file:
                return file.read()
        except FileNotFoundError:
            print('File `{}` not found'.format(url))
            sys.exit(0)",def,_text_file,(,self,",",url,),:,try,:,with,open,(,url,",",'r',",",encoding,=,'utf-8',),as,file,:,return,file,.,read,(,),except,FileNotFoundError,:,print,(,'File `{}` not found',.,format,(,url,),),sys,return the content of a file,return,the,content,of,a,file,,,,,,,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L86-L93,train,.,exit,(,0,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown._text_to_graphiz,"def _text_to_graphiz(self, text):
        """"""create a graphviz graph from text""""""
        dot = Source(text, format='svg')
        return dot.pipe().decode('utf-8')",python,"def _text_to_graphiz(self, text):
        """"""create a graphviz graph from text""""""
        dot = Source(text, format='svg')
        return dot.pipe().decode('utf-8')",def,_text_to_graphiz,(,self,",",text,),:,dot,=,Source,(,text,",",format,=,'svg',),return,dot,.,pipe,(,),.,decode,(,'utf-8',),,,,,,,,,,,,,,,create a graphviz graph from text,create,a,graphviz,graph,from,text,,,,,,,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L95-L98,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
madeindjs/Super-Markdown,SuperMarkdown/SuperMarkdown.py,SuperMarkdown._add_mermaid_js,"def _add_mermaid_js(self):
        """"""add js libraries and css files of mermaid js_file""""""
        self.add_javascripts('{}/js/jquery-1.11.3.min.js'.format(self.resources_path))
        self.add_javascripts('{}/js/mermaid.min.js'.format(self.resources_path))
        self.add_stylesheets('{}/css/mermaid.css'.format(self.resources_path))
        self.main_soup.script.append('mermaid.initialize({startOnLoad:true  });')",python,"def _add_mermaid_js(self):
        """"""add js libraries and css files of mermaid js_file""""""
        self.add_javascripts('{}/js/jquery-1.11.3.min.js'.format(self.resources_path))
        self.add_javascripts('{}/js/mermaid.min.js'.format(self.resources_path))
        self.add_stylesheets('{}/css/mermaid.css'.format(self.resources_path))
        self.main_soup.script.append('mermaid.initialize({startOnLoad:true  });')",def,_add_mermaid_js,(,self,),:,self,.,add_javascripts,(,'{}/js/jquery-1.11.3.min.js',.,format,(,self,.,resources_path,),),self,.,add_javascripts,(,'{}/js/mermaid.min.js',.,format,(,self,.,resources_path,),),self,.,add_stylesheets,(,'{}/css/mermaid.css',.,format,(,self,.,resources_path,add js libraries and css files of mermaid js_file,add,js,libraries,and,css,files,of,mermaid,js_file,,,,,,,fe2da746afa6a27aaaad27a2db1dca234f802eb0,https://github.com/madeindjs/Super-Markdown/blob/fe2da746afa6a27aaaad27a2db1dca234f802eb0/SuperMarkdown/SuperMarkdown.py#L100-L105,train,),),self,.,main_soup,.,script,.,append,(,'mermaid.initialize({startOnLoad:true  });',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
paul-wolf/strgen,strgen/__init__.py,StringGenerator.getCharacterSet,"def getCharacterSet(self):
        '''Get a character set with individual members or ranges.

        Current index is on '[', the start of the character set.

        '''
        
        chars = u''
        c = None
        cnt = 1
        start = 0

        while True:
            escaped_slash = False
            c = self.next()
            # print ""pattern   : "", self.pattern
            # print ""C         : "", c
            # print ""Slash     : "", c == u'\\'
            # print 'chars     : ', chars
            # print 'index     : ', self.index
            # print 'last      : ', self.last()
            # print 'lookahead : ', self.lookahead()
            if self.lookahead() == u'-' and not c == u'\\':
                f = c
                self.next()  # skip hyphen
                c = self.next()  # get far range
                if not c or (c in self.meta_chars):
                    raise StringGenerator.SyntaxError(u""unexpected end of class range"")
                chars += self.getCharacterRange(f, c)
            elif c == u'\\':
                if self.lookahead() in self.meta_chars:
                    c = self.next()
                    chars += c
                    continue
                elif self.lookahead() in self.string_code:
                    c = self.next()
                    chars += self.string_code[c]
            elif c and c not in self.meta_chars:
                chars += c
            if c == u']': 
                if self.lookahead() == u'{':
                    [start, cnt] = self.getQuantifier()
                else:
                    start = -1
                    cnt = 1
                break
            if c and c in self.meta_chars and not self.last() == u""\\"":
                raise StringGenerator.SyntaxError(u""Un-escaped character in class definition: %s"" % c)
            if not c:
                break

        return StringGenerator.CharacterSet(chars, start, cnt)",python,"def getCharacterSet(self):
        '''Get a character set with individual members or ranges.

        Current index is on '[', the start of the character set.

        '''
        
        chars = u''
        c = None
        cnt = 1
        start = 0

        while True:
            escaped_slash = False
            c = self.next()
            # print ""pattern   : "", self.pattern
            # print ""C         : "", c
            # print ""Slash     : "", c == u'\\'
            # print 'chars     : ', chars
            # print 'index     : ', self.index
            # print 'last      : ', self.last()
            # print 'lookahead : ', self.lookahead()
            if self.lookahead() == u'-' and not c == u'\\':
                f = c
                self.next()  # skip hyphen
                c = self.next()  # get far range
                if not c or (c in self.meta_chars):
                    raise StringGenerator.SyntaxError(u""unexpected end of class range"")
                chars += self.getCharacterRange(f, c)
            elif c == u'\\':
                if self.lookahead() in self.meta_chars:
                    c = self.next()
                    chars += c
                    continue
                elif self.lookahead() in self.string_code:
                    c = self.next()
                    chars += self.string_code[c]
            elif c and c not in self.meta_chars:
                chars += c
            if c == u']': 
                if self.lookahead() == u'{':
                    [start, cnt] = self.getQuantifier()
                else:
                    start = -1
                    cnt = 1
                break
            if c and c in self.meta_chars and not self.last() == u""\\"":
                raise StringGenerator.SyntaxError(u""Un-escaped character in class definition: %s"" % c)
            if not c:
                break

        return StringGenerator.CharacterSet(chars, start, cnt)",def,getCharacterSet,(,self,),:,chars,=,u'',c,=,None,cnt,=,1,start,=,0,while,True,:,escaped_slash,=,False,c,=,self,.,next,(,),"# print ""pattern   : "", self.pattern","# print ""C         : "", c","# print ""Slash     : "", c == u'\\'","# print 'chars     : ', chars","# print 'index     : ', self.index","# print 'last      : ', self.last()","# print 'lookahead : ', self.lookahead()",if,self,.,lookahead,(,"Get a character set with individual members or ranges.

        Current index is on '[', the start of the character set.",Get,a,character,set,with,individual,members,or,ranges,.,,,,,,ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f,https://github.com/paul-wolf/strgen/blob/ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f/strgen/__init__.py#L368-L419,train,),==,u'-',and,not,c,==,u'\\',:,f,=,c,self,.,next,(,),# skip hyphen,c,=,self,.,next,(,),# get far range,if,not,c,or,,,,,,,,,,,,,,,,,,,,(,c,in,self,.,meta_chars,),:,raise,StringGenerator,.,SyntaxError,(,"u""unexpected end of class range""",),chars,+=,self,.,getCharacterRange,(,f,",",c,),elif,c,==,u'\\',:,if,self,.,lookahead,(,),in,self,.,meta_chars,:,c,=,self,.,next,(,),chars,+=,c,continue,elif,self,.,lookahead,(,),in,self,.,string_code,:,c,=,self,.,next,(,),chars,+=,self,.,string_code,[,c,],elif,c,and,c,not,in,self,.,meta_chars,:,chars,+=,c,if,c,==,u']',:,if,self,.,lookahead,(,),==,u'{',:,[,start,",",cnt,],=,self,.,getQuantifier,(,),else,:,start,=,-,1,cnt,=,1,break,if,c,and,c,in,self,.,meta_chars,and,not,self,.,last,(,),==,"u""\\""",:,raise,StringGenerator,.,SyntaxError,(,"u""Un-escaped character in class definition: %s""",%,c,),if,not,c,:,break,return,StringGenerator,.,CharacterSet,(,chars,",",start,",",cnt,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
paul-wolf/strgen,strgen/__init__.py,StringGenerator.getLiteral,"def getLiteral(self):
        '''Get a sequence of non-special characters.'''
        # we are on the first non-special character
        chars = u''
        c = self.current()
        while True:
            if c and c == u""\\"":
                c = self.next()
                if c:
                    chars += c
                continue
            elif not c or (c in self.meta_chars):
                break
            else:
                chars += c
            if self.lookahead() and self.lookahead() in self.meta_chars:
                break
            c = self.next()
        return StringGenerator.Literal(chars)",python,"def getLiteral(self):
        '''Get a sequence of non-special characters.'''
        # we are on the first non-special character
        chars = u''
        c = self.current()
        while True:
            if c and c == u""\\"":
                c = self.next()
                if c:
                    chars += c
                continue
            elif not c or (c in self.meta_chars):
                break
            else:
                chars += c
            if self.lookahead() and self.lookahead() in self.meta_chars:
                break
            c = self.next()
        return StringGenerator.Literal(chars)",def,getLiteral,(,self,),:,# we are on the first non-special character,chars,=,u'',c,=,self,.,current,(,),while,True,:,if,c,and,c,==,"u""\\""",:,c,=,self,.,next,(,),if,c,:,chars,+=,c,continue,elif,not,Get a sequence of non-special characters.,Get,a,sequence,of,non,-,special,characters,.,,,,,,,ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f,https://github.com/paul-wolf/strgen/blob/ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f/strgen/__init__.py#L421-L439,train,c,or,(,c,in,self,.,meta_chars,),:,break,else,:,chars,+=,c,if,self,.,lookahead,(,),and,self,.,lookahead,(,),in,self,,,,,,,,,,,,,,,,,,,,.,meta_chars,:,break,c,=,self,.,next,(,),return,StringGenerator,.,Literal,(,chars,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
paul-wolf/strgen,strgen/__init__.py,StringGenerator.getSequence,"def getSequence(self, level=0):
        '''Get a sequence of nodes.'''

        seq = []
        op = ''
        left_operand = None
        right_operand = None
        sequence_closed = False
        while True:
            c = self.next()
            if not c:
                break
            if c and c not in self.meta_chars:
                seq.append(self.getLiteral())
            elif c and c == u'$' and self.lookahead() == u'{':
                seq.append(self.getSource())
            elif c == u'[' and not self.last() == u'\\':
                seq.append(self.getCharacterSet())
            elif c == u'(' and not self.last() == u'\\':
                seq.append(self.getSequence(level + 1))
            elif c == u')' and not self.last() == u'\\':
                # end of this sequence
                if level == 0:
                    # there should be no parens here
                    raise StringGenerator.SyntaxError(u""Extra closing parenthesis"")
                sequence_closed = True
                break
            elif c == u'|' and not self.last() == u'\\':
                op = c
            elif c == u'&' and not self.last() == u'\\':
                op = c
            else:
                if c in self.meta_chars and not self.last() == u""\\"":
                    raise StringGenerator.SyntaxError(u""Un-escaped special character: %s"" % c)
            
            #print( op,len(seq) )
            if op and not left_operand:
                if not seq or len(seq) < 1:
                    raise StringGenerator.SyntaxError(u""Operator: %s with no left operand"" % op)
                left_operand = seq.pop()
            elif op and len(seq) >= 1 and left_operand:
                right_operand = seq.pop()

                #print( ""popped: [%s] %s:%s""%( op, left_operand, right_operand) )
                if op == u'|':
                    seq.append(StringGenerator.SequenceOR([left_operand, right_operand]))
                elif op == u'&':
                    seq.append(StringGenerator.SequenceAND([left_operand, right_operand]))

                op = u''
                left_operand = None
                right_operand = None

        # check for syntax errors
        if op:
            raise StringGenerator.SyntaxError(u""Operator: %s with no right operand"" % op)
        if level > 0 and not sequence_closed:
            # it means we are finishing a non-first-level sequence without closing parens
            raise StringGenerator.SyntaxError(u""Missing closing parenthesis"")

        return StringGenerator.Sequence(seq)",python,"def getSequence(self, level=0):
        '''Get a sequence of nodes.'''

        seq = []
        op = ''
        left_operand = None
        right_operand = None
        sequence_closed = False
        while True:
            c = self.next()
            if not c:
                break
            if c and c not in self.meta_chars:
                seq.append(self.getLiteral())
            elif c and c == u'$' and self.lookahead() == u'{':
                seq.append(self.getSource())
            elif c == u'[' and not self.last() == u'\\':
                seq.append(self.getCharacterSet())
            elif c == u'(' and not self.last() == u'\\':
                seq.append(self.getSequence(level + 1))
            elif c == u')' and not self.last() == u'\\':
                # end of this sequence
                if level == 0:
                    # there should be no parens here
                    raise StringGenerator.SyntaxError(u""Extra closing parenthesis"")
                sequence_closed = True
                break
            elif c == u'|' and not self.last() == u'\\':
                op = c
            elif c == u'&' and not self.last() == u'\\':
                op = c
            else:
                if c in self.meta_chars and not self.last() == u""\\"":
                    raise StringGenerator.SyntaxError(u""Un-escaped special character: %s"" % c)
            
            #print( op,len(seq) )
            if op and not left_operand:
                if not seq or len(seq) < 1:
                    raise StringGenerator.SyntaxError(u""Operator: %s with no left operand"" % op)
                left_operand = seq.pop()
            elif op and len(seq) >= 1 and left_operand:
                right_operand = seq.pop()

                #print( ""popped: [%s] %s:%s""%( op, left_operand, right_operand) )
                if op == u'|':
                    seq.append(StringGenerator.SequenceOR([left_operand, right_operand]))
                elif op == u'&':
                    seq.append(StringGenerator.SequenceAND([left_operand, right_operand]))

                op = u''
                left_operand = None
                right_operand = None

        # check for syntax errors
        if op:
            raise StringGenerator.SyntaxError(u""Operator: %s with no right operand"" % op)
        if level > 0 and not sequence_closed:
            # it means we are finishing a non-first-level sequence without closing parens
            raise StringGenerator.SyntaxError(u""Missing closing parenthesis"")

        return StringGenerator.Sequence(seq)",def,getSequence,(,self,",",level,=,0,),:,seq,=,[,],op,=,'',left_operand,=,None,right_operand,=,None,sequence_closed,=,False,while,True,:,c,=,self,.,next,(,),if,not,c,:,break,if,c,Get a sequence of nodes.,Get,a,sequence,of,nodes,.,,,,,,,,,,ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f,https://github.com/paul-wolf/strgen/blob/ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f/strgen/__init__.py#L441-L501,train,and,c,not,in,self,.,meta_chars,:,seq,.,append,(,self,.,getLiteral,(,),),elif,c,and,c,==,u'$',and,self,.,lookahead,(,),,,,,,,,,,,,,,,,,,,,==,u'{',:,seq,.,append,(,self,.,getSource,(,),),elif,c,==,u'[',and,not,self,.,last,(,),==,u'\\',:,seq,.,append,(,self,.,getCharacterSet,(,),),elif,c,==,u'(',and,not,self,.,last,(,),==,u'\\',:,seq,.,append,(,self,.,getSequence,(,level,+,1,),),elif,c,==,u')',and,not,self,.,last,(,),==,u'\\',:,# end of this sequence,if,level,==,0,:,# there should be no parens here,raise,StringGenerator,.,SyntaxError,(,"u""Extra closing parenthesis""",),sequence_closed,=,True,break,elif,c,==,u'|',and,not,self,.,last,(,),==,u'\\',:,op,=,c,elif,c,==,u'&',and,not,self,.,last,(,),==,u'\\',:,op,=,c,else,:,if,c,in,self,.,meta_chars,and,not,self,.,last,(,),==,"u""\\""",:,raise,StringGenerator,.,SyntaxError,(,"u""Un-escaped special character: %s""",%,c,),"#print( op,len(seq) )",if,op,and,not,left_operand,:,if,not,seq,or,len,(,seq,),<,1,:,raise,StringGenerator,.,SyntaxError,(,"u""Operator: %s with no left operand""",%,op,),left_operand,=,seq,.,pop,(,),elif,op,and,len,(,seq,),>=,1,and,left_operand,:,right_operand,=,seq,.,pop,(,),"#print( ""popped: [%s] %s:%s""%( op, left_operand, right_operand) )",if,op,==,u'|',:,seq,.,append,(,StringGenerator,.,SequenceOR,(,[,left_operand,",",right_operand,],),),elif,op,==,u'&',:,seq,.,append,(,StringGenerator,.,SequenceAND,(,[,left_operand,",",right_operand,],),),op,=,u'',left_operand,=,None,right_operand,=,None,# check for syntax errors,if,op,:,raise,StringGenerator,.,SyntaxError,(,"u""Operator: %s with no right operand""",%,op,),if,level,>,0,and,not,sequence_closed,:,# it means we are finishing a non-first-level sequence without closing parens,raise,StringGenerator,.,SyntaxError,(,"u""Missing closing parenthesis""",),return,StringGenerator,.,Sequence,(,seq,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
paul-wolf/strgen,strgen/__init__.py,StringGenerator.dump,"def dump(self, **kwargs):
        import sys
        '''Print the parse tree and then call render for an example.'''
        if not self.seq:
            self.seq = self.getSequence()
        print(""StringGenerator version: %s"" % (__version__))
        print(""Python version: %s"" % sys.version)
        # this doesn't work anymore in p3
        # print(""Random method provider class: %s"" % randint.im_class.__name__)
        self.seq.dump()
        return self.render(**kwargs)",python,"def dump(self, **kwargs):
        import sys
        '''Print the parse tree and then call render for an example.'''
        if not self.seq:
            self.seq = self.getSequence()
        print(""StringGenerator version: %s"" % (__version__))
        print(""Python version: %s"" % sys.version)
        # this doesn't work anymore in p3
        # print(""Random method provider class: %s"" % randint.im_class.__name__)
        self.seq.dump()
        return self.render(**kwargs)",def,dump,(,self,",",*,*,kwargs,),:,import,sys,if,not,self,.,seq,:,self,.,seq,=,self,.,getSequence,(,),print,(,"""StringGenerator version: %s""",%,(,__version__,),),print,(,"""Python version: %s""",%,sys,.,version,),Print the parse tree and then call render for an example.,Print,the,parse,tree,and,then,call,render,for,an,example,.,,,,ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f,https://github.com/paul-wolf/strgen/blob/ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f/strgen/__init__.py#L521-L531,train,# this doesn't work anymore in p3,"# print(""Random method provider class: %s"" % randint.im_class.__name__)",self,.,seq,.,dump,(,),return,self,.,render,(,*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
paul-wolf/strgen,strgen/__init__.py,StringGenerator.render_list,"def render_list(self, cnt, unique=False, progress_callback=None, **kwargs):
        '''Return a list of generated strings.

        Args:
            cnt (int): length of list
            unique (bool): whether to make entries unique

        Returns:
            list.

        We keep track of total attempts because a template may
        specify something impossible to attain, like [1-9]{} with cnt==1000

        '''
        
        rendered_list = []
        i = 0
        total_attempts = 0
        while True:
            if i >= cnt:
                break
            if total_attempts > cnt * self.unique_attempts_factor:
                raise StringGenerator.UniquenessError(u""couldn't satisfy uniqueness"")
            s = self.render(**kwargs)
            if unique:
                if not s in rendered_list:
                    rendered_list.append(s)
                    i += 1
            else:
                rendered_list.append(s)
                i += 1
            total_attempts += 1

            # Optionally trigger the progress indicator to inform others about our progress
            if progress_callback and callable(progress_callback):
                progress_callback(i, cnt)

        return rendered_list",python,"def render_list(self, cnt, unique=False, progress_callback=None, **kwargs):
        '''Return a list of generated strings.

        Args:
            cnt (int): length of list
            unique (bool): whether to make entries unique

        Returns:
            list.

        We keep track of total attempts because a template may
        specify something impossible to attain, like [1-9]{} with cnt==1000

        '''
        
        rendered_list = []
        i = 0
        total_attempts = 0
        while True:
            if i >= cnt:
                break
            if total_attempts > cnt * self.unique_attempts_factor:
                raise StringGenerator.UniquenessError(u""couldn't satisfy uniqueness"")
            s = self.render(**kwargs)
            if unique:
                if not s in rendered_list:
                    rendered_list.append(s)
                    i += 1
            else:
                rendered_list.append(s)
                i += 1
            total_attempts += 1

            # Optionally trigger the progress indicator to inform others about our progress
            if progress_callback and callable(progress_callback):
                progress_callback(i, cnt)

        return rendered_list",def,render_list,(,self,",",cnt,",",unique,=,False,",",progress_callback,=,None,",",*,*,kwargs,),:,rendered_list,=,[,],i,=,0,total_attempts,=,0,while,True,:,if,i,>=,cnt,:,break,if,total_attempts,>,cnt,"Return a list of generated strings.

        Args:
            cnt (int): length of list
            unique (bool): whether to make entries unique

        Returns:
            list.

        We keep track of total attempts because a template may
        specify something impossible to attain, like [1-9]{} with cnt==1000",Return,a,list,of,generated,strings,.,,,,,,,,,ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f,https://github.com/paul-wolf/strgen/blob/ca1a1484bed5a31dc9ceaef1ab62dd5582cc0d9f/strgen/__init__.py#L533-L570,train,*,self,.,unique_attempts_factor,:,raise,StringGenerator,.,UniquenessError,(,"u""couldn't satisfy uniqueness""",),s,=,self,.,render,(,*,*,kwargs,),if,unique,:,if,not,s,in,rendered_list,,,,,,,,,,,,,,,,,,,,:,rendered_list,.,append,(,s,),i,+=,1,else,:,rendered_list,.,append,(,s,),i,+=,1,total_attempts,+=,1,# Optionally trigger the progress indicator to inform others about our progress,if,progress_callback,and,callable,(,progress_callback,),:,progress_callback,(,i,",",cnt,),return,rendered_list,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.connect,"def connect(self):
        """"""
        Establish the connection. This is done automatically for you.

        If you lose the connection, you can manually run this to be re-connected.
        """"""
        self.conn = boto.connect_s3(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, debug=self.S3UTILS_DEBUG_LEVEL)

        self.bucket = self.conn.get_bucket(self.AWS_STORAGE_BUCKET_NAME)

        self.k = Key(self.bucket)",python,"def connect(self):
        """"""
        Establish the connection. This is done automatically for you.

        If you lose the connection, you can manually run this to be re-connected.
        """"""
        self.conn = boto.connect_s3(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, debug=self.S3UTILS_DEBUG_LEVEL)

        self.bucket = self.conn.get_bucket(self.AWS_STORAGE_BUCKET_NAME)

        self.k = Key(self.bucket)",def,connect,(,self,),:,self,.,conn,=,boto,.,connect_s3,(,self,.,AWS_ACCESS_KEY_ID,",",self,.,AWS_SECRET_ACCESS_KEY,",",debug,=,self,.,S3UTILS_DEBUG_LEVEL,),self,.,bucket,=,self,.,conn,.,get_bucket,(,self,.,AWS_STORAGE_BUCKET_NAME,),self,"Establish the connection. This is done automatically for you.

        If you lose the connection, you can manually run this to be re-connected.",Establish,the,connection,.,This,is,done,automatically,for,you,.,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L145-L155,train,.,k,=,Key,(,self,.,bucket,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.connect_cloudfront,"def connect_cloudfront(self):
        ""Connect to Cloud Front. This is done automatically for you when needed.""
        self.conn_cloudfront = connect_cloudfront(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, debug=self.S3UTILS_DEBUG_LEVEL)",python,"def connect_cloudfront(self):
        ""Connect to Cloud Front. This is done automatically for you when needed.""
        self.conn_cloudfront = connect_cloudfront(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, debug=self.S3UTILS_DEBUG_LEVEL)",def,connect_cloudfront,(,self,),:,self,.,conn_cloudfront,=,connect_cloudfront,(,self,.,AWS_ACCESS_KEY_ID,",",self,.,AWS_SECRET_ACCESS_KEY,",",debug,=,self,.,S3UTILS_DEBUG_LEVEL,),,,,,,,,,,,,,,,,,,Connect to Cloud Front. This is done automatically for you when needed.,Connect,to,Cloud,Front,.,This,is,done,automatically,for,you,when,needed,.,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L166-L168,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.mkdir,"def mkdir(self, target_folder):
        """"""
        Create a folder on S3.

        Examples
        --------
            >>> s3utils.mkdir(""path/to/my_folder"")
            Making directory: path/to/my_folder
        """"""
        self.printv(""Making directory: %s"" % target_folder)
        self.k.key = re.sub(r""^/|/$"", """", target_folder) + ""/""
        self.k.set_contents_from_string('')
        self.k.close()",python,"def mkdir(self, target_folder):
        """"""
        Create a folder on S3.

        Examples
        --------
            >>> s3utils.mkdir(""path/to/my_folder"")
            Making directory: path/to/my_folder
        """"""
        self.printv(""Making directory: %s"" % target_folder)
        self.k.key = re.sub(r""^/|/$"", """", target_folder) + ""/""
        self.k.set_contents_from_string('')
        self.k.close()",def,mkdir,(,self,",",target_folder,),:,self,.,printv,(,"""Making directory: %s""",%,target_folder,),self,.,k,.,key,=,re,.,sub,(,"r""^/|/$""",",","""""",",",target_folder,),+,"""/""",self,.,k,.,set_contents_from_string,(,'',),self,"Create a folder on S3.

        Examples
        --------
            >>> s3utils.mkdir(""path/to/my_folder"")
            Making directory: path/to/my_folder",Create,a,folder,on,S3,.,,,,,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L171-L183,train,.,k,.,close,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.rm,"def rm(self, path):
        """"""
        Delete the path and anything under the path.

        Example
        -------
            >>> s3utils.rm(""path/to/file_or_folder"")
        """"""

        list_of_files = list(self.ls(path))

        if list_of_files:
            if len(list_of_files) == 1:
                self.bucket.delete_key(list_of_files[0])
            else:
                self.bucket.delete_keys(list_of_files)
            self.printv(""Deleted: %s"" % list_of_files)
        else:
            logger.error(""There was nothing to remove under %s"", path)",python,"def rm(self, path):
        """"""
        Delete the path and anything under the path.

        Example
        -------
            >>> s3utils.rm(""path/to/file_or_folder"")
        """"""

        list_of_files = list(self.ls(path))

        if list_of_files:
            if len(list_of_files) == 1:
                self.bucket.delete_key(list_of_files[0])
            else:
                self.bucket.delete_keys(list_of_files)
            self.printv(""Deleted: %s"" % list_of_files)
        else:
            logger.error(""There was nothing to remove under %s"", path)",def,rm,(,self,",",path,),:,list_of_files,=,list,(,self,.,ls,(,path,),),if,list_of_files,:,if,len,(,list_of_files,),==,1,:,self,.,bucket,.,delete_key,(,list_of_files,[,0,],),else,:,"Delete the path and anything under the path.

        Example
        -------
            >>> s3utils.rm(""path/to/file_or_folder"")",Delete,the,path,and,anything,under,the,path,.,,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L186-L204,train,self,.,bucket,.,delete_keys,(,list_of_files,),self,.,printv,(,"""Deleted: %s""",%,list_of_files,),else,:,logger,.,error,(,"""There was nothing to remove under %s""",",",path,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.__put_key,"def __put_key(self, local_file, target_file, acl='public-read', del_after_upload=False, overwrite=True, source=""filename""):
        """"""Copy a file to s3.""""""
        action_word = ""moving"" if del_after_upload else ""copying""

        try:
            self.k.key = target_file  # setting the path (key) of file in the container

            if source == ""filename"":
                # grabs the contents from local_file address. Note that it loads the whole file into memory
                self.k.set_contents_from_filename(local_file, self.AWS_HEADERS)
            elif source == ""fileobj"":
                self.k.set_contents_from_file(local_file, self.AWS_HEADERS)
            elif source == ""string"":
                self.k.set_contents_from_string(local_file, self.AWS_HEADERS)
            else:
                raise Exception(""%s is not implemented as a source."" % source)
            self.k.set_acl(acl)  # setting the file permissions
            self.k.close()  # not sure if it is needed. Somewhere I read it is recommended.

            self.printv(""%s %s to %s"" % (action_word, local_file, target_file))
            # if it is supposed to delete the local file after uploading
            if del_after_upload and source == ""filename"":
                try:
                    os.remove(local_file)
                except:
                    logger.error(""Unable to delete the file: "", local_file, exc_info=True)

            return True

        except:
            logger.error(""Error in writing to %s"", target_file, exc_info=True)
            return False",python,"def __put_key(self, local_file, target_file, acl='public-read', del_after_upload=False, overwrite=True, source=""filename""):
        """"""Copy a file to s3.""""""
        action_word = ""moving"" if del_after_upload else ""copying""

        try:
            self.k.key = target_file  # setting the path (key) of file in the container

            if source == ""filename"":
                # grabs the contents from local_file address. Note that it loads the whole file into memory
                self.k.set_contents_from_filename(local_file, self.AWS_HEADERS)
            elif source == ""fileobj"":
                self.k.set_contents_from_file(local_file, self.AWS_HEADERS)
            elif source == ""string"":
                self.k.set_contents_from_string(local_file, self.AWS_HEADERS)
            else:
                raise Exception(""%s is not implemented as a source."" % source)
            self.k.set_acl(acl)  # setting the file permissions
            self.k.close()  # not sure if it is needed. Somewhere I read it is recommended.

            self.printv(""%s %s to %s"" % (action_word, local_file, target_file))
            # if it is supposed to delete the local file after uploading
            if del_after_upload and source == ""filename"":
                try:
                    os.remove(local_file)
                except:
                    logger.error(""Unable to delete the file: "", local_file, exc_info=True)

            return True

        except:
            logger.error(""Error in writing to %s"", target_file, exc_info=True)
            return False",def,__put_key,(,self,",",local_file,",",target_file,",",acl,=,'public-read',",",del_after_upload,=,False,",",overwrite,=,True,",",source,=,"""filename""",),:,action_word,=,"""moving""",if,del_after_upload,else,"""copying""",try,:,self,.,k,.,key,=,target_file,# setting the path (key) of file in the container,Copy a file to s3.,Copy,a,file,to,s3,.,,,,,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L207-L238,train,if,source,==,"""filename""",:,# grabs the contents from local_file address. Note that it loads the whole file into memory,self,.,k,.,set_contents_from_filename,(,local_file,",",self,.,AWS_HEADERS,),elif,source,==,"""fileobj""",:,self,.,k,.,set_contents_from_file,(,local_file,,,,,,,,,,,,,,,,,,,,",",self,.,AWS_HEADERS,),elif,source,==,"""string""",:,self,.,k,.,set_contents_from_string,(,local_file,",",self,.,AWS_HEADERS,),else,:,raise,Exception,(,"""%s is not implemented as a source.""",%,source,),self,.,k,.,set_acl,(,acl,),# setting the file permissions,self,.,k,.,close,(,),# not sure if it is needed. Somewhere I read it is recommended.,self,.,printv,(,"""%s %s to %s""",%,(,action_word,",",local_file,",",target_file,),),# if it is supposed to delete the local file after uploading,if,del_after_upload,and,source,==,"""filename""",:,try,:,os,.,remove,(,local_file,),except,:,logger,.,error,(,"""Unable to delete the file: """,",",local_file,",",exc_info,=,True,),return,True,except,:,logger,.,error,(,"""Error in writing to %s""",",",target_file,",",exc_info,=,True,),return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.cp,"def cp(self, local_path, target_path, acl='public-read',
           del_after_upload=False, overwrite=True, invalidate=False):
        """"""
        Copy a file or folder from local to s3.

        Parameters
        ----------

        local_path : string
            Path to file or folder. Or if you want to copy only the contents of folder, add /* at the end of folder name

        target_path : string
            Target path on S3 bucket.

        acl : string, optional
            File permissions on S3. Default is public-read

            options:
                - private: Owner gets FULL_CONTROL. No one else has any access rights.
                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.
                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.
                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access


        del_after_upload : boolean, optional
            delete the local file after uploading. This is effectively like moving the file.
            You can use s3utils.mv instead of s3utils.cp to move files from local to S3.
            It basically sets this flag to True.
            default = False

        overwrite : boolean, optional
            overwrites files on S3 if set to True. Default is True

        invalidate : boolean, optional
            invalidates the CDN (a.k.a Distribution) cache if the file already exists on S3
            default = False
            Note that invalidation might take up to 15 minutes to take place. It is easier and faster to use cache buster
            to grab lastest version of your file on CDN than invalidation.

        **Returns**

        Nothing on success but it will return what went wrong if something fails.

        Examples
        --------
            >>> s3utils.cp(""path/to/folder"",""/test/"")
            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            copying /path/to/myfolder/test.txt to test/myfolder/test.txt
            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

            >>> # When overwrite is set to False, it returns the file(s) that were already existing on s3 and were not overwritten.
            >>> s3utils.cp(""/tmp/test3.txt"", ""test3.txt"", overwrite=False)
            ERROR:root:test3.txt already exist. Not overwriting.
            >>> {'existing_files': {'test3.txt'}}

            >>> # To overwrite the files on S3 and invalidate the CDN (cloudfront) cache so the new file goes on CDN:
            >>> s3utils.cp(""path/to/folder"",""/test/"", invalidate=True)
            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            copying /path/to/myfolder/test.txt to test/myfolder/test.txt
            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

            >>> # When file does not exist, it returns a dictionary of what went wrong.
            >>> s3utils.cp(""/tmp/does_not_exist"", ""somewhere"")
            ERROR:root:trying to upload to s3 but file doesn't exist: /tmp/does_not_exist
            >>> {'file_does_not_exist': '/tmp/does_not_exist'}
        """"""
        result = None
        if overwrite:
            list_of_files = []
        else:
            list_of_files = self.ls(folder=target_path, begin_from_file="""", num=-1, get_grants=False, all_grant_data=False)

        # copying the contents of the folder and not folder itself
        if local_path.endswith(""/*""):
            local_path = local_path[:-2]
            target_path = re.sub(r""^/|/$"", """", target_path)  # Amazon S3 doesn't let the name to begin with /
        # copying folder too
        else:
            local_base_name = os.path.basename(local_path)

            local_path = re.sub(r""/$"", """", local_path)
            target_path = re.sub(r""^/"", """", target_path)

            if not target_path.endswith(local_base_name):
                target_path = os.path.join(target_path, local_base_name)

        if os.path.exists(local_path):

            result = self.__find_files_and_copy(local_path, target_path, acl, del_after_upload, overwrite, invalidate, list_of_files)

        else:
            result = {'file_does_not_exist': local_path}
            logger.error(""trying to upload to s3 but file doesn't exist: %s"" % local_path)

        return result",python,"def cp(self, local_path, target_path, acl='public-read',
           del_after_upload=False, overwrite=True, invalidate=False):
        """"""
        Copy a file or folder from local to s3.

        Parameters
        ----------

        local_path : string
            Path to file or folder. Or if you want to copy only the contents of folder, add /* at the end of folder name

        target_path : string
            Target path on S3 bucket.

        acl : string, optional
            File permissions on S3. Default is public-read

            options:
                - private: Owner gets FULL_CONTROL. No one else has any access rights.
                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.
                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.
                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access


        del_after_upload : boolean, optional
            delete the local file after uploading. This is effectively like moving the file.
            You can use s3utils.mv instead of s3utils.cp to move files from local to S3.
            It basically sets this flag to True.
            default = False

        overwrite : boolean, optional
            overwrites files on S3 if set to True. Default is True

        invalidate : boolean, optional
            invalidates the CDN (a.k.a Distribution) cache if the file already exists on S3
            default = False
            Note that invalidation might take up to 15 minutes to take place. It is easier and faster to use cache buster
            to grab lastest version of your file on CDN than invalidation.

        **Returns**

        Nothing on success but it will return what went wrong if something fails.

        Examples
        --------
            >>> s3utils.cp(""path/to/folder"",""/test/"")
            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            copying /path/to/myfolder/test.txt to test/myfolder/test.txt
            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

            >>> # When overwrite is set to False, it returns the file(s) that were already existing on s3 and were not overwritten.
            >>> s3utils.cp(""/tmp/test3.txt"", ""test3.txt"", overwrite=False)
            ERROR:root:test3.txt already exist. Not overwriting.
            >>> {'existing_files': {'test3.txt'}}

            >>> # To overwrite the files on S3 and invalidate the CDN (cloudfront) cache so the new file goes on CDN:
            >>> s3utils.cp(""path/to/folder"",""/test/"", invalidate=True)
            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            copying /path/to/myfolder/test.txt to test/myfolder/test.txt
            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

            >>> # When file does not exist, it returns a dictionary of what went wrong.
            >>> s3utils.cp(""/tmp/does_not_exist"", ""somewhere"")
            ERROR:root:trying to upload to s3 but file doesn't exist: /tmp/does_not_exist
            >>> {'file_does_not_exist': '/tmp/does_not_exist'}
        """"""
        result = None
        if overwrite:
            list_of_files = []
        else:
            list_of_files = self.ls(folder=target_path, begin_from_file="""", num=-1, get_grants=False, all_grant_data=False)

        # copying the contents of the folder and not folder itself
        if local_path.endswith(""/*""):
            local_path = local_path[:-2]
            target_path = re.sub(r""^/|/$"", """", target_path)  # Amazon S3 doesn't let the name to begin with /
        # copying folder too
        else:
            local_base_name = os.path.basename(local_path)

            local_path = re.sub(r""/$"", """", local_path)
            target_path = re.sub(r""^/"", """", target_path)

            if not target_path.endswith(local_base_name):
                target_path = os.path.join(target_path, local_base_name)

        if os.path.exists(local_path):

            result = self.__find_files_and_copy(local_path, target_path, acl, del_after_upload, overwrite, invalidate, list_of_files)

        else:
            result = {'file_does_not_exist': local_path}
            logger.error(""trying to upload to s3 but file doesn't exist: %s"" % local_path)

        return result",def,cp,(,self,",",local_path,",",target_path,",",acl,=,'public-read',",",del_after_upload,=,False,",",overwrite,=,True,",",invalidate,=,False,),:,result,=,None,if,overwrite,:,list_of_files,=,[,],else,:,list_of_files,=,self,.,ls,"Copy a file or folder from local to s3.

        Parameters
        ----------

        local_path : string
            Path to file or folder. Or if you want to copy only the contents of folder, add /* at the end of folder name

        target_path : string
            Target path on S3 bucket.

        acl : string, optional
            File permissions on S3. Default is public-read

            options:
                - private: Owner gets FULL_CONTROL. No one else has any access rights.
                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.
                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.
                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access


        del_after_upload : boolean, optional
            delete the local file after uploading. This is effectively like moving the file.
            You can use s3utils.mv instead of s3utils.cp to move files from local to S3.
            It basically sets this flag to True.
            default = False

        overwrite : boolean, optional
            overwrites files on S3 if set to True. Default is True

        invalidate : boolean, optional
            invalidates the CDN (a.k.a Distribution) cache if the file already exists on S3
            default = False
            Note that invalidation might take up to 15 minutes to take place. It is easier and faster to use cache buster
            to grab lastest version of your file on CDN than invalidation.

        **Returns**

        Nothing on success but it will return what went wrong if something fails.

        Examples
        --------
            >>> s3utils.cp(""path/to/folder"",""/test/"")
            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            copying /path/to/myfolder/test.txt to test/myfolder/test.txt
            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

            >>> # When overwrite is set to False, it returns the file(s) that were already existing on s3 and were not overwritten.
            >>> s3utils.cp(""/tmp/test3.txt"", ""test3.txt"", overwrite=False)
            ERROR:root:test3.txt already exist. Not overwriting.
            >>> {'existing_files': {'test3.txt'}}

            >>> # To overwrite the files on S3 and invalidate the CDN (cloudfront) cache so the new file goes on CDN:
            >>> s3utils.cp(""path/to/folder"",""/test/"", invalidate=True)
            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            copying /path/to/myfolder/test.txt to test/myfolder/test.txt
            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

            >>> # When file does not exist, it returns a dictionary of what went wrong.
            >>> s3utils.cp(""/tmp/does_not_exist"", ""somewhere"")
            ERROR:root:trying to upload to s3 but file doesn't exist: /tmp/does_not_exist
            >>> {'file_does_not_exist': '/tmp/does_not_exist'}",Copy,a,file,or,folder,from,local,to,s3,.,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L240-L336,train,(,folder,=,target_path,",",begin_from_file,=,"""""",",",num,=,-,1,",",get_grants,=,False,",",all_grant_data,=,False,),# copying the contents of the folder and not folder itself,if,local_path,.,endswith,(,"""/*""",),,,,,,,,,,,,,,,,,,,,:,local_path,=,local_path,[,:,-,2,],target_path,=,re,.,sub,(,"r""^/|/$""",",","""""",",",target_path,),# Amazon S3 doesn't let the name to begin with /,# copying folder too,else,:,local_base_name,=,os,.,path,.,basename,(,local_path,),local_path,=,re,.,sub,(,"r""/$""",",","""""",",",local_path,),target_path,=,re,.,sub,(,"r""^/""",",","""""",",",target_path,),if,not,target_path,.,endswith,(,local_base_name,),:,target_path,=,os,.,path,.,join,(,target_path,",",local_base_name,),if,os,.,path,.,exists,(,local_path,),:,result,=,self,.,__find_files_and_copy,(,local_path,",",target_path,",",acl,",",del_after_upload,",",overwrite,",",invalidate,",",list_of_files,),else,:,result,=,{,'file_does_not_exist',:,local_path,},logger,.,error,(,"""trying to upload to s3 but file doesn't exist: %s""",%,local_path,),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.mv,"def mv(self, local_file, target_file, acl='public-read', overwrite=True, invalidate=False):
        """"""
        Similar to Linux mv command.

        Move the file to the S3 and deletes the local copy

        It is basically s3utils.cp that has del_after_upload=True

        Examples
        --------
            >>> s3utils.mv(""path/to/folder"",""/test/"")
            moving /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            moving /path/to/myfolder/test.txt to test/myfolder/test.txt
            moving /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            moving /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

        **Returns:**

        Nothing on success, otherwise what went wrong.

        Return type:
        dict

        """"""
        self.cp(local_file, target_file, acl=acl, del_after_upload=True, overwrite=overwrite, invalidate=invalidate)",python,"def mv(self, local_file, target_file, acl='public-read', overwrite=True, invalidate=False):
        """"""
        Similar to Linux mv command.

        Move the file to the S3 and deletes the local copy

        It is basically s3utils.cp that has del_after_upload=True

        Examples
        --------
            >>> s3utils.mv(""path/to/folder"",""/test/"")
            moving /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            moving /path/to/myfolder/test.txt to test/myfolder/test.txt
            moving /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            moving /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

        **Returns:**

        Nothing on success, otherwise what went wrong.

        Return type:
        dict

        """"""
        self.cp(local_file, target_file, acl=acl, del_after_upload=True, overwrite=overwrite, invalidate=invalidate)",def,mv,(,self,",",local_file,",",target_file,",",acl,=,'public-read',",",overwrite,=,True,",",invalidate,=,False,),:,self,.,cp,(,local_file,",",target_file,",",acl,=,acl,",",del_after_upload,=,True,",",overwrite,=,overwrite,",",invalidate,"Similar to Linux mv command.

        Move the file to the S3 and deletes the local copy

        It is basically s3utils.cp that has del_after_upload=True

        Examples
        --------
            >>> s3utils.mv(""path/to/folder"",""/test/"")
            moving /path/to/myfolder/test2.txt to test/myfolder/test2.txt
            moving /path/to/myfolder/test.txt to test/myfolder/test.txt
            moving /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG
            moving /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff

        **Returns:**

        Nothing on success, otherwise what went wrong.

        Return type:
        dict",Similar,to,Linux,mv,command,.,,,,,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L483-L507,train,=,invalidate,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.cp_cropduster_image,"def cp_cropduster_image(self, the_image_path, del_after_upload=False, overwrite=False, invalidate=False):
        """"""
        Deal with saving cropduster images to S3. Cropduster is a Django library for resizing editorial images.
        S3utils was originally written to put cropduster images on S3 bucket.

        Extra Items in your Django Settings
        -----------------------------------

        MEDIA_ROOT : string
            Django media root.
            Currently it is ONLY used in cp_cropduster_image method.
            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.

        S3_ROOT_BASE : string
            S3 media root base. This will be the root folder in S3.
            Currently it is ONLY used in cp_cropduster_image method.
            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.


        """"""

        local_file = os.path.join(settings.MEDIA_ROOT, the_image_path)

        # only try to upload things if the origin cropduster file exists (so it is not already uploaded to the CDN)
        if os.path.exists(local_file):

            the_image_crops_path = os.path.splitext(the_image_path)[0]
            the_image_crops_path_full_path = os.path.join(settings.MEDIA_ROOT, the_image_crops_path)

            self.cp(local_path=local_file,
                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_path),
                    del_after_upload=del_after_upload,
                    overwrite=overwrite,
                    invalidate=invalidate,
                    )

            self.cp(local_path=the_image_crops_path_full_path + ""/*"",
                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_crops_path),
                    del_after_upload=del_after_upload,
                    overwrite=overwrite,
                    invalidate=invalidate,
                    )",python,"def cp_cropduster_image(self, the_image_path, del_after_upload=False, overwrite=False, invalidate=False):
        """"""
        Deal with saving cropduster images to S3. Cropduster is a Django library for resizing editorial images.
        S3utils was originally written to put cropduster images on S3 bucket.

        Extra Items in your Django Settings
        -----------------------------------

        MEDIA_ROOT : string
            Django media root.
            Currently it is ONLY used in cp_cropduster_image method.
            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.

        S3_ROOT_BASE : string
            S3 media root base. This will be the root folder in S3.
            Currently it is ONLY used in cp_cropduster_image method.
            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.


        """"""

        local_file = os.path.join(settings.MEDIA_ROOT, the_image_path)

        # only try to upload things if the origin cropduster file exists (so it is not already uploaded to the CDN)
        if os.path.exists(local_file):

            the_image_crops_path = os.path.splitext(the_image_path)[0]
            the_image_crops_path_full_path = os.path.join(settings.MEDIA_ROOT, the_image_crops_path)

            self.cp(local_path=local_file,
                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_path),
                    del_after_upload=del_after_upload,
                    overwrite=overwrite,
                    invalidate=invalidate,
                    )

            self.cp(local_path=the_image_crops_path_full_path + ""/*"",
                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_crops_path),
                    del_after_upload=del_after_upload,
                    overwrite=overwrite,
                    invalidate=invalidate,
                    )",def,cp_cropduster_image,(,self,",",the_image_path,",",del_after_upload,=,False,",",overwrite,=,False,",",invalidate,=,False,),:,local_file,=,os,.,path,.,join,(,settings,.,MEDIA_ROOT,",",the_image_path,),# only try to upload things if the origin cropduster file exists (so it is not already uploaded to the CDN),if,os,.,path,.,exists,(,local_file,"Deal with saving cropduster images to S3. Cropduster is a Django library for resizing editorial images.
        S3utils was originally written to put cropduster images on S3 bucket.

        Extra Items in your Django Settings
        -----------------------------------

        MEDIA_ROOT : string
            Django media root.
            Currently it is ONLY used in cp_cropduster_image method.
            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.

        S3_ROOT_BASE : string
            S3 media root base. This will be the root folder in S3.
            Currently it is ONLY used in cp_cropduster_image method.
            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.",Deal,with,saving,cropduster,images,to,S3,.,Cropduster,is,a,Django,library,for,resizing,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L510-L551,train,),:,the_image_crops_path,=,os,.,path,.,splitext,(,the_image_path,),[,0,],the_image_crops_path_full_path,=,os,.,path,.,join,(,settings,.,MEDIA_ROOT,",",the_image_crops_path,),self,editorial,images,.,S3utils,was,originally,written,to,put,cropduster,images,on,S3,bucket,.,,,,,.,cp,(,local_path,=,local_file,",",target_path,=,os,.,path,.,join,(,settings,.,S3_ROOT_BASE,",",the_image_path,),",",del_after_upload,=,del_after_upload,",",overwrite,=,overwrite,",",invalidate,=,invalidate,",",),self,.,cp,(,local_path,=,the_image_crops_path_full_path,+,"""/*""",",",target_path,=,os,.,path,.,join,(,settings,.,S3_ROOT_BASE,",",the_image_crops_path,),",",del_after_upload,=,del_after_upload,",",overwrite,=,overwrite,",",invalidate,=,invalidate,",",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.chmod,"def chmod(self, target_file, acl='public-read'):
        """"""
        sets permissions for a file on S3

        Parameters
        ----------

        target_file : string
            Path to file on S3

        acl : string, optional
            File permissions on S3. Default is public-read

            options:
                - private: Owner gets FULL_CONTROL. No one else has any access rights.
                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.
                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.
                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access


        Examples
        --------
            >>> s3utils.chmod(""path/to/file"",""private"")


        """"""
        self.k.key = target_file  # setting the path (key) of file in the container
        self.k.set_acl(acl)  # setting the file permissions
        self.k.close()",python,"def chmod(self, target_file, acl='public-read'):
        """"""
        sets permissions for a file on S3

        Parameters
        ----------

        target_file : string
            Path to file on S3

        acl : string, optional
            File permissions on S3. Default is public-read

            options:
                - private: Owner gets FULL_CONTROL. No one else has any access rights.
                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.
                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.
                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access


        Examples
        --------
            >>> s3utils.chmod(""path/to/file"",""private"")


        """"""
        self.k.key = target_file  # setting the path (key) of file in the container
        self.k.set_acl(acl)  # setting the file permissions
        self.k.close()",def,chmod,(,self,",",target_file,",",acl,=,'public-read',),:,self,.,k,.,key,=,target_file,# setting the path (key) of file in the container,self,.,k,.,set_acl,(,acl,),# setting the file permissions,self,.,k,.,close,(,),,,,,,,,"sets permissions for a file on S3

        Parameters
        ----------

        target_file : string
            Path to file on S3

        acl : string, optional
            File permissions on S3. Default is public-read

            options:
                - private: Owner gets FULL_CONTROL. No one else has any access rights.
                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.
                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.
                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access


        Examples
        --------
            >>> s3utils.chmod(""path/to/file"",""private"")",sets,permissions,for,a,file,on,S3,,,,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L582-L610,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperman/s3utils,s3utils/s3utils.py,S3utils.ll,"def ll(self, folder="""", begin_from_file="""", num=-1, all_grant_data=False):
        """"""
        Get the list of files and permissions from S3.

        This is similar to LL (ls -lah) in Linux: List of files with permissions.

        Parameters
        ----------

        folder : string
            Path to file on S3

        num: integer, optional
            number of results to return, by default it returns all results.

        begin_from_file : string, optional
            which file to start from on S3.
            This is usedful in case you are iterating over lists of files and you need to page the result by
            starting listing from a certain file and fetching certain num (number) of files.

        all_grant_data : Boolean, optional
            More detailed file permission data will be returned.

        Examples
        --------

            >>> from s3utils import S3utils
            >>> s3utils = S3utils(
            ... AWS_ACCESS_KEY_ID = 'your access key',
            ... AWS_SECRET_ACCESS_KEY = 'your secret key',
            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',
            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose
            ... )
            >>> import json
            >>> # We use json.dumps to print the results more readable:
            >>> my_folder_stuff = s3utils.ll(""/test/"")
            >>> print(json.dumps(my_folder_stuff, indent=2))
            {
              ""test/myfolder/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/em/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/.DS_Store"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
              ""test/myfolder/hoho/haha/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/haha/ff"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
              ""test/myfolder/hoho/photo.JPG"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
            }

        """"""
        return self.ls(folder=folder, begin_from_file=begin_from_file, num=num, get_grants=True, all_grant_data=all_grant_data)",python,"def ll(self, folder="""", begin_from_file="""", num=-1, all_grant_data=False):
        """"""
        Get the list of files and permissions from S3.

        This is similar to LL (ls -lah) in Linux: List of files with permissions.

        Parameters
        ----------

        folder : string
            Path to file on S3

        num: integer, optional
            number of results to return, by default it returns all results.

        begin_from_file : string, optional
            which file to start from on S3.
            This is usedful in case you are iterating over lists of files and you need to page the result by
            starting listing from a certain file and fetching certain num (number) of files.

        all_grant_data : Boolean, optional
            More detailed file permission data will be returned.

        Examples
        --------

            >>> from s3utils import S3utils
            >>> s3utils = S3utils(
            ... AWS_ACCESS_KEY_ID = 'your access key',
            ... AWS_SECRET_ACCESS_KEY = 'your secret key',
            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',
            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose
            ... )
            >>> import json
            >>> # We use json.dumps to print the results more readable:
            >>> my_folder_stuff = s3utils.ll(""/test/"")
            >>> print(json.dumps(my_folder_stuff, indent=2))
            {
              ""test/myfolder/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/em/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/.DS_Store"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
              ""test/myfolder/hoho/haha/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/haha/ff"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
              ""test/myfolder/hoho/photo.JPG"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
            }

        """"""
        return self.ls(folder=folder, begin_from_file=begin_from_file, num=num, get_grants=True, all_grant_data=all_grant_data)",def,ll,(,self,",",folder,=,"""""",",",begin_from_file,=,"""""",",",num,=,-,1,",",all_grant_data,=,False,),:,return,self,.,ls,(,folder,=,folder,",",begin_from_file,=,begin_from_file,",",num,=,num,",",get_grants,=,True,"Get the list of files and permissions from S3.

        This is similar to LL (ls -lah) in Linux: List of files with permissions.

        Parameters
        ----------

        folder : string
            Path to file on S3

        num: integer, optional
            number of results to return, by default it returns all results.

        begin_from_file : string, optional
            which file to start from on S3.
            This is usedful in case you are iterating over lists of files and you need to page the result by
            starting listing from a certain file and fetching certain num (number) of files.

        all_grant_data : Boolean, optional
            More detailed file permission data will be returned.

        Examples
        --------

            >>> from s3utils import S3utils
            >>> s3utils = S3utils(
            ... AWS_ACCESS_KEY_ID = 'your access key',
            ... AWS_SECRET_ACCESS_KEY = 'your secret key',
            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',
            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose
            ... )
            >>> import json
            >>> # We use json.dumps to print the results more readable:
            >>> my_folder_stuff = s3utils.ll(""/test/"")
            >>> print(json.dumps(my_folder_stuff, indent=2))
            {
              ""test/myfolder/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/em/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/.DS_Store"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
              ""test/myfolder/hoho/haha/"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                }
              ],
              ""test/myfolder/hoho/haha/ff"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
              ""test/myfolder/hoho/photo.JPG"": [
                {
                  ""name"": ""owner's name"",
                  ""permission"": ""FULL_CONTROL""
                },
                {
                  ""name"": null,
                  ""permission"": ""READ""
                }
              ],
            }",Get,the,list,of,files,and,permissions,from,S3,.,,,,,,aea41388a023dcf1e95588402077e31097514cf1,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L669-L764,train,",",all_grant_data,=,all_grant_data,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
uktrade/directory-signature-auth,sigauth/helpers.py,get_path,"def get_path(url):
    """"""
    Get the path from a given url, including the querystring.

    Args:
        url (str)
    Returns:
        str

    """"""

    url = urlsplit(url)
    path = url.path
    if url.query:
        path += ""?{}"".format(url.query)
    return path",python,"def get_path(url):
    """"""
    Get the path from a given url, including the querystring.

    Args:
        url (str)
    Returns:
        str

    """"""

    url = urlsplit(url)
    path = url.path
    if url.query:
        path += ""?{}"".format(url.query)
    return path",def,get_path,(,url,),:,url,=,urlsplit,(,url,),path,=,url,.,path,if,url,.,query,:,path,+=,"""?{}""",.,format,(,url,.,query,),return,path,,,,,,,,,,"Get the path from a given url, including the querystring.

    Args:
        url (str)
    Returns:
        str",Get,the,path,from,a,given,url,including,the,querystring,.,,,,,1a1b1e887b25a938133d7bcc146d3fecf1079313,https://github.com/uktrade/directory-signature-auth/blob/1a1b1e887b25a938133d7bcc146d3fecf1079313/sigauth/helpers.py#L79-L94,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vkruoso/receita-tools,receita/tools/build.py,Build.run,"def run(self):
        """"""Reads data from disk and generates CSV files.""""""
        # Try to create the directory
        if not os.path.exists(self.output):
            try:
                os.mkdir(self.output)
            except:
                print 'failed to create output directory %s' % self.output

        # Be sure it is a directory
        if not os.path.isdir(self.output):
            print 'invalid output directory %s' % self.output
            sys.exit(1)

        # Create the CSV handlers
        visitors = [
            _CompaniesCSV(self.output),
            _ActivitiesCSV(self.output),
            _ActivitiesSeenCSV(self.output),
            _QSACSV(self.output),
        ]

        # Run by each company populating the CSV files
        for path in glob.glob(os.path.join(self.input, '*.json')):
            with open(path, 'r') as f:
                try:
                    data = json.load(f, encoding='utf-8')
                except ValueError:
                    continue

                for visitor in visitors:
                    visitor.visit(data)",python,"def run(self):
        """"""Reads data from disk and generates CSV files.""""""
        # Try to create the directory
        if not os.path.exists(self.output):
            try:
                os.mkdir(self.output)
            except:
                print 'failed to create output directory %s' % self.output

        # Be sure it is a directory
        if not os.path.isdir(self.output):
            print 'invalid output directory %s' % self.output
            sys.exit(1)

        # Create the CSV handlers
        visitors = [
            _CompaniesCSV(self.output),
            _ActivitiesCSV(self.output),
            _ActivitiesSeenCSV(self.output),
            _QSACSV(self.output),
        ]

        # Run by each company populating the CSV files
        for path in glob.glob(os.path.join(self.input, '*.json')):
            with open(path, 'r') as f:
                try:
                    data = json.load(f, encoding='utf-8')
                except ValueError:
                    continue

                for visitor in visitors:
                    visitor.visit(data)",def,run,(,self,),:,# Try to create the directory,if,not,os,.,path,.,exists,(,self,.,output,),:,try,:,os,.,mkdir,(,self,.,output,),except,:,print,'failed to create output directory %s',%,self,.,output,# Be sure it is a directory,if,not,os,.,Reads data from disk and generates CSV files.,Reads,data,from,disk,and,generates,CSV,files,.,,,,,,,fd62a252c76541c9feac6470b9048b31348ffe86,https://github.com/vkruoso/receita-tools/blob/fd62a252c76541c9feac6470b9048b31348ffe86/receita/tools/build.py#L144-L175,train,path,.,isdir,(,self,.,output,),:,print,'invalid output directory %s',%,self,.,output,sys,.,exit,(,1,),# Create the CSV handlers,visitors,=,[,_CompaniesCSV,(,self,.,output,,,,,,,,,,,,,,,,,,,,),",",_ActivitiesCSV,(,self,.,output,),",",_ActivitiesSeenCSV,(,self,.,output,),",",_QSACSV,(,self,.,output,),",",],# Run by each company populating the CSV files,for,path,in,glob,.,glob,(,os,.,path,.,join,(,self,.,input,",",'*.json',),),:,with,open,(,path,",",'r',),as,f,:,try,:,data,=,json,.,load,(,f,",",encoding,=,'utf-8',),except,ValueError,:,continue,for,visitor,in,visitors,:,visitor,.,visit,(,data,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/index.py,Index.process_fields,"def process_fields(self, fields):
		""""""Process a list of simple string field definitions and assign their order based on prefix.""""""
		
		result = []
		strip = ''.join(self.PREFIX_MAP)
		
		for field in fields:
			direction = self.PREFIX_MAP['']
			
			if field[0] in self.PREFIX_MAP:
				direction = self.PREFIX_MAP[field[0]]
				field = field.lstrip(strip)
			
			result.append((field, direction))
		
		return result",python,"def process_fields(self, fields):
		""""""Process a list of simple string field definitions and assign their order based on prefix.""""""
		
		result = []
		strip = ''.join(self.PREFIX_MAP)
		
		for field in fields:
			direction = self.PREFIX_MAP['']
			
			if field[0] in self.PREFIX_MAP:
				direction = self.PREFIX_MAP[field[0]]
				field = field.lstrip(strip)
			
			result.append((field, direction))
		
		return result",def,process_fields,(,self,",",fields,),:,result,=,[,],strip,=,'',.,join,(,self,.,PREFIX_MAP,),for,field,in,fields,:,direction,=,self,.,PREFIX_MAP,[,'',],if,field,[,0,],in,self,.,Process a list of simple string field definitions and assign their order based on prefix.,Process,a,list,of,simple,string,field,definitions,and,assign,their,order,based,on,prefix,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/index.py#L60-L75,train,PREFIX_MAP,:,direction,=,self,.,PREFIX_MAP,[,field,[,0,],],field,=,field,.,lstrip,(,strip,),result,.,append,(,(,field,",",direction,),.,,,,,,,,,,,,,,,,,,,),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
svartalf/python-2gis,dgis/__init__.py,API.search_in_rubric,"def search_in_rubric(self, **kwargs):
        """"""Firms search in rubric

        http://api.2gis.ru/doc/firms/searches/searchinrubric/
        """"""

        point = kwargs.pop('point', False)
        if point:
            kwargs['point'] = '%s,%s' % point

        bound = kwargs.pop('bound', False)
        if bound:
            kwargs['bound[point1]'] = bound[0]
            kwargs['bound[point2]'] = bound[1]

        filters = kwargs.pop('filters', False)
        if filters:
            for k, v in filters.items():
                kwargs['filters[%s]' % k] = v

        return self._search_in_rubric(**kwargs)",python,"def search_in_rubric(self, **kwargs):
        """"""Firms search in rubric

        http://api.2gis.ru/doc/firms/searches/searchinrubric/
        """"""

        point = kwargs.pop('point', False)
        if point:
            kwargs['point'] = '%s,%s' % point

        bound = kwargs.pop('bound', False)
        if bound:
            kwargs['bound[point1]'] = bound[0]
            kwargs['bound[point2]'] = bound[1]

        filters = kwargs.pop('filters', False)
        if filters:
            for k, v in filters.items():
                kwargs['filters[%s]' % k] = v

        return self._search_in_rubric(**kwargs)",def,search_in_rubric,(,self,",",*,*,kwargs,),:,point,=,kwargs,.,pop,(,'point',",",False,),if,point,:,kwargs,[,'point',],=,"'%s,%s'",%,point,bound,=,kwargs,.,pop,(,'bound',",",False,),if,bound,"Firms search in rubric

        http://api.2gis.ru/doc/firms/searches/searchinrubric/",Firms,search,in,rubric,,,,,,,,,,,,6eccd6073c99494b7abf20b38a5455cbd55d6420,https://github.com/svartalf/python-2gis/blob/6eccd6073c99494b7abf20b38a5455cbd55d6420/dgis/__init__.py#L89-L109,train,:,kwargs,[,'bound[point1]',],=,bound,[,0,],kwargs,[,'bound[point2]',],=,bound,[,1,],filters,=,kwargs,.,pop,(,'filters',",",False,),if,,,,,,,,,,,,,,,,,,,,filters,:,for,k,",",v,in,filters,.,items,(,),:,kwargs,[,'filters[%s]',%,k,],=,v,return,self,.,_search_in_rubric,(,*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/ui.py,RetoxRefreshMixin.refresh,"def refresh(self):
        '''
        Refresh the list and the screen
        '''
        self._screen.force_update()
        self._screen.refresh()
        self._update(1)",python,"def refresh(self):
        '''
        Refresh the list and the screen
        '''
        self._screen.force_update()
        self._screen.refresh()
        self._update(1)",def,refresh,(,self,),:,self,.,_screen,.,force_update,(,),self,.,_screen,.,refresh,(,),self,.,_update,(,1,),,,,,,,,,,,,,,,,,,Refresh the list and the screen,Refresh,the,list,and,the,screen,,,,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/ui.py#L54-L60,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/ui.py,VirtualEnvironmentFrame.start,"def start(self, activity, action):
        '''
        Mark an action as started

        :param activity: The virtualenv activity name
        :type  activity: ``str``

        :param action: The virtualenv action
        :type  action: :class:`tox.session.Action`
        '''
        try:
            self._start_action(activity, action)
        except ValueError:
            retox_log.debug(""Could not find action %s in env %s"" % (activity, self.name))
        self.refresh()",python,"def start(self, activity, action):
        '''
        Mark an action as started

        :param activity: The virtualenv activity name
        :type  activity: ``str``

        :param action: The virtualenv action
        :type  action: :class:`tox.session.Action`
        '''
        try:
            self._start_action(activity, action)
        except ValueError:
            retox_log.debug(""Could not find action %s in env %s"" % (activity, self.name))
        self.refresh()",def,start,(,self,",",activity,",",action,),:,try,:,self,.,_start_action,(,activity,",",action,),except,ValueError,:,retox_log,.,debug,(,"""Could not find action %s in env %s""",%,(,activity,",",self,.,name,),),self,.,refresh,(,),,"Mark an action as started

        :param activity: The virtualenv activity name
        :type  activity: ``str``

        :param action: The virtualenv action
        :type  action: :class:`tox.session.Action`",Mark,an,action,as,started,,,,,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/ui.py#L233-L247,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/ui.py,VirtualEnvironmentFrame.stop,"def stop(self, activity, action):
        '''
        Mark a task as completed

        :param activity: The virtualenv activity name
        :type  activity: ``str``

        :param action: The virtualenv action
        :type  action: :class:`tox.session.Action`
        '''
        try:
            self._remove_running_action(activity, action)
        except ValueError:
            retox_log.debug(""Could not find action %s in env %s"" % (activity, self.name))
        self._mark_action_completed(activity, action)
        self.refresh()",python,"def stop(self, activity, action):
        '''
        Mark a task as completed

        :param activity: The virtualenv activity name
        :type  activity: ``str``

        :param action: The virtualenv action
        :type  action: :class:`tox.session.Action`
        '''
        try:
            self._remove_running_action(activity, action)
        except ValueError:
            retox_log.debug(""Could not find action %s in env %s"" % (activity, self.name))
        self._mark_action_completed(activity, action)
        self.refresh()",def,stop,(,self,",",activity,",",action,),:,try,:,self,.,_remove_running_action,(,activity,",",action,),except,ValueError,:,retox_log,.,debug,(,"""Could not find action %s in env %s""",%,(,activity,",",self,.,name,),),self,.,_mark_action_completed,(,activity,",","Mark a task as completed

        :param activity: The virtualenv activity name
        :type  activity: ``str``

        :param action: The virtualenv action
        :type  action: :class:`tox.session.Action`",Mark,a,task,as,completed,,,,,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/ui.py#L249-L264,train,action,),self,.,refresh,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/ui.py,VirtualEnvironmentFrame.finish,"def finish(self, status):
        '''
        Move laggard tasks over

        :param activity: The virtualenv status
        :type  activity: ``str``
        '''
        retox_log.info(""Completing %s with status %s"" % (self.name, status))
        result = Screen.COLOUR_GREEN if not status else Screen.COLOUR_RED
        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, result)
        for item in list(self._task_view.options):
            self._task_view.options.remove(item)
            self._completed_view.options.append(item)
        self.refresh()",python,"def finish(self, status):
        '''
        Move laggard tasks over

        :param activity: The virtualenv status
        :type  activity: ``str``
        '''
        retox_log.info(""Completing %s with status %s"" % (self.name, status))
        result = Screen.COLOUR_GREEN if not status else Screen.COLOUR_RED
        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, result)
        for item in list(self._task_view.options):
            self._task_view.options.remove(item)
            self._completed_view.options.append(item)
        self.refresh()",def,finish,(,self,",",status,),:,retox_log,.,info,(,"""Completing %s with status %s""",%,(,self,.,name,",",status,),),result,=,Screen,.,COLOUR_GREEN,if,not,status,else,Screen,.,COLOUR_RED,self,.,palette,[,'title',],=,(,Screen,"Move laggard tasks over

        :param activity: The virtualenv status
        :type  activity: ``str``",Move,laggard,tasks,over,,,,,,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/ui.py#L266-L279,train,.,COLOUR_WHITE,",",Screen,.,A_BOLD,",",result,),for,item,in,list,(,self,.,_task_view,.,options,),:,self,.,_task_view,.,options,.,remove,(,item,,,,,,,,,,,,,,,,,,,,),self,.,_completed_view,.,options,.,append,(,item,),self,.,refresh,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/ui.py,VirtualEnvironmentFrame.reset,"def reset(self):
        '''
        Reset the frame between jobs
        '''
        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, Screen.COLOUR_BLUE)
        self._completed_view.options = []
        self._task_view.options = []
        self.refresh()",python,"def reset(self):
        '''
        Reset the frame between jobs
        '''
        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, Screen.COLOUR_BLUE)
        self._completed_view.options = []
        self._task_view.options = []
        self.refresh()",def,reset,(,self,),:,self,.,palette,[,'title',],=,(,Screen,.,COLOUR_WHITE,",",Screen,.,A_BOLD,",",Screen,.,COLOUR_BLUE,),self,.,_completed_view,.,options,=,[,],self,.,_task_view,.,options,=,[,],self,Reset the frame between jobs,Reset,the,frame,between,jobs,,,,,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/ui.py#L281-L288,train,.,refresh,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/decorator.py,Decorator.default_arguments,"def default_arguments(cls):
        """"""Returns the available kwargs of the called class""""""
        func = cls.__init__
        args = func.__code__.co_varnames
        defaults = func.__defaults__
        index = -len(defaults)
        return {k: v for k, v in zip(args[index:], defaults)}",python,"def default_arguments(cls):
        """"""Returns the available kwargs of the called class""""""
        func = cls.__init__
        args = func.__code__.co_varnames
        defaults = func.__defaults__
        index = -len(defaults)
        return {k: v for k, v in zip(args[index:], defaults)}",def,default_arguments,(,cls,),:,func,=,cls,.,__init__,args,=,func,.,__code__,.,co_varnames,defaults,=,func,.,__defaults__,index,=,-,len,(,defaults,),return,{,k,:,v,for,k,",",v,in,zip,(,args,Returns the available kwargs of the called class,Returns,the,available,kwargs,of,the,called,class,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/decorator.py#L134-L140,train,[,index,:,],",",defaults,),},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/decorator.py,Decorator.recreate,"def recreate(cls, *args, **kwargs):
        """"""Recreate the class based in your args, multiple uses""""""
        cls.check_arguments(kwargs)
        first_is_callable = True if any(args) and callable(args[0]) else False
        signature = cls.default_arguments()
        allowed_arguments = {k: v for k, v in kwargs.items() if k in signature}
        if (any(allowed_arguments) or any(args)) and not first_is_callable:
            if any(args) and not first_is_callable:
                return cls(args[0], **allowed_arguments)
            elif any(allowed_arguments):
                return cls(**allowed_arguments)

        return cls.instances[-1] if any(cls.instances) else cls()",python,"def recreate(cls, *args, **kwargs):
        """"""Recreate the class based in your args, multiple uses""""""
        cls.check_arguments(kwargs)
        first_is_callable = True if any(args) and callable(args[0]) else False
        signature = cls.default_arguments()
        allowed_arguments = {k: v for k, v in kwargs.items() if k in signature}
        if (any(allowed_arguments) or any(args)) and not first_is_callable:
            if any(args) and not first_is_callable:
                return cls(args[0], **allowed_arguments)
            elif any(allowed_arguments):
                return cls(**allowed_arguments)

        return cls.instances[-1] if any(cls.instances) else cls()",def,recreate,(,cls,",",*,args,",",*,*,kwargs,),:,cls,.,check_arguments,(,kwargs,),first_is_callable,=,True,if,any,(,args,),and,callable,(,args,[,0,],),else,False,signature,=,cls,.,default_arguments,(,"Recreate the class based in your args, multiple uses",Recreate,the,class,based,in,your,args,multiple,uses,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/decorator.py#L143-L155,train,),allowed_arguments,=,{,k,:,v,for,k,",",v,in,kwargs,.,items,(,),if,k,in,signature,},if,(,any,(,allowed_arguments,),or,any,,,,,,,,,,,,,,,,,,,,(,args,),),and,not,first_is_callable,:,if,any,(,args,),and,not,first_is_callable,:,return,cls,(,args,[,0,],",",*,*,allowed_arguments,),elif,any,(,allowed_arguments,),:,return,cls,(,*,*,allowed_arguments,),return,cls,.,instances,[,-,1,],if,any,(,cls,.,instances,),else,cls,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/decorator.py,Decorator.check_arguments,"def check_arguments(cls, passed):
        """"""Put warnings of arguments whose can't be handle by the class""""""
        defaults = list(cls.default_arguments().keys())
        template = (""Pass arg {argument:!r} in {cname:!r}, can be a typo? ""
                    ""Supported key arguments: {defaults}"")
        fails = []
        for arg in passed:
            if arg not in defaults:
                warn(template.format(argument=arg,
                                     cname=cls.__name__,
                                     defaults=defaults))
                fails.append(arg)

        return any(fails)",python,"def check_arguments(cls, passed):
        """"""Put warnings of arguments whose can't be handle by the class""""""
        defaults = list(cls.default_arguments().keys())
        template = (""Pass arg {argument:!r} in {cname:!r}, can be a typo? ""
                    ""Supported key arguments: {defaults}"")
        fails = []
        for arg in passed:
            if arg not in defaults:
                warn(template.format(argument=arg,
                                     cname=cls.__name__,
                                     defaults=defaults))
                fails.append(arg)

        return any(fails)",def,check_arguments,(,cls,",",passed,),:,defaults,=,list,(,cls,.,default_arguments,(,),.,keys,(,),),template,=,(,"""Pass arg {argument:!r} in {cname:!r}, can be a typo? ""","""Supported key arguments: {defaults}""",),fails,=,[,],for,arg,in,passed,:,if,arg,not,in,defaults,:,Put warnings of arguments whose can't be handle by the class,Put,warnings,of,arguments,whose,can,t,be,handle,by,the,class,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/decorator.py#L158-L171,train,warn,(,template,.,format,(,argument,=,arg,",",cname,=,cls,.,__name__,",",defaults,=,defaults,),),fails,.,append,(,arg,),return,any,(,,,,,,,,,,,,,,,,,,,,fails,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ovnicraft/suds2,suds/builder.py,Builder.process,"def process(self, data, type, history):
        """""" process the specified type then process its children """"""
        if type in history:
            return
        if type.enum():
            return
        history.append(type)
        resolved = type.resolve()
        value = None
        if type.multi_occurrence():
            value = []
        else:
            if len(resolved) > 0:
                if resolved.mixed():
                    value = Factory.property(resolved.name)
                    md = value.__metadata__
                    md.sxtype = resolved
                else:
                    value = Factory.object(resolved.name)
                    md = value.__metadata__
                    md.sxtype = resolved
                    md.ordering = self.ordering(resolved)
        setattr(data, type.name, value)
        if value is not None:
            data = value
        if not isinstance(data, list):
            self.add_attributes(data, resolved)
            for child, ancestry in resolved.children():
                if self.skip_child(child, ancestry):
                    continue
                self.process(data, child, history[:])",python,"def process(self, data, type, history):
        """""" process the specified type then process its children """"""
        if type in history:
            return
        if type.enum():
            return
        history.append(type)
        resolved = type.resolve()
        value = None
        if type.multi_occurrence():
            value = []
        else:
            if len(resolved) > 0:
                if resolved.mixed():
                    value = Factory.property(resolved.name)
                    md = value.__metadata__
                    md.sxtype = resolved
                else:
                    value = Factory.object(resolved.name)
                    md = value.__metadata__
                    md.sxtype = resolved
                    md.ordering = self.ordering(resolved)
        setattr(data, type.name, value)
        if value is not None:
            data = value
        if not isinstance(data, list):
            self.add_attributes(data, resolved)
            for child, ancestry in resolved.children():
                if self.skip_child(child, ancestry):
                    continue
                self.process(data, child, history[:])",def,process,(,self,",",data,",",type,",",history,),:,if,type,in,history,:,return,if,type,.,enum,(,),:,return,history,.,append,(,type,),resolved,=,type,.,resolve,(,),value,=,None,if,process the specified type then process its children,process,the,specified,type,then,process,its,children,,,,,,,,e5b540792206a41efc22f5d5b9cfac2dbe7a7992,https://github.com/ovnicraft/suds2/blob/e5b540792206a41efc22f5d5b9cfac2dbe7a7992/suds/builder.py#L60-L90,train,type,.,multi_occurrence,(,),:,value,=,[,],else,:,if,len,(,resolved,),>,0,:,if,resolved,.,mixed,(,),:,value,=,Factory,,,,,,,,,,,,,,,,,,,,.,property,(,resolved,.,name,),md,=,value,.,__metadata__,md,.,sxtype,=,resolved,else,:,value,=,Factory,.,object,(,resolved,.,name,),md,=,value,.,__metadata__,md,.,sxtype,=,resolved,md,.,ordering,=,self,.,ordering,(,resolved,),setattr,(,data,",",type,.,name,",",value,),if,value,is,not,None,:,data,=,value,if,not,isinstance,(,data,",",list,),:,self,.,add_attributes,(,data,",",resolved,),for,child,",",ancestry,in,resolved,.,children,(,),:,if,self,.,skip_child,(,child,",",ancestry,),:,continue,self,.,process,(,data,",",child,",",history,[,:,],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ovnicraft/suds2,suds/builder.py,Builder.skip_child,"def skip_child(self, child, ancestry):
        """""" get whether or not to skip the specified child """"""
        if child.any(): return True
        for x in ancestry:
            if x.choice():
                return True
        return False",python,"def skip_child(self, child, ancestry):
        """""" get whether or not to skip the specified child """"""
        if child.any(): return True
        for x in ancestry:
            if x.choice():
                return True
        return False",def,skip_child,(,self,",",child,",",ancestry,),:,if,child,.,any,(,),:,return,True,for,x,in,ancestry,:,if,x,.,choice,(,),:,return,True,return,False,,,,,,,,,get whether or not to skip the specified child,get,whether,or,not,to,skip,the,specified,child,,,,,,,e5b540792206a41efc22f5d5b9cfac2dbe7a7992,https://github.com/ovnicraft/suds2/blob/e5b540792206a41efc22f5d5b9cfac2dbe7a7992/suds/builder.py#L99-L105,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/signals.py,active_knocks,"def active_knocks(obj):
    """"""
    Checks whether knocks are enabled for the model given as argument

    :param obj: model instance
    :return True if knocks are active
    """"""
    if not hasattr(_thread_locals, 'knock_enabled'):
        return True
    return _thread_locals.knock_enabled.get(obj.__class__, True)",python,"def active_knocks(obj):
    """"""
    Checks whether knocks are enabled for the model given as argument

    :param obj: model instance
    :return True if knocks are active
    """"""
    if not hasattr(_thread_locals, 'knock_enabled'):
        return True
    return _thread_locals.knock_enabled.get(obj.__class__, True)",def,active_knocks,(,obj,),:,if,not,hasattr,(,_thread_locals,",",'knock_enabled',),:,return,True,return,_thread_locals,.,knock_enabled,.,get,(,obj,.,__class__,",",True,),,,,,,,,,,,,,,"Checks whether knocks are enabled for the model given as argument

    :param obj: model instance
    :return True if knocks are active",Checks,whether,knocks,are,enabled,for,the,model,given,as,argument,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/signals.py#L34-L43,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/signals.py,pause_knocks,"def pause_knocks(obj):
    """"""
    Context manager to suspend sending knocks for the given model

    :param obj: model instance
    """"""
    if not hasattr(_thread_locals, 'knock_enabled'):
        _thread_locals.knock_enabled = {}
    obj.__class__._disconnect()
    _thread_locals.knock_enabled[obj.__class__] = False
    yield
    _thread_locals.knock_enabled[obj.__class__] = True
    obj.__class__._connect()",python,"def pause_knocks(obj):
    """"""
    Context manager to suspend sending knocks for the given model

    :param obj: model instance
    """"""
    if not hasattr(_thread_locals, 'knock_enabled'):
        _thread_locals.knock_enabled = {}
    obj.__class__._disconnect()
    _thread_locals.knock_enabled[obj.__class__] = False
    yield
    _thread_locals.knock_enabled[obj.__class__] = True
    obj.__class__._connect()",def,pause_knocks,(,obj,),:,if,not,hasattr,(,_thread_locals,",",'knock_enabled',),:,_thread_locals,.,knock_enabled,=,{,},obj,.,__class__,.,_disconnect,(,),_thread_locals,.,knock_enabled,[,obj,.,__class__,],=,False,yield,_thread_locals,.,knock_enabled,[,"Context manager to suspend sending knocks for the given model

    :param obj: model instance",Context,manager,to,suspend,sending,knocks,for,the,given,model,,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/signals.py#L47-L59,train,obj,.,__class__,],=,True,obj,.,__class__,.,_connect,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/reporter.py,RetoxReporter._loopreport,"def _loopreport(self):
        '''
        Loop over the report progress
        '''
        while 1:
            eventlet.sleep(0.2)
            ac2popenlist = {}
            for action in self.session._actions:
                for popen in action._popenlist:
                    if popen.poll() is None:
                        lst = ac2popenlist.setdefault(action.activity, [])
                        lst.append(popen)
                if not action._popenlist and action in self._actionmayfinish:
                    super(RetoxReporter, self).logaction_finish(action)
                    self._actionmayfinish.remove(action)

            self.screen.draw_next_frame(repeat=False)",python,"def _loopreport(self):
        '''
        Loop over the report progress
        '''
        while 1:
            eventlet.sleep(0.2)
            ac2popenlist = {}
            for action in self.session._actions:
                for popen in action._popenlist:
                    if popen.poll() is None:
                        lst = ac2popenlist.setdefault(action.activity, [])
                        lst.append(popen)
                if not action._popenlist and action in self._actionmayfinish:
                    super(RetoxReporter, self).logaction_finish(action)
                    self._actionmayfinish.remove(action)

            self.screen.draw_next_frame(repeat=False)",def,_loopreport,(,self,),:,while,1,:,eventlet,.,sleep,(,0.2,),ac2popenlist,=,{,},for,action,in,self,.,session,.,_actions,:,for,popen,in,action,.,_popenlist,:,if,popen,.,poll,(,),is,None,Loop over the report progress,Loop,over,the,report,progress,,,,,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/reporter.py#L49-L65,train,:,lst,=,ac2popenlist,.,setdefault,(,action,.,activity,",",[,],),lst,.,append,(,popen,),if,not,action,.,_popenlist,and,action,in,self,.,,,,,,,,,,,,,,,,,,,,_actionmayfinish,:,super,(,RetoxReporter,",",self,),.,logaction_finish,(,action,),self,.,_actionmayfinish,.,remove,(,action,),self,.,screen,.,draw_next_frame,(,repeat,=,False,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yejianye/mdmail,mdmail/api.py,send,"def send(email, subject=None,
         from_email=None, to_email=None,
         cc=None, bcc=None, reply_to=None,
         smtp=None):
    """"""Send markdown email

    Args:
        email (str/obj): A markdown string or EmailContent object 
        subject (str): subject line
        from_email (str): sender email address
        to_email (str/list): recipient email addresses
        cc (str/list): CC email addresses (string or a list)
        bcc (str/list): BCC email addresses (string or a list)
        reply_to (str): Reply-to email address
        smtp (dict): SMTP configuration (dict)

    Schema of smtp dict:
        host (str): SMTP server host. Default: localhost
        port (int): SMTP server port. Default: 25
        tls (bool): Use TLS. Default: False
        ssl (bool): Use SSL. Default: False
        user (bool): SMTP login user. Default empty
        password (bool): SMTP login password. Default empty
    """"""
    if is_string(email):
        email = EmailContent(email)

    from_email = sanitize_email_address(from_email or email.headers.get('from'))
    to_email = sanitize_email_address(to_email or email.headers.get('to'))
    cc = sanitize_email_address(cc or email.headers.get('cc'))
    bcc = sanitize_email_address(bcc or email.headers.get('bcc'))
    reply_to = sanitize_email_address(reply_to or email.headers.get('reply-to'))

    message_args = {
        'html': email.html,
        'text': email.text,
        'subject': (subject or email.headers.get('subject', '')),
        'mail_from': from_email,
        'mail_to': to_email
    }
    if cc:
        message_args['cc'] = cc
    if bcc:
        message_args['bcc'] = bcc
    if reply_to:
        message_args['headers'] = {'reply-to': reply_to}

    message = emails.Message(**message_args)

    for filename, data in email.inline_images:
        message.attach(filename=filename, content_disposition='inline', data=data)

    message.send(smtp=smtp)",python,"def send(email, subject=None,
         from_email=None, to_email=None,
         cc=None, bcc=None, reply_to=None,
         smtp=None):
    """"""Send markdown email

    Args:
        email (str/obj): A markdown string or EmailContent object 
        subject (str): subject line
        from_email (str): sender email address
        to_email (str/list): recipient email addresses
        cc (str/list): CC email addresses (string or a list)
        bcc (str/list): BCC email addresses (string or a list)
        reply_to (str): Reply-to email address
        smtp (dict): SMTP configuration (dict)

    Schema of smtp dict:
        host (str): SMTP server host. Default: localhost
        port (int): SMTP server port. Default: 25
        tls (bool): Use TLS. Default: False
        ssl (bool): Use SSL. Default: False
        user (bool): SMTP login user. Default empty
        password (bool): SMTP login password. Default empty
    """"""
    if is_string(email):
        email = EmailContent(email)

    from_email = sanitize_email_address(from_email or email.headers.get('from'))
    to_email = sanitize_email_address(to_email or email.headers.get('to'))
    cc = sanitize_email_address(cc or email.headers.get('cc'))
    bcc = sanitize_email_address(bcc or email.headers.get('bcc'))
    reply_to = sanitize_email_address(reply_to or email.headers.get('reply-to'))

    message_args = {
        'html': email.html,
        'text': email.text,
        'subject': (subject or email.headers.get('subject', '')),
        'mail_from': from_email,
        'mail_to': to_email
    }
    if cc:
        message_args['cc'] = cc
    if bcc:
        message_args['bcc'] = bcc
    if reply_to:
        message_args['headers'] = {'reply-to': reply_to}

    message = emails.Message(**message_args)

    for filename, data in email.inline_images:
        message.attach(filename=filename, content_disposition='inline', data=data)

    message.send(smtp=smtp)",def,send,(,email,",",subject,=,None,",",from_email,=,None,",",to_email,=,None,",",cc,=,None,",",bcc,=,None,",",reply_to,=,None,",",smtp,=,None,),:,if,is_string,(,email,),:,email,=,EmailContent,"Send markdown email

    Args:
        email (str/obj): A markdown string or EmailContent object 
        subject (str): subject line
        from_email (str): sender email address
        to_email (str/list): recipient email addresses
        cc (str/list): CC email addresses (string or a list)
        bcc (str/list): BCC email addresses (string or a list)
        reply_to (str): Reply-to email address
        smtp (dict): SMTP configuration (dict)

    Schema of smtp dict:
        host (str): SMTP server host. Default: localhost
        port (int): SMTP server port. Default: 25
        tls (bool): Use TLS. Default: False
        ssl (bool): Use SSL. Default: False
        user (bool): SMTP login user. Default empty
        password (bool): SMTP login password. Default empty",Send,markdown,email,,,,,,,,,,,,,ef03da8d5836b5ae0a4ad8c44f2fe4936a896644,https://github.com/yejianye/mdmail/blob/ef03da8d5836b5ae0a4ad8c44f2fe4936a896644/mdmail/api.py#L11-L63,train,(,email,),from_email,=,sanitize_email_address,(,from_email,or,email,.,headers,.,get,(,'from',),),to_email,=,sanitize_email_address,(,to_email,or,email,.,headers,.,get,(,,,,,,,,,,,,,,,,,,,,'to',),),cc,=,sanitize_email_address,(,cc,or,email,.,headers,.,get,(,'cc',),),bcc,=,sanitize_email_address,(,bcc,or,email,.,headers,.,get,(,'bcc',),),reply_to,=,sanitize_email_address,(,reply_to,or,email,.,headers,.,get,(,'reply-to',),),message_args,=,{,'html',:,email,.,html,",",'text',:,email,.,text,",",'subject',:,(,subject,or,email,.,headers,.,get,(,'subject',",",'',),),",",'mail_from',:,from_email,",",'mail_to',:,to_email,},if,cc,:,message_args,[,'cc',],=,cc,if,bcc,:,message_args,[,'bcc',],=,bcc,if,reply_to,:,message_args,[,'headers',],=,{,'reply-to',:,reply_to,},message,=,emails,.,Message,(,*,*,message_args,),for,filename,",",data,in,email,.,inline_images,:,message,.,attach,(,filename,=,filename,",",content_disposition,=,'inline',",",data,=,data,),message,.,send,(,smtp,=,smtp,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/field/date.py,Date._process_tz,"def _process_tz(self, dt, naive, tz):
		""""""Process timezone casting and conversion.""""""
		
		def _tz(t):
			if t in (None, 'naive'):
				return t
			
			if t == 'local':
				if __debug__ and not localtz:
					raise ValueError(""Requested conversion to local timezone, but `localtz` not installed."")
				
				t = localtz
			
			if not isinstance(t, tzinfo):
				if __debug__ and not localtz:
					raise ValueError(""The `pytz` package must be installed to look up timezone: "" + repr(t))
				
				t = get_tz(t)
			
			if not hasattr(t, 'normalize') and get_tz:  # Attempt to handle non-pytz tzinfo.
				t = get_tz(t.tzname(dt))
			
			return t
		
		naive = _tz(naive)
		tz = _tz(tz)
		
		if not dt.tzinfo and naive:
			if hasattr(naive, 'localize'):
				dt = naive.localize(dt)
			else:
				dt = dt.replace(tzinfo=naive)
		
		if not tz:
			return dt
		
		if hasattr(tz, 'normalize'):
			dt = tz.normalize(dt.astimezone(tz))
		elif tz == 'naive':
			dt = dt.replace(tzinfo=None)
		else:
			dt = dt.astimezone(tz)  # Warning: this might not always be entirely correct!
		
		return dt",python,"def _process_tz(self, dt, naive, tz):
		""""""Process timezone casting and conversion.""""""
		
		def _tz(t):
			if t in (None, 'naive'):
				return t
			
			if t == 'local':
				if __debug__ and not localtz:
					raise ValueError(""Requested conversion to local timezone, but `localtz` not installed."")
				
				t = localtz
			
			if not isinstance(t, tzinfo):
				if __debug__ and not localtz:
					raise ValueError(""The `pytz` package must be installed to look up timezone: "" + repr(t))
				
				t = get_tz(t)
			
			if not hasattr(t, 'normalize') and get_tz:  # Attempt to handle non-pytz tzinfo.
				t = get_tz(t.tzname(dt))
			
			return t
		
		naive = _tz(naive)
		tz = _tz(tz)
		
		if not dt.tzinfo and naive:
			if hasattr(naive, 'localize'):
				dt = naive.localize(dt)
			else:
				dt = dt.replace(tzinfo=naive)
		
		if not tz:
			return dt
		
		if hasattr(tz, 'normalize'):
			dt = tz.normalize(dt.astimezone(tz))
		elif tz == 'naive':
			dt = dt.replace(tzinfo=None)
		else:
			dt = dt.astimezone(tz)  # Warning: this might not always be entirely correct!
		
		return dt",def,_process_tz,(,self,",",dt,",",naive,",",tz,),:,def,_tz,(,t,),:,if,t,in,(,None,",",'naive',),:,return,t,if,t,==,'local',:,if,__debug__,and,not,localtz,:,raise,ValueError,(,Process timezone casting and conversion.,Process,timezone,casting,and,conversion,.,,,,,,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/field/date.py#L59-L102,train,"""Requested conversion to local timezone, but `localtz` not installed.""",),t,=,localtz,if,not,isinstance,(,t,",",tzinfo,),:,if,__debug__,and,not,localtz,:,raise,ValueError,(,"""The `pytz` package must be installed to look up timezone: """,+,repr,(,t,),),,,,,,,,,,,,,,,,,,,,t,=,get_tz,(,t,),if,not,hasattr,(,t,",",'normalize',),and,get_tz,:,# Attempt to handle non-pytz tzinfo.,t,=,get_tz,(,t,.,tzname,(,dt,),),return,t,naive,=,_tz,(,naive,),tz,=,_tz,(,tz,),if,not,dt,.,tzinfo,and,naive,:,if,hasattr,(,naive,",",'localize',),:,dt,=,naive,.,localize,(,dt,),else,:,dt,=,dt,.,replace,(,tzinfo,=,naive,),if,not,tz,:,return,dt,if,hasattr,(,tz,",",'normalize',),:,dt,=,tz,.,normalize,(,dt,.,astimezone,(,tz,),),elif,tz,==,'naive',:,dt,=,dt,.,replace,(,tzinfo,=,None,),else,:,dt,=,dt,.,astimezone,(,tz,),# Warning: this might not always be entirely correct!,return,dt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/document.py,Document._prepare_defaults,"def _prepare_defaults(self):
		""""""Trigger assignment of default values.""""""
		
		for name, field in self.__fields__.items():
			if field.assign:
				getattr(self, name)",python,"def _prepare_defaults(self):
		""""""Trigger assignment of default values.""""""
		
		for name, field in self.__fields__.items():
			if field.assign:
				getattr(self, name)",def,_prepare_defaults,(,self,),:,for,name,",",field,in,self,.,__fields__,.,items,(,),:,if,field,.,assign,:,getattr,(,self,",",name,),,,,,,,,,,,,,,Trigger assignment of default values.,Trigger,assignment,of,default,values,.,,,,,,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/document.py#L71-L76,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/document.py,Document.from_mongo,"def from_mongo(cls, doc):
		""""""Convert data coming in from the MongoDB wire driver into a Document instance.""""""
		
		if doc is None:  # To support simplified iterative use, None should return None.
			return None
		
		if isinstance(doc, Document):  # No need to perform processing on existing Document instances.
			return doc
		
		if cls.__type_store__ and cls.__type_store__ in doc:  # Instantiate specific class mentioned in the data.
			cls = load(doc[cls.__type_store__], 'marrow.mongo.document')
		
		# Prepare a new instance in such a way that changes to the instance will be reflected in the originating doc.
		instance = cls(_prepare_defaults=False)  # Construct an instance, but delay default value processing.
		instance.__data__ = doc  # I am Popeye of Borg (pattern); you will be askimilgrated.
		instance._prepare_defaults()  # pylint:disable=protected-access -- deferred default value processing.
		
		return instance",python,"def from_mongo(cls, doc):
		""""""Convert data coming in from the MongoDB wire driver into a Document instance.""""""
		
		if doc is None:  # To support simplified iterative use, None should return None.
			return None
		
		if isinstance(doc, Document):  # No need to perform processing on existing Document instances.
			return doc
		
		if cls.__type_store__ and cls.__type_store__ in doc:  # Instantiate specific class mentioned in the data.
			cls = load(doc[cls.__type_store__], 'marrow.mongo.document')
		
		# Prepare a new instance in such a way that changes to the instance will be reflected in the originating doc.
		instance = cls(_prepare_defaults=False)  # Construct an instance, but delay default value processing.
		instance.__data__ = doc  # I am Popeye of Borg (pattern); you will be askimilgrated.
		instance._prepare_defaults()  # pylint:disable=protected-access -- deferred default value processing.
		
		return instance",def,from_mongo,(,cls,",",doc,),:,if,doc,is,None,:,"# To support simplified iterative use, None should return None.",return,None,if,isinstance,(,doc,",",Document,),:,# No need to perform processing on existing Document instances.,return,doc,if,cls,.,__type_store__,and,cls,.,__type_store__,in,doc,:,# Instantiate specific class mentioned in the data.,cls,=,load,(,Convert data coming in from the MongoDB wire driver into a Document instance.,Convert,data,coming,in,from,the,MongoDB,wire,driver,into,a,Document,instance,.,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/document.py#L81-L98,train,doc,[,cls,.,__type_store__,],",",'marrow.mongo.document',),# Prepare a new instance in such a way that changes to the instance will be reflected in the originating doc.,instance,=,cls,(,_prepare_defaults,=,False,),"# Construct an instance, but delay default value processing.",instance,.,__data__,=,doc,# I am Popeye of Borg (pattern); you will be askimilgrated.,instance,.,_prepare_defaults,(,),,,,,,,,,,,,,,,,,,,,# pylint:disable=protected-access -- deferred default value processing.,return,instance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/document.py,Document.pop,"def pop(self, name, default=SENTINEL):
		""""""Retrieve and remove a value from the backing store, optionally with a default.""""""
		
		if default is SENTINEL:
			return self.__data__.pop(name)
		
		return self.__data__.pop(name, default)",python,"def pop(self, name, default=SENTINEL):
		""""""Retrieve and remove a value from the backing store, optionally with a default.""""""
		
		if default is SENTINEL:
			return self.__data__.pop(name)
		
		return self.__data__.pop(name, default)",def,pop,(,self,",",name,",",default,=,SENTINEL,),:,if,default,is,SENTINEL,:,return,self,.,__data__,.,pop,(,name,),return,self,.,__data__,.,pop,(,name,",",default,),,,,,,,"Retrieve and remove a value from the backing store, optionally with a default.",Retrieve,and,remove,a,value,from,the,backing,store,optionally,with,a,default,.,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/document.py#L246-L252,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/query/query.py,Q._op,"def _op(self, operation, other, *allowed):
		""""""A basic operation operating on a single value.""""""
		
		f = self._field
		
		if self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).
			return reduce(self._combining,
					(q._op(operation, other, *allowed) for q in f))  # pylint:disable=protected-access
		
		# Optimize this away in production; diagnosic aide.
		if __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover
			raise NotImplementedError(""{self!r} does not allow {op} comparison."".format(self=self, op=operation))
		
		if other is not None:
			other = f.transformer.foreign(other, (f, self._document))
		
		return Filter({self._name: {operation: other}})",python,"def _op(self, operation, other, *allowed):
		""""""A basic operation operating on a single value.""""""
		
		f = self._field
		
		if self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).
			return reduce(self._combining,
					(q._op(operation, other, *allowed) for q in f))  # pylint:disable=protected-access
		
		# Optimize this away in production; diagnosic aide.
		if __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover
			raise NotImplementedError(""{self!r} does not allow {op} comparison."".format(self=self, op=operation))
		
		if other is not None:
			other = f.transformer.foreign(other, (f, self._document))
		
		return Filter({self._name: {operation: other}})",def,_op,(,self,",",operation,",",other,",",*,allowed,),:,f,=,self,.,_field,if,self,.,_combining,:,"# We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).",return,reduce,(,self,.,_combining,",",(,q,.,_op,(,operation,",",other,",",*,allowed,),A basic operation operating on a single value.,A,basic,operation,operating,on,a,single,value,.,,,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/query/query.py#L154-L170,train,for,q,in,f,),),# pylint:disable=protected-access,# Optimize this away in production; diagnosic aide.,if,__debug__,and,_complex_safety_check,(,f,",",{,operation,},|,set,(,allowed,),),:,# pragma: no cover,raise,NotImplementedError,(,"""{self!r} does not allow {op} comparison.""",,,,,,,,,,,,,,,,,,,,.,format,(,self,=,self,",",op,=,operation,),),if,other,is,not,None,:,other,=,f,.,transformer,.,foreign,(,other,",",(,f,",",self,.,_document,),),return,Filter,(,{,self,.,_name,:,{,operation,:,other,},},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/query/query.py,Q._iop,"def _iop(self, operation, other, *allowed):
		""""""An iterative operation operating on multiple values.
		
		Consumes iterators to construct a concrete list at time of execution.
		""""""
		
		f = self._field
		
		if self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).
			return reduce(self._combining,
					(q._iop(operation, other, *allowed) for q in f))  # pylint:disable=protected-access
		
		# Optimize this away in production; diagnosic aide.
		if __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover
			raise NotImplementedError(""{self!r} does not allow {op} comparison."".format(
					self=self, op=operation))
		
		def _t(o):
			for value in o:
				yield None if value is None else f.transformer.foreign(value, (f, self._document))
		
		other = other if len(other) > 1 else other[0]
		values = list(_t(other))
		
		return Filter({self._name: {operation: values}})",python,"def _iop(self, operation, other, *allowed):
		""""""An iterative operation operating on multiple values.
		
		Consumes iterators to construct a concrete list at time of execution.
		""""""
		
		f = self._field
		
		if self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).
			return reduce(self._combining,
					(q._iop(operation, other, *allowed) for q in f))  # pylint:disable=protected-access
		
		# Optimize this away in production; diagnosic aide.
		if __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover
			raise NotImplementedError(""{self!r} does not allow {op} comparison."".format(
					self=self, op=operation))
		
		def _t(o):
			for value in o:
				yield None if value is None else f.transformer.foreign(value, (f, self._document))
		
		other = other if len(other) > 1 else other[0]
		values = list(_t(other))
		
		return Filter({self._name: {operation: values}})",def,_iop,(,self,",",operation,",",other,",",*,allowed,),:,f,=,self,.,_field,if,self,.,_combining,:,"# We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).",return,reduce,(,self,.,_combining,",",(,q,.,_iop,(,operation,",",other,",",*,allowed,),"An iterative operation operating on multiple values.
		
		Consumes iterators to construct a concrete list at time of execution.",An,iterative,operation,operating,on,multiple,values,.,Consumes,iterators,to,construct,a,concrete,list,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/query/query.py#L172-L196,train,for,q,in,f,),),# pylint:disable=protected-access,# Optimize this away in production; diagnosic aide.,if,__debug__,and,_complex_safety_check,(,f,",",{,operation,},|,set,(,allowed,),),:,# pragma: no cover,raise,NotImplementedError,(,"""{self!r} does not allow {op} comparison.""",at,time,of,execution,.,,,,,,,,,,,,,,,.,format,(,self,=,self,",",op,=,operation,),),def,_t,(,o,),:,for,value,in,o,:,yield,None,if,value,is,None,else,f,.,transformer,.,foreign,(,value,",",(,f,",",self,.,_document,),),other,=,other,if,len,(,other,),>,1,else,other,[,0,],values,=,list,(,_t,(,other,),),return,Filter,(,{,self,.,_name,:,{,operation,:,values,},},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sofiatolaosebikan/hopcroftkarp,hopcroftkarp/__init__.py,HopcroftKarp.__dfs,"def __dfs(self, v, index, layers):
        """"""
        we recursively run dfs on each vertices in free_vertex,

        :param v: vertices in free_vertex
        :return: True if P is not empty (i.e., the maximal set of vertex-disjoint alternating path of length k)
        and false otherwise.
        """"""
        if index == 0:
            path = [v]
            while self._dfs_parent[v] != v:
                path.append(self._dfs_parent[v])
                v = self._dfs_parent[v]
            self._dfs_paths.append(path)
            return True
        for neighbour in self._graph[v]:  # check the neighbours of vertex
            if neighbour in layers[index - 1]:
                # if neighbour is in left, we are traversing unmatched edges..
                if neighbour in self._dfs_parent:
                    continue
                if (neighbour in self._left and (v not in self._matching or neighbour != self._matching[v])) or \
                        (neighbour in self._right and (v in self._matching and neighbour == self._matching[v])):
                    self._dfs_parent[neighbour] = v
                    if self.__dfs(neighbour, index-1, layers):
                        return True
        return False",python,"def __dfs(self, v, index, layers):
        """"""
        we recursively run dfs on each vertices in free_vertex,

        :param v: vertices in free_vertex
        :return: True if P is not empty (i.e., the maximal set of vertex-disjoint alternating path of length k)
        and false otherwise.
        """"""
        if index == 0:
            path = [v]
            while self._dfs_parent[v] != v:
                path.append(self._dfs_parent[v])
                v = self._dfs_parent[v]
            self._dfs_paths.append(path)
            return True
        for neighbour in self._graph[v]:  # check the neighbours of vertex
            if neighbour in layers[index - 1]:
                # if neighbour is in left, we are traversing unmatched edges..
                if neighbour in self._dfs_parent:
                    continue
                if (neighbour in self._left and (v not in self._matching or neighbour != self._matching[v])) or \
                        (neighbour in self._right and (v in self._matching and neighbour == self._matching[v])):
                    self._dfs_parent[neighbour] = v
                    if self.__dfs(neighbour, index-1, layers):
                        return True
        return False",def,__dfs,(,self,",",v,",",index,",",layers,),:,if,index,==,0,:,path,=,[,v,],while,self,.,_dfs_parent,[,v,],!=,v,:,path,.,append,(,self,.,_dfs_parent,[,v,],),"we recursively run dfs on each vertices in free_vertex,

        :param v: vertices in free_vertex
        :return: True if P is not empty (i.e., the maximal set of vertex-disjoint alternating path of length k)
        and false otherwise.",we,recursively,run,dfs,on,each,vertices,in,free_vertex,,,,,,,5e6cf4f95702304847307a07d369f8041edff8c9,https://github.com/sofiatolaosebikan/hopcroftkarp/blob/5e6cf4f95702304847307a07d369f8041edff8c9/hopcroftkarp/__init__.py#L84-L109,train,v,=,self,.,_dfs_parent,[,v,],self,.,_dfs_paths,.,append,(,path,),return,True,for,neighbour,in,self,.,_graph,[,v,],:,# check the neighbours of vertex,if,,,,,,,,,,,,,,,,,,,,neighbour,in,layers,[,index,-,1,],:,"# if neighbour is in left, we are traversing unmatched edges..",if,neighbour,in,self,.,_dfs_parent,:,continue,if,(,neighbour,in,self,.,_left,and,(,v,not,in,self,.,_matching,or,neighbour,!=,self,.,_matching,[,v,],),),or,(,neighbour,in,self,.,_right,and,(,v,in,self,.,_matching,and,neighbour,==,self,.,_matching,[,v,],),),:,self,.,_dfs_parent,[,neighbour,],=,v,if,self,.,__dfs,(,neighbour,",",index,-,1,",",layers,),:,return,True,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tehmaze/parser,parser/base.py,Parser.method,"def method(self, symbol):
        '''
        Symbol decorator.
        '''
        assert issubclass(symbol, SymbolBase)
        def wrapped(fn):
            setattr(symbol, fn.__name__, fn)
        return wrapped",python,"def method(self, symbol):
        '''
        Symbol decorator.
        '''
        assert issubclass(symbol, SymbolBase)
        def wrapped(fn):
            setattr(symbol, fn.__name__, fn)
        return wrapped",def,method,(,self,",",symbol,),:,assert,issubclass,(,symbol,",",SymbolBase,),def,wrapped,(,fn,),:,setattr,(,symbol,",",fn,.,__name__,",",fn,),return,wrapped,,,,,,,,,,,Symbol decorator.,Symbol,decorator,.,,,,,,,,,,,,,ccc69236304b2f00671f14c62433e8830b838101,https://github.com/tehmaze/parser/blob/ccc69236304b2f00671f14c62433e8830b838101/parser/base.py#L69-L76,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
antoniobotelho/py-business-calendar,business_calendar/business_calendar.py,_simpleparsefun,"def _simpleparsefun(date):
    """"""Simple date parsing function""""""
    if hasattr(date, 'year'):
        return date
    try:
        date = datetime.datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        date = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')
    return date",python,"def _simpleparsefun(date):
    """"""Simple date parsing function""""""
    if hasattr(date, 'year'):
        return date
    try:
        date = datetime.datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        date = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')
    return date",def,_simpleparsefun,(,date,),:,if,hasattr,(,date,",",'year',),:,return,date,try,:,date,=,datetime,.,datetime,.,strptime,(,date,",",'%Y-%m-%d',),except,ValueError,:,date,=,datetime,.,datetime,.,strptime,(,date,",",Simple date parsing function,Simple,date,parsing,function,,,,,,,,,,,,92365fbddd043e41e33b01f1ddd9dd6a5094c031,https://github.com/antoniobotelho/py-business-calendar/blob/92365fbddd043e41e33b01f1ddd9dd6a5094c031/business_calendar/business_calendar.py#L63-L71,train,'%Y-%m-%d %H:%M:%S',),return,date,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/mixins.py,KnockerModel._connect,"def _connect(cls):
        """"""
        Connect signal to current model
        """"""
        post_save.connect(
            notify_items, sender=cls,
            dispatch_uid='knocker_{0}'.format(cls.__name__)
        )",python,"def _connect(cls):
        """"""
        Connect signal to current model
        """"""
        post_save.connect(
            notify_items, sender=cls,
            dispatch_uid='knocker_{0}'.format(cls.__name__)
        )",def,_connect,(,cls,),:,post_save,.,connect,(,notify_items,",",sender,=,cls,",",dispatch_uid,=,'knocker_{0}',.,format,(,cls,.,__name__,),),,,,,,,,,,,,,,,,,Connect signal to current model,Connect,signal,to,current,model,,,,,,,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/mixins.py#L31-L38,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/mixins.py,KnockerModel._disconnect,"def _disconnect(cls):
        """"""
        Disconnect signal from current model
        """"""
        post_save.disconnect(
            notify_items, sender=cls,
            dispatch_uid='knocker_{0}'.format(cls.__name__)
        )",python,"def _disconnect(cls):
        """"""
        Disconnect signal from current model
        """"""
        post_save.disconnect(
            notify_items, sender=cls,
            dispatch_uid='knocker_{0}'.format(cls.__name__)
        )",def,_disconnect,(,cls,),:,post_save,.,disconnect,(,notify_items,",",sender,=,cls,",",dispatch_uid,=,'knocker_{0}',.,format,(,cls,.,__name__,),),,,,,,,,,,,,,,,,,Disconnect signal from current model,Disconnect,signal,from,current,model,,,,,,,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/mixins.py#L41-L48,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/mixins.py,KnockerModel.as_knock,"def as_knock(self, created=False):
        """"""
        Returns a dictionary with the knock data built from _knocker_data
        """"""
        knock = {}
        if self.should_knock(created):
            for field, data in self._retrieve_data(None, self._knocker_data):
                knock[field] = data
        return knock",python,"def as_knock(self, created=False):
        """"""
        Returns a dictionary with the knock data built from _knocker_data
        """"""
        knock = {}
        if self.should_knock(created):
            for field, data in self._retrieve_data(None, self._knocker_data):
                knock[field] = data
        return knock",def,as_knock,(,self,",",created,=,False,),:,knock,=,{,},if,self,.,should_knock,(,created,),:,for,field,",",data,in,self,.,_retrieve_data,(,None,",",self,.,_knocker_data,),:,knock,[,field,],=,Returns a dictionary with the knock data built from _knocker_data,Returns,a,dictionary,with,the,knock,data,built,from,_knocker_data,,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/mixins.py#L97-L105,train,data,return,knock,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/mixins.py,KnockerModel.send_knock,"def send_knock(self, created=False):
        """"""
        Send the knock in the associated channels Group
        """"""
        knock = self.as_knock(created)
        if knock:
            gr = Group('knocker-{0}'.format(knock['language']))
            gr.send({'text': json.dumps(knock)})",python,"def send_knock(self, created=False):
        """"""
        Send the knock in the associated channels Group
        """"""
        knock = self.as_knock(created)
        if knock:
            gr = Group('knocker-{0}'.format(knock['language']))
            gr.send({'text': json.dumps(knock)})",def,send_knock,(,self,",",created,=,False,),:,knock,=,self,.,as_knock,(,created,),if,knock,:,gr,=,Group,(,'knocker-{0}',.,format,(,knock,[,'language',],),),gr,.,send,(,{,'text',:,json,Send the knock in the associated channels Group,Send,the,knock,in,the,associated,channels,Group,,,,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/mixins.py#L107-L114,train,.,dumps,(,knock,),},),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/color.py,colorize,"def colorize(printable, color, style='normal', autoreset=True):
    """"""Colorize some message with ANSI colors specification

    :param printable: interface whose has __str__ or __repr__ method
    :param color: the colors defined in COLOR_MAP to colorize the text
    :style: can be 'normal', 'bold' or 'underline'

    :returns: the 'printable' colorized with style
    """"""
    if not COLORED:  # disable color
        return printable
    if color not in COLOR_MAP:
        raise RuntimeError('invalid color set, no {}'.format(color))

    return '{color}{printable}{reset}'.format(
        printable=printable,
        color=COLOR_MAP[color].format(style=STYLE_MAP[style]),
        reset=COLOR_MAP['reset'] if autoreset else ''
    )",python,"def colorize(printable, color, style='normal', autoreset=True):
    """"""Colorize some message with ANSI colors specification

    :param printable: interface whose has __str__ or __repr__ method
    :param color: the colors defined in COLOR_MAP to colorize the text
    :style: can be 'normal', 'bold' or 'underline'

    :returns: the 'printable' colorized with style
    """"""
    if not COLORED:  # disable color
        return printable
    if color not in COLOR_MAP:
        raise RuntimeError('invalid color set, no {}'.format(color))

    return '{color}{printable}{reset}'.format(
        printable=printable,
        color=COLOR_MAP[color].format(style=STYLE_MAP[style]),
        reset=COLOR_MAP['reset'] if autoreset else ''
    )",def,colorize,(,printable,",",color,",",style,=,'normal',",",autoreset,=,True,),:,if,not,COLORED,:,# disable color,return,printable,if,color,not,in,COLOR_MAP,:,raise,RuntimeError,(,"'invalid color set, no {}'",.,format,(,color,),),return,'{color}{printable}{reset}',.,format,"Colorize some message with ANSI colors specification

    :param printable: interface whose has __str__ or __repr__ method
    :param color: the colors defined in COLOR_MAP to colorize the text
    :style: can be 'normal', 'bold' or 'underline'

    :returns: the 'printable' colorized with style",Colorize,some,message,with,ANSI,colors,specification,,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/color.py#L45-L63,train,(,printable,=,printable,",",color,=,COLOR_MAP,[,color,],.,format,(,style,=,STYLE_MAP,[,style,],),",",reset,=,COLOR_MAP,[,'reset',],if,autoreset,,,,,,,,,,,,,,,,,,,,else,'',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FortyNorthSecurity/Hasher,hashes/common/helpers.py,color,"def color(string, status=True, warning=False, bold=True):
    """"""
    Change text color for the linux terminal, defaults to green.
    Set ""warning=True"" for red.
    """"""
    attr = []
    if status:
        # green
        attr.append('32')
    if warning:
        # red
        attr.append('31')
    if bold:
        attr.append('1')
    return '\x1b[%sm%s\x1b[0m' % (';'.join(attr), string)",python,"def color(string, status=True, warning=False, bold=True):
    """"""
    Change text color for the linux terminal, defaults to green.
    Set ""warning=True"" for red.
    """"""
    attr = []
    if status:
        # green
        attr.append('32')
    if warning:
        # red
        attr.append('31')
    if bold:
        attr.append('1')
    return '\x1b[%sm%s\x1b[0m' % (';'.join(attr), string)",def,color,(,string,",",status,=,True,",",warning,=,False,",",bold,=,True,),:,attr,=,[,],if,status,:,# green,attr,.,append,(,'32',),if,warning,:,# red,attr,.,append,(,'31',),if,"Change text color for the linux terminal, defaults to green.
    Set ""warning=True"" for red.",Change,text,color,for,the,linux,terminal,defaults,to,green,.,Set,warning,=,True,40173c56b36680ab1ddc57a9c13c36b3a1ec51c3,https://github.com/FortyNorthSecurity/Hasher/blob/40173c56b36680ab1ddc57a9c13c36b3a1ec51c3/hashes/common/helpers.py#L10-L24,train,bold,:,attr,.,append,(,'1',),return,'\x1b[%sm%s\x1b[0m',%,(,';',.,join,(,attr,),",",string,),,,,,,,,,,for,red,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/util/capped.py,_patch,"def _patch():
	""""""Patch pymongo's Collection object to add a tail method.
	
	While not nessicarily recommended, you can use this to inject `tail` as a method into Collection, making it
	generally accessible.
	""""""
	
	if not __debug__:  # pragma: no cover
		import warnings
		warnings.warn(""A catgirl has died."", ImportWarning)
	
	from pymongo.collection import Collection
	Collection.tail = tail",python,"def _patch():
	""""""Patch pymongo's Collection object to add a tail method.
	
	While not nessicarily recommended, you can use this to inject `tail` as a method into Collection, making it
	generally accessible.
	""""""
	
	if not __debug__:  # pragma: no cover
		import warnings
		warnings.warn(""A catgirl has died."", ImportWarning)
	
	from pymongo.collection import Collection
	Collection.tail = tail",def,_patch,(,),:,if,not,__debug__,:,# pragma: no cover,import,warnings,warnings,.,warn,(,"""A catgirl has died.""",",",ImportWarning,),from,pymongo,.,collection,import,Collection,Collection,.,tail,=,tail,,,,,,,,,,,,,"Patch pymongo's Collection object to add a tail method.
	
	While not nessicarily recommended, you can use this to inject `tail` as a method into Collection, making it
	generally accessible.",Patch,pymongo,s,Collection,object,to,add,a,tail,method,.,While,not,nessicarily,recommended,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/util/capped.py#L49-L61,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,you,can,use,this,to,inject,tail,as,a,method,into,Collection,making,it,generally,accessible,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/queryable.py,Queryable._prepare_find,"def _prepare_find(cls, *args, **kw):
		""""""Execute a find and return the resulting queryset using combined plain and parametric query generation.
		
		Additionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.
		""""""
		
		cls, collection, query, options = cls._prepare_query(
				cls.FIND_MAPPING,
				cls.FIND_OPTIONS,
				*args,
				**kw
			)
		
		if 'await' in options:
			raise TypeError(""Await is hard-deprecated as reserved keyword in Python 3.7, use wait instead."")
		
		if 'cursor_type' in options and {'tail', 'wait'} & set(options):
			raise TypeError(""Can not combine cursor_type and tail/wait arguments."")
		
		elif options.pop('tail', False):
			options['cursor_type'] = CursorType.TAILABLE_AWAIT if options.pop('wait', True) else CursorType.TAILABLE
		
		elif 'wait' in options:
			raise TypeError(""Wait option only applies to tailing cursors."")
		
		modifiers = options.get('modifiers', dict())
		
		if 'max_time_ms' in options:
			modifiers['$maxTimeMS'] = options.pop('max_time_ms')
		
		if modifiers:
			options['modifiers'] = modifiers
		
		return cls, collection, query, options",python,"def _prepare_find(cls, *args, **kw):
		""""""Execute a find and return the resulting queryset using combined plain and parametric query generation.
		
		Additionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.
		""""""
		
		cls, collection, query, options = cls._prepare_query(
				cls.FIND_MAPPING,
				cls.FIND_OPTIONS,
				*args,
				**kw
			)
		
		if 'await' in options:
			raise TypeError(""Await is hard-deprecated as reserved keyword in Python 3.7, use wait instead."")
		
		if 'cursor_type' in options and {'tail', 'wait'} & set(options):
			raise TypeError(""Can not combine cursor_type and tail/wait arguments."")
		
		elif options.pop('tail', False):
			options['cursor_type'] = CursorType.TAILABLE_AWAIT if options.pop('wait', True) else CursorType.TAILABLE
		
		elif 'wait' in options:
			raise TypeError(""Wait option only applies to tailing cursors."")
		
		modifiers = options.get('modifiers', dict())
		
		if 'max_time_ms' in options:
			modifiers['$maxTimeMS'] = options.pop('max_time_ms')
		
		if modifiers:
			options['modifiers'] = modifiers
		
		return cls, collection, query, options",def,_prepare_find,(,cls,",",*,args,",",*,*,kw,),:,cls,",",collection,",",query,",",options,=,cls,.,_prepare_query,(,cls,.,FIND_MAPPING,",",cls,.,FIND_OPTIONS,",",*,args,",",*,*,kw,),if,'await',in,"Execute a find and return the resulting queryset using combined plain and parametric query generation.
		
		Additionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.",Execute,a,find,and,return,the,resulting,queryset,using,combined,plain,and,parametric,query,generation,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/queryable.py#L113-L146,train,options,:,raise,TypeError,(,"""Await is hard-deprecated as reserved keyword in Python 3.7, use wait instead.""",),if,'cursor_type',in,options,and,{,'tail',",",'wait',},&,set,(,options,),:,raise,TypeError,(,"""Can not combine cursor_type and tail/wait arguments.""",),elif,options,.,Additionally,performs,argument,case,normalization,refer,to,the,_prepare_query,method,s,docstring,.,,,,,,.,pop,(,'tail',",",False,),:,options,[,'cursor_type',],=,CursorType,.,TAILABLE_AWAIT,if,options,.,pop,(,'wait',",",True,),else,CursorType,.,TAILABLE,elif,'wait',in,options,:,raise,TypeError,(,"""Wait option only applies to tailing cursors.""",),modifiers,=,options,.,get,(,'modifiers',",",dict,(,),),if,'max_time_ms',in,options,:,modifiers,[,'$maxTimeMS',],=,options,.,pop,(,'max_time_ms',),if,modifiers,:,options,[,'modifiers',],=,modifiers,return,cls,",",collection,",",query,",",options,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/queryable.py,Queryable.reload,"def reload(self, *fields, **kw):
		""""""Reload the entire document from the database, or refresh specific named top-level fields.""""""
		
		Doc, collection, query, options = self._prepare_find(id=self.id, projection=fields, **kw)
		result = collection.find_one(query, **options)
		
		if fields:  # Refresh only the requested data.
			for k in result:  # TODO: Better merge algorithm.
				if k == ~Doc.id: continue
				self.__data__[k] = result[k]
		else:
			self.__data__ = result
		
		return self",python,"def reload(self, *fields, **kw):
		""""""Reload the entire document from the database, or refresh specific named top-level fields.""""""
		
		Doc, collection, query, options = self._prepare_find(id=self.id, projection=fields, **kw)
		result = collection.find_one(query, **options)
		
		if fields:  # Refresh only the requested data.
			for k in result:  # TODO: Better merge algorithm.
				if k == ~Doc.id: continue
				self.__data__[k] = result[k]
		else:
			self.__data__ = result
		
		return self",def,reload,(,self,",",*,fields,",",*,*,kw,),:,Doc,",",collection,",",query,",",options,=,self,.,_prepare_find,(,id,=,self,.,id,",",projection,=,fields,",",*,*,kw,),result,=,collection,.,"Reload the entire document from the database, or refresh specific named top-level fields.",Reload,the,entire,document,from,the,database,or,refresh,specific,named,top,-,level,fields,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/queryable.py#L271-L284,train,find_one,(,query,",",*,*,options,),if,fields,:,# Refresh only the requested data.,for,k,in,result,:,# TODO: Better merge algorithm.,if,k,==,~,Doc,.,id,:,continue,self,.,__data__,.,,,,,,,,,,,,,,,,,,,[,k,],=,result,[,k,],else,:,self,.,__data__,=,result,return,self,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
andrasmaroy/pconf,pconf/pconf.py,Pconf.get,"def get(cls):
        """"""Get values gathered from the previously set hierarchy.

        Respects the order in which sources are set, the first source set
        has the highest priority, overrides values with the same key that
        exist in sources with lower priority.

        Returns:
            dict: The dictionary containing values gathered from all set sources.
        """"""
        results = {}

        hierarchy = cls.__hierarchy
        hierarchy.reverse()

        for storeMethod in hierarchy:
            cls.merger.merge(results, storeMethod.get())

        return results",python,"def get(cls):
        """"""Get values gathered from the previously set hierarchy.

        Respects the order in which sources are set, the first source set
        has the highest priority, overrides values with the same key that
        exist in sources with lower priority.

        Returns:
            dict: The dictionary containing values gathered from all set sources.
        """"""
        results = {}

        hierarchy = cls.__hierarchy
        hierarchy.reverse()

        for storeMethod in hierarchy:
            cls.merger.merge(results, storeMethod.get())

        return results",def,get,(,cls,),:,results,=,{,},hierarchy,=,cls,.,__hierarchy,hierarchy,.,reverse,(,),for,storeMethod,in,hierarchy,:,cls,.,merger,.,merge,(,results,",",storeMethod,.,get,(,),),return,results,,,"Get values gathered from the previously set hierarchy.

        Respects the order in which sources are set, the first source set
        has the highest priority, overrides values with the same key that
        exist in sources with lower priority.

        Returns:
            dict: The dictionary containing values gathered from all set sources.",Get,values,gathered,from,the,previously,set,hierarchy,.,,,,,,,1f930bf4e88bf8b4732fcc95557c66f3608b8821,https://github.com/andrasmaroy/pconf/blob/1f930bf4e88bf8b4732fcc95557c66f3608b8821/pconf/pconf.py#L27-L45,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
andrasmaroy/pconf,pconf/pconf.py,Pconf.argv,"def argv(cls, name, short_name=None, type=None, help=None):
        """""" Set command line arguments as a source

        Parses the command line arguments described by the parameters.

        Args:
            name: the long name of the argument (foo)
            short_name: the optional short name of the argument (f)
            type: the optional type of the argument, defaults to bool
            help: the optional help text for the argument
        """"""
        cls.__hierarchy.append(argv.Argv(name, short_name, type, help))",python,"def argv(cls, name, short_name=None, type=None, help=None):
        """""" Set command line arguments as a source

        Parses the command line arguments described by the parameters.

        Args:
            name: the long name of the argument (foo)
            short_name: the optional short name of the argument (f)
            type: the optional type of the argument, defaults to bool
            help: the optional help text for the argument
        """"""
        cls.__hierarchy.append(argv.Argv(name, short_name, type, help))",def,argv,(,cls,",",name,",",short_name,=,None,",",type,=,None,",",help,=,None,),:,cls,.,__hierarchy,.,append,(,argv,.,Argv,(,name,",",short_name,",",type,",",help,),),,,,,"Set command line arguments as a source

        Parses the command line arguments described by the parameters.

        Args:
            name: the long name of the argument (foo)
            short_name: the optional short name of the argument (f)
            type: the optional type of the argument, defaults to bool
            help: the optional help text for the argument",Set,command,line,arguments,as,a,source,,,,,,,,,1f930bf4e88bf8b4732fcc95557c66f3608b8821,https://github.com/andrasmaroy/pconf/blob/1f930bf4e88bf8b4732fcc95557c66f3608b8821/pconf/pconf.py#L72-L83,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
andrasmaroy/pconf,pconf/pconf.py,Pconf.env,"def env(cls, separator=None, match=None, whitelist=None, parse_values=None, to_lower=None, convert_underscores=None):
        """"""Set environment variables as a source.

        By default all environment variables available to the process are used.
        This can be narrowed by the args.

        Args:
            separator: Keys are split along this character, the resulting
                splits are considered nested values.
            match: Regular expression for key matching. Keys matching the
                expression are considered whitelisted.
            whitelist: Only use environment variables that are listed in this
                list.
            parse_values: Try to parse all variable for well-known types.
            to_lower: Convert all variable names to lower case.
            convert_underscores: Convert all underscores in the name to dashes,
                this takes place after separation via the separator option.
        """"""
        cls.__hierarchy.append(env.Env(separator, match, whitelist, parse_values, to_lower, convert_underscores))",python,"def env(cls, separator=None, match=None, whitelist=None, parse_values=None, to_lower=None, convert_underscores=None):
        """"""Set environment variables as a source.

        By default all environment variables available to the process are used.
        This can be narrowed by the args.

        Args:
            separator: Keys are split along this character, the resulting
                splits are considered nested values.
            match: Regular expression for key matching. Keys matching the
                expression are considered whitelisted.
            whitelist: Only use environment variables that are listed in this
                list.
            parse_values: Try to parse all variable for well-known types.
            to_lower: Convert all variable names to lower case.
            convert_underscores: Convert all underscores in the name to dashes,
                this takes place after separation via the separator option.
        """"""
        cls.__hierarchy.append(env.Env(separator, match, whitelist, parse_values, to_lower, convert_underscores))",def,env,(,cls,",",separator,=,None,",",match,=,None,",",whitelist,=,None,",",parse_values,=,None,",",to_lower,=,None,",",convert_underscores,=,None,),:,cls,.,__hierarchy,.,append,(,env,.,Env,(,separator,",",match,"Set environment variables as a source.

        By default all environment variables available to the process are used.
        This can be narrowed by the args.

        Args:
            separator: Keys are split along this character, the resulting
                splits are considered nested values.
            match: Regular expression for key matching. Keys matching the
                expression are considered whitelisted.
            whitelist: Only use environment variables that are listed in this
                list.
            parse_values: Try to parse all variable for well-known types.
            to_lower: Convert all variable names to lower case.
            convert_underscores: Convert all underscores in the name to dashes,
                this takes place after separation via the separator option.",Set,environment,variables,as,a,source,.,,,,,,,,,1f930bf4e88bf8b4732fcc95557c66f3608b8821,https://github.com/andrasmaroy/pconf/blob/1f930bf4e88bf8b4732fcc95557c66f3608b8821/pconf/pconf.py#L86-L104,train,",",whitelist,",",parse_values,",",to_lower,",",convert_underscores,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
andrasmaroy/pconf,pconf/pconf.py,Pconf.file,"def file(cls, path, encoding=None, parser=None):
        """"""Set a file as a source.

        File are parsed as literal python dicts by default, this behaviour
        can be configured.

        Args:
            path: The path to the file to be parsed
            encoding: The encoding of the file.
                Defaults to 'raw'. Available built-in values: 'ini', 'json', 'yaml'.
                Custom value can be used in conjunction with parser.
            parser: A parser function for a custom encoder.
                It is expected to return a dict containing the parsed values
                when called with the contents of the file as an argument.
        """"""
        cls.__hierarchy.append(file.File(path, encoding, parser))",python,"def file(cls, path, encoding=None, parser=None):
        """"""Set a file as a source.

        File are parsed as literal python dicts by default, this behaviour
        can be configured.

        Args:
            path: The path to the file to be parsed
            encoding: The encoding of the file.
                Defaults to 'raw'. Available built-in values: 'ini', 'json', 'yaml'.
                Custom value can be used in conjunction with parser.
            parser: A parser function for a custom encoder.
                It is expected to return a dict containing the parsed values
                when called with the contents of the file as an argument.
        """"""
        cls.__hierarchy.append(file.File(path, encoding, parser))",def,file,(,cls,",",path,",",encoding,=,None,",",parser,=,None,),:,cls,.,__hierarchy,.,append,(,file,.,File,(,path,",",encoding,",",parser,),),,,,,,,,,,,"Set a file as a source.

        File are parsed as literal python dicts by default, this behaviour
        can be configured.

        Args:
            path: The path to the file to be parsed
            encoding: The encoding of the file.
                Defaults to 'raw'. Available built-in values: 'ini', 'json', 'yaml'.
                Custom value can be used in conjunction with parser.
            parser: A parser function for a custom encoder.
                It is expected to return a dict containing the parsed values
                when called with the contents of the file as an argument.",Set,a,file,as,a,source,.,,,,,,,,,1f930bf4e88bf8b4732fcc95557c66f3608b8821,https://github.com/andrasmaroy/pconf/blob/1f930bf4e88bf8b4732fcc95557c66f3608b8821/pconf/pconf.py#L107-L122,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/param/project.py,P,"def P(Document, *fields, **kw):
	""""""Generate a MongoDB projection dictionary using the Django ORM style.""""""
	
	__always__ = kw.pop('__always__', set())
	projected = set()
	omitted = set()
	
	for field in fields:
		if field[0] in ('-', '!'):
			omitted.add(field[1:])
		elif field[0] == '+':
			projected.add(field[1:])
		else:
			projected.add(field)
	
	if not projected:  # We only have exclusions from the default projection.
		names = set(getattr(Document, '__projection__', Document.__fields__) or Document.__fields__)
		projected = {name for name in (names - omitted)}
	
	projected |= __always__
	
	if not projected:
		projected = {'_id'}
	
	return {unicode(traverse(Document, name, name)): True for name in projected}",python,"def P(Document, *fields, **kw):
	""""""Generate a MongoDB projection dictionary using the Django ORM style.""""""
	
	__always__ = kw.pop('__always__', set())
	projected = set()
	omitted = set()
	
	for field in fields:
		if field[0] in ('-', '!'):
			omitted.add(field[1:])
		elif field[0] == '+':
			projected.add(field[1:])
		else:
			projected.add(field)
	
	if not projected:  # We only have exclusions from the default projection.
		names = set(getattr(Document, '__projection__', Document.__fields__) or Document.__fields__)
		projected = {name for name in (names - omitted)}
	
	projected |= __always__
	
	if not projected:
		projected = {'_id'}
	
	return {unicode(traverse(Document, name, name)): True for name in projected}",def,P,(,Document,",",*,fields,",",*,*,kw,),:,__always__,=,kw,.,pop,(,'__always__',",",set,(,),),projected,=,set,(,),omitted,=,set,(,),for,field,in,fields,:,if,field,[,Generate a MongoDB projection dictionary using the Django ORM style.,Generate,a,MongoDB,projection,dictionary,using,the,Django,ORM,style,.,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/param/project.py#L11-L35,train,0,],in,(,'-',",",'!',),:,omitted,.,add,(,field,[,1,:,],),elif,field,[,0,],==,'+',:,projected,.,add,,,,,,,,,,,,,,,,,,,,(,field,[,1,:,],),else,:,projected,.,add,(,field,),if,not,projected,:,# We only have exclusions from the default projection.,names,=,set,(,getattr,(,Document,",",'__projection__',",",Document,.,__fields__,),or,Document,.,__fields__,),projected,=,{,name,for,name,in,(,names,-,omitted,),},projected,|=,__always__,if,not,projected,:,projected,=,{,'_id',},return,{,unicode,(,traverse,(,Document,",",name,",",name,),),:,True,for,name,in,projected,},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,web/session/mongo.py,MongoSession.is_valid,"def is_valid(self, context, sid):
		""""""Identify if the given session ID is currently valid.
		
		Return True if valid, False if explicitly invalid, None if unknown.
		""""""
		
		record = self._Document.find_one(sid, project=('expires', ))
		
		if not record:
			return
		
		return not record._expired",python,"def is_valid(self, context, sid):
		""""""Identify if the given session ID is currently valid.
		
		Return True if valid, False if explicitly invalid, None if unknown.
		""""""
		
		record = self._Document.find_one(sid, project=('expires', ))
		
		if not record:
			return
		
		return not record._expired",def,is_valid,(,self,",",context,",",sid,),:,record,=,self,.,_Document,.,find_one,(,sid,",",project,=,(,'expires',",",),),if,not,record,:,return,return,not,record,.,_expired,,,,,,,"Identify if the given session ID is currently valid.
		
		Return True if valid, False if explicitly invalid, None if unknown.",Identify,if,the,given,session,ID,is,currently,valid,.,Return,True,if,valid,False,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/web/session/mongo.py#L55-L66,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,if,explicitly,invalid,None,if,unknown,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,web/session/mongo.py,MongoSession.invalidate,"def invalidate(self, context, sid):
		""""""Immediately expire a session from the backing store.""""""
		
		result = self._Document.get_collection().delete_one({'_id': sid})
		
		return result.deleted_count == 1",python,"def invalidate(self, context, sid):
		""""""Immediately expire a session from the backing store.""""""
		
		result = self._Document.get_collection().delete_one({'_id': sid})
		
		return result.deleted_count == 1",def,invalidate,(,self,",",context,",",sid,),:,result,=,self,.,_Document,.,get_collection,(,),.,delete_one,(,{,'_id',:,sid,},),return,result,.,deleted_count,==,1,,,,,,,,,,Immediately expire a session from the backing store.,Immediately,expire,a,session,from,the,backing,store,.,,,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/web/session/mongo.py#L68-L73,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,web/session/mongo.py,MongoSession.persist,"def persist(self, context):
		""""""Update or insert the session document into the configured collection""""""
		
		D = self._Document
		document = context.session[self.name]
		
		D.get_collection().replace_one(D.id == document.id, document, True)",python,"def persist(self, context):
		""""""Update or insert the session document into the configured collection""""""
		
		D = self._Document
		document = context.session[self.name]
		
		D.get_collection().replace_one(D.id == document.id, document, True)",def,persist,(,self,",",context,),:,D,=,self,.,_Document,document,=,context,.,session,[,self,.,name,],D,.,get_collection,(,),.,replace_one,(,D,.,id,==,document,.,id,",",document,",",True,),Update or insert the session document into the configured collection,Update,or,insert,the,session,document,into,the,configured,collection,,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/web/session/mongo.py#L92-L98,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/consumers.py,ws_connect,"def ws_connect(message):
    """"""
    Channels connection setup.
    Register the current client on the related Group according to the language
    """"""
    prefix, language = message['path'].strip('/').split('/')
    gr = Group('knocker-{0}'.format(language))
    gr.add(message.reply_channel)
    message.channel_session['knocker'] = language
    message.reply_channel.send({""accept"": True})",python,"def ws_connect(message):
    """"""
    Channels connection setup.
    Register the current client on the related Group according to the language
    """"""
    prefix, language = message['path'].strip('/').split('/')
    gr = Group('knocker-{0}'.format(language))
    gr.add(message.reply_channel)
    message.channel_session['knocker'] = language
    message.reply_channel.send({""accept"": True})",def,ws_connect,(,message,),:,prefix,",",language,=,message,[,'path',],.,strip,(,'/',),.,split,(,'/',),gr,=,Group,(,'knocker-{0}',.,format,(,language,),),gr,.,add,(,message,.,reply_channel,),"Channels connection setup.
    Register the current client on the related Group according to the language",Channels,connection,setup,.,Register,the,current,client,on,the,related,Group,according,to,the,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/consumers.py#L9-L18,train,message,.,channel_session,[,'knocker',],=,language,message,.,reply_channel,.,send,(,{,"""accept""",:,True,},),,,,,,,,,,,language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nephila/django-knocker,knocker/consumers.py,ws_disconnect,"def ws_disconnect(message):
    """"""
    Channels connection close.
    Deregister the client
    """"""
    language = message.channel_session['knocker']
    gr = Group('knocker-{0}'.format(language))
    gr.discard(message.reply_channel)",python,"def ws_disconnect(message):
    """"""
    Channels connection close.
    Deregister the client
    """"""
    language = message.channel_session['knocker']
    gr = Group('knocker-{0}'.format(language))
    gr.discard(message.reply_channel)",def,ws_disconnect,(,message,),:,language,=,message,.,channel_session,[,'knocker',],gr,=,Group,(,'knocker-{0}',.,format,(,language,),),gr,.,discard,(,message,.,reply_channel,),,,,,,,,,,,"Channels connection close.
    Deregister the client",Channels,connection,close,.,Deregister,the,client,,,,,,,,,d25380d43a1f91285f1581dcf9db8510fe87f354,https://github.com/nephila/django-knocker/blob/d25380d43a1f91285f1581dcf9db8510fe87f354/knocker/consumers.py#L30-L37,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/animation.py,AnimatedDecorator.start,"def start(self, autopush=True):
        """"""Start a new animation instance""""""
        if self.enabled:
            if autopush:
                self.push_message(self.message)
                self.spinner.message = ' - '.join(self.animation.messages)
            if not self.spinner.running:
                self.animation.thread = threading.Thread(target=_spinner,
                                                         args=(self.spinner,))
                self.spinner.running = True
                self.animation.thread.start()
                sys.stdout = stream.Clean(sys.stdout, self.spinner.stream)",python,"def start(self, autopush=True):
        """"""Start a new animation instance""""""
        if self.enabled:
            if autopush:
                self.push_message(self.message)
                self.spinner.message = ' - '.join(self.animation.messages)
            if not self.spinner.running:
                self.animation.thread = threading.Thread(target=_spinner,
                                                         args=(self.spinner,))
                self.spinner.running = True
                self.animation.thread.start()
                sys.stdout = stream.Clean(sys.stdout, self.spinner.stream)",def,start,(,self,",",autopush,=,True,),:,if,self,.,enabled,:,if,autopush,:,self,.,push_message,(,self,.,message,),self,.,spinner,.,message,=,' - ',.,join,(,self,.,animation,.,messages,),if,Start a new animation instance,Start,a,new,animation,instance,,,,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/animation.py#L235-L246,train,not,self,.,spinner,.,running,:,self,.,animation,.,thread,=,threading,.,Thread,(,target,=,_spinner,",",args,=,(,self,.,spinner,",",),),,,,,,,,,,,,,,,,,,,,self,.,spinner,.,running,=,True,self,.,animation,.,thread,.,start,(,),sys,.,stdout,=,stream,.,Clean,(,sys,.,stdout,",",self,.,spinner,.,stream,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/animation.py,AnimatedDecorator.stop,"def stop(cls):
        """"""Stop the thread animation gracefully and reset_message""""""
        if AnimatedDecorator._enabled:
            if cls.spinner.running:
                cls.spinner.running = False
                cls.animation.thread.join()

            if any(cls.animation.messages):
                cls.pop_message()

            sys.stdout = sys.__stdout__",python,"def stop(cls):
        """"""Stop the thread animation gracefully and reset_message""""""
        if AnimatedDecorator._enabled:
            if cls.spinner.running:
                cls.spinner.running = False
                cls.animation.thread.join()

            if any(cls.animation.messages):
                cls.pop_message()

            sys.stdout = sys.__stdout__",def,stop,(,cls,),:,if,AnimatedDecorator,.,_enabled,:,if,cls,.,spinner,.,running,:,cls,.,spinner,.,running,=,False,cls,.,animation,.,thread,.,join,(,),if,any,(,cls,.,animation,.,messages,),Stop the thread animation gracefully and reset_message,Stop,the,thread,animation,gracefully,and,reset_message,,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/animation.py#L249-L259,train,:,cls,.,pop_message,(,),sys,.,stdout,=,sys,.,__stdout__,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/animation.py,AnimatedDecorator.auto_message,"def auto_message(self, args):
        """"""Try guess the message by the args passed

        args: a set of args passed on the wrapper __call__ in
              the definition above.

        if the object already have some message (defined in __init__),
        we don't change that. If the first arg is a function, so is decorated
        without argument, use the func name as the message.

        If not self.message anyway, use the default_message global,
        another else  use the default self.message already

        """"""
        if any(args) and callable(args[0]) and not self.message:
            return args[0].__name__
        elif not self.message:
            return self.default_message
        else:
            return self.message",python,"def auto_message(self, args):
        """"""Try guess the message by the args passed

        args: a set of args passed on the wrapper __call__ in
              the definition above.

        if the object already have some message (defined in __init__),
        we don't change that. If the first arg is a function, so is decorated
        without argument, use the func name as the message.

        If not self.message anyway, use the default_message global,
        another else  use the default self.message already

        """"""
        if any(args) and callable(args[0]) and not self.message:
            return args[0].__name__
        elif not self.message:
            return self.default_message
        else:
            return self.message",def,auto_message,(,self,",",args,),:,if,any,(,args,),and,callable,(,args,[,0,],),and,not,self,.,message,:,return,args,[,0,],.,__name__,elif,not,self,.,message,:,return,self,.,"Try guess the message by the args passed

        args: a set of args passed on the wrapper __call__ in
              the definition above.

        if the object already have some message (defined in __init__),
        we don't change that. If the first arg is a function, so is decorated
        without argument, use the func name as the message.

        If not self.message anyway, use the default_message global,
        another else  use the default self.message already",Try,guess,the,message,by,the,args,passed,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/animation.py#L296-L315,train,default_message,else,:,return,self,.,message,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/animation.py,WritingDecorator.start,"def start(self):
        """"""Activate the TypingStream on stdout""""""
        self.streams.append(sys.stdout)
        sys.stdout = self.stream",python,"def start(self):
        """"""Activate the TypingStream on stdout""""""
        self.streams.append(sys.stdout)
        sys.stdout = self.stream",def,start,(,self,),:,self,.,streams,.,append,(,sys,.,stdout,),sys,.,stdout,=,self,.,stream,,,,,,,,,,,,,,,,,,,,,Activate the TypingStream on stdout,Activate,the,TypingStream,on,stdout,,,,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/animation.py#L353-L356,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/animation.py,WritingDecorator.stop,"def stop(cls):
        """"""Change back the normal stdout after the end""""""
        if any(cls.streams):
            sys.stdout = cls.streams.pop(-1)
        else:
            sys.stdout = sys.__stdout__",python,"def stop(cls):
        """"""Change back the normal stdout after the end""""""
        if any(cls.streams):
            sys.stdout = cls.streams.pop(-1)
        else:
            sys.stdout = sys.__stdout__",def,stop,(,cls,),:,if,any,(,cls,.,streams,),:,sys,.,stdout,=,cls,.,streams,.,pop,(,-,1,),else,:,sys,.,stdout,=,sys,.,__stdout__,,,,,,,,Change back the normal stdout after the end,Change,back,the,normal,stdout,after,the,end,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/animation.py#L359-L364,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/lockable.py,Lockable.prolong,"def prolong(self):
		""""""Prolong the working duration of an already held lock.
		
		Attempting to prolong a lock not already owned will result in a Locked exception.
		""""""
		
		D = self.__class__
		collection = self.get_collection()
		identity = self.Lock()
		
		query = D.id == self
		query &= D.lock.instance == identity.instance
		query &= D.lock.time >= (identity.time - identity.__period__)
		
		previous = collection.find_one_and_update(query, {'$set': {~D.lock.time: identity.time}}, {~D.lock: True})
		
		if previous is None:
			lock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)
			
			if lock and lock.expires <= identity.time:
				lock.expired(self)
			
			raise self.Locked(""Unable to prolong lock."", lock)
		
		identity.prolonged(self)
		
		return identity",python,"def prolong(self):
		""""""Prolong the working duration of an already held lock.
		
		Attempting to prolong a lock not already owned will result in a Locked exception.
		""""""
		
		D = self.__class__
		collection = self.get_collection()
		identity = self.Lock()
		
		query = D.id == self
		query &= D.lock.instance == identity.instance
		query &= D.lock.time >= (identity.time - identity.__period__)
		
		previous = collection.find_one_and_update(query, {'$set': {~D.lock.time: identity.time}}, {~D.lock: True})
		
		if previous is None:
			lock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)
			
			if lock and lock.expires <= identity.time:
				lock.expired(self)
			
			raise self.Locked(""Unable to prolong lock."", lock)
		
		identity.prolonged(self)
		
		return identity",def,prolong,(,self,),:,D,=,self,.,__class__,collection,=,self,.,get_collection,(,),identity,=,self,.,Lock,(,),query,=,D,.,id,==,self,query,&=,D,.,lock,.,instance,==,identity,.,instance,"Prolong the working duration of an already held lock.
		
		Attempting to prolong a lock not already owned will result in a Locked exception.",Prolong,the,working,duration,of,an,already,held,lock,.,Attempting,to,prolong,a,lock,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/lockable.py#L245-L271,train,query,&=,D,.,lock,.,time,>=,(,identity,.,time,-,identity,.,__period__,),previous,=,collection,.,find_one_and_update,(,query,",",{,'$set',:,{,~,not,already,owned,will,result,in,a,Locked,exception,.,,,,,,,,,,D,.,lock,.,time,:,identity,.,time,},},",",{,~,D,.,lock,:,True,},),if,previous,is,None,:,lock,=,getattr,(,self,.,find_one,(,self,",",projection,=,{,~,D,.,lock,:,True,},),",",'lock',",",None,),if,lock,and,lock,.,expires,<=,identity,.,time,:,lock,.,expired,(,self,),raise,self,.,Locked,(,"""Unable to prolong lock.""",",",lock,),identity,.,prolonged,(,self,),return,identity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/lockable.py,Lockable.release,"def release(self, force=False):
		""""""Release an exclusive lock on this integration task.
		
		Unless forcing, if we are not the current owners of the lock a Locked exception will be raised.
		""""""
		
		D = self.__class__
		collection = self.get_collection()
		identity = self.Lock()
		
		query = D.id == self
		
		if not force:
			query &= D.lock.instance == identity.instance
		
		previous = collection.find_one_and_update(query, {'$unset': {~D.lock: True}}, {~D.lock: True})
		
		if previous is None:
			lock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)
			raise self.Locked(""Unable to release lock."", lock)
		
		lock = self.Lock.from_mongo(previous[~D.lock])
		
		if lock and lock.expires <= identity.time:
			lock.expired(self)
		
		identity.released(self, force)",python,"def release(self, force=False):
		""""""Release an exclusive lock on this integration task.
		
		Unless forcing, if we are not the current owners of the lock a Locked exception will be raised.
		""""""
		
		D = self.__class__
		collection = self.get_collection()
		identity = self.Lock()
		
		query = D.id == self
		
		if not force:
			query &= D.lock.instance == identity.instance
		
		previous = collection.find_one_and_update(query, {'$unset': {~D.lock: True}}, {~D.lock: True})
		
		if previous is None:
			lock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)
			raise self.Locked(""Unable to release lock."", lock)
		
		lock = self.Lock.from_mongo(previous[~D.lock])
		
		if lock and lock.expires <= identity.time:
			lock.expired(self)
		
		identity.released(self, force)",def,release,(,self,",",force,=,False,),:,D,=,self,.,__class__,collection,=,self,.,get_collection,(,),identity,=,self,.,Lock,(,),query,=,D,.,id,==,self,if,not,force,:,query,&=,D,"Release an exclusive lock on this integration task.
		
		Unless forcing, if we are not the current owners of the lock a Locked exception will be raised.",Release,an,exclusive,lock,on,this,integration,task,.,Unless,forcing,if,we,are,not,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/lockable.py#L273-L299,train,.,lock,.,instance,==,identity,.,instance,previous,=,collection,.,find_one_and_update,(,query,",",{,'$unset',:,{,~,D,.,lock,:,True,},},",",{,the,current,owners,of,the,lock,a,Locked,exception,will,be,raised,.,,,,,,,~,D,.,lock,:,True,},),if,previous,is,None,:,lock,=,getattr,(,self,.,find_one,(,self,",",projection,=,{,~,D,.,lock,:,True,},),",",'lock',",",None,),raise,self,.,Locked,(,"""Unable to release lock.""",",",lock,),lock,=,self,.,Lock,.,from_mongo,(,previous,[,~,D,.,lock,],),if,lock,and,lock,.,expires,<=,identity,.,time,:,lock,.,expired,(,self,),identity,.,released,(,self,",",force,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/stream.py,Animation.write,"def write(self, message, autoerase=True):
        """"""Send something for stdout and erased after delay""""""
        super(Animation, self).write(message)
        self.last_message = message
        if autoerase:
            time.sleep(self.interval)
            self.erase(message)",python,"def write(self, message, autoerase=True):
        """"""Send something for stdout and erased after delay""""""
        super(Animation, self).write(message)
        self.last_message = message
        if autoerase:
            time.sleep(self.interval)
            self.erase(message)",def,write,(,self,",",message,",",autoerase,=,True,),:,super,(,Animation,",",self,),.,write,(,message,),self,.,last_message,=,message,if,autoerase,:,time,.,sleep,(,self,.,interval,),self,.,erase,(,Send something for stdout and erased after delay,Send,something,for,stdout,and,erased,after,delay,,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/stream.py#L82-L88,train,message,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/stream.py,Clean.write,"def write(self, message, flush=False):
        """"""Write something on the default stream with a prefixed message""""""
        # this need be threadsafe because the concurrent spinning running on
        # the stderr
        with self.lock:
            self.paralell_stream.erase()
            super(Clean, self).write(message, flush)",python,"def write(self, message, flush=False):
        """"""Write something on the default stream with a prefixed message""""""
        # this need be threadsafe because the concurrent spinning running on
        # the stderr
        with self.lock:
            self.paralell_stream.erase()
            super(Clean, self).write(message, flush)",def,write,(,self,",",message,",",flush,=,False,),:,# this need be threadsafe because the concurrent spinning running on,# the stderr,with,self,.,lock,:,self,.,paralell_stream,.,erase,(,),super,(,Clean,",",self,),.,write,(,message,",",flush,),,,,,Write something on the default stream with a prefixed message,Write,something,on,the,default,stream,with,a,prefixed,message,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/stream.py#L120-L126,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ryukinix/decorating,decorating/stream.py,Writting.write,"def write(self, message, flush=True):
        if isinstance(message, bytes):  # pragma: no cover
            message = message.decode('utf-8')

        """"""A Writting like write method, delayed at each char""""""
        for char in message:
            time.sleep(self.delay * (4 if char == '\n' else 1))
            super(Writting, self).write(char, flush)",python,"def write(self, message, flush=True):
        if isinstance(message, bytes):  # pragma: no cover
            message = message.decode('utf-8')

        """"""A Writting like write method, delayed at each char""""""
        for char in message:
            time.sleep(self.delay * (4 if char == '\n' else 1))
            super(Writting, self).write(char, flush)",def,write,(,self,",",message,",",flush,=,True,),:,if,isinstance,(,message,",",bytes,),:,# pragma: no cover,message,=,message,.,decode,(,'utf-8',),for,char,in,message,:,time,.,sleep,(,self,.,delay,*,(,"A Writting like write method, delayed at each char",A,Writting,like,write,method,delayed,at,each,char,,,,,,,df78c3f87800205701704c0bc0fb9b6bb908ba7e,https://github.com/ryukinix/decorating/blob/df78c3f87800205701704c0bc0fb9b6bb908ba7e/decorating/stream.py#L148-L155,train,4,if,char,==,'\n',else,1,),),super,(,Writting,",",self,),.,write,(,char,",",flush,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/collection.py,Collection._get_default_projection,"def _get_default_projection(cls):
		""""""Construct the default projection document.""""""
		
		projected = []  # The fields explicitly requested for inclusion.
		neutral = []  # Fields returning neutral (None) status.
		omitted = False  # Have any fields been explicitly omitted?
		
		for name, field in cls.__fields__.items():
			if field.project is None:
				neutral.append(name)
			elif field.project:
				projected.append(name)
			else:
				omitted = True
		
		if not projected and not omitted:
			# No preferences specified.
			return None
			
		elif not projected and omitted:
			# No positive inclusions given, but negative ones were.
			projected = neutral
		
		return {field: True for field in projected}",python,"def _get_default_projection(cls):
		""""""Construct the default projection document.""""""
		
		projected = []  # The fields explicitly requested for inclusion.
		neutral = []  # Fields returning neutral (None) status.
		omitted = False  # Have any fields been explicitly omitted?
		
		for name, field in cls.__fields__.items():
			if field.project is None:
				neutral.append(name)
			elif field.project:
				projected.append(name)
			else:
				omitted = True
		
		if not projected and not omitted:
			# No preferences specified.
			return None
			
		elif not projected and omitted:
			# No positive inclusions given, but negative ones were.
			projected = neutral
		
		return {field: True for field in projected}",def,_get_default_projection,(,cls,),:,projected,=,[,],# The fields explicitly requested for inclusion.,neutral,=,[,],# Fields returning neutral (None) status.,omitted,=,False,# Have any fields been explicitly omitted?,for,name,",",field,in,cls,.,__fields__,.,items,(,),:,if,field,.,project,is,None,:,neutral,.,append,Construct the default projection document.,Construct,the,default,projection,document,.,,,,,,,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/collection.py#L203-L226,train,(,name,),elif,field,.,project,:,projected,.,append,(,name,),else,:,omitted,=,True,if,not,projected,and,not,omitted,:,# No preferences specified.,return,None,elif,,,,,,,,,,,,,,,,,,,,not,projected,and,omitted,:,"# No positive inclusions given, but negative ones were.",projected,=,neutral,return,{,field,:,True,for,field,in,projected,},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/util/__init__.py,adjust_attribute_sequence,"def adjust_attribute_sequence(*fields):
	""""""Move marrow.schema fields around to control positional instantiation order.""""""
	
	amount = None
	
	if fields and isinstance(fields[0], int):
		amount, fields = fields[0], fields[1:]
	
	def adjust_inner(cls):
		for field in fields:
			if field not in cls.__dict__:
				# TODO: Copy the field definition.
				raise TypeError(""Can only override sequence on non-inherited attributes."")
			
			# Adjust the sequence to re-order the field.
			if amount is None:
				cls.__dict__[field].__sequence__ = ElementMeta.sequence
			else:
				cls.__dict__[field].__sequence__ += amount  # Add the given amount.
		
		# Update the attribute collection.
		cls.__attributes__ = OrderedDict(
					(k, v) for k, v in \
					sorted(cls.__attributes__.items(),
						key=lambda i: i[1].__sequence__)
				)
		
		return cls
	
	return adjust_inner",python,"def adjust_attribute_sequence(*fields):
	""""""Move marrow.schema fields around to control positional instantiation order.""""""
	
	amount = None
	
	if fields and isinstance(fields[0], int):
		amount, fields = fields[0], fields[1:]
	
	def adjust_inner(cls):
		for field in fields:
			if field not in cls.__dict__:
				# TODO: Copy the field definition.
				raise TypeError(""Can only override sequence on non-inherited attributes."")
			
			# Adjust the sequence to re-order the field.
			if amount is None:
				cls.__dict__[field].__sequence__ = ElementMeta.sequence
			else:
				cls.__dict__[field].__sequence__ += amount  # Add the given amount.
		
		# Update the attribute collection.
		cls.__attributes__ = OrderedDict(
					(k, v) for k, v in \
					sorted(cls.__attributes__.items(),
						key=lambda i: i[1].__sequence__)
				)
		
		return cls
	
	return adjust_inner",def,adjust_attribute_sequence,(,*,fields,),:,amount,=,None,if,fields,and,isinstance,(,fields,[,0,],",",int,),:,amount,",",fields,=,fields,[,0,],",",fields,[,1,:,],def,adjust_inner,(,cls,),:,Move marrow.schema fields around to control positional instantiation order.,Move,marrow,.,schema,fields,around,to,control,positional,instantiation,order,.,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/util/__init__.py#L26-L55,train,for,field,in,fields,:,if,field,not,in,cls,.,__dict__,:,# TODO: Copy the field definition.,raise,TypeError,(,"""Can only override sequence on non-inherited attributes.""",),# Adjust the sequence to re-order the field.,if,amount,is,None,:,cls,.,__dict__,[,field,,,,,,,,,,,,,,,,,,,,],.,__sequence__,=,ElementMeta,.,sequence,else,:,cls,.,__dict__,[,field,],.,__sequence__,+=,amount,# Add the given amount.,# Update the attribute collection.,cls,.,__attributes__,=,OrderedDict,(,(,k,",",v,),for,k,",",v,in,sorted,(,cls,.,__attributes__,.,items,(,),",",key,=,lambda,i,:,i,[,1,],.,__sequence__,),),return,cls,return,adjust_inner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tonybaloney/retox,retox/__main__.py,get_hashes,"def get_hashes(path, exclude=None):
    '''
    Get a dictionary of file paths and timestamps.

    Paths matching `exclude` regex will be excluded.
    '''
    out = {}
    for f in Path(path).rglob('*'):
        if f.is_dir():
            # We want to watch files, not directories.
            continue
        if exclude and re.match(exclude, f.as_posix()):
            retox_log.debug(""excluding '{}'"".format(f.as_posix()))
            continue
        pytime = f.stat().st_mtime
        out[f.as_posix()] = pytime
    return out",python,"def get_hashes(path, exclude=None):
    '''
    Get a dictionary of file paths and timestamps.

    Paths matching `exclude` regex will be excluded.
    '''
    out = {}
    for f in Path(path).rglob('*'):
        if f.is_dir():
            # We want to watch files, not directories.
            continue
        if exclude and re.match(exclude, f.as_posix()):
            retox_log.debug(""excluding '{}'"".format(f.as_posix()))
            continue
        pytime = f.stat().st_mtime
        out[f.as_posix()] = pytime
    return out",def,get_hashes,(,path,",",exclude,=,None,),:,out,=,{,},for,f,in,Path,(,path,),.,rglob,(,'*',),:,if,f,.,is_dir,(,),:,"# We want to watch files, not directories.",continue,if,exclude,and,re,.,match,(,"Get a dictionary of file paths and timestamps.

    Paths matching `exclude` regex will be excluded.",Get,a,dictionary,of,file,paths,and,timestamps,.,,,,,,,4635e31001d2ac083423f46766249ac8daca7c9c,https://github.com/tonybaloney/retox/blob/4635e31001d2ac083423f46766249ac8daca7c9c/retox/__main__.py#L103-L119,train,exclude,",",f,.,as_posix,(,),),:,retox_log,.,debug,(,"""excluding '{}'""",.,format,(,f,.,as_posix,(,),),),continue,pytime,=,f,.,stat,,,,,,,,,,,,,,,,,,,,(,),.,st_mtime,out,[,f,.,as_posix,(,),],=,pytime,return,out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mediawiki-utilities/python-mwapi,mwapi/session.py,Session.request,"def request(self, method, params=None, query_continue=None,
                files=None, auth=None, continuation=False):
        """"""
        Sends an HTTP request to the API.

        :Parameters:
            method : `str`
                Which HTTP method to use for the request?
                (Usually ""POST"" or ""GET"")
            params : `dict`
                A set of parameters to send with the request.  These parameters
                will be included in the POST body for post requests or a query
                string otherwise.
            query_continue : `dict`
                A 'continue' field from a past request.  This field represents
                the point from which a query should be continued.
            files : `dict`
                A dictionary of (filename : `str`, data : `bytes`) pairs to
                send with the request.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)
        """"""
        normal_params = _normalize_params(params, query_continue)
        if continuation:
            return self._continuation(method, params=normal_params, auth=auth,
                                      files=files)
        else:
            return self._request(method, params=normal_params, auth=auth,
                                 files=files)",python,"def request(self, method, params=None, query_continue=None,
                files=None, auth=None, continuation=False):
        """"""
        Sends an HTTP request to the API.

        :Parameters:
            method : `str`
                Which HTTP method to use for the request?
                (Usually ""POST"" or ""GET"")
            params : `dict`
                A set of parameters to send with the request.  These parameters
                will be included in the POST body for post requests or a query
                string otherwise.
            query_continue : `dict`
                A 'continue' field from a past request.  This field represents
                the point from which a query should be continued.
            files : `dict`
                A dictionary of (filename : `str`, data : `bytes`) pairs to
                send with the request.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)
        """"""
        normal_params = _normalize_params(params, query_continue)
        if continuation:
            return self._continuation(method, params=normal_params, auth=auth,
                                      files=files)
        else:
            return self._request(method, params=normal_params, auth=auth,
                                 files=files)",def,request,(,self,",",method,",",params,=,None,",",query_continue,=,None,",",files,=,None,",",auth,=,None,",",continuation,=,False,),:,normal_params,=,_normalize_params,(,params,",",query_continue,),if,continuation,:,return,self,.,_continuation,"Sends an HTTP request to the API.

        :Parameters:
            method : `str`
                Which HTTP method to use for the request?
                (Usually ""POST"" or ""GET"")
            params : `dict`
                A set of parameters to send with the request.  These parameters
                will be included in the POST body for post requests or a query
                string otherwise.
            query_continue : `dict`
                A 'continue' field from a past request.  This field represents
                the point from which a query should be continued.
            files : `dict`
                A dictionary of (filename : `str`, data : `bytes`) pairs to
                send with the request.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)",Sends,an,HTTP,request,to,the,API,.,,,,,,,,7a653c29207ecd318ae4b369d398aed13f26951d,https://github.com/mediawiki-utilities/python-mwapi/blob/7a653c29207ecd318ae4b369d398aed13f26951d/mwapi/session.py#L136-L171,train,(,method,",",params,=,normal_params,",",auth,=,auth,",",files,=,files,),else,:,return,self,.,_request,(,method,",",params,=,normal_params,",",auth,=,,,,,,,,,,,,,,,,,,,,auth,",",files,=,files,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mediawiki-utilities/python-mwapi,mwapi/session.py,Session.login,"def login(self, username, password, login_token=None):
        """"""
        Authenticate with the given credentials.  If authentication is
        successful, all further requests sent will be signed the authenticated
        user.

        Note that passwords are sent as plaintext. This is a limitation of the
        Mediawiki API.  Use a https host if you want your password to be secure

        :Parameters:
            username : str
                The username of the user to be authenticated
            password : str
                The password of the user to be authenticated

        :Raises:
            :class:`mwapi.errors.LoginError` : if authentication fails
            :class:`mwapi.errors.ClientInteractionRequest` : if authentication requires a continue_login() call
            :class:`mwapi.errors.APIError` : if the API responds with an error
        """"""
        if login_token is None:
            token_doc = self.post(action='query', meta='tokens', type='login')
            login_token = token_doc['query']['tokens']['logintoken']

        login_doc = self.post(
            action=""clientlogin"", username=username, password=password,
            logintoken=login_token, loginreturnurl=""http://example.org/"")

        if login_doc['clientlogin']['status'] == ""UI"":
            raise ClientInteractionRequest.from_doc(
                login_token, login_doc['clientlogin'])
        elif login_doc['clientlogin']['status'] != 'PASS':
            raise LoginError.from_doc(login_doc['clientlogin'])
        return login_doc['clientlogin']",python,"def login(self, username, password, login_token=None):
        """"""
        Authenticate with the given credentials.  If authentication is
        successful, all further requests sent will be signed the authenticated
        user.

        Note that passwords are sent as plaintext. This is a limitation of the
        Mediawiki API.  Use a https host if you want your password to be secure

        :Parameters:
            username : str
                The username of the user to be authenticated
            password : str
                The password of the user to be authenticated

        :Raises:
            :class:`mwapi.errors.LoginError` : if authentication fails
            :class:`mwapi.errors.ClientInteractionRequest` : if authentication requires a continue_login() call
            :class:`mwapi.errors.APIError` : if the API responds with an error
        """"""
        if login_token is None:
            token_doc = self.post(action='query', meta='tokens', type='login')
            login_token = token_doc['query']['tokens']['logintoken']

        login_doc = self.post(
            action=""clientlogin"", username=username, password=password,
            logintoken=login_token, loginreturnurl=""http://example.org/"")

        if login_doc['clientlogin']['status'] == ""UI"":
            raise ClientInteractionRequest.from_doc(
                login_token, login_doc['clientlogin'])
        elif login_doc['clientlogin']['status'] != 'PASS':
            raise LoginError.from_doc(login_doc['clientlogin'])
        return login_doc['clientlogin']",def,login,(,self,",",username,",",password,",",login_token,=,None,),:,if,login_token,is,None,:,token_doc,=,self,.,post,(,action,=,'query',",",meta,=,'tokens',",",type,=,'login',),login_token,=,token_doc,[,'query',],"Authenticate with the given credentials.  If authentication is
        successful, all further requests sent will be signed the authenticated
        user.

        Note that passwords are sent as plaintext. This is a limitation of the
        Mediawiki API.  Use a https host if you want your password to be secure

        :Parameters:
            username : str
                The username of the user to be authenticated
            password : str
                The password of the user to be authenticated

        :Raises:
            :class:`mwapi.errors.LoginError` : if authentication fails
            :class:`mwapi.errors.ClientInteractionRequest` : if authentication requires a continue_login() call
            :class:`mwapi.errors.APIError` : if the API responds with an error",Authenticate,with,the,given,credentials,.,If,authentication,is,successful,all,further,requests,sent,will,7a653c29207ecd318ae4b369d398aed13f26951d,https://github.com/mediawiki-utilities/python-mwapi/blob/7a653c29207ecd318ae4b369d398aed13f26951d/mwapi/session.py#L213-L246,train,[,'tokens',],[,'logintoken',],login_doc,=,self,.,post,(,action,=,"""clientlogin""",",",username,=,username,",",password,=,password,",",logintoken,=,login_token,",",loginreturnurl,=,be,signed,the,authenticated,user,.,,,,,,,,,,,,,,"""http://example.org/""",),if,login_doc,[,'clientlogin',],[,'status',],==,"""UI""",:,raise,ClientInteractionRequest,.,from_doc,(,login_token,",",login_doc,[,'clientlogin',],),elif,login_doc,[,'clientlogin',],[,'status',],!=,'PASS',:,raise,LoginError,.,from_doc,(,login_doc,[,'clientlogin',],),return,login_doc,[,'clientlogin',],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mediawiki-utilities/python-mwapi,mwapi/session.py,Session.continue_login,"def continue_login(self, login_token, **params):
        """"""
        Continues a login that requires an additional step.  This is common
        for when login requires completing a captcha or supplying a two-factor
        authentication token.

        :Parameters:
            login_token : `str`
                A login token generated by the MediaWiki API (and used in a
                previous call to login())
            params : `mixed`
                A set of parameters to include with the request.  This depends
                on what ""requests"" for additional information were made by the
                MediaWiki API.
        """"""

        login_params = {
            'action': ""clientlogin"",
            'logintoken': login_token,
            'logincontinue': 1
        }
        login_params.update(params)
        login_doc = self.post(**login_params)
        if login_doc['clientlogin']['status'] != 'PASS':
            raise LoginError.from_doc(login_doc['clientlogin'])
        return login_doc['clientlogin']",python,"def continue_login(self, login_token, **params):
        """"""
        Continues a login that requires an additional step.  This is common
        for when login requires completing a captcha or supplying a two-factor
        authentication token.

        :Parameters:
            login_token : `str`
                A login token generated by the MediaWiki API (and used in a
                previous call to login())
            params : `mixed`
                A set of parameters to include with the request.  This depends
                on what ""requests"" for additional information were made by the
                MediaWiki API.
        """"""

        login_params = {
            'action': ""clientlogin"",
            'logintoken': login_token,
            'logincontinue': 1
        }
        login_params.update(params)
        login_doc = self.post(**login_params)
        if login_doc['clientlogin']['status'] != 'PASS':
            raise LoginError.from_doc(login_doc['clientlogin'])
        return login_doc['clientlogin']",def,continue_login,(,self,",",login_token,",",*,*,params,),:,login_params,=,{,'action',:,"""clientlogin""",",",'logintoken',:,login_token,",",'logincontinue',:,1,},login_params,.,update,(,params,),login_doc,=,self,.,post,(,*,*,login_params,),"Continues a login that requires an additional step.  This is common
        for when login requires completing a captcha or supplying a two-factor
        authentication token.

        :Parameters:
            login_token : `str`
                A login token generated by the MediaWiki API (and used in a
                previous call to login())
            params : `mixed`
                A set of parameters to include with the request.  This depends
                on what ""requests"" for additional information were made by the
                MediaWiki API.",Continues,a,login,that,requires,an,additional,step,.,This,is,common,for,when,login,7a653c29207ecd318ae4b369d398aed13f26951d,https://github.com/mediawiki-utilities/python-mwapi/blob/7a653c29207ecd318ae4b369d398aed13f26951d/mwapi/session.py#L248-L273,train,if,login_doc,[,'clientlogin',],[,'status',],!=,'PASS',:,raise,LoginError,.,from_doc,(,login_doc,[,'clientlogin',],),return,login_doc,[,'clientlogin',],,,,,requires,completing,a,captcha,or,supplying,a,two,-,factor,authentication,token,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mediawiki-utilities/python-mwapi,mwapi/session.py,Session.get,"def get(self, query_continue=None, auth=None, continuation=False,
            **params):
        """"""Makes an API request with the GET method

        :Parameters:
            query_continue : `dict`
                Optionally, the value of a query continuation 'continue' field.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.
            params :
                Keyword parameters to be sent in the query string.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)

        :Raises:
            :class:`mwapi.errors.APIError` : if the API responds with an error
        """"""

        return self.request('GET', params=params, auth=auth,
                            query_continue=query_continue,
                            continuation=continuation)",python,"def get(self, query_continue=None, auth=None, continuation=False,
            **params):
        """"""Makes an API request with the GET method

        :Parameters:
            query_continue : `dict`
                Optionally, the value of a query continuation 'continue' field.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.
            params :
                Keyword parameters to be sent in the query string.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)

        :Raises:
            :class:`mwapi.errors.APIError` : if the API responds with an error
        """"""

        return self.request('GET', params=params, auth=auth,
                            query_continue=query_continue,
                            continuation=continuation)",def,get,(,self,",",query_continue,=,None,",",auth,=,None,",",continuation,=,False,",",*,*,params,),:,return,self,.,request,(,'GET',",",params,=,params,",",auth,=,auth,",",query_continue,=,query_continue,",",continuation,=,"Makes an API request with the GET method

        :Parameters:
            query_continue : `dict`
                Optionally, the value of a query continuation 'continue' field.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.
            params :
                Keyword parameters to be sent in the query string.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)

        :Raises:
            :class:`mwapi.errors.APIError` : if the API responds with an error",Makes,an,API,request,with,the,GET,method,,,,,,,,7a653c29207ecd318ae4b369d398aed13f26951d,https://github.com/mediawiki-utilities/python-mwapi/blob/7a653c29207ecd318ae4b369d398aed13f26951d/mwapi/session.py#L284-L309,train,continuation,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mediawiki-utilities/python-mwapi,mwapi/session.py,Session.post,"def post(self, query_continue=None, upload_file=None, auth=None,
             continuation=False, **params):
        """"""Makes an API request with the POST method

        :Parameters:
            query_continue : `dict`
                Optionally, the value of a query continuation 'continue' field.
            upload_file : `bytes`
                The bytes of a file to upload.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.
            params :
                Keyword parameters to be sent in the POST message body.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)

        :Raises:
            :class:`mwapi.errors.APIError` : if the API responds with an error
        """"""
        if upload_file is not None:
            files = {'file': upload_file}
        else:
            files = None

        return self.request('POST', params=params, auth=auth,
                            query_continue=query_continue, files=files,
                            continuation=continuation)",python,"def post(self, query_continue=None, upload_file=None, auth=None,
             continuation=False, **params):
        """"""Makes an API request with the POST method

        :Parameters:
            query_continue : `dict`
                Optionally, the value of a query continuation 'continue' field.
            upload_file : `bytes`
                The bytes of a file to upload.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.
            params :
                Keyword parameters to be sent in the POST message body.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)

        :Raises:
            :class:`mwapi.errors.APIError` : if the API responds with an error
        """"""
        if upload_file is not None:
            files = {'file': upload_file}
        else:
            files = None

        return self.request('POST', params=params, auth=auth,
                            query_continue=query_continue, files=files,
                            continuation=continuation)",def,post,(,self,",",query_continue,=,None,",",upload_file,=,None,",",auth,=,None,",",continuation,=,False,",",*,*,params,),:,if,upload_file,is,not,None,:,files,=,{,'file',:,upload_file,},else,:,files,=,"Makes an API request with the POST method

        :Parameters:
            query_continue : `dict`
                Optionally, the value of a query continuation 'continue' field.
            upload_file : `bytes`
                The bytes of a file to upload.
            auth : mixed
                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
            continuation : `bool`
                If true, a continuation will be attempted and a generator of
                JSON response documents will be returned.
            params :
                Keyword parameters to be sent in the POST message body.

        :Returns:
            A response JSON documents (or a generator of documents if
            `continuation == True`)

        :Raises:
            :class:`mwapi.errors.APIError` : if the API responds with an error",Makes,an,API,request,with,the,POST,method,,,,,,,,7a653c29207ecd318ae4b369d398aed13f26951d,https://github.com/mediawiki-utilities/python-mwapi/blob/7a653c29207ecd318ae4b369d398aed13f26951d/mwapi/session.py#L311-L342,train,None,return,self,.,request,(,'POST',",",params,=,params,",",auth,=,auth,",",query_continue,=,query_continue,",",files,=,files,",",continuation,=,continuation,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/derived.py,Derived.promote,"def promote(self, cls, update=False, preserve=True):
		""""""Transform this record into an instance of a more specialized subclass.""""""
		
		if not issubclass(cls, self.__class__):
			raise TypeError(""Must promote to a subclass of "" + self.__class__.__name__)
		
		return self._as(cls, update, preserve)",python,"def promote(self, cls, update=False, preserve=True):
		""""""Transform this record into an instance of a more specialized subclass.""""""
		
		if not issubclass(cls, self.__class__):
			raise TypeError(""Must promote to a subclass of "" + self.__class__.__name__)
		
		return self._as(cls, update, preserve)",def,promote,(,self,",",cls,",",update,=,False,",",preserve,=,True,),:,if,not,issubclass,(,cls,",",self,.,__class__,),:,raise,TypeError,(,"""Must promote to a subclass of """,+,self,.,__class__,.,__name__,),return,self,.,_as,(,Transform this record into an instance of a more specialized subclass.,Transform,this,record,into,an,instance,of,a,more,specialized,subclass,.,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/derived.py#L36-L42,train,cls,",",update,",",preserve,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jrief/djangocms-bootstrap,cms_bootstrap/templatetags/bootstrap_tags.py,cut_levels,"def cut_levels(nodes, start_level):
    """"""
    cutting nodes away from menus
    """"""
    final = []
    removed = []
    for node in nodes:
        if not hasattr(node, 'level'):
            # remove and ignore nodes that don't have level information
            remove(node, removed)
            continue
        if node.attr.get('soft_root', False):
            # remove and ignore nodes that are behind a node marked as 'soft_root'
            remove(node, removed)
            continue
        if node.level == start_level:
            # turn nodes that are on from_level into root nodes
            final.append(node)
            node.parent = None
            if not node.visible and not node.children:
                remove(node, removed)
        elif node.level == start_level + 1:
            # remove nodes that are deeper than one level
            node.children = []
        else:
            remove(node, removed)
        if not node.visible:
            keep_node = False
            for child in node.children:
                keep_node = keep_node or child.visible
            if not keep_node:
                remove(node, removed)
    for node in removed:
        if node in final:
            final.remove(node)
    return final",python,"def cut_levels(nodes, start_level):
    """"""
    cutting nodes away from menus
    """"""
    final = []
    removed = []
    for node in nodes:
        if not hasattr(node, 'level'):
            # remove and ignore nodes that don't have level information
            remove(node, removed)
            continue
        if node.attr.get('soft_root', False):
            # remove and ignore nodes that are behind a node marked as 'soft_root'
            remove(node, removed)
            continue
        if node.level == start_level:
            # turn nodes that are on from_level into root nodes
            final.append(node)
            node.parent = None
            if not node.visible and not node.children:
                remove(node, removed)
        elif node.level == start_level + 1:
            # remove nodes that are deeper than one level
            node.children = []
        else:
            remove(node, removed)
        if not node.visible:
            keep_node = False
            for child in node.children:
                keep_node = keep_node or child.visible
            if not keep_node:
                remove(node, removed)
    for node in removed:
        if node in final:
            final.remove(node)
    return final",def,cut_levels,(,nodes,",",start_level,),:,final,=,[,],removed,=,[,],for,node,in,nodes,:,if,not,hasattr,(,node,",",'level',),:,# remove and ignore nodes that don't have level information,remove,(,node,",",removed,),continue,if,node,.,attr,.,cutting nodes away from menus,cutting,nodes,away,from,menus,,,,,,,,,,,293a7050602d6e9a728acea2fb13893e5ec7992e,https://github.com/jrief/djangocms-bootstrap/blob/293a7050602d6e9a728acea2fb13893e5ec7992e/cms_bootstrap/templatetags/bootstrap_tags.py#L18-L53,train,get,(,'soft_root',",",False,),:,# remove and ignore nodes that are behind a node marked as 'soft_root',remove,(,node,",",removed,),continue,if,node,.,level,==,start_level,:,# turn nodes that are on from_level into root nodes,final,.,append,(,node,),node,,,,,,,,,,,,,,,,,,,,.,parent,=,None,if,not,node,.,visible,and,not,node,.,children,:,remove,(,node,",",removed,),elif,node,.,level,==,start_level,+,1,:,# remove nodes that are deeper than one level,node,.,children,=,[,],else,:,remove,(,node,",",removed,),if,not,node,.,visible,:,keep_node,=,False,for,child,in,node,.,children,:,keep_node,=,keep_node,or,child,.,visible,if,not,keep_node,:,remove,(,node,",",removed,),for,node,in,removed,:,if,node,in,final,:,final,.,remove,(,node,),return,final,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/core/trait/expires.py,Expires.from_mongo,"def from_mongo(cls, data, expired=False, **kw):
		""""""In the event a value that has technically already expired is loaded, swap it for None.""""""
		
		value = super(Expires, cls).from_mongo(data, **kw)
		
		if not expired and value.is_expired:
			return None
		
		return value",python,"def from_mongo(cls, data, expired=False, **kw):
		""""""In the event a value that has technically already expired is loaded, swap it for None.""""""
		
		value = super(Expires, cls).from_mongo(data, **kw)
		
		if not expired and value.is_expired:
			return None
		
		return value",def,from_mongo,(,cls,",",data,",",expired,=,False,",",*,*,kw,),:,value,=,super,(,Expires,",",cls,),.,from_mongo,(,data,",",*,*,kw,),if,not,expired,and,value,.,is_expired,:,return,None,"In the event a value that has technically already expired is loaded, swap it for None.",In,the,event,a,value,that,has,technically,already,expired,is,loaded,swap,it,for,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/core/trait/expires.py#L36-L44,train,return,value,,,,,,,,,,,,,,,,,,,,,,,,,,,,,None,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
marrow/mongo,marrow/mongo/param/sort.py,S,"def S(Document, *fields):
	""""""Generate a MongoDB sort order list using the Django ORM style.""""""
	
	result = []
	
	for field in fields:
		if isinstance(field, tuple):  # Unpack existing tuple.
			field, direction = field
			result.append((field, direction))
			continue
		
		direction = ASCENDING
		
		if not field.startswith('__'):
			field = field.replace('__', '.')
		
		if field[0] == '-':
			direction = DESCENDING
		
		if field[0] in ('+', '-'):
			field = field[1:]
		
		_field = traverse(Document, field, default=None)
		
		result.append(((~_field) if _field else field, direction))
	
	return result",python,"def S(Document, *fields):
	""""""Generate a MongoDB sort order list using the Django ORM style.""""""
	
	result = []
	
	for field in fields:
		if isinstance(field, tuple):  # Unpack existing tuple.
			field, direction = field
			result.append((field, direction))
			continue
		
		direction = ASCENDING
		
		if not field.startswith('__'):
			field = field.replace('__', '.')
		
		if field[0] == '-':
			direction = DESCENDING
		
		if field[0] in ('+', '-'):
			field = field[1:]
		
		_field = traverse(Document, field, default=None)
		
		result.append(((~_field) if _field else field, direction))
	
	return result",def,S,(,Document,",",*,fields,),:,result,=,[,],for,field,in,fields,:,if,isinstance,(,field,",",tuple,),:,# Unpack existing tuple.,field,",",direction,=,field,result,.,append,(,(,field,",",direction,),),continue,Generate a MongoDB sort order list using the Django ORM style.,Generate,a,MongoDB,sort,order,list,using,the,Django,ORM,style,.,,,,2066dc73e281b8a46cb5fc965267d6b8e1b18467,https://github.com/marrow/mongo/blob/2066dc73e281b8a46cb5fc965267d6b8e1b18467/marrow/mongo/param/sort.py#L12-L38,train,direction,=,ASCENDING,if,not,field,.,startswith,(,'__',),:,field,=,field,.,replace,(,'__',",",'.',),if,field,[,0,],==,'-',:,,,,,,,,,,,,,,,,,,,,direction,=,DESCENDING,if,field,[,0,],in,(,'+',",",'-',),:,field,=,field,[,1,:,],_field,=,traverse,(,Document,",",field,",",default,=,None,),result,.,append,(,(,(,~,_field,),if,_field,else,field,",",direction,),),return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vkruoso/receita-tools,receita/tools/get.py,Get.run,"def run(self):
        """"""Reads data from CNPJ list and write results to output directory.""""""
        self._assure_output_dir(self.output)
        companies = self.read()
        print '%s CNPJs found' % len(companies)

        pbar = ProgressBar(
            widgets=[Counter(), ' ', Percentage(), ' ', Bar(), ' ', Timer()],
            maxval=len(companies)).start()

        resolved = 0
        runner = Runner(companies, self.days, self.token)

        try:
            for data in runner:
                self.write(data)
                resolved = resolved + 1
                pbar.update(resolved)
        except KeyboardInterrupt:
            print '\naborted: waiting current requests to finish.'
            runner.stop()
            return

        pbar.finish()",python,"def run(self):
        """"""Reads data from CNPJ list and write results to output directory.""""""
        self._assure_output_dir(self.output)
        companies = self.read()
        print '%s CNPJs found' % len(companies)

        pbar = ProgressBar(
            widgets=[Counter(), ' ', Percentage(), ' ', Bar(), ' ', Timer()],
            maxval=len(companies)).start()

        resolved = 0
        runner = Runner(companies, self.days, self.token)

        try:
            for data in runner:
                self.write(data)
                resolved = resolved + 1
                pbar.update(resolved)
        except KeyboardInterrupt:
            print '\naborted: waiting current requests to finish.'
            runner.stop()
            return

        pbar.finish()",def,run,(,self,),:,self,.,_assure_output_dir,(,self,.,output,),companies,=,self,.,read,(,),print,'%s CNPJs found',%,len,(,companies,),pbar,=,ProgressBar,(,widgets,=,[,Counter,(,),",",' ',",",Percentage,(,Reads data from CNPJ list and write results to output directory.,Reads,data,from,CNPJ,list,and,write,results,to,output,directory,.,,,,fd62a252c76541c9feac6470b9048b31348ffe86,https://github.com/vkruoso/receita-tools/blob/fd62a252c76541c9feac6470b9048b31348ffe86/receita/tools/get.py#L25-L48,train,),",",' ',",",Bar,(,),",",' ',",",Timer,(,),],",",maxval,=,len,(,companies,),),.,start,(,),resolved,=,0,runner,,,,,,,,,,,,,,,,,,,,=,Runner,(,companies,",",self,.,days,",",self,.,token,),try,:,for,data,in,runner,:,self,.,write,(,data,),resolved,=,resolved,+,1,pbar,.,update,(,resolved,),except,KeyboardInterrupt,:,print,'\naborted: waiting current requests to finish.',runner,.,stop,(,),return,pbar,.,finish,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vkruoso/receita-tools,receita/tools/get.py,Get.read,"def read(self):
        """"""Reads data from the CSV file.""""""
        companies = []
        with open(self.file) as f:
            reader = unicodecsv.reader(f)
            for line in reader:
                if len(line) >= 1:
                    cnpj = self.format(line[0])
                    if self.valid(cnpj):
                        companies.append(cnpj)
        return companies",python,"def read(self):
        """"""Reads data from the CSV file.""""""
        companies = []
        with open(self.file) as f:
            reader = unicodecsv.reader(f)
            for line in reader:
                if len(line) >= 1:
                    cnpj = self.format(line[0])
                    if self.valid(cnpj):
                        companies.append(cnpj)
        return companies",def,read,(,self,),:,companies,=,[,],with,open,(,self,.,file,),as,f,:,reader,=,unicodecsv,.,reader,(,f,),for,line,in,reader,:,if,len,(,line,),>=,1,:,cnpj,=,Reads data from the CSV file.,Reads,data,from,the,CSV,file,.,,,,,,,,,fd62a252c76541c9feac6470b9048b31348ffe86,https://github.com/vkruoso/receita-tools/blob/fd62a252c76541c9feac6470b9048b31348ffe86/receita/tools/get.py#L50-L60,train,self,.,format,(,line,[,0,],),if,self,.,valid,(,cnpj,),:,companies,.,append,(,cnpj,),return,companies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vkruoso/receita-tools,receita/tools/get.py,Get.write,"def write(self, data):
        """"""Writes json data to the output directory.""""""
        cnpj, data = data

        path = os.path.join(self.output, '%s.json' % cnpj)
        with open(path, 'w') as f:
            json.dump(data, f, encoding='utf-8')",python,"def write(self, data):
        """"""Writes json data to the output directory.""""""
        cnpj, data = data

        path = os.path.join(self.output, '%s.json' % cnpj)
        with open(path, 'w') as f:
            json.dump(data, f, encoding='utf-8')",def,write,(,self,",",data,),:,cnpj,",",data,=,data,path,=,os,.,path,.,join,(,self,.,output,",",'%s.json',%,cnpj,),with,open,(,path,",",'w',),as,f,:,json,.,dump,(,Writes json data to the output directory.,Writes,json,data,to,the,output,directory,.,,,,,,,,fd62a252c76541c9feac6470b9048b31348ffe86,https://github.com/vkruoso/receita-tools/blob/fd62a252c76541c9feac6470b9048b31348ffe86/receita/tools/get.py#L62-L68,train,data,",",f,",",encoding,=,'utf-8',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vkruoso/receita-tools,receita/tools/get.py,Get.valid,"def valid(self, cnpj):
        """"""Check if a CNPJ is valid.

        We should avoid sending invalid CNPJ to the web service as we know
        it is going to be a waste of bandwidth. Assumes CNPJ is a string.
        """"""
        if len(cnpj) != 14:
            return False

        tam = 12
        nums = cnpj[:tam]
        digs = cnpj[tam:]

        tot = 0
        pos = tam-7
        for i in range(tam, 0, -1):
            tot = tot + int(nums[tam-i])*pos
            pos = pos - 1
            if pos < 2:
                pos = 9
        res = 0 if tot % 11 < 2 else 11 - (tot % 11)
        if res != int(digs[0]):
            return False

        tam = tam + 1
        nums = cnpj[:tam]
        tot = 0
        pos = tam-7
        for i in range(tam, 0, -1):
            tot = tot + int(nums[tam-i])*pos
            pos = pos - 1
            if pos < 2:
                pos = 9
        res = 0 if tot % 11 < 2 else 11 - (tot % 11)
        if res != int(digs[1]):
            return False

        return True",python,"def valid(self, cnpj):
        """"""Check if a CNPJ is valid.

        We should avoid sending invalid CNPJ to the web service as we know
        it is going to be a waste of bandwidth. Assumes CNPJ is a string.
        """"""
        if len(cnpj) != 14:
            return False

        tam = 12
        nums = cnpj[:tam]
        digs = cnpj[tam:]

        tot = 0
        pos = tam-7
        for i in range(tam, 0, -1):
            tot = tot + int(nums[tam-i])*pos
            pos = pos - 1
            if pos < 2:
                pos = 9
        res = 0 if tot % 11 < 2 else 11 - (tot % 11)
        if res != int(digs[0]):
            return False

        tam = tam + 1
        nums = cnpj[:tam]
        tot = 0
        pos = tam-7
        for i in range(tam, 0, -1):
            tot = tot + int(nums[tam-i])*pos
            pos = pos - 1
            if pos < 2:
                pos = 9
        res = 0 if tot % 11 < 2 else 11 - (tot % 11)
        if res != int(digs[1]):
            return False

        return True",def,valid,(,self,",",cnpj,),:,if,len,(,cnpj,),!=,14,:,return,False,tam,=,12,nums,=,cnpj,[,:,tam,],digs,=,cnpj,[,tam,:,],tot,=,0,pos,=,tam,-,7,"Check if a CNPJ is valid.

        We should avoid sending invalid CNPJ to the web service as we know
        it is going to be a waste of bandwidth. Assumes CNPJ is a string.",Check,if,a,CNPJ,is,valid,.,,,,,,,,,fd62a252c76541c9feac6470b9048b31348ffe86,https://github.com/vkruoso/receita-tools/blob/fd62a252c76541c9feac6470b9048b31348ffe86/receita/tools/get.py#L88-L125,train,for,i,in,range,(,tam,",",0,",",-,1,),:,tot,=,tot,+,int,(,nums,[,tam,-,i,],),*,pos,pos,=,,,,,,,,,,,,,,,,,,,,pos,-,1,if,pos,<,2,:,pos,=,9,res,=,0,if,tot,%,11,<,2,else,11,-,(,tot,%,11,),if,res,!=,int,(,digs,[,0,],),:,return,False,tam,=,tam,+,1,nums,=,cnpj,[,:,tam,],tot,=,0,pos,=,tam,-,7,for,i,in,range,(,tam,",",0,",",-,1,),:,tot,=,tot,+,int,(,nums,[,tam,-,i,],),*,pos,pos,=,pos,-,1,if,pos,<,2,:,pos,=,9,res,=,0,if,tot,%,11,<,2,else,11,-,(,tot,%,11,),if,res,!=,int,(,digs,[,1,],),:,return,False,return,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/get_config.py,get_default_config_filename,"def get_default_config_filename():
    """"""Returns the configuration filepath.

    If PEYOTL_CONFIG_FILE is in the env that is the preferred choice; otherwise ~/.peyotl/config is preferred.
    If the preferred file does not exist, then the packaged peyotl/default.conf from the installation of peyotl is
    used.
    A RuntimeError is raised if that fails.
    """"""
    global _CONFIG_FN
    if _CONFIG_FN is not None:
        return _CONFIG_FN
    with _CONFIG_FN_LOCK:
        if _CONFIG_FN is not None:
            return _CONFIG_FN
        if 'PEYOTL_CONFIG_FILE' in os.environ:
            cfn = os.path.abspath(os.environ['PEYOTL_CONFIG_FILE'])
        else:
            cfn = os.path.expanduser(""~/.peyotl/config"")
        if not os.path.isfile(cfn):
            # noinspection PyProtectedMember
            if 'PEYOTL_CONFIG_FILE' in os.environ:
                from peyotl.utility.get_logger import warn_from_util_logger
                msg = 'Filepath ""{}"" specified via PEYOTL_CONFIG_FILE={} was not found'.format(cfn, os.environ[
                    'PEYOTL_CONFIG_FILE'])
                warn_from_util_logger(msg)
            from pkg_resources import Requirement, resource_filename
            pr = Requirement.parse('peyotl')
            cfn = resource_filename(pr, 'peyotl/default.conf')
        if not os.path.isfile(cfn):
            raise RuntimeError('The peyotl configuration file cascade failed looking for ""{}""'.format(cfn))
        _CONFIG_FN = os.path.abspath(cfn)
    return _CONFIG_FN",python,"def get_default_config_filename():
    """"""Returns the configuration filepath.

    If PEYOTL_CONFIG_FILE is in the env that is the preferred choice; otherwise ~/.peyotl/config is preferred.
    If the preferred file does not exist, then the packaged peyotl/default.conf from the installation of peyotl is
    used.
    A RuntimeError is raised if that fails.
    """"""
    global _CONFIG_FN
    if _CONFIG_FN is not None:
        return _CONFIG_FN
    with _CONFIG_FN_LOCK:
        if _CONFIG_FN is not None:
            return _CONFIG_FN
        if 'PEYOTL_CONFIG_FILE' in os.environ:
            cfn = os.path.abspath(os.environ['PEYOTL_CONFIG_FILE'])
        else:
            cfn = os.path.expanduser(""~/.peyotl/config"")
        if not os.path.isfile(cfn):
            # noinspection PyProtectedMember
            if 'PEYOTL_CONFIG_FILE' in os.environ:
                from peyotl.utility.get_logger import warn_from_util_logger
                msg = 'Filepath ""{}"" specified via PEYOTL_CONFIG_FILE={} was not found'.format(cfn, os.environ[
                    'PEYOTL_CONFIG_FILE'])
                warn_from_util_logger(msg)
            from pkg_resources import Requirement, resource_filename
            pr = Requirement.parse('peyotl')
            cfn = resource_filename(pr, 'peyotl/default.conf')
        if not os.path.isfile(cfn):
            raise RuntimeError('The peyotl configuration file cascade failed looking for ""{}""'.format(cfn))
        _CONFIG_FN = os.path.abspath(cfn)
    return _CONFIG_FN",def,get_default_config_filename,(,),:,global,_CONFIG_FN,if,_CONFIG_FN,is,not,None,:,return,_CONFIG_FN,with,_CONFIG_FN_LOCK,:,if,_CONFIG_FN,is,not,None,:,return,_CONFIG_FN,if,'PEYOTL_CONFIG_FILE',in,os,.,environ,:,cfn,=,os,.,path,.,abspath,(,os,.,"Returns the configuration filepath.

    If PEYOTL_CONFIG_FILE is in the env that is the preferred choice; otherwise ~/.peyotl/config is preferred.
    If the preferred file does not exist, then the packaged peyotl/default.conf from the installation of peyotl is
    used.
    A RuntimeError is raised if that fails.",Returns,the,configuration,filepath,.,,,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/get_config.py#L73-L104,train,environ,[,'PEYOTL_CONFIG_FILE',],),else,:,cfn,=,os,.,path,.,expanduser,(,"""~/.peyotl/config""",),if,not,os,.,path,.,isfile,(,cfn,),:,# noinspection PyProtectedMember,if,,,,,,,,,,,,,,,,,,,,'PEYOTL_CONFIG_FILE',in,os,.,environ,:,from,peyotl,.,utility,.,get_logger,import,warn_from_util_logger,msg,=,"'Filepath ""{}"" specified via PEYOTL_CONFIG_FILE={} was not found'",.,format,(,cfn,",",os,.,environ,[,'PEYOTL_CONFIG_FILE',],),warn_from_util_logger,(,msg,),from,pkg_resources,import,Requirement,",",resource_filename,pr,=,Requirement,.,parse,(,'peyotl',),cfn,=,resource_filename,(,pr,",",'peyotl/default.conf',),if,not,os,.,path,.,isfile,(,cfn,),:,raise,RuntimeError,(,"'The peyotl configuration file cascade failed looking for ""{}""'",.,format,(,cfn,),),_CONFIG_FN,=,os,.,path,.,abspath,(,cfn,),return,_CONFIG_FN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/get_config.py,get_raw_default_config_and_read_file_list,"def get_raw_default_config_and_read_file_list():
    """"""Returns a ConfigParser object and a list of filenames that were parsed to initialize it""""""
    global _CONFIG, _READ_DEFAULT_FILES
    if _CONFIG is not None:
        return _CONFIG, _READ_DEFAULT_FILES
    with _CONFIG_LOCK:
        if _CONFIG is not None:
            return _CONFIG, _READ_DEFAULT_FILES
        try:
            # noinspection PyCompatibility
            from ConfigParser import SafeConfigParser
        except ImportError:
            # noinspection PyCompatibility,PyUnresolvedReferences
            from configparser import ConfigParser as SafeConfigParser  # pylint: disable=F0401
        cfg = SafeConfigParser()
        read_files = cfg.read(get_default_config_filename())
        _CONFIG, _READ_DEFAULT_FILES = cfg, read_files
        return _CONFIG, _READ_DEFAULT_FILES",python,"def get_raw_default_config_and_read_file_list():
    """"""Returns a ConfigParser object and a list of filenames that were parsed to initialize it""""""
    global _CONFIG, _READ_DEFAULT_FILES
    if _CONFIG is not None:
        return _CONFIG, _READ_DEFAULT_FILES
    with _CONFIG_LOCK:
        if _CONFIG is not None:
            return _CONFIG, _READ_DEFAULT_FILES
        try:
            # noinspection PyCompatibility
            from ConfigParser import SafeConfigParser
        except ImportError:
            # noinspection PyCompatibility,PyUnresolvedReferences
            from configparser import ConfigParser as SafeConfigParser  # pylint: disable=F0401
        cfg = SafeConfigParser()
        read_files = cfg.read(get_default_config_filename())
        _CONFIG, _READ_DEFAULT_FILES = cfg, read_files
        return _CONFIG, _READ_DEFAULT_FILES",def,get_raw_default_config_and_read_file_list,(,),:,global,_CONFIG,",",_READ_DEFAULT_FILES,if,_CONFIG,is,not,None,:,return,_CONFIG,",",_READ_DEFAULT_FILES,with,_CONFIG_LOCK,:,if,_CONFIG,is,not,None,:,return,_CONFIG,",",_READ_DEFAULT_FILES,try,:,# noinspection PyCompatibility,from,ConfigParser,import,SafeConfigParser,except,ImportError,:,"# noinspection PyCompatibility,PyUnresolvedReferences",Returns a ConfigParser object and a list of filenames that were parsed to initialize it,Returns,a,ConfigParser,object,and,a,list,of,filenames,that,were,parsed,to,initialize,it,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/get_config.py#L107-L124,train,from,configparser,import,ConfigParser,as,SafeConfigParser,# pylint: disable=F0401,cfg,=,SafeConfigParser,(,),read_files,=,cfg,.,read,(,get_default_config_filename,(,),),_CONFIG,",",_READ_DEFAULT_FILES,=,cfg,",",read_files,return,,,,,,,,,,,,,,,,,,,,_CONFIG,",",_READ_DEFAULT_FILES,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/get_config.py,get_config_object,"def get_config_object():
    """"""Thread-safe accessor for the immutable default ConfigWrapper object""""""
    global _DEFAULT_CONFIG_WRAPPER
    if _DEFAULT_CONFIG_WRAPPER is not None:
        return _DEFAULT_CONFIG_WRAPPER
    with _DEFAULT_CONFIG_WRAPPER_LOCK:
        if _DEFAULT_CONFIG_WRAPPER is not None:
            return _DEFAULT_CONFIG_WRAPPER
        _DEFAULT_CONFIG_WRAPPER = ConfigWrapper()
        return _DEFAULT_CONFIG_WRAPPER",python,"def get_config_object():
    """"""Thread-safe accessor for the immutable default ConfigWrapper object""""""
    global _DEFAULT_CONFIG_WRAPPER
    if _DEFAULT_CONFIG_WRAPPER is not None:
        return _DEFAULT_CONFIG_WRAPPER
    with _DEFAULT_CONFIG_WRAPPER_LOCK:
        if _DEFAULT_CONFIG_WRAPPER is not None:
            return _DEFAULT_CONFIG_WRAPPER
        _DEFAULT_CONFIG_WRAPPER = ConfigWrapper()
        return _DEFAULT_CONFIG_WRAPPER",def,get_config_object,(,),:,global,_DEFAULT_CONFIG_WRAPPER,if,_DEFAULT_CONFIG_WRAPPER,is,not,None,:,return,_DEFAULT_CONFIG_WRAPPER,with,_DEFAULT_CONFIG_WRAPPER_LOCK,:,if,_DEFAULT_CONFIG_WRAPPER,is,not,None,:,return,_DEFAULT_CONFIG_WRAPPER,_DEFAULT_CONFIG_WRAPPER,=,ConfigWrapper,(,),return,_DEFAULT_CONFIG_WRAPPER,,,,,,,,,,,Thread-safe accessor for the immutable default ConfigWrapper object,Thread,-,safe,accessor,for,the,immutable,default,ConfigWrapper,object,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/get_config.py#L315-L324,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/get_config.py,ConfigWrapper.get_from_config_setting_cascade,"def get_from_config_setting_cascade(self, sec_param_list, default=None, warn_on_none_level=logging.WARN):
        """"""return the first non-None setting from a series where each
        element in `sec_param_list` is a section, param pair suitable for
        a get_config_setting call.

        Note that non-None values for overrides for this ConfigWrapper instance will cause
            this call to only evaluate the first element in the cascade.
        """"""
        for section, param in sec_param_list:
            r = self.get_config_setting(section, param, default=None, warn_on_none_level=None)
            if r is not None:
                return r
        section, param = sec_param_list[-1]
        if default is None:
            _warn_missing_setting(section, param, self._config_filename, warn_on_none_level)
        return default",python,"def get_from_config_setting_cascade(self, sec_param_list, default=None, warn_on_none_level=logging.WARN):
        """"""return the first non-None setting from a series where each
        element in `sec_param_list` is a section, param pair suitable for
        a get_config_setting call.

        Note that non-None values for overrides for this ConfigWrapper instance will cause
            this call to only evaluate the first element in the cascade.
        """"""
        for section, param in sec_param_list:
            r = self.get_config_setting(section, param, default=None, warn_on_none_level=None)
            if r is not None:
                return r
        section, param = sec_param_list[-1]
        if default is None:
            _warn_missing_setting(section, param, self._config_filename, warn_on_none_level)
        return default",def,get_from_config_setting_cascade,(,self,",",sec_param_list,",",default,=,None,",",warn_on_none_level,=,logging,.,WARN,),:,for,section,",",param,in,sec_param_list,:,r,=,self,.,get_config_setting,(,section,",",param,",",default,=,None,",",warn_on_none_level,=,None,),"return the first non-None setting from a series where each
        element in `sec_param_list` is a section, param pair suitable for
        a get_config_setting call.

        Note that non-None values for overrides for this ConfigWrapper instance will cause
            this call to only evaluate the first element in the cascade.",return,the,first,non,-,None,setting,from,a,series,where,each,element,in,sec_param_list,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/get_config.py#L213-L228,train,if,r,is,not,None,:,return,r,section,",",param,=,sec_param_list,[,-,1,],if,default,is,None,:,_warn_missing_setting,(,section,",",param,",",self,.,is,a,section,param,pair,suitable,for,a,get_config_setting,call,.,,,,,,,,,_config_filename,",",warn_on_none_level,),return,default,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/generate_python.py,parse,"def parse(input_: Union[str, FileStream], source: str) -> Optional[str]:
    """"""Parse the text in infile and save the results in outfile

    :param input_: string or stream to parse
    :param source: source name for python file header
    :return: python text if successful
    """"""

    # Step 1: Tokenize the input stream
    error_listener = ParseErrorListener()
    if not isinstance(input_, FileStream):
        input_ = InputStream(input_)
    lexer = jsgLexer(input_)
    lexer.addErrorListener(error_listener)
    tokens = CommonTokenStream(lexer)
    tokens.fill()
    if error_listener.n_errors:
        return None

    # Step 2: Generate the parse tree
    parser = jsgParser(tokens)
    parser.addErrorListener(error_listener)
    parse_tree = parser.doc()
    if error_listener.n_errors:
        return None

    # Step 3: Transform the results the results
    parser = JSGDocParser()
    parser.visit(parse_tree)

    if parser.undefined_tokens():
        for tkn in parser.undefined_tokens():
            print(""Undefined token: "" + tkn)
        return None

    return parser.as_python(source)",python,"def parse(input_: Union[str, FileStream], source: str) -> Optional[str]:
    """"""Parse the text in infile and save the results in outfile

    :param input_: string or stream to parse
    :param source: source name for python file header
    :return: python text if successful
    """"""

    # Step 1: Tokenize the input stream
    error_listener = ParseErrorListener()
    if not isinstance(input_, FileStream):
        input_ = InputStream(input_)
    lexer = jsgLexer(input_)
    lexer.addErrorListener(error_listener)
    tokens = CommonTokenStream(lexer)
    tokens.fill()
    if error_listener.n_errors:
        return None

    # Step 2: Generate the parse tree
    parser = jsgParser(tokens)
    parser.addErrorListener(error_listener)
    parse_tree = parser.doc()
    if error_listener.n_errors:
        return None

    # Step 3: Transform the results the results
    parser = JSGDocParser()
    parser.visit(parse_tree)

    if parser.undefined_tokens():
        for tkn in parser.undefined_tokens():
            print(""Undefined token: "" + tkn)
        return None

    return parser.as_python(source)",def,parse,(,input_,:,Union,[,str,",",FileStream,],",",source,:,str,),->,Optional,[,str,],:,# Step 1: Tokenize the input stream,error_listener,=,ParseErrorListener,(,),if,not,isinstance,(,input_,",",FileStream,),:,input_,=,InputStream,(,input_,),"Parse the text in infile and save the results in outfile

    :param input_: string or stream to parse
    :param source: source name for python file header
    :return: python text if successful",Parse,the,text,in,infile,and,save,the,results,in,outfile,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/generate_python.py#L56-L91,train,lexer,=,jsgLexer,(,input_,),lexer,.,addErrorListener,(,error_listener,),tokens,=,CommonTokenStream,(,lexer,),tokens,.,fill,(,),if,error_listener,.,n_errors,:,return,None,,,,,,,,,,,,,,,,,,,,# Step 2: Generate the parse tree,parser,=,jsgParser,(,tokens,),parser,.,addErrorListener,(,error_listener,),parse_tree,=,parser,.,doc,(,),if,error_listener,.,n_errors,:,return,None,# Step 3: Transform the results the results,parser,=,JSGDocParser,(,),parser,.,visit,(,parse_tree,),if,parser,.,undefined_tokens,(,),:,for,tkn,in,parser,.,undefined_tokens,(,),:,print,(,"""Undefined token: """,+,tkn,),return,None,return,parser,.,as_python,(,source,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
globocom/tornado-alf,tornadoalf/client.py,Client.fetch,"def fetch(self, request, callback=None, raise_error=True, **kwargs):
        """"""Executes a request by AsyncHTTPClient,
        asynchronously returning an `tornado.HTTPResponse`.

           The ``raise_error=False`` argument currently suppresses
           *all* errors, encapsulating them in `HTTPResponse` objects
           following the tornado http-client standard
        """"""
        # accepts request as string then convert it to HTTPRequest
        if isinstance(request, str):
            request = HTTPRequest(request, **kwargs)

        try:
            # The first request calls tornado-client ignoring the
            # possible exception, in case of 401 response,
            # renews the access token and replay it
            response = yield self._authorized_fetch(request,
                                                    callback,
                                                    raise_error=False,
                                                    **kwargs)

            if response.code == BAD_TOKEN:
                yield self._token_manager.reset_token()
            elif response.error and raise_error:
                raise response.error
            else:
                raise gen.Return(response)

            # The request with renewed token
            response = yield self._authorized_fetch(request,
                                                    callback,
                                                    raise_error=raise_error,
                                                    **kwargs)
            raise gen.Return(response)

        except TokenError as err:
            yield self._token_manager.reset_token()
            raise err",python,"def fetch(self, request, callback=None, raise_error=True, **kwargs):
        """"""Executes a request by AsyncHTTPClient,
        asynchronously returning an `tornado.HTTPResponse`.

           The ``raise_error=False`` argument currently suppresses
           *all* errors, encapsulating them in `HTTPResponse` objects
           following the tornado http-client standard
        """"""
        # accepts request as string then convert it to HTTPRequest
        if isinstance(request, str):
            request = HTTPRequest(request, **kwargs)

        try:
            # The first request calls tornado-client ignoring the
            # possible exception, in case of 401 response,
            # renews the access token and replay it
            response = yield self._authorized_fetch(request,
                                                    callback,
                                                    raise_error=False,
                                                    **kwargs)

            if response.code == BAD_TOKEN:
                yield self._token_manager.reset_token()
            elif response.error and raise_error:
                raise response.error
            else:
                raise gen.Return(response)

            # The request with renewed token
            response = yield self._authorized_fetch(request,
                                                    callback,
                                                    raise_error=raise_error,
                                                    **kwargs)
            raise gen.Return(response)

        except TokenError as err:
            yield self._token_manager.reset_token()
            raise err",def,fetch,(,self,",",request,",",callback,=,None,",",raise_error,=,True,",",*,*,kwargs,),:,# accepts request as string then convert it to HTTPRequest,if,isinstance,(,request,",",str,),:,request,=,HTTPRequest,(,request,",",*,*,kwargs,),try,:,# The first request calls tornado-client ignoring the,"# possible exception, in case of 401 response,","Executes a request by AsyncHTTPClient,
        asynchronously returning an `tornado.HTTPResponse`.

           The ``raise_error=False`` argument currently suppresses
           *all* errors, encapsulating them in `HTTPResponse` objects
           following the tornado http-client standard",Executes,a,request,by,AsyncHTTPClient,asynchronously,returning,an,tornado,.,HTTPResponse,.,,,,3c3ec58c33f2d4ddfbed4ac18ca89d6beedf9c87,https://github.com/globocom/tornado-alf/blob/3c3ec58c33f2d4ddfbed4ac18ca89d6beedf9c87/tornadoalf/client.py#L29-L66,train,# renews the access token and replay it,response,=,yield,self,.,_authorized_fetch,(,request,",",callback,",",raise_error,=,False,",",*,*,kwargs,),if,response,.,code,==,BAD_TOKEN,:,yield,self,.,,,,,,,,,,,,,,,,,,,,_token_manager,.,reset_token,(,),elif,response,.,error,and,raise_error,:,raise,response,.,error,else,:,raise,gen,.,Return,(,response,),# The request with renewed token,response,=,yield,self,.,_authorized_fetch,(,request,",",callback,",",raise_error,=,raise_error,",",*,*,kwargs,),raise,gen,.,Return,(,response,),except,TokenError,as,err,:,yield,self,.,_token_manager,.,reset_token,(,),raise,err,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/validate_config.py,validate_config,"def validate_config(key: str, config: dict) -> None:
    """"""
    Call jsonschema validation to raise JSONValidation on non-compliance or silently pass.

    :param key: validation schema key of interest
    :param config: configuration dict to validate
    """"""

    try:
        jsonschema.validate(config, CONFIG_JSON_SCHEMA[key])
    except jsonschema.ValidationError as x_validation:
        raise JSONValidation('JSON validation error on {} configuration: {}'.format(key, x_validation.message))
    except jsonschema.SchemaError as x_schema:
        raise JSONValidation('JSON schema error on {} specification: {}'.format(key, x_schema.message))",python,"def validate_config(key: str, config: dict) -> None:
    """"""
    Call jsonschema validation to raise JSONValidation on non-compliance or silently pass.

    :param key: validation schema key of interest
    :param config: configuration dict to validate
    """"""

    try:
        jsonschema.validate(config, CONFIG_JSON_SCHEMA[key])
    except jsonschema.ValidationError as x_validation:
        raise JSONValidation('JSON validation error on {} configuration: {}'.format(key, x_validation.message))
    except jsonschema.SchemaError as x_schema:
        raise JSONValidation('JSON schema error on {} specification: {}'.format(key, x_schema.message))",def,validate_config,(,key,:,str,",",config,:,dict,),->,None,:,try,:,jsonschema,.,validate,(,config,",",CONFIG_JSON_SCHEMA,[,key,],),except,jsonschema,.,ValidationError,as,x_validation,:,raise,JSONValidation,(,'JSON validation error on {} configuration: {}',.,format,(,key,",","Call jsonschema validation to raise JSONValidation on non-compliance or silently pass.

    :param key: validation schema key of interest
    :param config: configuration dict to validate",Call,jsonschema,validation,to,raise,JSONValidation,on,non,-,compliance,or,silently,pass,.,,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/validate_config.py#L86-L99,train,x_validation,.,message,),),except,jsonschema,.,SchemaError,as,x_schema,:,raise,JSONValidation,(,'JSON schema error on {} specification: {}',.,format,(,key,",",x_schema,.,message,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/signals.py,__make_id,"def __make_id(receiver):
    """"""Generate an identifier for a callable signal receiver.

    This is used when disconnecting receivers, where we need to correctly
    establish equivalence between the input receiver and the receivers assigned
    to a signal.

    Args:
        receiver: A callable object.

    Returns:
        An identifier for the receiver.
    """"""
    if __is_bound_method(receiver):
        return (id(receiver.__func__), id(receiver.__self__))
    return id(receiver)",python,"def __make_id(receiver):
    """"""Generate an identifier for a callable signal receiver.

    This is used when disconnecting receivers, where we need to correctly
    establish equivalence between the input receiver and the receivers assigned
    to a signal.

    Args:
        receiver: A callable object.

    Returns:
        An identifier for the receiver.
    """"""
    if __is_bound_method(receiver):
        return (id(receiver.__func__), id(receiver.__self__))
    return id(receiver)",def,__make_id,(,receiver,),:,if,__is_bound_method,(,receiver,),:,return,(,id,(,receiver,.,__func__,),",",id,(,receiver,.,__self__,),),return,id,(,receiver,),,,,,,,,,,,"Generate an identifier for a callable signal receiver.

    This is used when disconnecting receivers, where we need to correctly
    establish equivalence between the input receiver and the receivers assigned
    to a signal.

    Args:
        receiver: A callable object.

    Returns:
        An identifier for the receiver.",Generate,an,identifier,for,a,callable,signal,receiver,.,,,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/signals.py#L41-L56,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/signals.py,__purge,"def __purge():
    """"""Remove all dead signal receivers from the global receivers collection.

    Note:
        It is assumed that the caller holds the __lock.
    """"""
    global __receivers
    newreceivers = collections.defaultdict(list)

    for signal, receivers in six.iteritems(__receivers):
        alive = [x for x in receivers if not __is_dead(x)]
        newreceivers[signal] = alive

    __receivers = newreceivers",python,"def __purge():
    """"""Remove all dead signal receivers from the global receivers collection.

    Note:
        It is assumed that the caller holds the __lock.
    """"""
    global __receivers
    newreceivers = collections.defaultdict(list)

    for signal, receivers in six.iteritems(__receivers):
        alive = [x for x in receivers if not __is_dead(x)]
        newreceivers[signal] = alive

    __receivers = newreceivers",def,__purge,(,),:,global,__receivers,newreceivers,=,collections,.,defaultdict,(,list,),for,signal,",",receivers,in,six,.,iteritems,(,__receivers,),:,alive,=,[,x,for,x,in,receivers,if,not,__is_dead,(,x,),],newreceivers,"Remove all dead signal receivers from the global receivers collection.

    Note:
        It is assumed that the caller holds the __lock.",Remove,all,dead,signal,receivers,from,the,global,receivers,collection,.,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/signals.py#L59-L72,train,[,signal,],=,alive,__receivers,=,newreceivers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/signals.py,__live_receivers,"def __live_receivers(signal):
    """"""Return all signal handlers that are currently still alive for the
    input `signal`.

    Args:
        signal: A signal name.

    Returns:
        A list of callable receivers for the input signal.
    """"""
    with __lock:
        __purge()
        receivers = [funcref() for funcref in __receivers[signal]]

    return receivers",python,"def __live_receivers(signal):
    """"""Return all signal handlers that are currently still alive for the
    input `signal`.

    Args:
        signal: A signal name.

    Returns:
        A list of callable receivers for the input signal.
    """"""
    with __lock:
        __purge()
        receivers = [funcref() for funcref in __receivers[signal]]

    return receivers",def,__live_receivers,(,signal,),:,with,__lock,:,__purge,(,),receivers,=,[,funcref,(,),for,funcref,in,__receivers,[,signal,],],return,receivers,,,,,,,,,,,,,,,,"Return all signal handlers that are currently still alive for the
    input `signal`.

    Args:
        signal: A signal name.

    Returns:
        A list of callable receivers for the input signal.",Return,all,signal,handlers,that,are,currently,still,alive,for,the,input,signal,.,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/signals.py#L75-L89,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/signals.py,__is_bound_method,"def __is_bound_method(method):
    """"""Return ``True`` if the `method` is a bound method (attached to an class
    instance.

    Args:
        method: A method or function type object.
    """"""
    if not(hasattr(method, ""__func__"") and hasattr(method, ""__self__"")):
        return False

    # Bound methods have a __self__ attribute pointing to the owner instance
    return six.get_method_self(method) is not None",python,"def __is_bound_method(method):
    """"""Return ``True`` if the `method` is a bound method (attached to an class
    instance.

    Args:
        method: A method or function type object.
    """"""
    if not(hasattr(method, ""__func__"") and hasattr(method, ""__self__"")):
        return False

    # Bound methods have a __self__ attribute pointing to the owner instance
    return six.get_method_self(method) is not None",def,__is_bound_method,(,method,),:,if,not,(,hasattr,(,method,",","""__func__""",),and,hasattr,(,method,",","""__self__""",),),:,return,False,# Bound methods have a __self__ attribute pointing to the owner instance,return,six,.,get_method_self,(,method,),is,not,None,,,,,,,"Return ``True`` if the `method` is a bound method (attached to an class
    instance.

    Args:
        method: A method or function type object.",Return,True,if,the,method,is,a,bound,method,(,attached,to,an,class,instance,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/signals.py#L92-L103,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/signals.py,disconnect,"def disconnect(signal, receiver):
    """"""Disconnect the receiver `func` from the signal, identified by
    `signal_id`.

    Args:
        signal: The signal identifier.
        receiver: The callable receiver to disconnect.

    Returns:
        True if the receiver was successfully disconnected. False otherwise.
    """"""
    inputkey = __make_id(receiver)

    with __lock:
        __purge()
        receivers = __receivers.get(signal)

        for idx in six.moves.range(len(receivers)):
            connected = receivers[idx]()

            if inputkey != __make_id(connected):
                continue

            del receivers[idx]
            return True  # receiver successfully disconnected!

    return False",python,"def disconnect(signal, receiver):
    """"""Disconnect the receiver `func` from the signal, identified by
    `signal_id`.

    Args:
        signal: The signal identifier.
        receiver: The callable receiver to disconnect.

    Returns:
        True if the receiver was successfully disconnected. False otherwise.
    """"""
    inputkey = __make_id(receiver)

    with __lock:
        __purge()
        receivers = __receivers.get(signal)

        for idx in six.moves.range(len(receivers)):
            connected = receivers[idx]()

            if inputkey != __make_id(connected):
                continue

            del receivers[idx]
            return True  # receiver successfully disconnected!

    return False",def,disconnect,(,signal,",",receiver,),:,inputkey,=,__make_id,(,receiver,),with,__lock,:,__purge,(,),receivers,=,__receivers,.,get,(,signal,),for,idx,in,six,.,moves,.,range,(,len,(,receivers,),),:,"Disconnect the receiver `func` from the signal, identified by
    `signal_id`.

    Args:
        signal: The signal identifier.
        receiver: The callable receiver to disconnect.

    Returns:
        True if the receiver was successfully disconnected. False otherwise.",Disconnect,the,receiver,func,from,the,signal,identified,by,signal_id,.,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/signals.py#L149-L175,train,connected,=,receivers,[,idx,],(,),if,inputkey,!=,__make_id,(,connected,),:,continue,del,receivers,[,idx,],return,True,# receiver successfully disconnected!,return,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/signals.py,emit,"def emit(signal, *args, **kwargs):
    """"""Emit a signal by serially calling each registered signal receiver for
    the `signal`.

    Note:
        The receiver must accept the *args and/or **kwargs that have been
        passed to it. There expected parameters are not dictated by
        mixbox.
        
    Args:
        signal: A signal identifier or name.
        *args: A variable-length argument list to pass to the receiver.
        **kwargs: Keyword-arguments to pass to the receiver.
    """"""
    if signal not in __receivers:
        return

    receivers = __live_receivers(signal)

    for func in receivers:
        func(*args, **kwargs)",python,"def emit(signal, *args, **kwargs):
    """"""Emit a signal by serially calling each registered signal receiver for
    the `signal`.

    Note:
        The receiver must accept the *args and/or **kwargs that have been
        passed to it. There expected parameters are not dictated by
        mixbox.
        
    Args:
        signal: A signal identifier or name.
        *args: A variable-length argument list to pass to the receiver.
        **kwargs: Keyword-arguments to pass to the receiver.
    """"""
    if signal not in __receivers:
        return

    receivers = __live_receivers(signal)

    for func in receivers:
        func(*args, **kwargs)",def,emit,(,signal,",",*,args,",",*,*,kwargs,),:,if,signal,not,in,__receivers,:,return,receivers,=,__live_receivers,(,signal,),for,func,in,receivers,:,func,(,*,args,",",*,*,kwargs,),,,,"Emit a signal by serially calling each registered signal receiver for
    the `signal`.

    Note:
        The receiver must accept the *args and/or **kwargs that have been
        passed to it. There expected parameters are not dictated by
        mixbox.
        
    Args:
        signal: A signal identifier or name.
        *args: A variable-length argument list to pass to the receiver.
        **kwargs: Keyword-arguments to pass to the receiver.",Emit,a,signal,by,serially,calling,each,registered,signal,receiver,for,the,signal,.,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/signals.py#L194-L214,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/fast.py,arrayuniqify,"def arrayuniqify(X, retainorder=False):
    """"""
    Very fast uniqify routine for numpy arrays.

    **Parameters**

            **X** :  numpy array

                    Determine the unique elements of this numpy array.

            **retainorder** :  Boolean, optional

                    Whether or not to return indices corresponding to unique 
                    values of `X` that also sort the values.  Default value is 
                    `False`, in which case `[D,s]` is returned.  This can be 
                    used to produce a uniqified version of `X` by simply 
                    taking::

                            X[s][D]

                    or::

                            X[s[D.nonzero()[0]]]

    **Returns**

            **D** :  numpy array

                    List of ""first differences"" in the sorted verion of `X`.  
                    Returned when `retainorder` is `False` (default).

            **s** :  numpy array

                    Permutation that will sort `X`.  Returned when 
                    `retainorder` is `False` (default).

            **ind** :  numpy array

                    List of indices that correspond to unique values of `X`, 
                    without sorting those values.  Returned when `retainorder` 
                    is `True`.

    **See Also:**

            :func:`tabular.fast.recarrayuniqify`

    """"""
    s = X.argsort()
    X = X[s]
    D = np.append([True],X[1:] != X[:-1])
    if retainorder:
        DD = np.append(D.nonzero()[0],len(X))
        ind = [min(s[x:DD[i+1]]) for (i,x) in enumerate(DD[:-1])]
        ind.sort()
        return ind
    else:
        return [D,s]",python,"def arrayuniqify(X, retainorder=False):
    """"""
    Very fast uniqify routine for numpy arrays.

    **Parameters**

            **X** :  numpy array

                    Determine the unique elements of this numpy array.

            **retainorder** :  Boolean, optional

                    Whether or not to return indices corresponding to unique 
                    values of `X` that also sort the values.  Default value is 
                    `False`, in which case `[D,s]` is returned.  This can be 
                    used to produce a uniqified version of `X` by simply 
                    taking::

                            X[s][D]

                    or::

                            X[s[D.nonzero()[0]]]

    **Returns**

            **D** :  numpy array

                    List of ""first differences"" in the sorted verion of `X`.  
                    Returned when `retainorder` is `False` (default).

            **s** :  numpy array

                    Permutation that will sort `X`.  Returned when 
                    `retainorder` is `False` (default).

            **ind** :  numpy array

                    List of indices that correspond to unique values of `X`, 
                    without sorting those values.  Returned when `retainorder` 
                    is `True`.

    **See Also:**

            :func:`tabular.fast.recarrayuniqify`

    """"""
    s = X.argsort()
    X = X[s]
    D = np.append([True],X[1:] != X[:-1])
    if retainorder:
        DD = np.append(D.nonzero()[0],len(X))
        ind = [min(s[x:DD[i+1]]) for (i,x) in enumerate(DD[:-1])]
        ind.sort()
        return ind
    else:
        return [D,s]",def,arrayuniqify,(,X,",",retainorder,=,False,),:,s,=,X,.,argsort,(,),X,=,X,[,s,],D,=,np,.,append,(,[,True,],",",X,[,1,:,],!=,X,[,:,-,"Very fast uniqify routine for numpy arrays.

    **Parameters**

            **X** :  numpy array

                    Determine the unique elements of this numpy array.

            **retainorder** :  Boolean, optional

                    Whether or not to return indices corresponding to unique 
                    values of `X` that also sort the values.  Default value is 
                    `False`, in which case `[D,s]` is returned.  This can be 
                    used to produce a uniqified version of `X` by simply 
                    taking::

                            X[s][D]

                    or::

                            X[s[D.nonzero()[0]]]

    **Returns**

            **D** :  numpy array

                    List of ""first differences"" in the sorted verion of `X`.  
                    Returned when `retainorder` is `False` (default).

            **s** :  numpy array

                    Permutation that will sort `X`.  Returned when 
                    `retainorder` is `False` (default).

            **ind** :  numpy array

                    List of indices that correspond to unique values of `X`, 
                    without sorting those values.  Returned when `retainorder` 
                    is `True`.

    **See Also:**

            :func:`tabular.fast.recarrayuniqify`",Very,fast,uniqify,routine,for,numpy,arrays,.,,,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/fast.py#L19-L75,train,1,],),if,retainorder,:,DD,=,np,.,append,(,D,.,nonzero,(,),[,0,],",",len,(,X,),),ind,=,[,min,,,,,,,,,,,,,,,,,,,,(,s,[,x,:,DD,[,i,+,1,],],),for,(,i,",",x,),in,enumerate,(,DD,[,:,-,1,],),],ind,.,sort,(,),return,ind,else,:,return,[,D,",",s,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/fast.py,equalspairs,"def equalspairs(X, Y):
    """"""
    Indices of elements in a sorted numpy array equal to those in another.

    Given numpy array `X` and sorted numpy array `Y`, determine the indices in 
    Y equal to indices in X.

    Returns `[A,B]` where `A` and `B` are numpy arrays of indices in `X` such 
    that::

            Y[A[i]:B[i]] = Y[Y == X[i]]`

    `A[i] = B[i] = 0` if `X[i]` is not in `Y`.

    **Parameters**

            **X** :  numpy array

                    Numpy array to compare to the sorted numpy array `Y`.

            **Y** :  numpy array

                    Sorted numpy array.  Determine the indices of elements of 
                    `Y` equal to those in numpy array `X`.

    **Returns**

            **A** :  numpy array

                    List of indices in `Y`, `len(A) = len(Y)`.

            **B** :  numpy array

                    List of indices in `Y`, `len(B) = len(Y)`.

    **See Also:**

            :func:`tabular.fast.recarrayequalspairs`

    """"""
    T = Y.copy()
    R = (T[1:] != T[:-1]).nonzero()[0]
    R = np.append(R,np.array([len(T)-1]))
    M = R[R.searchsorted(range(len(T)))]
    D = T.searchsorted(X)
    T = np.append(T,np.array([0]))
    M = np.append(M,np.array([0]))
    A = (T[D] == X) * D
    B = (T[D] == X) * (M[D] + 1)
    return [A,B]",python,"def equalspairs(X, Y):
    """"""
    Indices of elements in a sorted numpy array equal to those in another.

    Given numpy array `X` and sorted numpy array `Y`, determine the indices in 
    Y equal to indices in X.

    Returns `[A,B]` where `A` and `B` are numpy arrays of indices in `X` such 
    that::

            Y[A[i]:B[i]] = Y[Y == X[i]]`

    `A[i] = B[i] = 0` if `X[i]` is not in `Y`.

    **Parameters**

            **X** :  numpy array

                    Numpy array to compare to the sorted numpy array `Y`.

            **Y** :  numpy array

                    Sorted numpy array.  Determine the indices of elements of 
                    `Y` equal to those in numpy array `X`.

    **Returns**

            **A** :  numpy array

                    List of indices in `Y`, `len(A) = len(Y)`.

            **B** :  numpy array

                    List of indices in `Y`, `len(B) = len(Y)`.

    **See Also:**

            :func:`tabular.fast.recarrayequalspairs`

    """"""
    T = Y.copy()
    R = (T[1:] != T[:-1]).nonzero()[0]
    R = np.append(R,np.array([len(T)-1]))
    M = R[R.searchsorted(range(len(T)))]
    D = T.searchsorted(X)
    T = np.append(T,np.array([0]))
    M = np.append(M,np.array([0]))
    A = (T[D] == X) * D
    B = (T[D] == X) * (M[D] + 1)
    return [A,B]",def,equalspairs,(,X,",",Y,),:,T,=,Y,.,copy,(,),R,=,(,T,[,1,:,],!=,T,[,:,-,1,],),.,nonzero,(,),[,0,],R,=,np,.,append,"Indices of elements in a sorted numpy array equal to those in another.

    Given numpy array `X` and sorted numpy array `Y`, determine the indices in 
    Y equal to indices in X.

    Returns `[A,B]` where `A` and `B` are numpy arrays of indices in `X` such 
    that::

            Y[A[i]:B[i]] = Y[Y == X[i]]`

    `A[i] = B[i] = 0` if `X[i]` is not in `Y`.

    **Parameters**

            **X** :  numpy array

                    Numpy array to compare to the sorted numpy array `Y`.

            **Y** :  numpy array

                    Sorted numpy array.  Determine the indices of elements of 
                    `Y` equal to those in numpy array `X`.

    **Returns**

            **A** :  numpy array

                    List of indices in `Y`, `len(A) = len(Y)`.

            **B** :  numpy array

                    List of indices in `Y`, `len(B) = len(Y)`.

    **See Also:**

            :func:`tabular.fast.recarrayequalspairs`",Indices,of,elements,in,a,sorted,numpy,array,equal,to,those,in,another,.,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/fast.py#L142-L191,train,(,R,",",np,.,array,(,[,len,(,T,),-,1,],),),M,=,R,[,R,.,searchsorted,(,range,(,len,(,T,,,,,,,,,,,,,,,,,,,,),),),],D,=,T,.,searchsorted,(,X,),T,=,np,.,append,(,T,",",np,.,array,(,[,0,],),),M,=,np,.,append,(,M,",",np,.,array,(,[,0,],),),A,=,(,T,[,D,],==,X,),*,D,B,=,(,T,[,D,],==,X,),*,(,M,[,D,],+,1,),return,[,A,",",B,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/fast.py,isin,"def isin(X,Y):
    """"""
    Indices of elements in a numpy array that appear in another.

    Fast routine for determining indices of elements in numpy array `X` that 
    appear in numpy array `Y`, returning a boolean array `Z` such that::

            Z[i] = X[i] in Y

    **Parameters**

            **X** :  numpy array

                    Numpy array to comapare to numpy array `Y`.  For each 
                    element of `X`, ask if it is in `Y`.

            **Y** :  numpy array

                    Numpy array to which numpy array `X` is compared.  For each 
                    element of `X`, ask if it is in `Y`.

    **Returns**

            **b** :  numpy array (bool)

                    Boolean numpy array, `len(b) = len(X)`.

    **See Also:**

            :func:`tabular.fast.recarrayisin`, 
            :func:`tabular.fast.arraydifference`

    """"""
    if len(Y) > 0:
        T = Y.copy()
        T.sort()
        D = T.searchsorted(X)
        T = np.append(T,np.array([0]))
        W = (T[D] == X)
        if isinstance(W,bool):
            return np.zeros((len(X),),bool)
        else:
            return (T[D] == X)
    else:
        return np.zeros((len(X),),bool)",python,"def isin(X,Y):
    """"""
    Indices of elements in a numpy array that appear in another.

    Fast routine for determining indices of elements in numpy array `X` that 
    appear in numpy array `Y`, returning a boolean array `Z` such that::

            Z[i] = X[i] in Y

    **Parameters**

            **X** :  numpy array

                    Numpy array to comapare to numpy array `Y`.  For each 
                    element of `X`, ask if it is in `Y`.

            **Y** :  numpy array

                    Numpy array to which numpy array `X` is compared.  For each 
                    element of `X`, ask if it is in `Y`.

    **Returns**

            **b** :  numpy array (bool)

                    Boolean numpy array, `len(b) = len(X)`.

    **See Also:**

            :func:`tabular.fast.recarrayisin`, 
            :func:`tabular.fast.arraydifference`

    """"""
    if len(Y) > 0:
        T = Y.copy()
        T.sort()
        D = T.searchsorted(X)
        T = np.append(T,np.array([0]))
        W = (T[D] == X)
        if isinstance(W,bool):
            return np.zeros((len(X),),bool)
        else:
            return (T[D] == X)
    else:
        return np.zeros((len(X),),bool)",def,isin,(,X,",",Y,),:,if,len,(,Y,),>,0,:,T,=,Y,.,copy,(,),T,.,sort,(,),D,=,T,.,searchsorted,(,X,),T,=,np,.,append,(,T,"Indices of elements in a numpy array that appear in another.

    Fast routine for determining indices of elements in numpy array `X` that 
    appear in numpy array `Y`, returning a boolean array `Z` such that::

            Z[i] = X[i] in Y

    **Parameters**

            **X** :  numpy array

                    Numpy array to comapare to numpy array `Y`.  For each 
                    element of `X`, ask if it is in `Y`.

            **Y** :  numpy array

                    Numpy array to which numpy array `X` is compared.  For each 
                    element of `X`, ask if it is in `Y`.

    **Returns**

            **b** :  numpy array (bool)

                    Boolean numpy array, `len(b) = len(X)`.

    **See Also:**

            :func:`tabular.fast.recarrayisin`, 
            :func:`tabular.fast.arraydifference`",Indices,of,elements,in,a,numpy,array,that,appear,in,another,.,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/fast.py#L260-L304,train,",",np,.,array,(,[,0,],),),W,=,(,T,[,D,],==,X,),if,isinstance,(,W,",",bool,),:,return,np,,,,,,,,,,,,,,,,,,,,.,zeros,(,(,len,(,X,),",",),",",bool,),else,:,return,(,T,[,D,],==,X,),else,:,return,np,.,zeros,(,(,len,(,X,),",",),",",bool,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/fast.py,arraydifference,"def arraydifference(X,Y):
    """"""
    Elements of a numpy array that do not appear in another.

    Fast routine for determining which elements in numpy array `X`
    do not appear in numpy array `Y`.

    **Parameters**

            **X** :  numpy array

                    Numpy array to comapare to numpy array `Y`.
                    Return subset of `X` corresponding to elements not in `Y`.

            **Y** :  numpy array

                    Numpy array to which numpy array `X` is compared.
                    Return subset of `X` corresponding to elements not in `Y`.

    **Returns**

            **Z** :  numpy array

                    Subset of `X` corresponding to elements not in `Y`.

    **See Also:**

            :func:`tabular.fast.recarraydifference`, :func:`tabular.fast.isin`

    """"""
    if len(Y) > 0:
        Z = isin(X,Y)
        return X[np.invert(Z)]
    else:
        return X",python,"def arraydifference(X,Y):
    """"""
    Elements of a numpy array that do not appear in another.

    Fast routine for determining which elements in numpy array `X`
    do not appear in numpy array `Y`.

    **Parameters**

            **X** :  numpy array

                    Numpy array to comapare to numpy array `Y`.
                    Return subset of `X` corresponding to elements not in `Y`.

            **Y** :  numpy array

                    Numpy array to which numpy array `X` is compared.
                    Return subset of `X` corresponding to elements not in `Y`.

    **Returns**

            **Z** :  numpy array

                    Subset of `X` corresponding to elements not in `Y`.

    **See Also:**

            :func:`tabular.fast.recarraydifference`, :func:`tabular.fast.isin`

    """"""
    if len(Y) > 0:
        Z = isin(X,Y)
        return X[np.invert(Z)]
    else:
        return X",def,arraydifference,(,X,",",Y,),:,if,len,(,Y,),>,0,:,Z,=,isin,(,X,",",Y,),return,X,[,np,.,invert,(,Z,),],else,:,return,X,,,,,,"Elements of a numpy array that do not appear in another.

    Fast routine for determining which elements in numpy array `X`
    do not appear in numpy array `Y`.

    **Parameters**

            **X** :  numpy array

                    Numpy array to comapare to numpy array `Y`.
                    Return subset of `X` corresponding to elements not in `Y`.

            **Y** :  numpy array

                    Numpy array to which numpy array `X` is compared.
                    Return subset of `X` corresponding to elements not in `Y`.

    **Returns**

            **Z** :  numpy array

                    Subset of `X` corresponding to elements not in `Y`.

    **See Also:**

            :func:`tabular.fast.recarraydifference`, :func:`tabular.fast.isin`",Elements,of,a,numpy,array,that,do,not,appear,in,another,.,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/fast.py#L357-L391,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/fast.py,arraymax,"def arraymax(X,Y):
    """"""
    Fast ""vectorized"" max function for element-wise comparison of two numpy arrays.

    For two numpy arrays `X` and `Y` of equal length,
    return numpy array `Z` such that::

            Z[i] = max(X[i],Y[i])

    **Parameters**

            **X** :  numpy array

                    Numpy array; `len(X) = len(Y)`.

            **Y** :  numpy array

                    Numpy array; `len(Y) = len(X)`.

    **Returns**

            **Z** :  numpy array

                    Numpy array such that `Z[i] = max(X[i],Y[i])`.

    **See Also**

            :func:`tabular.fast.arraymin`

    """"""
    Z = np.zeros((len(X),), int)
    A = X <= Y
    B = Y < X
    Z[A] = Y[A]
    Z[B] = X[B]
    return Z",python,"def arraymax(X,Y):
    """"""
    Fast ""vectorized"" max function for element-wise comparison of two numpy arrays.

    For two numpy arrays `X` and `Y` of equal length,
    return numpy array `Z` such that::

            Z[i] = max(X[i],Y[i])

    **Parameters**

            **X** :  numpy array

                    Numpy array; `len(X) = len(Y)`.

            **Y** :  numpy array

                    Numpy array; `len(Y) = len(X)`.

    **Returns**

            **Z** :  numpy array

                    Numpy array such that `Z[i] = max(X[i],Y[i])`.

    **See Also**

            :func:`tabular.fast.arraymin`

    """"""
    Z = np.zeros((len(X),), int)
    A = X <= Y
    B = Y < X
    Z[A] = Y[A]
    Z[B] = X[B]
    return Z",def,arraymax,(,X,",",Y,),:,Z,=,np,.,zeros,(,(,len,(,X,),",",),",",int,),A,=,X,<=,Y,B,=,Y,<,X,Z,[,A,],=,Y,[,A,],"Fast ""vectorized"" max function for element-wise comparison of two numpy arrays.

    For two numpy arrays `X` and `Y` of equal length,
    return numpy array `Z` such that::

            Z[i] = max(X[i],Y[i])

    **Parameters**

            **X** :  numpy array

                    Numpy array; `len(X) = len(Y)`.

            **Y** :  numpy array

                    Numpy array; `len(Y) = len(X)`.

    **Returns**

            **Z** :  numpy array

                    Numpy array such that `Z[i] = max(X[i],Y[i])`.

    **See Also**

            :func:`tabular.fast.arraymin`",Fast,vectorized,max,function,for,element,-,wise,comparison,of,two,numpy,arrays,.,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/fast.py#L434-L469,train,Z,[,B,],=,X,[,B,],return,Z,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/wallet.py,Wallet._seed2did,"async def _seed2did(self) -> str:
        """"""
        Derive DID, as per indy-sdk, from seed.

        :return: DID
        """"""

        rv = None
        dids_with_meta = json.loads(await did.list_my_dids_with_meta(self.handle))  # list

        if dids_with_meta:
            for did_with_meta in dids_with_meta:  # dict
                if 'metadata' in did_with_meta:
                    try:
                        meta = json.loads(did_with_meta['metadata'])
                        if isinstance(meta, dict) and meta.get('seed', None) == self._seed:
                            rv = did_with_meta.get('did')
                    except json.decoder.JSONDecodeError:
                        continue  # it's not one of ours, carry on

        if not rv:  # seed not in metadata, generate did again on temp wallet
            temp_wallet = await Wallet(
                self._seed,
                '{}.seed2did'.format(self.name),
                None,
                {'auto-remove': True}).create()

            rv = temp_wallet.did
            await temp_wallet.remove()

        return rv",python,"async def _seed2did(self) -> str:
        """"""
        Derive DID, as per indy-sdk, from seed.

        :return: DID
        """"""

        rv = None
        dids_with_meta = json.loads(await did.list_my_dids_with_meta(self.handle))  # list

        if dids_with_meta:
            for did_with_meta in dids_with_meta:  # dict
                if 'metadata' in did_with_meta:
                    try:
                        meta = json.loads(did_with_meta['metadata'])
                        if isinstance(meta, dict) and meta.get('seed', None) == self._seed:
                            rv = did_with_meta.get('did')
                    except json.decoder.JSONDecodeError:
                        continue  # it's not one of ours, carry on

        if not rv:  # seed not in metadata, generate did again on temp wallet
            temp_wallet = await Wallet(
                self._seed,
                '{}.seed2did'.format(self.name),
                None,
                {'auto-remove': True}).create()

            rv = temp_wallet.did
            await temp_wallet.remove()

        return rv",async,def,_seed2did,(,self,),->,str,:,rv,=,None,dids_with_meta,=,json,.,loads,(,await,did,.,list_my_dids_with_meta,(,self,.,handle,),),# list,if,dids_with_meta,:,for,did_with_meta,in,dids_with_meta,:,# dict,if,'metadata',in,did_with_meta,:,"Derive DID, as per indy-sdk, from seed.

        :return: DID",Derive,DID,as,per,indy,-,sdk,from,seed,.,,,,,,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/wallet.py#L181-L211,train,try,:,meta,=,json,.,loads,(,did_with_meta,[,'metadata',],),if,isinstance,(,meta,",",dict,),and,meta,.,get,(,'seed',",",None,),==,,,,,,,,,,,,,,,,,,,,self,.,_seed,:,rv,=,did_with_meta,.,get,(,'did',),except,json,.,decoder,.,JSONDecodeError,:,continue,"# it's not one of ours, carry on",if,not,rv,:,"# seed not in metadata, generate did again on temp wallet",temp_wallet,=,await,Wallet,(,self,.,_seed,",",'{}.seed2did',.,format,(,self,.,name,),",",None,",",{,'auto-remove',:,True,},),.,create,(,),rv,=,temp_wallet,.,did,await,temp_wallet,.,remove,(,),return,rv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/wallet.py,Wallet.remove,"async def remove(self) -> None:
        """"""
        Remove serialized wallet if it exists.
        """"""

        LOGGER.debug('Wallet.remove >>>')

        try:
            LOGGER.info('Removing wallet: %s', self.name)
            await wallet.delete_wallet(json.dumps(self.cfg), json.dumps(self.access_creds))
        except IndyError as x_indy:
            LOGGER.info('Abstaining from wallet removal; indy-sdk error code %s', x_indy.error_code)

        LOGGER.debug('Wallet.remove <<<')",python,"async def remove(self) -> None:
        """"""
        Remove serialized wallet if it exists.
        """"""

        LOGGER.debug('Wallet.remove >>>')

        try:
            LOGGER.info('Removing wallet: %s', self.name)
            await wallet.delete_wallet(json.dumps(self.cfg), json.dumps(self.access_creds))
        except IndyError as x_indy:
            LOGGER.info('Abstaining from wallet removal; indy-sdk error code %s', x_indy.error_code)

        LOGGER.debug('Wallet.remove <<<')",async,def,remove,(,self,),->,None,:,LOGGER,.,debug,(,'Wallet.remove >>>',),try,:,LOGGER,.,info,(,'Removing wallet: %s',",",self,.,name,),await,wallet,.,delete_wallet,(,json,.,dumps,(,self,.,cfg,),",",json,.,Remove serialized wallet if it exists.,Remove,serialized,wallet,if,it,exists,.,,,,,,,,,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/wallet.py#L353-L366,train,dumps,(,self,.,access_creds,),),except,IndyError,as,x_indy,:,LOGGER,.,info,(,'Abstaining from wallet removal; indy-sdk error code %s',",",x_indy,.,error_code,),LOGGER,.,debug,(,'Wallet.remove <<<',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/io.py,loadSV,"def loadSV(fname, shape=None, titles=None, aligned=False, byteorder=None,  
           renamer=None, **kwargs):
    """"""
    Load a delimited text file to a numpy record array.

    Basically, this function calls loadSVcols and combines columns returned by 
    that function into a numpy ndarray with stuctured dtype.  Also uses and 
    returns metadata including column names, formats, coloring, &c. if these 
    items are determined during the loading process.

    **Parameters**

        **fname** :  string or file object

            Path (or file object) corresponding to a separated variable
            (CSV) text file.

         **names** : list of strings
                
            Sets the names of the columns of the resulting tabarray.   If 
            not specified, `names` value is determined first by looking for 
            metadata in the header of the file, and if that is not found, 
            are assigned by NumPy's `f0, f1, ... fn` convention.  See 
            **namesinheader** parameter below.
                
        **formats** :  string or list of strings
            
            Sets the datatypes of the columns.  The value of `formats` can 
            be a list or comma-delimited string of values describing values 
            for each column (e.g. ""str,str,int,float"" or 
            [""str"", ""str"", ""int"", ""float""]), a single value to apply to all 
            columns, or anything that can be used in numpy.rec.array 
            constructor.   
                
            If the **formats** (or **dtype**) parameter are not  specified, 
            typing is done by inference.  See **typer** parameter below.  
                                    
        **dtype** : numpy dtype object
             
            Sets the numpy dtype of the resulting tabarray, combining column 
            format and column name information.  If dtype is set, any 
            **names** and **formats** specifications will be overriden.  If 
            the **dtype** (or **formats**) parameter are not  specified, 
            typing is done by inference.  See **typer** parameter below.   

        The **names**, **formats** and **dtype** parameters duplicate 
        parameters of the NumPy record array creation inferface.  Additional 
        paramters of the NumPy inferface that are passed through are 
        **shape**, **titles**, **byteorder** and **aligned** (see NumPy 
        documentation for more information.)

    **kwargs**: keyword argument dictionary of variable length

        Contains various parameters to be passed down to loadSVcols.  These may 
        include  **skiprows**, **comments**, **delimiter**, **lineterminator**, 
        **uselines**, **usecols**, **excludecols**, **metametadata**, 
        **namesinheader**,**headerlines**, **valuefixer**, **linefixer**, 
        **colfixer**, **delimiter_regex**, **inflines**, **typer**, 
        **missingvalues**, **fillingvalues**, **verbosity**, and various CSV 
        module parameters like **escapechar**, **quoting**, **quotechar**, 
        **doublequote**, **skipinitialspace**.              

    **Returns**

        **R** :  numpy record array

            Record array constructed from data in the SV file

        **metadata** :  dictionary

            Metadata read and constructed during process of reading file.

    **See Also:**

            :func:`tabular.io.loadSVcols`, :func:`tabular.io.saveSV`, 
            :func:`tabular.io.DEFAULT_TYPEINFERER`

    """"""    
    [columns, metadata] = loadSVcols(fname, **kwargs)
    
    if 'names' in metadata.keys():
        names = metadata['names']
    else:
        names = None
 
    if 'formats' in metadata.keys():
        formats = metadata['formats']
    else:
        formats = None
    
    if 'dtype' in metadata.keys():
        dtype = metadata['dtype']
    else:
        dtype = None
 
    if renamer is not None:
        print 'Trying user-given renamer ...'
        renamed = renamer(names)
        if len(renamed) == len(uniqify(renamed)):
            names = renamed
            print '''... using renamed names (original names will be in return 
                     metadata)'''
        else:
            print '... renamer failed to produce unique names, not using.'
            
    if names and len(names) != len(uniqify(names)):
        print 'Names are not unique, reverting to default naming scheme.'
        names = None


    return [utils.fromarrays(columns, type=np.ndarray, dtype=dtype, 
                             shape=shape, formats=formats, names=names, 
                             titles=titles, aligned=aligned, 
                             byteorder=byteorder), metadata]",python,"def loadSV(fname, shape=None, titles=None, aligned=False, byteorder=None,  
           renamer=None, **kwargs):
    """"""
    Load a delimited text file to a numpy record array.

    Basically, this function calls loadSVcols and combines columns returned by 
    that function into a numpy ndarray with stuctured dtype.  Also uses and 
    returns metadata including column names, formats, coloring, &c. if these 
    items are determined during the loading process.

    **Parameters**

        **fname** :  string or file object

            Path (or file object) corresponding to a separated variable
            (CSV) text file.

         **names** : list of strings
                
            Sets the names of the columns of the resulting tabarray.   If 
            not specified, `names` value is determined first by looking for 
            metadata in the header of the file, and if that is not found, 
            are assigned by NumPy's `f0, f1, ... fn` convention.  See 
            **namesinheader** parameter below.
                
        **formats** :  string or list of strings
            
            Sets the datatypes of the columns.  The value of `formats` can 
            be a list or comma-delimited string of values describing values 
            for each column (e.g. ""str,str,int,float"" or 
            [""str"", ""str"", ""int"", ""float""]), a single value to apply to all 
            columns, or anything that can be used in numpy.rec.array 
            constructor.   
                
            If the **formats** (or **dtype**) parameter are not  specified, 
            typing is done by inference.  See **typer** parameter below.  
                                    
        **dtype** : numpy dtype object
             
            Sets the numpy dtype of the resulting tabarray, combining column 
            format and column name information.  If dtype is set, any 
            **names** and **formats** specifications will be overriden.  If 
            the **dtype** (or **formats**) parameter are not  specified, 
            typing is done by inference.  See **typer** parameter below.   

        The **names**, **formats** and **dtype** parameters duplicate 
        parameters of the NumPy record array creation inferface.  Additional 
        paramters of the NumPy inferface that are passed through are 
        **shape**, **titles**, **byteorder** and **aligned** (see NumPy 
        documentation for more information.)

    **kwargs**: keyword argument dictionary of variable length

        Contains various parameters to be passed down to loadSVcols.  These may 
        include  **skiprows**, **comments**, **delimiter**, **lineterminator**, 
        **uselines**, **usecols**, **excludecols**, **metametadata**, 
        **namesinheader**,**headerlines**, **valuefixer**, **linefixer**, 
        **colfixer**, **delimiter_regex**, **inflines**, **typer**, 
        **missingvalues**, **fillingvalues**, **verbosity**, and various CSV 
        module parameters like **escapechar**, **quoting**, **quotechar**, 
        **doublequote**, **skipinitialspace**.              

    **Returns**

        **R** :  numpy record array

            Record array constructed from data in the SV file

        **metadata** :  dictionary

            Metadata read and constructed during process of reading file.

    **See Also:**

            :func:`tabular.io.loadSVcols`, :func:`tabular.io.saveSV`, 
            :func:`tabular.io.DEFAULT_TYPEINFERER`

    """"""    
    [columns, metadata] = loadSVcols(fname, **kwargs)
    
    if 'names' in metadata.keys():
        names = metadata['names']
    else:
        names = None
 
    if 'formats' in metadata.keys():
        formats = metadata['formats']
    else:
        formats = None
    
    if 'dtype' in metadata.keys():
        dtype = metadata['dtype']
    else:
        dtype = None
 
    if renamer is not None:
        print 'Trying user-given renamer ...'
        renamed = renamer(names)
        if len(renamed) == len(uniqify(renamed)):
            names = renamed
            print '''... using renamed names (original names will be in return 
                     metadata)'''
        else:
            print '... renamer failed to produce unique names, not using.'
            
    if names and len(names) != len(uniqify(names)):
        print 'Names are not unique, reverting to default naming scheme.'
        names = None


    return [utils.fromarrays(columns, type=np.ndarray, dtype=dtype, 
                             shape=shape, formats=formats, names=names, 
                             titles=titles, aligned=aligned, 
                             byteorder=byteorder), metadata]",def,loadSV,(,fname,",",shape,=,None,",",titles,=,None,",",aligned,=,False,",",byteorder,=,None,",",renamer,=,None,",",*,*,kwargs,),:,[,columns,",",metadata,],=,loadSVcols,(,fname,",",*,*,kwargs,"Load a delimited text file to a numpy record array.

    Basically, this function calls loadSVcols and combines columns returned by 
    that function into a numpy ndarray with stuctured dtype.  Also uses and 
    returns metadata including column names, formats, coloring, &c. if these 
    items are determined during the loading process.

    **Parameters**

        **fname** :  string or file object

            Path (or file object) corresponding to a separated variable
            (CSV) text file.

         **names** : list of strings
                
            Sets the names of the columns of the resulting tabarray.   If 
            not specified, `names` value is determined first by looking for 
            metadata in the header of the file, and if that is not found, 
            are assigned by NumPy's `f0, f1, ... fn` convention.  See 
            **namesinheader** parameter below.
                
        **formats** :  string or list of strings
            
            Sets the datatypes of the columns.  The value of `formats` can 
            be a list or comma-delimited string of values describing values 
            for each column (e.g. ""str,str,int,float"" or 
            [""str"", ""str"", ""int"", ""float""]), a single value to apply to all 
            columns, or anything that can be used in numpy.rec.array 
            constructor.   
                
            If the **formats** (or **dtype**) parameter are not  specified, 
            typing is done by inference.  See **typer** parameter below.  
                                    
        **dtype** : numpy dtype object
             
            Sets the numpy dtype of the resulting tabarray, combining column 
            format and column name information.  If dtype is set, any 
            **names** and **formats** specifications will be overriden.  If 
            the **dtype** (or **formats**) parameter are not  specified, 
            typing is done by inference.  See **typer** parameter below.   

        The **names**, **formats** and **dtype** parameters duplicate 
        parameters of the NumPy record array creation inferface.  Additional 
        paramters of the NumPy inferface that are passed through are 
        **shape**, **titles**, **byteorder** and **aligned** (see NumPy 
        documentation for more information.)

    **kwargs**: keyword argument dictionary of variable length

        Contains various parameters to be passed down to loadSVcols.  These may 
        include  **skiprows**, **comments**, **delimiter**, **lineterminator**, 
        **uselines**, **usecols**, **excludecols**, **metametadata**, 
        **namesinheader**,**headerlines**, **valuefixer**, **linefixer**, 
        **colfixer**, **delimiter_regex**, **inflines**, **typer**, 
        **missingvalues**, **fillingvalues**, **verbosity**, and various CSV 
        module parameters like **escapechar**, **quoting**, **quotechar**, 
        **doublequote**, **skipinitialspace**.              

    **Returns**

        **R** :  numpy record array

            Record array constructed from data in the SV file

        **metadata** :  dictionary

            Metadata read and constructed during process of reading file.

    **See Also:**

            :func:`tabular.io.loadSVcols`, :func:`tabular.io.saveSV`, 
            :func:`tabular.io.DEFAULT_TYPEINFERER`",Load,a,delimited,text,file,to,a,numpy,record,array,.,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/io.py#L39-L152,train,),if,'names',in,metadata,.,keys,(,),:,names,=,metadata,[,'names',],else,:,names,=,None,if,'formats',in,metadata,.,keys,(,),:,,,,,,,,,,,,,,,,,,,,formats,=,metadata,[,'formats',],else,:,formats,=,None,if,'dtype',in,metadata,.,keys,(,),:,dtype,=,metadata,[,'dtype',],else,:,dtype,=,None,if,renamer,is,not,None,:,print,'Trying user-given renamer ...',renamed,=,renamer,(,names,),if,len,(,renamed,),==,len,(,uniqify,(,renamed,),),:,names,=,renamed,print,"'''... using renamed names (original names will be in return 
                     metadata)'''",else,:,print,"'... renamer failed to produce unique names, not using.'",if,names,and,len,(,names,),!=,len,(,uniqify,(,names,),),:,print,"'Names are not unique, reverting to default naming scheme.'",names,=,None,return,[,utils,.,fromarrays,(,columns,",",type,=,np,.,ndarray,",",dtype,=,dtype,",",shape,=,shape,",",formats,=,formats,",",names,=,names,",",titles,=,titles,",",aligned,=,aligned,",",byteorder,=,byteorder,),",",metadata,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/io.py,loadSVrecs,"def loadSVrecs(fname, uselines=None, skiprows=0, linefixer=None, 
               delimiter_regex=None, verbosity=DEFAULT_VERBOSITY, **metadata):
    """"""
    Load a separated value text file to a list of lists of strings of records.

    Takes a tabular text file with a specified delimeter and end-of-line 
    character, and return data as a list of lists of strings corresponding to 
    records (rows).  Also uses and returns metadata (including column names, 
    formats, coloring, &c.) if these items are determined during the loading 
    process.   

    **Parameters**

        **fname** :  string or file object

            Path (or file object) corresponding to a separated variable
            (CSV) text file.
 
        **delimiter** : single-character string
        
            When reading text file, character to use as delimiter to split 
            fields.  If not specified, the delimiter is determined first by 
            looking for special-format metadata specifying the delimiter, and 
            then if no specification is found, attempts are made to infer 
            delimiter from file contents.  (See **inflines** parameter below.)  
            
        **delimiter_regex** : regular expression (compiled or in string format)                    
         
            Regular expression to use to recognize delimiters, in place of a 
            single character.  (For instance, to have whitespace delimiting, 
            using delimiter_regex = '[\s*]+')
                         
        **lineterminator** : single-character string
        
            Line terminator to use when reading in using SVfile.
            
        **skipinitialspace** : boolean
        
            If true, strips whitespace following the delimiter from field.   
            
       The **delimiter**, **linterminator** and **skipinitialspace** 
       parameters are passed on as parameters to the python CSV module, which is 
       used for reading in delimited text files.  Additional parameters from 
       that interface that are replicated in this constructor include 
       **quotechar**, **escapechar**, **quoting**, **doublequote** and 
       **dialect** (see CSV module documentation for more information).

        **skiprows** :  non-negative integer, optional

            When reading from a text file, the first `skiprows` lines are 
            ignored.  Default is 0, e.g no rows are skipped. 

        **uselines** : pair of non-negative integer, optional
        
            When reading from a text file, range of lines of data to load.  (In 
            contrast to **skiprows**, which specifies file rows to ignore 
            before looking for header information, **uselines** specifies which 
            data (non-header) lines to use, after header has been striped and 
            processed.)  See **headerlines** below.
            
        **comments** : single-character string, optional
            
            When reading from a text file, character used to distinguish header 
            lines.  If specified, any lines beginning with this character at the 
            top of the file are assumed to contain header information and not 
            row data. 
      
        **headerlines** : integer, optional

            When reading from a text file, the number of lines at the top of the 
            file (after the first  `skiprows` lines) corresponding to the header 
            of the file, where metadata can be found.  Lines after headerlines 
            are assumed to contain row contents.  If not specified, value is 
            determined first by looking for special metametadata  in first line 
            of file (see Tabular reference documentation for more information 
            about this), and if no such metadata is found, is inferred by 
            looking at file contents.    
            
        **namesinheader** : Boolean, optional

            When reading from a text file, if `namesinheader == True`, then 
            assume the column names are in the last header line (unless 
            overridden by existing metadata or metametadata directive).  Default 
            is True.                        
            
        **linefixer** : callable, optional

           This callable is applied to every line in the file.  If specified, 
           the called is applied directly to the strings in the file, after 
           they're split in lines but before they're split into fields.  The 
           purpose is to make lines with errors or mistakes amenable to 
           delimiter inference and field-splitting. 
            
        **inflines** :  integer, optional
        
            Number of lines of file to use as sample data when inferring 
            delimiter and header.   

        **metametadata** :  dictionary of integers or pairs of integers
            
            Specifies supplementary metametadata information for use 
            with SVfile loading.  See Tabular reference documentation for more 
            information
            
    **Returns**

            **records** :  list of lists of strings

                List of lists corresponding to records (rows) of data.

            **metadata** :  dictionary

                Metadata read and constructed during process of reading file.

    **See Also:**

            :func:`tabular.io.loadSV`, :func:`tabular.io.saveSV`, 
            :func:`tabular.io.DEFAULT_TYPEINFERER`

    """"""
    if delimiter_regex and isinstance(delimiter_regex, types.StringType):
        import re
        delimiter_regex = re.compile(delimiter_regex) 
   
    [metadata, inferedlines, WHOLETHING] = getmetadata(fname, skiprows=skiprows,
                                                linefixer=linefixer, 
                                                delimiter_regex=delimiter_regex, 
                                                verbosity=verbosity, **metadata)

    if uselines is None:
        uselines = (0,False)
    
    if is_string_like(fname):
        fh = file(fname, 'rU')
    elif hasattr(fname, 'readline'):
        fh = fname
    else:
        raise ValueError('fname must be a string or file handle') 
 
    for _ind in range(skiprows+uselines[0] + metadata['headerlines']):
        fh.readline()
        
    if linefixer or delimiter_regex:
        fh2 = tempfile.TemporaryFile('w+b')
        F = fh.read().strip('\n').split('\n')
        if linefixer:
            F = map(linefixer,F)
        if delimiter_regex:
            F = map(lambda line: 
                    delimiter_regex.sub(metadata['dialect'].delimiter, line), F)       
        fh2.write('\n'.join(F))        
        fh2.seek(0)
        fh = fh2        

    reader = csv.reader(fh, dialect=metadata['dialect'])

    if uselines[1]:
        linelist = []
        for ln in reader:
            if reader.line_num <= uselines[1] - uselines[0]:
                linelist.append(ln)
            else:
                break
    else:
        linelist = list(reader)
      
    fh.close()

    if linelist[-1] == []:
        linelist.pop(-1)

    return [linelist,metadata]",python,"def loadSVrecs(fname, uselines=None, skiprows=0, linefixer=None, 
               delimiter_regex=None, verbosity=DEFAULT_VERBOSITY, **metadata):
    """"""
    Load a separated value text file to a list of lists of strings of records.

    Takes a tabular text file with a specified delimeter and end-of-line 
    character, and return data as a list of lists of strings corresponding to 
    records (rows).  Also uses and returns metadata (including column names, 
    formats, coloring, &c.) if these items are determined during the loading 
    process.   

    **Parameters**

        **fname** :  string or file object

            Path (or file object) corresponding to a separated variable
            (CSV) text file.
 
        **delimiter** : single-character string
        
            When reading text file, character to use as delimiter to split 
            fields.  If not specified, the delimiter is determined first by 
            looking for special-format metadata specifying the delimiter, and 
            then if no specification is found, attempts are made to infer 
            delimiter from file contents.  (See **inflines** parameter below.)  
            
        **delimiter_regex** : regular expression (compiled or in string format)                    
         
            Regular expression to use to recognize delimiters, in place of a 
            single character.  (For instance, to have whitespace delimiting, 
            using delimiter_regex = '[\s*]+')
                         
        **lineterminator** : single-character string
        
            Line terminator to use when reading in using SVfile.
            
        **skipinitialspace** : boolean
        
            If true, strips whitespace following the delimiter from field.   
            
       The **delimiter**, **linterminator** and **skipinitialspace** 
       parameters are passed on as parameters to the python CSV module, which is 
       used for reading in delimited text files.  Additional parameters from 
       that interface that are replicated in this constructor include 
       **quotechar**, **escapechar**, **quoting**, **doublequote** and 
       **dialect** (see CSV module documentation for more information).

        **skiprows** :  non-negative integer, optional

            When reading from a text file, the first `skiprows` lines are 
            ignored.  Default is 0, e.g no rows are skipped. 

        **uselines** : pair of non-negative integer, optional
        
            When reading from a text file, range of lines of data to load.  (In 
            contrast to **skiprows**, which specifies file rows to ignore 
            before looking for header information, **uselines** specifies which 
            data (non-header) lines to use, after header has been striped and 
            processed.)  See **headerlines** below.
            
        **comments** : single-character string, optional
            
            When reading from a text file, character used to distinguish header 
            lines.  If specified, any lines beginning with this character at the 
            top of the file are assumed to contain header information and not 
            row data. 
      
        **headerlines** : integer, optional

            When reading from a text file, the number of lines at the top of the 
            file (after the first  `skiprows` lines) corresponding to the header 
            of the file, where metadata can be found.  Lines after headerlines 
            are assumed to contain row contents.  If not specified, value is 
            determined first by looking for special metametadata  in first line 
            of file (see Tabular reference documentation for more information 
            about this), and if no such metadata is found, is inferred by 
            looking at file contents.    
            
        **namesinheader** : Boolean, optional

            When reading from a text file, if `namesinheader == True`, then 
            assume the column names are in the last header line (unless 
            overridden by existing metadata or metametadata directive).  Default 
            is True.                        
            
        **linefixer** : callable, optional

           This callable is applied to every line in the file.  If specified, 
           the called is applied directly to the strings in the file, after 
           they're split in lines but before they're split into fields.  The 
           purpose is to make lines with errors or mistakes amenable to 
           delimiter inference and field-splitting. 
            
        **inflines** :  integer, optional
        
            Number of lines of file to use as sample data when inferring 
            delimiter and header.   

        **metametadata** :  dictionary of integers or pairs of integers
            
            Specifies supplementary metametadata information for use 
            with SVfile loading.  See Tabular reference documentation for more 
            information
            
    **Returns**

            **records** :  list of lists of strings

                List of lists corresponding to records (rows) of data.

            **metadata** :  dictionary

                Metadata read and constructed during process of reading file.

    **See Also:**

            :func:`tabular.io.loadSV`, :func:`tabular.io.saveSV`, 
            :func:`tabular.io.DEFAULT_TYPEINFERER`

    """"""
    if delimiter_regex and isinstance(delimiter_regex, types.StringType):
        import re
        delimiter_regex = re.compile(delimiter_regex) 
   
    [metadata, inferedlines, WHOLETHING] = getmetadata(fname, skiprows=skiprows,
                                                linefixer=linefixer, 
                                                delimiter_regex=delimiter_regex, 
                                                verbosity=verbosity, **metadata)

    if uselines is None:
        uselines = (0,False)
    
    if is_string_like(fname):
        fh = file(fname, 'rU')
    elif hasattr(fname, 'readline'):
        fh = fname
    else:
        raise ValueError('fname must be a string or file handle') 
 
    for _ind in range(skiprows+uselines[0] + metadata['headerlines']):
        fh.readline()
        
    if linefixer or delimiter_regex:
        fh2 = tempfile.TemporaryFile('w+b')
        F = fh.read().strip('\n').split('\n')
        if linefixer:
            F = map(linefixer,F)
        if delimiter_regex:
            F = map(lambda line: 
                    delimiter_regex.sub(metadata['dialect'].delimiter, line), F)       
        fh2.write('\n'.join(F))        
        fh2.seek(0)
        fh = fh2        

    reader = csv.reader(fh, dialect=metadata['dialect'])

    if uselines[1]:
        linelist = []
        for ln in reader:
            if reader.line_num <= uselines[1] - uselines[0]:
                linelist.append(ln)
            else:
                break
    else:
        linelist = list(reader)
      
    fh.close()

    if linelist[-1] == []:
        linelist.pop(-1)

    return [linelist,metadata]",def,loadSVrecs,(,fname,",",uselines,=,None,",",skiprows,=,0,",",linefixer,=,None,",",delimiter_regex,=,None,",",verbosity,=,DEFAULT_VERBOSITY,",",*,*,metadata,),:,if,delimiter_regex,and,isinstance,(,delimiter_regex,",",types,.,StringType,),:,import,"Load a separated value text file to a list of lists of strings of records.

    Takes a tabular text file with a specified delimeter and end-of-line 
    character, and return data as a list of lists of strings corresponding to 
    records (rows).  Also uses and returns metadata (including column names, 
    formats, coloring, &c.) if these items are determined during the loading 
    process.   

    **Parameters**

        **fname** :  string or file object

            Path (or file object) corresponding to a separated variable
            (CSV) text file.
 
        **delimiter** : single-character string
        
            When reading text file, character to use as delimiter to split 
            fields.  If not specified, the delimiter is determined first by 
            looking for special-format metadata specifying the delimiter, and 
            then if no specification is found, attempts are made to infer 
            delimiter from file contents.  (See **inflines** parameter below.)  
            
        **delimiter_regex** : regular expression (compiled or in string format)                    
         
            Regular expression to use to recognize delimiters, in place of a 
            single character.  (For instance, to have whitespace delimiting, 
            using delimiter_regex = '[\s*]+')
                         
        **lineterminator** : single-character string
        
            Line terminator to use when reading in using SVfile.
            
        **skipinitialspace** : boolean
        
            If true, strips whitespace following the delimiter from field.   
            
       The **delimiter**, **linterminator** and **skipinitialspace** 
       parameters are passed on as parameters to the python CSV module, which is 
       used for reading in delimited text files.  Additional parameters from 
       that interface that are replicated in this constructor include 
       **quotechar**, **escapechar**, **quoting**, **doublequote** and 
       **dialect** (see CSV module documentation for more information).

        **skiprows** :  non-negative integer, optional

            When reading from a text file, the first `skiprows` lines are 
            ignored.  Default is 0, e.g no rows are skipped. 

        **uselines** : pair of non-negative integer, optional
        
            When reading from a text file, range of lines of data to load.  (In 
            contrast to **skiprows**, which specifies file rows to ignore 
            before looking for header information, **uselines** specifies which 
            data (non-header) lines to use, after header has been striped and 
            processed.)  See **headerlines** below.
            
        **comments** : single-character string, optional
            
            When reading from a text file, character used to distinguish header 
            lines.  If specified, any lines beginning with this character at the 
            top of the file are assumed to contain header information and not 
            row data. 
      
        **headerlines** : integer, optional

            When reading from a text file, the number of lines at the top of the 
            file (after the first  `skiprows` lines) corresponding to the header 
            of the file, where metadata can be found.  Lines after headerlines 
            are assumed to contain row contents.  If not specified, value is 
            determined first by looking for special metametadata  in first line 
            of file (see Tabular reference documentation for more information 
            about this), and if no such metadata is found, is inferred by 
            looking at file contents.    
            
        **namesinheader** : Boolean, optional

            When reading from a text file, if `namesinheader == True`, then 
            assume the column names are in the last header line (unless 
            overridden by existing metadata or metametadata directive).  Default 
            is True.                        
            
        **linefixer** : callable, optional

           This callable is applied to every line in the file.  If specified, 
           the called is applied directly to the strings in the file, after 
           they're split in lines but before they're split into fields.  The 
           purpose is to make lines with errors or mistakes amenable to 
           delimiter inference and field-splitting. 
            
        **inflines** :  integer, optional
        
            Number of lines of file to use as sample data when inferring 
            delimiter and header.   

        **metametadata** :  dictionary of integers or pairs of integers
            
            Specifies supplementary metametadata information for use 
            with SVfile loading.  See Tabular reference documentation for more 
            information
            
    **Returns**

            **records** :  list of lists of strings

                List of lists corresponding to records (rows) of data.

            **metadata** :  dictionary

                Metadata read and constructed during process of reading file.

    **See Also:**

            :func:`tabular.io.loadSV`, :func:`tabular.io.saveSV`, 
            :func:`tabular.io.DEFAULT_TYPEINFERER`",Load,a,separated,value,text,file,to,a,list,of,lists,of,strings,of,records,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/io.py#L401-L572,train,re,delimiter_regex,=,re,.,compile,(,delimiter_regex,),[,metadata,",",inferedlines,",",WHOLETHING,],=,getmetadata,(,fname,",",skiprows,=,skiprows,",",linefixer,=,linefixer,",",delimiter_regex,.,,,,,,,,,,,,,,,,,,,=,delimiter_regex,",",verbosity,=,verbosity,",",*,*,metadata,),if,uselines,is,None,:,uselines,=,(,0,",",False,),if,is_string_like,(,fname,),:,fh,=,file,(,fname,",",'rU',),elif,hasattr,(,fname,",",'readline',),:,fh,=,fname,else,:,raise,ValueError,(,'fname must be a string or file handle',),for,_ind,in,range,(,skiprows,+,uselines,[,0,],+,metadata,[,'headerlines',],),:,fh,.,readline,(,),if,linefixer,or,delimiter_regex,:,fh2,=,tempfile,.,TemporaryFile,(,'w+b',),F,=,fh,.,read,(,),.,strip,(,'\n',),.,split,(,'\n',),if,linefixer,:,F,=,map,(,linefixer,",",F,),if,delimiter_regex,:,F,=,map,(,lambda,line,:,delimiter_regex,.,sub,(,metadata,[,'dialect',],.,delimiter,",",line,),",",F,),fh2,.,write,(,'\n',.,join,(,F,),),fh2,.,seek,(,0,),fh,=,fh2,reader,=,csv,.,reader,(,fh,",",dialect,=,metadata,[,'dialect',],),if,uselines,[,1,],:,linelist,=,[,],for,ln,in,reader,:,if,reader,.,line_num,<=,uselines,[,1,],-,uselines,[,0,],:,linelist,.,append,(,ln,),else,:,break,else,:,linelist,=,list,(,reader,),fh,.,close,(,),if,linelist,[,-,1,],==,[,],:,linelist,.,pop,(,-,1,),return,[,linelist,",",metadata,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/io.py,parsetypes,"def parsetypes(dtype):
    """"""
    Parse the types from a structured numpy dtype object.

    Return list of string representations of types from a structured numpy 
    dtype object, e.g. ['int', 'float', 'str'].

    Used by :func:`tabular.io.saveSV` to write out type information in the 
    header.

    **Parameters**

        **dtype** :  numpy dtype object

            Structured numpy dtype object to parse.

    **Returns**

        **out** :  list of strings

            List of strings corresponding to numpy types::

                [dtype[i].name.strip('1234567890').rstrip('ing') \ 
                 for i in range(len(dtype))]

    """"""
    return [dtype[i].name.strip('1234567890').rstrip('ing') 
            for i in range(len(dtype))]",python,"def parsetypes(dtype):
    """"""
    Parse the types from a structured numpy dtype object.

    Return list of string representations of types from a structured numpy 
    dtype object, e.g. ['int', 'float', 'str'].

    Used by :func:`tabular.io.saveSV` to write out type information in the 
    header.

    **Parameters**

        **dtype** :  numpy dtype object

            Structured numpy dtype object to parse.

    **Returns**

        **out** :  list of strings

            List of strings corresponding to numpy types::

                [dtype[i].name.strip('1234567890').rstrip('ing') \ 
                 for i in range(len(dtype))]

    """"""
    return [dtype[i].name.strip('1234567890').rstrip('ing') 
            for i in range(len(dtype))]",def,parsetypes,(,dtype,),:,return,[,dtype,[,i,],.,name,.,strip,(,'1234567890',),.,rstrip,(,'ing',),for,i,in,range,(,len,(,dtype,),),],,,,,,,,,"Parse the types from a structured numpy dtype object.

    Return list of string representations of types from a structured numpy 
    dtype object, e.g. ['int', 'float', 'str'].

    Used by :func:`tabular.io.saveSV` to write out type information in the 
    header.

    **Parameters**

        **dtype** :  numpy dtype object

            Structured numpy dtype object to parse.

    **Returns**

        **out** :  list of strings

            List of strings corresponding to numpy types::

                [dtype[i].name.strip('1234567890').rstrip('ing') \ 
                 for i in range(len(dtype))]",Parse,the,types,from,a,structured,numpy,dtype,object,.,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/io.py#L1808-L1835,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/io.py,thresholdcoloring,"def thresholdcoloring(coloring, names):
    """"""
    Threshold a coloring dictionary for a given list of column names.

    Threshold `coloring` based on `names`, a list of strings in::

        coloring.values()

    **Parameters**

        **coloring** :  dictionary

            Hierarchical structure on the columns given in the header of the 
            file; an attribute of tabarrays.

            See :func:`tabular.tab.tabarray.__new__` for more information about 
            coloring.

        **names** :  list of strings

            List of strings giving column names.

    **Returns**

        **newcoloring** :  dictionary

            The thresholded coloring dictionary.

    """"""
    for key in coloring.keys():
        if len([k for k in coloring[key] if k in names]) == 0:
            coloring.pop(key)
        else:
            coloring[key] = utils.uniqify([k for k in coloring[key] if k in 
                                           names])
    return coloring",python,"def thresholdcoloring(coloring, names):
    """"""
    Threshold a coloring dictionary for a given list of column names.

    Threshold `coloring` based on `names`, a list of strings in::

        coloring.values()

    **Parameters**

        **coloring** :  dictionary

            Hierarchical structure on the columns given in the header of the 
            file; an attribute of tabarrays.

            See :func:`tabular.tab.tabarray.__new__` for more information about 
            coloring.

        **names** :  list of strings

            List of strings giving column names.

    **Returns**

        **newcoloring** :  dictionary

            The thresholded coloring dictionary.

    """"""
    for key in coloring.keys():
        if len([k for k in coloring[key] if k in names]) == 0:
            coloring.pop(key)
        else:
            coloring[key] = utils.uniqify([k for k in coloring[key] if k in 
                                           names])
    return coloring",def,thresholdcoloring,(,coloring,",",names,),:,for,key,in,coloring,.,keys,(,),:,if,len,(,[,k,for,k,in,coloring,[,key,],if,k,in,names,],),==,0,:,coloring,.,pop,(,key,"Threshold a coloring dictionary for a given list of column names.

    Threshold `coloring` based on `names`, a list of strings in::

        coloring.values()

    **Parameters**

        **coloring** :  dictionary

            Hierarchical structure on the columns given in the header of the 
            file; an attribute of tabarrays.

            See :func:`tabular.tab.tabarray.__new__` for more information about 
            coloring.

        **names** :  list of strings

            List of strings giving column names.

    **Returns**

        **newcoloring** :  dictionary

            The thresholded coloring dictionary.",Threshold,a,coloring,dictionary,for,a,given,list,of,column,names,.,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/io.py#L1838-L1873,train,),else,:,coloring,[,key,],=,utils,.,uniqify,(,[,k,for,k,in,coloring,[,key,],if,k,in,names,],),return,coloring,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/io.py,makedir,"def makedir(dir_name):
    """"""
     ""Strong"" directory maker.

    ""Strong"" version of `os.mkdir`.  If `dir_name` already exists, this deletes 
    it first.

    **Parameters**

        **dir_name** :  string

            Path to a file directory that may or may not already exist.

    **See Also:**

        :func:`tabular.io.delete`, 
        `os <http://docs.python.org/library/os.html>`_

    """"""
    if os.path.exists(dir_name):
        delete(dir_name)
    os.mkdir(dir_name)",python,"def makedir(dir_name):
    """"""
     ""Strong"" directory maker.

    ""Strong"" version of `os.mkdir`.  If `dir_name` already exists, this deletes 
    it first.

    **Parameters**

        **dir_name** :  string

            Path to a file directory that may or may not already exist.

    **See Also:**

        :func:`tabular.io.delete`, 
        `os <http://docs.python.org/library/os.html>`_

    """"""
    if os.path.exists(dir_name):
        delete(dir_name)
    os.mkdir(dir_name)",def,makedir,(,dir_name,),:,if,os,.,path,.,exists,(,dir_name,),:,delete,(,dir_name,),os,.,mkdir,(,dir_name,),,,,,,,,,,,,,,,,,,"""Strong"" directory maker.

    ""Strong"" version of `os.mkdir`.  If `dir_name` already exists, this deletes 
    it first.

    **Parameters**

        **dir_name** :  string

            Path to a file directory that may or may not already exist.

    **See Also:**

        :func:`tabular.io.delete`, 
        `os <http://docs.python.org/library/os.html>`_",Strong,directory,maker,.,,,,,,,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/io.py#L1924-L1945,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/views/ui.py,pass_community,"def pass_community(f):
    """"""Decorator to pass community.""""""
    @wraps(f)
    def inner(community_id, *args, **kwargs):
        c = Community.get(community_id)
        if c is None:
            abort(404)
        return f(c, *args, **kwargs)
    return inner",python,"def pass_community(f):
    """"""Decorator to pass community.""""""
    @wraps(f)
    def inner(community_id, *args, **kwargs):
        c = Community.get(community_id)
        if c is None:
            abort(404)
        return f(c, *args, **kwargs)
    return inner",def,pass_community,(,f,),:,@,wraps,(,f,),def,inner,(,community_id,",",*,args,",",*,*,kwargs,),:,c,=,Community,.,get,(,community_id,),if,c,is,None,:,abort,(,404,),return,f,Decorator to pass community.,Decorator,to,pass,community,.,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/views/ui.py#L56-L64,train,(,c,",",*,args,",",*,*,kwargs,),return,inner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/views/ui.py,permission_required,"def permission_required(action):
    """"""Decorator to require permission.""""""
    def decorator(f):
        @wraps(f)
        def inner(community, *args, **kwargs):
            permission = current_permission_factory(community, action=action)
            if not permission.can():
                abort(403)
            return f(community, *args, **kwargs)
        return inner
    return decorator",python,"def permission_required(action):
    """"""Decorator to require permission.""""""
    def decorator(f):
        @wraps(f)
        def inner(community, *args, **kwargs):
            permission = current_permission_factory(community, action=action)
            if not permission.can():
                abort(403)
            return f(community, *args, **kwargs)
        return inner
    return decorator",def,permission_required,(,action,),:,def,decorator,(,f,),:,@,wraps,(,f,),def,inner,(,community,",",*,args,",",*,*,kwargs,),:,permission,=,current_permission_factory,(,community,",",action,=,action,),if,not,permission,Decorator to require permission.,Decorator,to,require,permission,.,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/views/ui.py#L67-L77,train,.,can,(,),:,abort,(,403,),return,f,(,community,",",*,args,",",*,*,kwargs,),return,inner,return,decorator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/views/ui.py,format_item,"def format_item(item, template, name='item'):
    """"""Render a template to a string with the provided item in context.""""""
    ctx = {name: item}
    return render_template_to_string(template, **ctx)",python,"def format_item(item, template, name='item'):
    """"""Render a template to a string with the provided item in context.""""""
    ctx = {name: item}
    return render_template_to_string(template, **ctx)",def,format_item,(,item,",",template,",",name,=,'item',),:,ctx,=,{,name,:,item,},return,render_template_to_string,(,template,",",*,*,ctx,),,,,,,,,,,,,,,,,Render a template to a string with the provided item in context.,Render,a,template,to,a,string,with,the,provided,item,in,context,.,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/views/ui.py#L81-L84,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/views/ui.py,new,"def new():
    """"""Create a new community.""""""
    form = CommunityForm(formdata=request.values)

    ctx = mycommunities_ctx()
    ctx.update({
        'form': form,
        'is_new': True,
        'community': None,
    })

    if form.validate_on_submit():
        data = copy.deepcopy(form.data)

        community_id = data.pop('identifier')
        del data['logo']

        community = Community.create(
            community_id, current_user.get_id(), **data)

        file = request.files.get('logo', None)
        if file:
            if not community.save_logo(file.stream, file.filename):
                form.logo.errors.append(_(
                    'Cannot add this file as a logo. Supported formats: '
                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))
                db.session.rollback()
                community = None

        if community:
            db.session.commit()
            flash(""Community was successfully created."", category='success')
            return redirect(url_for('.edit', community_id=community.id))

    return render_template(
        current_app.config['COMMUNITIES_NEW_TEMPLATE'],
        community_form=form,
        **ctx
    )",python,"def new():
    """"""Create a new community.""""""
    form = CommunityForm(formdata=request.values)

    ctx = mycommunities_ctx()
    ctx.update({
        'form': form,
        'is_new': True,
        'community': None,
    })

    if form.validate_on_submit():
        data = copy.deepcopy(form.data)

        community_id = data.pop('identifier')
        del data['logo']

        community = Community.create(
            community_id, current_user.get_id(), **data)

        file = request.files.get('logo', None)
        if file:
            if not community.save_logo(file.stream, file.filename):
                form.logo.errors.append(_(
                    'Cannot add this file as a logo. Supported formats: '
                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))
                db.session.rollback()
                community = None

        if community:
            db.session.commit()
            flash(""Community was successfully created."", category='success')
            return redirect(url_for('.edit', community_id=community.id))

    return render_template(
        current_app.config['COMMUNITIES_NEW_TEMPLATE'],
        community_form=form,
        **ctx
    )",def,new,(,),:,form,=,CommunityForm,(,formdata,=,request,.,values,),ctx,=,mycommunities_ctx,(,),ctx,.,update,(,{,'form',:,form,",",'is_new',:,True,",",'community',:,None,",",},),if,form,.,validate_on_submit,Create a new community.,Create,a,new,community,.,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/views/ui.py#L171-L209,train,(,),:,data,=,copy,.,deepcopy,(,form,.,data,),community_id,=,data,.,pop,(,'identifier',),del,data,[,'logo',],community,=,Community,.,,,,,,,,,,,,,,,,,,,,create,(,community_id,",",current_user,.,get_id,(,),",",*,*,data,),file,=,request,.,files,.,get,(,'logo',",",None,),if,file,:,if,not,community,.,save_logo,(,file,.,stream,",",file,.,filename,),:,form,.,logo,.,errors,.,append,(,_,(,'Cannot add this file as a logo. Supported formats: ',"'PNG, JPG and SVG. Max file size: 1.5 MB.'",),),db,.,session,.,rollback,(,),community,=,None,if,community,:,db,.,session,.,commit,(,),flash,(,"""Community was successfully created.""",",",category,=,'success',),return,redirect,(,url_for,(,'.edit',",",community_id,=,community,.,id,),),return,render_template,(,current_app,.,config,[,'COMMUNITIES_NEW_TEMPLATE',],",",community_form,=,form,",",*,*,ctx,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/views/ui.py,edit,"def edit(community):
    """"""Create or edit a community.""""""
    form = EditCommunityForm(formdata=request.values, obj=community)
    deleteform = DeleteCommunityForm()
    ctx = mycommunities_ctx()
    ctx.update({
        'form': form,
        'is_new': False,
        'community': community,
        'deleteform': deleteform,
    })

    if form.validate_on_submit():
        for field, val in form.data.items():
            setattr(community, field, val)

        file = request.files.get('logo', None)
        if file:
            if not community.save_logo(file.stream, file.filename):
                form.logo.errors.append(_(
                    'Cannot add this file as a logo. Supported formats: '
                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))

        if not form.logo.errors:
            db.session.commit()
            flash(""Community successfully edited."", category='success')
            return redirect(url_for('.edit', community_id=community.id))

    return render_template(
        current_app.config['COMMUNITIES_EDIT_TEMPLATE'],
        **ctx
    )",python,"def edit(community):
    """"""Create or edit a community.""""""
    form = EditCommunityForm(formdata=request.values, obj=community)
    deleteform = DeleteCommunityForm()
    ctx = mycommunities_ctx()
    ctx.update({
        'form': form,
        'is_new': False,
        'community': community,
        'deleteform': deleteform,
    })

    if form.validate_on_submit():
        for field, val in form.data.items():
            setattr(community, field, val)

        file = request.files.get('logo', None)
        if file:
            if not community.save_logo(file.stream, file.filename):
                form.logo.errors.append(_(
                    'Cannot add this file as a logo. Supported formats: '
                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))

        if not form.logo.errors:
            db.session.commit()
            flash(""Community successfully edited."", category='success')
            return redirect(url_for('.edit', community_id=community.id))

    return render_template(
        current_app.config['COMMUNITIES_EDIT_TEMPLATE'],
        **ctx
    )",def,edit,(,community,),:,form,=,EditCommunityForm,(,formdata,=,request,.,values,",",obj,=,community,),deleteform,=,DeleteCommunityForm,(,),ctx,=,mycommunities_ctx,(,),ctx,.,update,(,{,'form',:,form,",",'is_new',:,False,",",Create or edit a community.,Create,or,edit,a,community,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/views/ui.py#L216-L247,train,'community',:,community,",",'deleteform',:,deleteform,",",},),if,form,.,validate_on_submit,(,),:,for,field,",",val,in,form,.,data,.,items,(,),:,,,,,,,,,,,,,,,,,,,,setattr,(,community,",",field,",",val,),file,=,request,.,files,.,get,(,'logo',",",None,),if,file,:,if,not,community,.,save_logo,(,file,.,stream,",",file,.,filename,),:,form,.,logo,.,errors,.,append,(,_,(,'Cannot add this file as a logo. Supported formats: ',"'PNG, JPG and SVG. Max file size: 1.5 MB.'",),),if,not,form,.,logo,.,errors,:,db,.,session,.,commit,(,),flash,(,"""Community successfully edited.""",",",category,=,'success',),return,redirect,(,url_for,(,'.edit',",",community_id,=,community,.,id,),),return,render_template,(,current_app,.,config,[,'COMMUNITIES_EDIT_TEMPLATE',],",",*,*,ctx,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/views/ui.py,delete,"def delete(community):
    """"""Delete a community.""""""
    deleteform = DeleteCommunityForm(formdata=request.values)
    ctx = mycommunities_ctx()
    ctx.update({
        'deleteform': deleteform,
        'is_new': False,
        'community': community,
    })

    if deleteform.validate_on_submit():
        community.delete()
        db.session.commit()
        flash(""Community was deleted."", category='success')
        return redirect(url_for('.index'))
    else:
        flash(""Community could not be deleted."", category='warning')
        return redirect(url_for('.edit', community_id=community.id))",python,"def delete(community):
    """"""Delete a community.""""""
    deleteform = DeleteCommunityForm(formdata=request.values)
    ctx = mycommunities_ctx()
    ctx.update({
        'deleteform': deleteform,
        'is_new': False,
        'community': community,
    })

    if deleteform.validate_on_submit():
        community.delete()
        db.session.commit()
        flash(""Community was deleted."", category='success')
        return redirect(url_for('.index'))
    else:
        flash(""Community could not be deleted."", category='warning')
        return redirect(url_for('.edit', community_id=community.id))",def,delete,(,community,),:,deleteform,=,DeleteCommunityForm,(,formdata,=,request,.,values,),ctx,=,mycommunities_ctx,(,),ctx,.,update,(,{,'deleteform',:,deleteform,",",'is_new',:,False,",",'community',:,community,",",},),if,deleteform,.,Delete a community.,Delete,a,community,.,,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/views/ui.py#L254-L271,train,validate_on_submit,(,),:,community,.,delete,(,),db,.,session,.,commit,(,),flash,(,"""Community was deleted.""",",",category,=,'success',),return,redirect,(,url_for,(,'.index',,,,,,,,,,,,,,,,,,,,),),else,:,flash,(,"""Community could not be deleted.""",",",category,=,'warning',),return,redirect,(,url_for,(,'.edit',",",community_id,=,community,.,id,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,tutorials/ot-oti-find-tree.py,ot_find_tree,"def ot_find_tree(arg_dict, exact=True, verbose=False, oti_wrapper=None):
    """"""Uses a peyotl wrapper around an Open Tree web service to get a list of trees including values `value` for a given property to be searched on `porperty`.

    The oti_wrapper can be None (in which case the default wrapper from peyotl.sugar will be used.
    All other arguments correspond to the arguments of the web-service call.
    """"""
    if oti_wrapper is None:
        from peyotl.sugar import oti
        oti_wrapper = oti
    return oti_wrapper.find_trees(arg_dict,
                                  exact=exact,
                                  verbose=verbose,
                                  wrap_response=True)",python,"def ot_find_tree(arg_dict, exact=True, verbose=False, oti_wrapper=None):
    """"""Uses a peyotl wrapper around an Open Tree web service to get a list of trees including values `value` for a given property to be searched on `porperty`.

    The oti_wrapper can be None (in which case the default wrapper from peyotl.sugar will be used.
    All other arguments correspond to the arguments of the web-service call.
    """"""
    if oti_wrapper is None:
        from peyotl.sugar import oti
        oti_wrapper = oti
    return oti_wrapper.find_trees(arg_dict,
                                  exact=exact,
                                  verbose=verbose,
                                  wrap_response=True)",def,ot_find_tree,(,arg_dict,",",exact,=,True,",",verbose,=,False,",",oti_wrapper,=,None,),:,if,oti_wrapper,is,None,:,from,peyotl,.,sugar,import,oti,oti_wrapper,=,oti,return,oti_wrapper,.,find_trees,(,arg_dict,",",exact,=,exact,",","Uses a peyotl wrapper around an Open Tree web service to get a list of trees including values `value` for a given property to be searched on `porperty`.

    The oti_wrapper can be None (in which case the default wrapper from peyotl.sugar will be used.
    All other arguments correspond to the arguments of the web-service call.",Uses,a,peyotl,wrapper,around,an,Open,Tree,web,service,to,get,a,list,of,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/tutorials/ot-oti-find-tree.py#L12-L24,train,verbose,=,verbose,",",wrap_response,=,True,),,,,,,,,,,,,,,,,,,,,,,,trees,including,values,value,for,a,given,property,to,be,searched,on,porperty,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/typing_patch_36.py,is_iterable,"def is_iterable(etype) -> bool:
    """""" Determine whether etype is a List or other iterable """"""
    return type(etype) is GenericMeta and issubclass(etype.__extra__, Iterable)",python,"def is_iterable(etype) -> bool:
    """""" Determine whether etype is a List or other iterable """"""
    return type(etype) is GenericMeta and issubclass(etype.__extra__, Iterable)",def,is_iterable,(,etype,),->,bool,:,return,type,(,etype,),is,GenericMeta,and,issubclass,(,etype,.,__extra__,",",Iterable,),,,,,,,,,,,,,,,,,,,,Determine whether etype is a List or other iterable,Determine,whether,etype,is,a,List,or,other,iterable,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/typing_patch_36.py#L29-L31,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,tutorials/ot-tree-of-life-mrca.py,main,"def main(argv):
    """"""This function sets up a command-line option parser and then calls fetch_and_write_mrca
    to do all of the real work.
    """"""
    import argparse
    description = 'Uses Open Tree of Life web services to the MRCA for a set of OTT IDs.'
    parser = argparse.ArgumentParser(prog='ot-tree-of-life-mrca', description=description)
    parser.add_argument('ottid', nargs='*', type=int, help='OTT IDs')
    parser.add_argument('--subtree', action='store_true', default=False, required=False,
                        help='write a newick representation of the subtree rooted at this mrca')
    parser.add_argument('--induced-subtree', action='store_true', default=False, required=False,
                        help='write a newick representation of the topology of the requested taxa in the synthetic tree (the subtree pruned to just the queried taxa)')
    parser.add_argument('--details', action='store_true', default=False, required=False,
                        help='report more details about the mrca node')
    args = parser.parse_args(argv)
    id_list = args.ottid
    if not id_list:
        sys.stderr.write('No OTT IDs provided. Running a dummy query with 770302 770315\n')
        id_list = [770302, 770315]
    fetch_and_write_mrca(id_list, args.details, args.subtree, args.induced_subtree, sys.stdout, sys.stderr)",python,"def main(argv):
    """"""This function sets up a command-line option parser and then calls fetch_and_write_mrca
    to do all of the real work.
    """"""
    import argparse
    description = 'Uses Open Tree of Life web services to the MRCA for a set of OTT IDs.'
    parser = argparse.ArgumentParser(prog='ot-tree-of-life-mrca', description=description)
    parser.add_argument('ottid', nargs='*', type=int, help='OTT IDs')
    parser.add_argument('--subtree', action='store_true', default=False, required=False,
                        help='write a newick representation of the subtree rooted at this mrca')
    parser.add_argument('--induced-subtree', action='store_true', default=False, required=False,
                        help='write a newick representation of the topology of the requested taxa in the synthetic tree (the subtree pruned to just the queried taxa)')
    parser.add_argument('--details', action='store_true', default=False, required=False,
                        help='report more details about the mrca node')
    args = parser.parse_args(argv)
    id_list = args.ottid
    if not id_list:
        sys.stderr.write('No OTT IDs provided. Running a dummy query with 770302 770315\n')
        id_list = [770302, 770315]
    fetch_and_write_mrca(id_list, args.details, args.subtree, args.induced_subtree, sys.stdout, sys.stderr)",def,main,(,argv,),:,import,argparse,description,=,'Uses Open Tree of Life web services to the MRCA for a set of OTT IDs.',parser,=,argparse,.,ArgumentParser,(,prog,=,'ot-tree-of-life-mrca',",",description,=,description,),parser,.,add_argument,(,'ottid',",",nargs,=,'*',",",type,=,int,",",help,=,'OTT IDs',),"This function sets up a command-line option parser and then calls fetch_and_write_mrca
    to do all of the real work.",This,function,sets,up,a,command,-,line,option,parser,and,then,calls,fetch_and_write_mrca,to,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/tutorials/ot-tree-of-life-mrca.py#L63-L82,train,parser,.,add_argument,(,'--subtree',",",action,=,'store_true',",",default,=,False,",",required,=,False,",",help,=,'write a newick representation of the subtree rooted at this mrca',),parser,.,add_argument,(,'--induced-subtree',",",action,=,do,all,of,the,real,work,.,,,,,,,,,,,,,'store_true',",",default,=,False,",",required,=,False,",",help,=,'write a newick representation of the topology of the requested taxa in the synthetic tree (the subtree pruned to just the queried taxa)',),parser,.,add_argument,(,'--details',",",action,=,'store_true',",",default,=,False,",",required,=,False,",",help,=,'report more details about the mrca node',),args,=,parser,.,parse_args,(,argv,),id_list,=,args,.,ottid,if,not,id_list,:,sys,.,stderr,.,write,(,'No OTT IDs provided. Running a dummy query with 770302 770315\n',),id_list,=,[,770302,",",770315,],fetch_and_write_mrca,(,id_list,",",args,.,details,",",args,.,subtree,",",args,.,induced_subtree,",",sys,.,stdout,",",sys,.,stderr,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/agent/origin.py,Origin.send_schema,"async def send_schema(self, schema_data_json: str) -> str:
        """"""
        Send schema to ledger, then retrieve it as written to the ledger and return it.
        If schema already exists on ledger, log error and return schema.

        :param schema_data_json: schema data json with name, version, attribute names; e.g.,

        ::

            {
                'name': 'my-schema',
                'version': '1.234',
                'attr_names': ['favourite_drink', 'height', 'last_visit_date']
            }

        :return: schema json as written to ledger (or existed a priori)
        """"""

        LOGGER.debug('Origin.send_schema >>> schema_data_json: %s', schema_data_json)

        schema_data = json.loads(schema_data_json)
        s_key = schema_key(schema_id(self.did, schema_data['name'], schema_data['version']))
        with SCHEMA_CACHE.lock:
            try:
                rv_json = await self.get_schema(s_key)
                LOGGER.error(
                    'Schema %s version %s already exists on ledger for origin-did %s: not sending',
                    schema_data['name'],
                    schema_data['version'],
                    self.did)
            except AbsentSchema:  # OK - about to create and send it
                (_, schema_json) = await anoncreds.issuer_create_schema(
                    self.did,
                    schema_data['name'],
                    schema_data['version'],
                    json.dumps(schema_data['attr_names']))
                req_json = await ledger.build_schema_request(self.did, schema_json)
                resp_json = await self._sign_submit(req_json)
                resp = json.loads(resp_json)
                resp_result_txn = resp['result']['txn']
                rv_json = await self.get_schema(schema_key(schema_id(
                    resp_result_txn['metadata']['from'],
                    resp_result_txn['data']['data']['name'],
                    resp_result_txn['data']['data']['version'])))  # add to cache en passant

        LOGGER.debug('Origin.send_schema <<< %s', rv_json)
        return rv_json",python,"async def send_schema(self, schema_data_json: str) -> str:
        """"""
        Send schema to ledger, then retrieve it as written to the ledger and return it.
        If schema already exists on ledger, log error and return schema.

        :param schema_data_json: schema data json with name, version, attribute names; e.g.,

        ::

            {
                'name': 'my-schema',
                'version': '1.234',
                'attr_names': ['favourite_drink', 'height', 'last_visit_date']
            }

        :return: schema json as written to ledger (or existed a priori)
        """"""

        LOGGER.debug('Origin.send_schema >>> schema_data_json: %s', schema_data_json)

        schema_data = json.loads(schema_data_json)
        s_key = schema_key(schema_id(self.did, schema_data['name'], schema_data['version']))
        with SCHEMA_CACHE.lock:
            try:
                rv_json = await self.get_schema(s_key)
                LOGGER.error(
                    'Schema %s version %s already exists on ledger for origin-did %s: not sending',
                    schema_data['name'],
                    schema_data['version'],
                    self.did)
            except AbsentSchema:  # OK - about to create and send it
                (_, schema_json) = await anoncreds.issuer_create_schema(
                    self.did,
                    schema_data['name'],
                    schema_data['version'],
                    json.dumps(schema_data['attr_names']))
                req_json = await ledger.build_schema_request(self.did, schema_json)
                resp_json = await self._sign_submit(req_json)
                resp = json.loads(resp_json)
                resp_result_txn = resp['result']['txn']
                rv_json = await self.get_schema(schema_key(schema_id(
                    resp_result_txn['metadata']['from'],
                    resp_result_txn['data']['data']['name'],
                    resp_result_txn['data']['data']['version'])))  # add to cache en passant

        LOGGER.debug('Origin.send_schema <<< %s', rv_json)
        return rv_json",async,def,send_schema,(,self,",",schema_data_json,:,str,),->,str,:,LOGGER,.,debug,(,'Origin.send_schema >>> schema_data_json: %s',",",schema_data_json,),schema_data,=,json,.,loads,(,schema_data_json,),s_key,=,schema_key,(,schema_id,(,self,.,did,",",schema_data,[,'name',],"Send schema to ledger, then retrieve it as written to the ledger and return it.
        If schema already exists on ledger, log error and return schema.

        :param schema_data_json: schema data json with name, version, attribute names; e.g.,

        ::

            {
                'name': 'my-schema',
                'version': '1.234',
                'attr_names': ['favourite_drink', 'height', 'last_visit_date']
            }

        :return: schema json as written to ledger (or existed a priori)",Send,schema,to,ledger,then,retrieve,it,as,written,to,the,ledger,and,return,it,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/agent/origin.py#L36-L82,train,",",schema_data,[,'version',],),),with,SCHEMA_CACHE,.,lock,:,try,:,rv_json,=,await,self,.,get_schema,(,s_key,),LOGGER,.,error,(,'Schema %s version %s already exists on ledger for origin-did %s: not sending',",",schema_data,.,If,schema,already,exists,on,ledger,log,error,and,return,schema,.,,,,,,,[,'name',],",",schema_data,[,'version',],",",self,.,did,),except,AbsentSchema,:,# OK - about to create and send it,(,_,",",schema_json,),=,await,anoncreds,.,issuer_create_schema,(,self,.,did,",",schema_data,[,'name',],",",schema_data,[,'version',],",",json,.,dumps,(,schema_data,[,'attr_names',],),),req_json,=,await,ledger,.,build_schema_request,(,self,.,did,",",schema_json,),resp_json,=,await,self,.,_sign_submit,(,req_json,),resp,=,json,.,loads,(,resp_json,),resp_result_txn,=,resp,[,'result',],[,'txn',],rv_json,=,await,self,.,get_schema,(,schema_key,(,schema_id,(,resp_result_txn,[,'metadata',],[,'from',],",",resp_result_txn,[,'data',],[,'data',],[,'name',],",",resp_result_txn,[,'data',],[,'data',],[,'version',],),),),# add to cache en passant,LOGGER,.,debug,(,'Origin.send_schema <<< %s',",",rv_json,),return,rv_json,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/type_aware_doc_store.py,TypeAwareDocStore._locked_refresh_doc_ids,"def _locked_refresh_doc_ids(self):
        """"""Assumes that the caller has the _index_lock !
        """"""
        d = {}
        for s in self._shards:
            for k in s.doc_index.keys():
                if k in d:
                    raise KeyError('doc ""{i}"" found in multiple repos'.format(i=k))
                d[k] = s
        self._doc2shard_map = d",python,"def _locked_refresh_doc_ids(self):
        """"""Assumes that the caller has the _index_lock !
        """"""
        d = {}
        for s in self._shards:
            for k in s.doc_index.keys():
                if k in d:
                    raise KeyError('doc ""{i}"" found in multiple repos'.format(i=k))
                d[k] = s
        self._doc2shard_map = d",def,_locked_refresh_doc_ids,(,self,),:,d,=,{,},for,s,in,self,.,_shards,:,for,k,in,s,.,doc_index,.,keys,(,),:,if,k,in,d,:,raise,KeyError,(,"'doc ""{i}"" found in multiple repos'",.,format,(,i,=,k,Assumes that the caller has the _index_lock !,Assumes,that,the,caller,has,the,_index_lock,!,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/type_aware_doc_store.py#L157-L166,train,),),d,[,k,],=,s,self,.,_doc2shard_map,=,d,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/type_aware_doc_store.py,TypeAwareDocStore.push_doc_to_remote,"def push_doc_to_remote(self, remote_name, doc_id=None):
        """"""This will push the master branch to the remote named `remote_name`
        using the mirroring strategy to cut down on locking of the working repo.

        `doc_id` is used to determine which shard should be pushed.
        if `doc_id` is None, all shards are pushed.
        """"""
        if doc_id is None:
            ret = True
            # @TODO should spawn a thread of each shard...
            for shard in self._shards:
                if not shard.push_to_remote(remote_name):
                    ret = False
            return ret
        shard = self.get_shard(doc_id)
        return shard.push_to_remote(remote_name)",python,"def push_doc_to_remote(self, remote_name, doc_id=None):
        """"""This will push the master branch to the remote named `remote_name`
        using the mirroring strategy to cut down on locking of the working repo.

        `doc_id` is used to determine which shard should be pushed.
        if `doc_id` is None, all shards are pushed.
        """"""
        if doc_id is None:
            ret = True
            # @TODO should spawn a thread of each shard...
            for shard in self._shards:
                if not shard.push_to_remote(remote_name):
                    ret = False
            return ret
        shard = self.get_shard(doc_id)
        return shard.push_to_remote(remote_name)",def,push_doc_to_remote,(,self,",",remote_name,",",doc_id,=,None,),:,if,doc_id,is,None,:,ret,=,True,# @TODO should spawn a thread of each shard...,for,shard,in,self,.,_shards,:,if,not,shard,.,push_to_remote,(,remote_name,),:,ret,=,False,return,ret,shard,"This will push the master branch to the remote named `remote_name`
        using the mirroring strategy to cut down on locking of the working repo.

        `doc_id` is used to determine which shard should be pushed.
        if `doc_id` is None, all shards are pushed.",This,will,push,the,master,branch,to,the,remote,named,remote_name,using,the,mirroring,strategy,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/type_aware_doc_store.py#L214-L229,train,=,self,.,get_shard,(,doc_id,),return,shard,.,push_to_remote,(,remote_name,),,,,,,,,,,,,,,,,,to,cut,down,on,locking,of,the,working,repo,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/type_aware_doc_store.py,TypeAwareDocStore.iter_doc_filepaths,"def iter_doc_filepaths(self, **kwargs):
        """"""Generator that iterates over all detected documents.
        and returns the filesystem path to each doc.
        Order is by shard, but arbitrary within shards.
        @TEMP not locked to prevent doc creation/deletion
        """"""
        for shard in self._shards:
            for doc_id, blob in shard.iter_doc_filepaths(**kwargs):
                yield doc_id, blob",python,"def iter_doc_filepaths(self, **kwargs):
        """"""Generator that iterates over all detected documents.
        and returns the filesystem path to each doc.
        Order is by shard, but arbitrary within shards.
        @TEMP not locked to prevent doc creation/deletion
        """"""
        for shard in self._shards:
            for doc_id, blob in shard.iter_doc_filepaths(**kwargs):
                yield doc_id, blob",def,iter_doc_filepaths,(,self,",",*,*,kwargs,),:,for,shard,in,self,.,_shards,:,for,doc_id,",",blob,in,shard,.,iter_doc_filepaths,(,*,*,kwargs,),:,yield,doc_id,",",blob,,,,,,,,,"Generator that iterates over all detected documents.
        and returns the filesystem path to each doc.
        Order is by shard, but arbitrary within shards.
        @TEMP not locked to prevent doc creation/deletion",Generator,that,iterates,over,all,detected,documents,.,and,returns,the,filesystem,path,to,each,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/type_aware_doc_store.py#L323-L331,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,doc,.,Order,is,by,shard,but,arbitrary,within,shards,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/forms.py,CommunityForm.data,"def data(self):
        """"""Form data.""""""
        d = super(CommunityForm, self).data
        d.pop('csrf_token', None)
        return d",python,"def data(self):
        """"""Form data.""""""
        d = super(CommunityForm, self).data
        d.pop('csrf_token', None)
        return d",def,data,(,self,),:,d,=,super,(,CommunityForm,",",self,),.,data,d,.,pop,(,'csrf_token',",",None,),return,d,,,,,,,,,,,,,,,,,,Form data.,Form,data,.,,,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/forms.py#L54-L58,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/forms.py,CommunityForm.validate_identifier,"def validate_identifier(self, field):
        """"""Validate field identifier.""""""
        if field.data:
            field.data = field.data.lower()
            if Community.get(field.data, with_deleted=True):
                raise validators.ValidationError(
                    _('The identifier already exists. '
                      'Please choose a different one.'))",python,"def validate_identifier(self, field):
        """"""Validate field identifier.""""""
        if field.data:
            field.data = field.data.lower()
            if Community.get(field.data, with_deleted=True):
                raise validators.ValidationError(
                    _('The identifier already exists. '
                      'Please choose a different one.'))",def,validate_identifier,(,self,",",field,),:,if,field,.,data,:,field,.,data,=,field,.,data,.,lower,(,),if,Community,.,get,(,field,.,data,",",with_deleted,=,True,),:,raise,validators,.,ValidationError,(,Validate field identifier.,Validate,field,identifier,.,,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/forms.py#L154-L161,train,_,(,'The identifier already exists. ','Please choose a different one.',),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/input_output.py,read_filepath,"def read_filepath(filepath, encoding='utf-8'):
    """"""Returns the text content of `filepath`""""""
    with codecs.open(filepath, 'r', encoding=encoding) as fo:
        return fo.read()",python,"def read_filepath(filepath, encoding='utf-8'):
    """"""Returns the text content of `filepath`""""""
    with codecs.open(filepath, 'r', encoding=encoding) as fo:
        return fo.read()",def,read_filepath,(,filepath,",",encoding,=,'utf-8',),:,with,codecs,.,open,(,filepath,",",'r',",",encoding,=,encoding,),as,fo,:,return,fo,.,read,(,),,,,,,,,,,,,Returns the text content of `filepath`,Returns,the,text,content,of,filepath,,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/input_output.py#L25-L28,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/input_output.py,download,"def download(url, encoding='utf-8'):
    """"""Returns the text fetched via http GET from URL, read as `encoding`""""""
    import requests
    response = requests.get(url)
    response.encoding = encoding
    return response.text",python,"def download(url, encoding='utf-8'):
    """"""Returns the text fetched via http GET from URL, read as `encoding`""""""
    import requests
    response = requests.get(url)
    response.encoding = encoding
    return response.text",def,download,(,url,",",encoding,=,'utf-8',),:,import,requests,response,=,requests,.,get,(,url,),response,.,encoding,=,encoding,return,response,.,text,,,,,,,,,,,,,,,"Returns the text fetched via http GET from URL, read as `encoding`",Returns,the,text,fetched,via,http,GET,from,URL,read,as,encoding,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/input_output.py#L53-L58,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/input_output.py,pretty_dict_str,"def pretty_dict_str(d, indent=2):
    """"""shows JSON indented representation of d""""""
    b = StringIO()
    write_pretty_dict_str(b, d, indent=indent)
    return b.getvalue()",python,"def pretty_dict_str(d, indent=2):
    """"""shows JSON indented representation of d""""""
    b = StringIO()
    write_pretty_dict_str(b, d, indent=indent)
    return b.getvalue()",def,pretty_dict_str,(,d,",",indent,=,2,),:,b,=,StringIO,(,),write_pretty_dict_str,(,b,",",d,",",indent,=,indent,),return,b,.,getvalue,(,),,,,,,,,,,,,,shows JSON indented representation of d,shows,JSON,indented,representation,of,d,,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/input_output.py#L80-L84,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/utility/input_output.py,write_pretty_dict_str,"def write_pretty_dict_str(out, obj, indent=2):
    """"""writes JSON indented representation of `obj` to `out`""""""
    json.dump(obj,
              out,
              indent=indent,
              sort_keys=True,
              separators=(',', ': '),
              ensure_ascii=False,
              encoding=""utf-8"")",python,"def write_pretty_dict_str(out, obj, indent=2):
    """"""writes JSON indented representation of `obj` to `out`""""""
    json.dump(obj,
              out,
              indent=indent,
              sort_keys=True,
              separators=(',', ': '),
              ensure_ascii=False,
              encoding=""utf-8"")",def,write_pretty_dict_str,(,out,",",obj,",",indent,=,2,),:,json,.,dump,(,obj,",",out,",",indent,=,indent,",",sort_keys,=,True,",",separators,=,(,"','",",",': ',),",",ensure_ascii,=,False,",",encoding,=,"""utf-8""",writes JSON indented representation of `obj` to `out`,writes,JSON,indented,representation,of,obj,to,out,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/input_output.py#L87-L95,train,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/serializers/response.py,community_responsify,"def community_responsify(schema_class, mimetype):
    """"""Create a community response serializer.

    :param serializer: Serializer instance.
    :param mimetype: MIME type of response.
    """"""
    def view(data, code=200, headers=None, links_item_factory=None,
             page=None, urlkwargs=None, links_pagination_factory=None):
        """"""Generate the response object.""""""
        if isinstance(data, Community):
            last_modified = data.updated
            response_data = schema_class(
                context=dict(item_links_factory=links_item_factory)
            ).dump(data).data
        else:
            last_modified = None
            response_data = schema_class(
                context=dict(
                    total=data.query.count(),
                    item_links_factory=links_item_factory,
                    page=page,
                    urlkwargs=urlkwargs,
                    pagination_links_factory=links_pagination_factory)
            ).dump(data.items, many=True).data

        response = current_app.response_class(
            json.dumps(response_data, **_format_args()),
            mimetype=mimetype)
        response.status_code = code

        if last_modified:
            response.last_modified = last_modified

        if headers is not None:
            response.headers.extend(headers)
        return response
    return view",python,"def community_responsify(schema_class, mimetype):
    """"""Create a community response serializer.

    :param serializer: Serializer instance.
    :param mimetype: MIME type of response.
    """"""
    def view(data, code=200, headers=None, links_item_factory=None,
             page=None, urlkwargs=None, links_pagination_factory=None):
        """"""Generate the response object.""""""
        if isinstance(data, Community):
            last_modified = data.updated
            response_data = schema_class(
                context=dict(item_links_factory=links_item_factory)
            ).dump(data).data
        else:
            last_modified = None
            response_data = schema_class(
                context=dict(
                    total=data.query.count(),
                    item_links_factory=links_item_factory,
                    page=page,
                    urlkwargs=urlkwargs,
                    pagination_links_factory=links_pagination_factory)
            ).dump(data.items, many=True).data

        response = current_app.response_class(
            json.dumps(response_data, **_format_args()),
            mimetype=mimetype)
        response.status_code = code

        if last_modified:
            response.last_modified = last_modified

        if headers is not None:
            response.headers.extend(headers)
        return response
    return view",def,community_responsify,(,schema_class,",",mimetype,),:,def,view,(,data,",",code,=,200,",",headers,=,None,",",links_item_factory,=,None,",",page,=,None,",",urlkwargs,=,None,",",links_pagination_factory,=,None,),:,"""""""Generate the response object.""""""",if,isinstance,(,data,"Create a community response serializer.

    :param serializer: Serializer instance.
    :param mimetype: MIME type of response.",Create,a,community,response,serializer,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/serializers/response.py#L58-L94,train,",",Community,),:,last_modified,=,data,.,updated,response_data,=,schema_class,(,context,=,dict,(,item_links_factory,=,links_item_factory,),),.,dump,(,data,),.,data,else,,,,,,,,,,,,,,,,,,,,:,last_modified,=,None,response_data,=,schema_class,(,context,=,dict,(,total,=,data,.,query,.,count,(,),",",item_links_factory,=,links_item_factory,",",page,=,page,",",urlkwargs,=,urlkwargs,",",pagination_links_factory,=,links_pagination_factory,),),.,dump,(,data,.,items,",",many,=,True,),.,data,response,=,current_app,.,response_class,(,json,.,dumps,(,response_data,",",*,*,_format_args,(,),),",",mimetype,=,mimetype,),response,.,status_code,=,code,if,last_modified,:,response,.,last_modified,=,last_modified,if,headers,is,not,None,:,response,.,headers,.,extend,(,headers,),return,response,return,view,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
palantir/typedjsonrpc,typedjsonrpc/errors.py,InternalError.from_error,"def from_error(exc_info, json_encoder, debug_url=None):
        """"""Wraps another Exception in an InternalError.

        :param exc_info: The exception info for the wrapped exception
        :type exc_info: (type, object, traceback)
        :type json_encoder: json.JSONEncoder
        :type debug_url: str | None
        :rtype: InternalError

        .. versionadded:: 0.1.0
        .. versionchanged:: 0.2.0
            Stringifies non-JSON-serializable objects
        """"""
        exc = exc_info[1]
        data = exc.__dict__.copy()
        for key, value in data.items():
            try:
                json_encoder.encode(value)
            except TypeError:
                data[key] = repr(value)
        data[""traceback""] = """".join(traceback.format_exception(*exc_info))
        if debug_url is not None:
            data[""debug_url""] = debug_url
        return InternalError(data)",python,"def from_error(exc_info, json_encoder, debug_url=None):
        """"""Wraps another Exception in an InternalError.

        :param exc_info: The exception info for the wrapped exception
        :type exc_info: (type, object, traceback)
        :type json_encoder: json.JSONEncoder
        :type debug_url: str | None
        :rtype: InternalError

        .. versionadded:: 0.1.0
        .. versionchanged:: 0.2.0
            Stringifies non-JSON-serializable objects
        """"""
        exc = exc_info[1]
        data = exc.__dict__.copy()
        for key, value in data.items():
            try:
                json_encoder.encode(value)
            except TypeError:
                data[key] = repr(value)
        data[""traceback""] = """".join(traceback.format_exception(*exc_info))
        if debug_url is not None:
            data[""debug_url""] = debug_url
        return InternalError(data)",def,from_error,(,exc_info,",",json_encoder,",",debug_url,=,None,),:,exc,=,exc_info,[,1,],data,=,exc,.,__dict__,.,copy,(,),for,key,",",value,in,data,.,items,(,),:,try,:,json_encoder,.,encode,"Wraps another Exception in an InternalError.

        :param exc_info: The exception info for the wrapped exception
        :type exc_info: (type, object, traceback)
        :type json_encoder: json.JSONEncoder
        :type debug_url: str | None
        :rtype: InternalError

        .. versionadded:: 0.1.0
        .. versionchanged:: 0.2.0
            Stringifies non-JSON-serializable objects",Wraps,another,Exception,in,an,InternalError,.,,,,,,,,,274218fcd236ff9643506caa629029c9ba25a0fb,https://github.com/palantir/typedjsonrpc/blob/274218fcd236ff9643506caa629029c9ba25a0fb/typedjsonrpc/errors.py#L99-L122,train,(,value,),except,TypeError,:,data,[,key,],=,repr,(,value,),data,[,"""traceback""",],=,"""""",.,join,(,traceback,.,format_exception,(,*,exc_info,,,,,,,,,,,,,,,,,,,,),),if,debug_url,is,not,None,:,data,[,"""debug_url""",],=,debug_url,return,InternalError,(,data,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/cache.py,SchemaCache.contains,"def contains(self, index: Union[SchemaKey, int, str]) -> bool:
        """"""
        Return whether the cache contains a schema for the input key, sequence number, or schema identifier.

        :param index: schema key, sequence number, or sequence identifier
        :return: whether the cache contains a schema for the input index
        """"""

        LOGGER.debug('SchemaCache.contains >>> index: %s', index)

        rv = None
        if isinstance(index, SchemaKey):
            rv = (index in self._schema_key2schema)
        elif isinstance(index, int) or (isinstance(index, str) and ':2:' not in index):
            rv = (int(index) in self._seq_no2schema_key)
        elif isinstance(index, str):
            rv = (schema_key(index) in self._schema_key2schema)
        else:
            rv = False

        LOGGER.debug('SchemaCache.contains <<< %s', rv)
        return rv",python,"def contains(self, index: Union[SchemaKey, int, str]) -> bool:
        """"""
        Return whether the cache contains a schema for the input key, sequence number, or schema identifier.

        :param index: schema key, sequence number, or sequence identifier
        :return: whether the cache contains a schema for the input index
        """"""

        LOGGER.debug('SchemaCache.contains >>> index: %s', index)

        rv = None
        if isinstance(index, SchemaKey):
            rv = (index in self._schema_key2schema)
        elif isinstance(index, int) or (isinstance(index, str) and ':2:' not in index):
            rv = (int(index) in self._seq_no2schema_key)
        elif isinstance(index, str):
            rv = (schema_key(index) in self._schema_key2schema)
        else:
            rv = False

        LOGGER.debug('SchemaCache.contains <<< %s', rv)
        return rv",def,contains,(,self,",",index,:,Union,[,SchemaKey,",",int,",",str,],),->,bool,:,LOGGER,.,debug,(,'SchemaCache.contains >>> index: %s',",",index,),rv,=,None,if,isinstance,(,index,",",SchemaKey,),:,rv,=,(,index,in,"Return whether the cache contains a schema for the input key, sequence number, or schema identifier.

        :param index: schema key, sequence number, or sequence identifier
        :return: whether the cache contains a schema for the input index",Return,whether,the,cache,contains,a,schema,for,the,input,key,sequence,number,or,schema,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/cache.py#L115-L136,train,self,.,_schema_key2schema,),elif,isinstance,(,index,",",int,),or,(,isinstance,(,index,",",str,),and,':2:',not,in,index,),:,rv,=,(,int,identifier,.,,,,,,,,,,,,,,,,,,(,index,),in,self,.,_seq_no2schema_key,),elif,isinstance,(,index,",",str,),:,rv,=,(,schema_key,(,index,),in,self,.,_schema_key2schema,),else,:,rv,=,False,LOGGER,.,debug,(,'SchemaCache.contains <<< %s',",",rv,),return,rv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/cache.py,RevoCacheEntry.cull,"def cull(self, delta: bool) -> None:
        """"""
        Cull cache entry frame list to size, favouring most recent query time.

        :param delta: True to operate on rev reg deltas, False for rev reg states
        """"""

        LOGGER.debug('RevoCacheEntry.cull >>> delta: %s', delta)

        rr_frames = self.rr_delta_frames if delta else self.rr_state_frames
        mark = 4096**0.5  # max rev reg size = 4096; heuristic: hover max around sqrt(4096) = 64
        if len(rr_frames) > int(mark * 1.25):
            rr_frames.sort(key=lambda x: -x.qtime)  # order by descending query time
            del rr_frames[int(mark * 0.75):]  # retain most recent, grow again from here
            LOGGER.info(
                'Pruned revocation cache entry %s to %s %s frames',
                self.rev_reg_def['id'],
                len(rr_frames),
                'delta' if delta else 'state')

        LOGGER.debug('RevoCacheEntry.cull <<<')",python,"def cull(self, delta: bool) -> None:
        """"""
        Cull cache entry frame list to size, favouring most recent query time.

        :param delta: True to operate on rev reg deltas, False for rev reg states
        """"""

        LOGGER.debug('RevoCacheEntry.cull >>> delta: %s', delta)

        rr_frames = self.rr_delta_frames if delta else self.rr_state_frames
        mark = 4096**0.5  # max rev reg size = 4096; heuristic: hover max around sqrt(4096) = 64
        if len(rr_frames) > int(mark * 1.25):
            rr_frames.sort(key=lambda x: -x.qtime)  # order by descending query time
            del rr_frames[int(mark * 0.75):]  # retain most recent, grow again from here
            LOGGER.info(
                'Pruned revocation cache entry %s to %s %s frames',
                self.rev_reg_def['id'],
                len(rr_frames),
                'delta' if delta else 'state')

        LOGGER.debug('RevoCacheEntry.cull <<<')",def,cull,(,self,",",delta,:,bool,),->,None,:,LOGGER,.,debug,(,'RevoCacheEntry.cull >>> delta: %s',",",delta,),rr_frames,=,self,.,rr_delta_frames,if,delta,else,self,.,rr_state_frames,mark,=,4096,**,0.5,# max rev reg size = 4096; heuristic: hover max around sqrt(4096) = 64,if,len,(,rr_frames,),>,"Cull cache entry frame list to size, favouring most recent query time.

        :param delta: True to operate on rev reg deltas, False for rev reg states",Cull,cache,entry,frame,list,to,size,favouring,most,recent,query,time,.,,,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/cache.py#L413-L433,train,int,(,mark,*,1.25,),:,rr_frames,.,sort,(,key,=,lambda,x,:,-,x,.,qtime,),# order by descending query time,del,rr_frames,[,int,(,mark,*,0.75,,,,,,,,,,,,,,,,,,,,),:,],"# retain most recent, grow again from here",LOGGER,.,info,(,'Pruned revocation cache entry %s to %s %s frames',",",self,.,rev_reg_def,[,'id',],",",len,(,rr_frames,),",",'delta',if,delta,else,'state',),LOGGER,.,debug,(,'RevoCacheEntry.cull <<<',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/cache.py,RevocationCache.dflt_interval,"def dflt_interval(self, cd_id: str) -> (int, int):
        """"""
        Return default non-revocation interval from latest 'to' times on delta frames
        of revocation cache entries on indices stemming from input cred def id.

        Compute the 'from'/'to' values as the earliest/latest 'to' values of all
        cached delta frames on all rev reg ids stemming from the input cred def id.

        E.g., on frames for
            rev-reg-0: -[xx]---[xxxx]-[x]---[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]--> time
            rev-reg-1: ----------------------[xxxx]----[xxx]---[xxxxxxxxxxxxxxxxxxxx]---------> time
            rev-reg-2: -------------------------------------------[xx]-----[xxxx]-----[xxxxx]-> time
            rev-reg-3: -----------------------------------------------------------[xxxxxxxx]--> time

        return the most recent interval covering all matching revocation registries in the cache; i.e.,:
            interval:  -------------------------------------------------------------[*******]-> time

        Raise CacheIndex if there are no matching entries.

        :param cd_id: cred def identifier to match
        :return: default non-revocation interval as 2-tuple (fro, to)
        """"""

        LOGGER.debug('RevocationCache.dflt_interval >>>')

        fro = None
        to = None

        for rr_id in self:
            if cd_id != rev_reg_id2cred_def_id(rr_id):
                continue
            entry = self[rr_id]
            if entry.rr_delta_frames:
                to = max(entry.rr_delta_frames, key=lambda f: f.to).to
                fro = min(fro or to, to)

        if not (fro and to):
            LOGGER.debug(
                'RevocationCache.dflt_interval <!< No data for default non-revoc interval on cred def id %s',
                cd_id)
            raise CacheIndex('No data for default non-revoc interval on cred def id {}'.format(cd_id))

        rv = (fro, to)
        LOGGER.debug('RevocationCache.dflt_interval <<< %s', rv)
        return rv",python,"def dflt_interval(self, cd_id: str) -> (int, int):
        """"""
        Return default non-revocation interval from latest 'to' times on delta frames
        of revocation cache entries on indices stemming from input cred def id.

        Compute the 'from'/'to' values as the earliest/latest 'to' values of all
        cached delta frames on all rev reg ids stemming from the input cred def id.

        E.g., on frames for
            rev-reg-0: -[xx]---[xxxx]-[x]---[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]--> time
            rev-reg-1: ----------------------[xxxx]----[xxx]---[xxxxxxxxxxxxxxxxxxxx]---------> time
            rev-reg-2: -------------------------------------------[xx]-----[xxxx]-----[xxxxx]-> time
            rev-reg-3: -----------------------------------------------------------[xxxxxxxx]--> time

        return the most recent interval covering all matching revocation registries in the cache; i.e.,:
            interval:  -------------------------------------------------------------[*******]-> time

        Raise CacheIndex if there are no matching entries.

        :param cd_id: cred def identifier to match
        :return: default non-revocation interval as 2-tuple (fro, to)
        """"""

        LOGGER.debug('RevocationCache.dflt_interval >>>')

        fro = None
        to = None

        for rr_id in self:
            if cd_id != rev_reg_id2cred_def_id(rr_id):
                continue
            entry = self[rr_id]
            if entry.rr_delta_frames:
                to = max(entry.rr_delta_frames, key=lambda f: f.to).to
                fro = min(fro or to, to)

        if not (fro and to):
            LOGGER.debug(
                'RevocationCache.dflt_interval <!< No data for default non-revoc interval on cred def id %s',
                cd_id)
            raise CacheIndex('No data for default non-revoc interval on cred def id {}'.format(cd_id))

        rv = (fro, to)
        LOGGER.debug('RevocationCache.dflt_interval <<< %s', rv)
        return rv",def,dflt_interval,(,self,",",cd_id,:,str,),->,(,int,",",int,),:,LOGGER,.,debug,(,'RevocationCache.dflt_interval >>>',),fro,=,None,to,=,None,for,rr_id,in,self,:,if,cd_id,!=,rev_reg_id2cred_def_id,(,rr_id,),:,continue,entry,"Return default non-revocation interval from latest 'to' times on delta frames
        of revocation cache entries on indices stemming from input cred def id.

        Compute the 'from'/'to' values as the earliest/latest 'to' values of all
        cached delta frames on all rev reg ids stemming from the input cred def id.

        E.g., on frames for
            rev-reg-0: -[xx]---[xxxx]-[x]---[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]--> time
            rev-reg-1: ----------------------[xxxx]----[xxx]---[xxxxxxxxxxxxxxxxxxxx]---------> time
            rev-reg-2: -------------------------------------------[xx]-----[xxxx]-----[xxxxx]-> time
            rev-reg-3: -----------------------------------------------------------[xxxxxxxx]--> time

        return the most recent interval covering all matching revocation registries in the cache; i.e.,:
            interval:  -------------------------------------------------------------[*******]-> time

        Raise CacheIndex if there are no matching entries.

        :param cd_id: cred def identifier to match
        :return: default non-revocation interval as 2-tuple (fro, to)",Return,default,non,-,revocation,interval,from,latest,to,times,on,delta,frames,of,revocation,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/cache.py#L662-L706,train,=,self,[,rr_id,],if,entry,.,rr_delta_frames,:,to,=,max,(,entry,.,rr_delta_frames,",",key,=,lambda,f,:,f,.,to,),.,to,fro,cache,entries,on,indices,stemming,from,input,cred,def,id,.,,,,,,,,,=,min,(,fro,or,to,",",to,),if,not,(,fro,and,to,),:,LOGGER,.,debug,(,'RevocationCache.dflt_interval <!< No data for default non-revoc interval on cred def id %s',",",cd_id,),raise,CacheIndex,(,'No data for default non-revoc interval on cred def id {}',.,format,(,cd_id,),),rv,=,(,fro,",",to,),LOGGER,.,debug,(,'RevocationCache.dflt_interval <<< %s',",",rv,),return,rv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/cache.py,Caches.parse,"def parse(base_dir: str, timestamp: int = None) -> int:
        """"""
        Parse and update from archived cache files. Only accept new content;
        do not overwrite any existing cache content.

        :param base_dir: archive base directory
        :param timestamp: epoch time of cache serving as subdirectory, default most recent
        :return: epoch time of cache serving as subdirectory, None if there is no such archive.
        """"""

        LOGGER.debug('parse >>> base_dir: %s, timestamp: %s', base_dir, timestamp)

        if not isdir(base_dir):
            LOGGER.info('No cache archives available: not feeding cache')
            LOGGER.debug('parse <<< None')
            return None

        if not timestamp:
            timestamps = [int(t) for t in listdir(base_dir) if t.isdigit()]
            if timestamps:
                timestamp = max(timestamps)
            else:
                LOGGER.info('No cache archives available: not feeding cache')
                LOGGER.debug('parse <<< None')
                return None

        timestamp_dir = join(base_dir, str(timestamp))
        if not isdir(timestamp_dir):
            LOGGER.error('No such archived cache directory: %s', timestamp_dir)
            LOGGER.debug('parse <<< None')
            return None

        with SCHEMA_CACHE.lock:
            with open(join(timestamp_dir, 'schema'), 'r') as archive:
                schemata = json.loads(archive.read())
                SCHEMA_CACHE.feed(schemata)

        with CRED_DEF_CACHE.lock:
            with open(join(timestamp_dir, 'cred_def'), 'r') as archive:
                cred_defs = json.loads(archive.read())
                for cd_id in cred_defs:
                    if cd_id in CRED_DEF_CACHE:
                        LOGGER.warning('Cred def cache already has cred def on %s: skipping', cd_id)
                    else:
                        CRED_DEF_CACHE[cd_id] = cred_defs[cd_id]
                        LOGGER.info('Cred def cache imported cred def for cred def id %s', cd_id)

        with REVO_CACHE.lock:
            with open(join(timestamp_dir, 'revocation'), 'r') as archive:
                rr_cache_entries = json.loads(archive.read())
                for (rr_id, entry) in rr_cache_entries.items():
                    if rr_id in REVO_CACHE:
                        LOGGER.warning('Revocation cache already has entry on %s: skipping', rr_id)
                    else:
                        rr_cache_entry = RevoCacheEntry(entry['rev_reg_def'])

                        rr_cache_entry.rr_delta_frames = [
                            RevRegUpdateFrame(
                                f['_to'],
                                f['_timestamp'],
                                f['_rr_update']) for f in entry['rr_delta_frames']
                        ]
                        rr_cache_entry.cull(True)

                        rr_cache_entry.rr_state_frames = [
                            RevRegUpdateFrame(
                                f['_to'],
                                f['_timestamp'],
                                f['_rr_update']) for f in entry['rr_state_frames']
                        ]
                        rr_cache_entry.cull(False)

                        REVO_CACHE[rr_id] = rr_cache_entry
                        LOGGER.info('Revocation cache imported entry for rev reg id %s', rr_id)

        LOGGER.debug('parse <<< %s', timestamp)
        return timestamp",python,"def parse(base_dir: str, timestamp: int = None) -> int:
        """"""
        Parse and update from archived cache files. Only accept new content;
        do not overwrite any existing cache content.

        :param base_dir: archive base directory
        :param timestamp: epoch time of cache serving as subdirectory, default most recent
        :return: epoch time of cache serving as subdirectory, None if there is no such archive.
        """"""

        LOGGER.debug('parse >>> base_dir: %s, timestamp: %s', base_dir, timestamp)

        if not isdir(base_dir):
            LOGGER.info('No cache archives available: not feeding cache')
            LOGGER.debug('parse <<< None')
            return None

        if not timestamp:
            timestamps = [int(t) for t in listdir(base_dir) if t.isdigit()]
            if timestamps:
                timestamp = max(timestamps)
            else:
                LOGGER.info('No cache archives available: not feeding cache')
                LOGGER.debug('parse <<< None')
                return None

        timestamp_dir = join(base_dir, str(timestamp))
        if not isdir(timestamp_dir):
            LOGGER.error('No such archived cache directory: %s', timestamp_dir)
            LOGGER.debug('parse <<< None')
            return None

        with SCHEMA_CACHE.lock:
            with open(join(timestamp_dir, 'schema'), 'r') as archive:
                schemata = json.loads(archive.read())
                SCHEMA_CACHE.feed(schemata)

        with CRED_DEF_CACHE.lock:
            with open(join(timestamp_dir, 'cred_def'), 'r') as archive:
                cred_defs = json.loads(archive.read())
                for cd_id in cred_defs:
                    if cd_id in CRED_DEF_CACHE:
                        LOGGER.warning('Cred def cache already has cred def on %s: skipping', cd_id)
                    else:
                        CRED_DEF_CACHE[cd_id] = cred_defs[cd_id]
                        LOGGER.info('Cred def cache imported cred def for cred def id %s', cd_id)

        with REVO_CACHE.lock:
            with open(join(timestamp_dir, 'revocation'), 'r') as archive:
                rr_cache_entries = json.loads(archive.read())
                for (rr_id, entry) in rr_cache_entries.items():
                    if rr_id in REVO_CACHE:
                        LOGGER.warning('Revocation cache already has entry on %s: skipping', rr_id)
                    else:
                        rr_cache_entry = RevoCacheEntry(entry['rev_reg_def'])

                        rr_cache_entry.rr_delta_frames = [
                            RevRegUpdateFrame(
                                f['_to'],
                                f['_timestamp'],
                                f['_rr_update']) for f in entry['rr_delta_frames']
                        ]
                        rr_cache_entry.cull(True)

                        rr_cache_entry.rr_state_frames = [
                            RevRegUpdateFrame(
                                f['_to'],
                                f['_timestamp'],
                                f['_rr_update']) for f in entry['rr_state_frames']
                        ]
                        rr_cache_entry.cull(False)

                        REVO_CACHE[rr_id] = rr_cache_entry
                        LOGGER.info('Revocation cache imported entry for rev reg id %s', rr_id)

        LOGGER.debug('parse <<< %s', timestamp)
        return timestamp",def,parse,(,base_dir,:,str,",",timestamp,:,int,=,None,),->,int,:,LOGGER,.,debug,(,"'parse >>> base_dir: %s, timestamp: %s'",",",base_dir,",",timestamp,),if,not,isdir,(,base_dir,),:,LOGGER,.,info,(,'No cache archives available: not feeding cache',),LOGGER,.,debug,(,"Parse and update from archived cache files. Only accept new content;
        do not overwrite any existing cache content.

        :param base_dir: archive base directory
        :param timestamp: epoch time of cache serving as subdirectory, default most recent
        :return: epoch time of cache serving as subdirectory, None if there is no such archive.",Parse,and,update,from,archived,cache,files,.,Only,accept,new,content,;,do,not,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/cache.py#L774-L850,train,'parse <<< None',),return,None,if,not,timestamp,:,timestamps,=,[,int,(,t,),for,t,in,listdir,(,base_dir,),if,t,.,isdigit,(,),],if,overwrite,any,existing,cache,content,.,,,,,,,,,,,,,,timestamps,:,timestamp,=,max,(,timestamps,),else,:,LOGGER,.,info,(,'No cache archives available: not feeding cache',),LOGGER,.,debug,(,'parse <<< None',),return,None,timestamp_dir,=,join,(,base_dir,",",str,(,timestamp,),),if,not,isdir,(,timestamp_dir,),:,LOGGER,.,error,(,'No such archived cache directory: %s',",",timestamp_dir,),LOGGER,.,debug,(,'parse <<< None',),return,None,with,SCHEMA_CACHE,.,lock,:,with,open,(,join,(,timestamp_dir,",",'schema',),",",'r',),as,archive,:,schemata,=,json,.,loads,(,archive,.,read,(,),),SCHEMA_CACHE,.,feed,(,schemata,),with,CRED_DEF_CACHE,.,lock,:,with,open,(,join,(,timestamp_dir,",",'cred_def',),",",'r',),as,archive,:,cred_defs,=,json,.,loads,(,archive,.,read,(,),),for,cd_id,in,cred_defs,:,if,cd_id,in,CRED_DEF_CACHE,:,LOGGER,.,warning,(,'Cred def cache already has cred def on %s: skipping',",",cd_id,),else,:,CRED_DEF_CACHE,[,cd_id,],=,cred_defs,[,cd_id,],LOGGER,.,info,(,'Cred def cache imported cred def for cred def id %s',",",cd_id,),with,REVO_CACHE,.,lock,:,with,open,(,join,(,timestamp_dir,",",'revocation',),",",'r',),as,archive,:,rr_cache_entries,=,json,.,loads,(,archive,.,read,(,),),for,(,rr_id,",",entry,),in,rr_cache_entries,.,items,(,),:,if,rr_id,in,REVO_CACHE,:,LOGGER,.,warning,(,'Revocation cache already has entry on %s: skipping',",",rr_id,),else,:,rr_cache_entry,=,RevoCacheEntry,(,entry,[,'rev_reg_def',],),rr_cache_entry,.,rr_delta_frames,=,[,RevRegUpdateFrame,(,f,[,'_to',],",",f,[,'_timestamp',],",",f,[,'_rr_update',],),for,f,in,entry,[,'rr_delta_frames',],],rr_cache_entry,.,cull,(,True,),rr_cache_entry,.,rr_state_frames,=,[,RevRegUpdateFrame,(,f,[,'_to',],",",f,[,'_timestamp',],",",f,[,'_rr_update',],),for,f,in,entry,[,'rr_state_frames',],],rr_cache_entry,.,cull,(,False,),REVO_CACHE,[,rr_id,],=,rr_cache_entry,LOGGER,.,info,(,'Revocation cache imported entry for rev reg id %s',",",rr_id,),LOGGER,.,debug,,,,,,,,,,,,(,'parse <<< %s',",",timestamp,),return,timestamp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/helper.py,detect_nexson_version,"def detect_nexson_version(blob):
    """"""Returns the nexml2json attribute or the default code for badgerfish""""""
    n = get_nexml_el(blob)
    assert isinstance(n, dict)
    return n.get('@nexml2json', BADGER_FISH_NEXSON_VERSION)",python,"def detect_nexson_version(blob):
    """"""Returns the nexml2json attribute or the default code for badgerfish""""""
    n = get_nexml_el(blob)
    assert isinstance(n, dict)
    return n.get('@nexml2json', BADGER_FISH_NEXSON_VERSION)",def,detect_nexson_version,(,blob,),:,n,=,get_nexml_el,(,blob,),assert,isinstance,(,n,",",dict,),return,n,.,get,(,'@nexml2json',",",BADGER_FISH_NEXSON_VERSION,),,,,,,,,,,,,,,,,Returns the nexml2json attribute or the default code for badgerfish,Returns,the,nexml2json,attribute,or,the,default,code,for,badgerfish,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/helper.py#L40-L44,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/helper.py,_add_value_to_dict_bf,"def _add_value_to_dict_bf(d, k, v):
    """"""Adds the `k`->`v` mapping to `d`, but if a previous element exists it changes
    the value of for the key to list.

    This is used in the BadgerFish mapping convention.

    This is a simple multi-dict that is only suitable when you know that you'll never
    store a list or `None` as a value in the dict.
    """"""
    prev = d.get(k)
    if prev is None:
        d[k] = v
    elif isinstance(prev, list):
        if isinstance(v, list):
            prev.extend(v)
        else:
            prev.append(v)
    else:
        if isinstance(v, list):
            x = [prev]
            x.extend(v)
            d[k] = x
        else:
            d[k] = [prev, v]",python,"def _add_value_to_dict_bf(d, k, v):
    """"""Adds the `k`->`v` mapping to `d`, but if a previous element exists it changes
    the value of for the key to list.

    This is used in the BadgerFish mapping convention.

    This is a simple multi-dict that is only suitable when you know that you'll never
    store a list or `None` as a value in the dict.
    """"""
    prev = d.get(k)
    if prev is None:
        d[k] = v
    elif isinstance(prev, list):
        if isinstance(v, list):
            prev.extend(v)
        else:
            prev.append(v)
    else:
        if isinstance(v, list):
            x = [prev]
            x.extend(v)
            d[k] = x
        else:
            d[k] = [prev, v]",def,_add_value_to_dict_bf,(,d,",",k,",",v,),:,prev,=,d,.,get,(,k,),if,prev,is,None,:,d,[,k,],=,v,elif,isinstance,(,prev,",",list,),:,if,isinstance,(,v,",",list,"Adds the `k`->`v` mapping to `d`, but if a previous element exists it changes
    the value of for the key to list.

    This is used in the BadgerFish mapping convention.

    This is a simple multi-dict that is only suitable when you know that you'll never
    store a list or `None` as a value in the dict.",Adds,the,k,-,>,v,mapping,to,d,but,if,a,previous,element,exists,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/helper.py#L110-L133,train,),:,prev,.,extend,(,v,),else,:,prev,.,append,(,v,),else,:,if,isinstance,(,v,",",list,),:,x,=,[,prev,it,changes,the,value,of,for,the,key,to,list,.,,,,,,,,,],x,.,extend,(,v,),d,[,k,],=,x,else,:,d,[,k,],=,[,prev,",",v,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/helper.py,_add_uniq_value_to_dict_bf,"def _add_uniq_value_to_dict_bf(d, k, v):
    """"""Like _add_value_to_dict_bf but will not add v if another
    element in under key `k` has the same value.
    """"""
    prev = d.get(k)
    if prev is None:
        d[k] = v
    elif isinstance(prev, list):
        if not isinstance(v, list):
            v = [v]
        for sel in v:
            found = False
            for el in prev:
                if el == sel:
                    found = True
                    break
            if not found:
                prev.append(sel)
    else:
        if isinstance(v, list):
            prev = [prev]
            for sel in v:
                found = False
                for el in prev:
                    if el == sel:
                        found = True
                        break
                if not found:
                    prev.append(sel)
            if len(prev) > 1:
                d[k] = prev
        elif prev != v:
            d[k] = [prev, v]",python,"def _add_uniq_value_to_dict_bf(d, k, v):
    """"""Like _add_value_to_dict_bf but will not add v if another
    element in under key `k` has the same value.
    """"""
    prev = d.get(k)
    if prev is None:
        d[k] = v
    elif isinstance(prev, list):
        if not isinstance(v, list):
            v = [v]
        for sel in v:
            found = False
            for el in prev:
                if el == sel:
                    found = True
                    break
            if not found:
                prev.append(sel)
    else:
        if isinstance(v, list):
            prev = [prev]
            for sel in v:
                found = False
                for el in prev:
                    if el == sel:
                        found = True
                        break
                if not found:
                    prev.append(sel)
            if len(prev) > 1:
                d[k] = prev
        elif prev != v:
            d[k] = [prev, v]",def,_add_uniq_value_to_dict_bf,(,d,",",k,",",v,),:,prev,=,d,.,get,(,k,),if,prev,is,None,:,d,[,k,],=,v,elif,isinstance,(,prev,",",list,),:,if,not,isinstance,(,v,",","Like _add_value_to_dict_bf but will not add v if another
    element in under key `k` has the same value.",Like,_add_value_to_dict_bf,but,will,not,add,v,if,another,element,in,under,key,k,has,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/helper.py#L136-L168,train,list,),:,v,=,[,v,],for,sel,in,v,:,found,=,False,for,el,in,prev,:,if,el,==,sel,:,found,=,True,break,the,same,value,.,,,,,,,,,,,,,,,,if,not,found,:,prev,.,append,(,sel,),else,:,if,isinstance,(,v,",",list,),:,prev,=,[,prev,],for,sel,in,v,:,found,=,False,for,el,in,prev,:,if,el,==,sel,:,found,=,True,break,if,not,found,:,prev,.,append,(,sel,),if,len,(,prev,),>,1,:,d,[,k,],=,prev,elif,prev,!=,v,:,d,[,k,],=,[,prev,",",v,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/helper.py,_debug_dump_dom,"def _debug_dump_dom(el):
    """"""Debugging helper. Prints out `el` contents.""""""
    import xml.dom.minidom
    s = [el.nodeName]
    att_container = el.attributes
    for i in range(att_container.length):
        attr = att_container.item(i)
        s.append('  @{a}=""{v}""'.format(a=attr.name, v=attr.value))
    for c in el.childNodes:
        if c.nodeType == xml.dom.minidom.Node.TEXT_NODE:
            s.append('  {a} type=""TEXT"" data=""{d}""'.format(a=c.nodeName, d=c.data))
        else:
            s.append('  {a} child'.format(a=c.nodeName))
    return '\n'.join(s)",python,"def _debug_dump_dom(el):
    """"""Debugging helper. Prints out `el` contents.""""""
    import xml.dom.minidom
    s = [el.nodeName]
    att_container = el.attributes
    for i in range(att_container.length):
        attr = att_container.item(i)
        s.append('  @{a}=""{v}""'.format(a=attr.name, v=attr.value))
    for c in el.childNodes:
        if c.nodeType == xml.dom.minidom.Node.TEXT_NODE:
            s.append('  {a} type=""TEXT"" data=""{d}""'.format(a=c.nodeName, d=c.data))
        else:
            s.append('  {a} child'.format(a=c.nodeName))
    return '\n'.join(s)",def,_debug_dump_dom,(,el,),:,import,xml,.,dom,.,minidom,s,=,[,el,.,nodeName,],att_container,=,el,.,attributes,for,i,in,range,(,att_container,.,length,),:,attr,=,att_container,.,item,(,i,),s,Debugging helper. Prints out `el` contents.,Debugging,helper,.,Prints,out,el,contents,.,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/helper.py#L177-L190,train,.,append,(,"'  @{a}=""{v}""'",.,format,(,a,=,attr,.,name,",",v,=,attr,.,value,),),for,c,in,el,.,childNodes,:,if,c,.,,,,,,,,,,,,,,,,,,,,nodeType,==,xml,.,dom,.,minidom,.,Node,.,TEXT_NODE,:,s,.,append,(,"'  {a} type=""TEXT"" data=""{d}""'",.,format,(,a,=,c,.,nodeName,",",d,=,c,.,data,),),else,:,s,.,append,(,'  {a} child',.,format,(,a,=,c,.,nodeName,),),return,'\n',.,join,(,s,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/helper.py,_convert_hbf_meta_val_for_xml,"def _convert_hbf_meta_val_for_xml(key, val):
    """"""Convert to a BadgerFish-style dict for addition to a dict suitable for
    addition to XML tree or for v1.0 to v0.0 conversion.""""""
    if isinstance(val, list):
        return [_convert_hbf_meta_val_for_xml(key, i) for i in val]
    is_literal = True
    content = None
    if isinstance(val, dict):
        ret = val
        if '@href' in val:
            is_literal = False
        else:
            content = val.get('$')
            if isinstance(content, dict) and _contains_hbf_meta_keys(val):
                is_literal = False
    else:
        ret = {}
        content = val
    if is_literal:
        ret.setdefault('@xsi:type', 'nex:LiteralMeta')
        ret.setdefault('@property', key)
        if content is not None:
            ret.setdefault('@datatype', _python_instance_to_nexml_meta_datatype(content))
        if ret is not val:
            ret['$'] = content
    else:
        ret.setdefault('@xsi:type', 'nex:ResourceMeta')
        ret.setdefault('@rel', key)
    return ret",python,"def _convert_hbf_meta_val_for_xml(key, val):
    """"""Convert to a BadgerFish-style dict for addition to a dict suitable for
    addition to XML tree or for v1.0 to v0.0 conversion.""""""
    if isinstance(val, list):
        return [_convert_hbf_meta_val_for_xml(key, i) for i in val]
    is_literal = True
    content = None
    if isinstance(val, dict):
        ret = val
        if '@href' in val:
            is_literal = False
        else:
            content = val.get('$')
            if isinstance(content, dict) and _contains_hbf_meta_keys(val):
                is_literal = False
    else:
        ret = {}
        content = val
    if is_literal:
        ret.setdefault('@xsi:type', 'nex:LiteralMeta')
        ret.setdefault('@property', key)
        if content is not None:
            ret.setdefault('@datatype', _python_instance_to_nexml_meta_datatype(content))
        if ret is not val:
            ret['$'] = content
    else:
        ret.setdefault('@xsi:type', 'nex:ResourceMeta')
        ret.setdefault('@rel', key)
    return ret",def,_convert_hbf_meta_val_for_xml,(,key,",",val,),:,if,isinstance,(,val,",",list,),:,return,[,_convert_hbf_meta_val_for_xml,(,key,",",i,),for,i,in,val,],is_literal,=,True,content,=,None,if,isinstance,(,val,",",dict,),:,"Convert to a BadgerFish-style dict for addition to a dict suitable for
    addition to XML tree or for v1.0 to v0.0 conversion.",Convert,to,a,BadgerFish,-,style,dict,for,addition,to,a,dict,suitable,for,addition,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/helper.py#L249-L277,train,ret,=,val,if,'@href',in,val,:,is_literal,=,False,else,:,content,=,val,.,get,(,'$',),if,isinstance,(,content,",",dict,),and,_contains_hbf_meta_keys,to,XML,tree,or,for,v1,.,0,to,v0,.,0,conversion,.,,,,,,(,val,),:,is_literal,=,False,else,:,ret,=,{,},content,=,val,if,is_literal,:,ret,.,setdefault,(,'@xsi:type',",",'nex:LiteralMeta',),ret,.,setdefault,(,'@property',",",key,),if,content,is,not,None,:,ret,.,setdefault,(,'@datatype',",",_python_instance_to_nexml_meta_datatype,(,content,),),if,ret,is,not,val,:,ret,[,'$',],=,content,else,:,ret,.,setdefault,(,'@xsi:type',",",'nex:ResourceMeta',),ret,.,setdefault,(,'@rel',",",key,),return,ret,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/helper.py,find_nested_meta_first,"def find_nested_meta_first(d, prop_name, version):
    """"""Returns obj. for badgerfish and val for hbf. Appropriate for nested literals""""""
    if _is_badgerfish_version(version):
        return find_nested_meta_first_bf(d, prop_name)
    p = '^' + prop_name
    return d.get(p)",python,"def find_nested_meta_first(d, prop_name, version):
    """"""Returns obj. for badgerfish and val for hbf. Appropriate for nested literals""""""
    if _is_badgerfish_version(version):
        return find_nested_meta_first_bf(d, prop_name)
    p = '^' + prop_name
    return d.get(p)",def,find_nested_meta_first,(,d,",",prop_name,",",version,),:,if,_is_badgerfish_version,(,version,),:,return,find_nested_meta_first_bf,(,d,",",prop_name,),p,=,'^',+,prop_name,return,d,.,get,(,p,),,,,,,,,,Returns obj. for badgerfish and val for hbf. Appropriate for nested literals,Returns,obj,.,for,badgerfish,and,val,for,hbf,.,Appropriate,for,nested,literals,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/helper.py#L361-L366,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/codec.py,decode,"def decode(value: str) -> Union[str, None, bool, int, float]:
    """"""
    Decode encoded credential attribute value.

    :param value: numeric string to decode
    :return: decoded value, stringified if original was neither str, bool, int, nor float
    """"""

    assert value.isdigit() or value[0] == '-' and value[1:].isdigit()

    if -I32_BOUND <= int(value) < I32_BOUND:  # it's an i32: it is its own encoding
        return int(value)
    elif int(value) == I32_BOUND:
        return None

    (prefix, value) = (int(value[0]), int(value[1:]))
    ival = int(value) - I32_BOUND
    if ival == 0:
        return ''  # special case: empty string encodes as 2**31
    elif ival == 1:
        return False  # sentinel for bool False
    elif ival == 2:
        return True  # sentinel for bool True

    blen = ceil(log(ival, 16)/2)
    ibytes = unhexlify(ival.to_bytes(blen, 'big'))
    return DECODE_PREFIX.get(prefix, str)(ibytes.decode())",python,"def decode(value: str) -> Union[str, None, bool, int, float]:
    """"""
    Decode encoded credential attribute value.

    :param value: numeric string to decode
    :return: decoded value, stringified if original was neither str, bool, int, nor float
    """"""

    assert value.isdigit() or value[0] == '-' and value[1:].isdigit()

    if -I32_BOUND <= int(value) < I32_BOUND:  # it's an i32: it is its own encoding
        return int(value)
    elif int(value) == I32_BOUND:
        return None

    (prefix, value) = (int(value[0]), int(value[1:]))
    ival = int(value) - I32_BOUND
    if ival == 0:
        return ''  # special case: empty string encodes as 2**31
    elif ival == 1:
        return False  # sentinel for bool False
    elif ival == 2:
        return True  # sentinel for bool True

    blen = ceil(log(ival, 16)/2)
    ibytes = unhexlify(ival.to_bytes(blen, 'big'))
    return DECODE_PREFIX.get(prefix, str)(ibytes.decode())",def,decode,(,value,:,str,),->,Union,[,str,",",None,",",bool,",",int,",",float,],:,assert,value,.,isdigit,(,),or,value,[,0,],==,'-',and,value,[,1,:,],.,isdigit,(,"Decode encoded credential attribute value.

    :param value: numeric string to decode
    :return: decoded value, stringified if original was neither str, bool, int, nor float",Decode,encoded,credential,attribute,value,.,,,,,,,,,,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/codec.py#L70-L96,train,),if,-,I32_BOUND,<=,int,(,value,),<,I32_BOUND,:,# it's an i32: it is its own encoding,return,int,(,value,),elif,int,(,value,),==,I32_BOUND,:,return,None,(,prefix,,,,,,,,,,,,,,,,,,,,",",value,),=,(,int,(,value,[,0,],),",",int,(,value,[,1,:,],),),ival,=,int,(,value,),-,I32_BOUND,if,ival,==,0,:,return,'',# special case: empty string encodes as 2**31,elif,ival,==,1,:,return,False,# sentinel for bool False,elif,ival,==,2,:,return,True,# sentinel for bool True,blen,=,ceil,(,log,(,ival,",",16,),/,2,),ibytes,=,unhexlify,(,ival,.,to_bytes,(,blen,",",'big',),),return,DECODE_PREFIX,.,get,(,prefix,",",str,),(,ibytes,.,decode,(,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
palantir/typedjsonrpc,typedjsonrpc/parameter_checker.py,validate_params_match,"def validate_params_match(method, parameters):
    """"""Validates that the given parameters are exactly the method's declared parameters.

    :param method: The method to be called
    :type method: function
    :param parameters: The parameters to use in the call
    :type parameters: dict[str, object] | list[object]
    """"""
    argspec = inspect.getargspec(method)  # pylint: disable=deprecated-method
    default_length = len(argspec.defaults) if argspec.defaults is not None else 0

    if isinstance(parameters, list):
        if len(parameters) > len(argspec.args) and argspec.varargs is None:
            raise InvalidParamsError(""Too many parameters"")

        remaining_parameters = len(argspec.args) - len(parameters)
        if remaining_parameters > default_length:
            raise InvalidParamsError(""Not enough parameters"")

    elif isinstance(parameters, dict):
        missing_parameters = [key for key in argspec.args if key not in parameters]
        default_parameters = set(argspec.args[len(argspec.args) - default_length:])
        for key in missing_parameters:
            if key not in default_parameters:
                raise InvalidParamsError(""Parameter {} has not been satisfied"".format(key))

        extra_params = [key for key in parameters if key not in argspec.args]
        if len(extra_params) > 0 and argspec.keywords is None:
            raise InvalidParamsError(""Too many parameters"")",python,"def validate_params_match(method, parameters):
    """"""Validates that the given parameters are exactly the method's declared parameters.

    :param method: The method to be called
    :type method: function
    :param parameters: The parameters to use in the call
    :type parameters: dict[str, object] | list[object]
    """"""
    argspec = inspect.getargspec(method)  # pylint: disable=deprecated-method
    default_length = len(argspec.defaults) if argspec.defaults is not None else 0

    if isinstance(parameters, list):
        if len(parameters) > len(argspec.args) and argspec.varargs is None:
            raise InvalidParamsError(""Too many parameters"")

        remaining_parameters = len(argspec.args) - len(parameters)
        if remaining_parameters > default_length:
            raise InvalidParamsError(""Not enough parameters"")

    elif isinstance(parameters, dict):
        missing_parameters = [key for key in argspec.args if key not in parameters]
        default_parameters = set(argspec.args[len(argspec.args) - default_length:])
        for key in missing_parameters:
            if key not in default_parameters:
                raise InvalidParamsError(""Parameter {} has not been satisfied"".format(key))

        extra_params = [key for key in parameters if key not in argspec.args]
        if len(extra_params) > 0 and argspec.keywords is None:
            raise InvalidParamsError(""Too many parameters"")",def,validate_params_match,(,method,",",parameters,),:,argspec,=,inspect,.,getargspec,(,method,),# pylint: disable=deprecated-method,default_length,=,len,(,argspec,.,defaults,),if,argspec,.,defaults,is,not,None,else,0,if,isinstance,(,parameters,",",list,),:,if,"Validates that the given parameters are exactly the method's declared parameters.

    :param method: The method to be called
    :type method: function
    :param parameters: The parameters to use in the call
    :type parameters: dict[str, object] | list[object]",Validates,that,the,given,parameters,are,exactly,the,method,s,declared,parameters,.,,,274218fcd236ff9643506caa629029c9ba25a0fb,https://github.com/palantir/typedjsonrpc/blob/274218fcd236ff9643506caa629029c9ba25a0fb/typedjsonrpc/parameter_checker.py#L27-L55,train,len,(,parameters,),>,len,(,argspec,.,args,),and,argspec,.,varargs,is,None,:,raise,InvalidParamsError,(,"""Too many parameters""",),remaining_parameters,=,len,(,argspec,.,args,,,,,,,,,,,,,,,,,,,,),-,len,(,parameters,),if,remaining_parameters,>,default_length,:,raise,InvalidParamsError,(,"""Not enough parameters""",),elif,isinstance,(,parameters,",",dict,),:,missing_parameters,=,[,key,for,key,in,argspec,.,args,if,key,not,in,parameters,],default_parameters,=,set,(,argspec,.,args,[,len,(,argspec,.,args,),-,default_length,:,],),for,key,in,missing_parameters,:,if,key,not,in,default_parameters,:,raise,InvalidParamsError,(,"""Parameter {} has not been satisfied""",.,format,(,key,),),extra_params,=,[,key,for,key,in,parameters,if,key,not,in,argspec,.,args,],if,len,(,extra_params,),>,0,and,argspec,.,keywords,is,None,:,raise,InvalidParamsError,(,"""Too many parameters""",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
palantir/typedjsonrpc,typedjsonrpc/parameter_checker.py,check_types,"def check_types(parameters, parameter_types, strict_floats):
    """"""Checks that the given parameters have the correct types.

    :param parameters: List of (name, value) pairs of the given parameters
    :type parameters: dict[str, object]
    :param parameter_types: Parameter type by name.
    :type parameter_types: dict[str, type]
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool
    """"""
    for name, parameter_type in parameter_types.items():
        if name not in parameters:
            raise InvalidParamsError(""Parameter '{}' is missing."".format(name))
        if not _is_instance(parameters[name], parameter_type, strict_floats):
            raise InvalidParamsError(""Value '{}' for parameter '{}' is not of expected type {}.""
                                     .format(parameters[name], name, parameter_type))",python,"def check_types(parameters, parameter_types, strict_floats):
    """"""Checks that the given parameters have the correct types.

    :param parameters: List of (name, value) pairs of the given parameters
    :type parameters: dict[str, object]
    :param parameter_types: Parameter type by name.
    :type parameter_types: dict[str, type]
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool
    """"""
    for name, parameter_type in parameter_types.items():
        if name not in parameters:
            raise InvalidParamsError(""Parameter '{}' is missing."".format(name))
        if not _is_instance(parameters[name], parameter_type, strict_floats):
            raise InvalidParamsError(""Value '{}' for parameter '{}' is not of expected type {}.""
                                     .format(parameters[name], name, parameter_type))",def,check_types,(,parameters,",",parameter_types,",",strict_floats,),:,for,name,",",parameter_type,in,parameter_types,.,items,(,),:,if,name,not,in,parameters,:,raise,InvalidParamsError,(,"""Parameter '{}' is missing.""",.,format,(,name,),),if,not,_is_instance,(,parameters,[,"Checks that the given parameters have the correct types.

    :param parameters: List of (name, value) pairs of the given parameters
    :type parameters: dict[str, object]
    :param parameter_types: Parameter type by name.
    :type parameter_types: dict[str, type]
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool",Checks,that,the,given,parameters,have,the,correct,types,.,,,,,,274218fcd236ff9643506caa629029c9ba25a0fb,https://github.com/palantir/typedjsonrpc/blob/274218fcd236ff9643506caa629029c9ba25a0fb/typedjsonrpc/parameter_checker.py#L58-L73,train,name,],",",parameter_type,",",strict_floats,),:,raise,InvalidParamsError,(,"""Value '{}' for parameter '{}' is not of expected type {}.""",.,format,(,parameters,[,name,],",",name,",",parameter_type,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
palantir/typedjsonrpc,typedjsonrpc/parameter_checker.py,check_type_declaration,"def check_type_declaration(parameter_names, parameter_types):
    """"""Checks that exactly the given parameter names have declared types.

    :param parameter_names: The names of the parameters in the method declaration
    :type parameter_names: list[str]
    :param parameter_types: Parameter type by name
    :type parameter_types: dict[str, type]
    """"""
    if len(parameter_names) != len(parameter_types):
        raise Exception(""Number of method parameters ({}) does not match number of ""
                        ""declared types ({})""
                        .format(len(parameter_names), len(parameter_types)))
    for parameter_name in parameter_names:
        if parameter_name not in parameter_types:
            raise Exception(""Parameter '{}' does not have a declared type"".format(parameter_name))",python,"def check_type_declaration(parameter_names, parameter_types):
    """"""Checks that exactly the given parameter names have declared types.

    :param parameter_names: The names of the parameters in the method declaration
    :type parameter_names: list[str]
    :param parameter_types: Parameter type by name
    :type parameter_types: dict[str, type]
    """"""
    if len(parameter_names) != len(parameter_types):
        raise Exception(""Number of method parameters ({}) does not match number of ""
                        ""declared types ({})""
                        .format(len(parameter_names), len(parameter_types)))
    for parameter_name in parameter_names:
        if parameter_name not in parameter_types:
            raise Exception(""Parameter '{}' does not have a declared type"".format(parameter_name))",def,check_type_declaration,(,parameter_names,",",parameter_types,),:,if,len,(,parameter_names,),!=,len,(,parameter_types,),:,raise,Exception,(,"""Number of method parameters ({}) does not match number of ""","""declared types ({})""",.,format,(,len,(,parameter_names,),",",len,(,parameter_types,),),),for,parameter_name,in,parameter_names,:,"Checks that exactly the given parameter names have declared types.

    :param parameter_names: The names of the parameters in the method declaration
    :type parameter_names: list[str]
    :param parameter_types: Parameter type by name
    :type parameter_types: dict[str, type]",Checks,that,exactly,the,given,parameter,names,have,declared,types,.,,,,,274218fcd236ff9643506caa629029c9ba25a0fb,https://github.com/palantir/typedjsonrpc/blob/274218fcd236ff9643506caa629029c9ba25a0fb/typedjsonrpc/parameter_checker.py#L76-L90,train,if,parameter_name,not,in,parameter_types,:,raise,Exception,(,"""Parameter '{}' does not have a declared type""",.,format,(,parameter_name,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
palantir/typedjsonrpc,typedjsonrpc/parameter_checker.py,check_return_type,"def check_return_type(value, expected_type, strict_floats):
    """"""Checks that the given return value has the correct type.

    :param value: Value returned by the method
    :type value: object
    :param expected_type: Expected return type
    :type expected_type: type
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool
    """"""
    if expected_type is None:
        if value is not None:
            raise InvalidReturnTypeError(""Returned value is '{}' but None was expected""
                                         .format(value))
    elif not _is_instance(value, expected_type, strict_floats):
        raise InvalidReturnTypeError(""Type of return value '{}' does not match expected type {}""
                                     .format(value, expected_type))",python,"def check_return_type(value, expected_type, strict_floats):
    """"""Checks that the given return value has the correct type.

    :param value: Value returned by the method
    :type value: object
    :param expected_type: Expected return type
    :type expected_type: type
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool
    """"""
    if expected_type is None:
        if value is not None:
            raise InvalidReturnTypeError(""Returned value is '{}' but None was expected""
                                         .format(value))
    elif not _is_instance(value, expected_type, strict_floats):
        raise InvalidReturnTypeError(""Type of return value '{}' does not match expected type {}""
                                     .format(value, expected_type))",def,check_return_type,(,value,",",expected_type,",",strict_floats,),:,if,expected_type,is,None,:,if,value,is,not,None,:,raise,InvalidReturnTypeError,(,"""Returned value is '{}' but None was expected""",.,format,(,value,),),elif,not,_is_instance,(,value,",",expected_type,",",strict_floats,),:,raise,"Checks that the given return value has the correct type.

    :param value: Value returned by the method
    :type value: object
    :param expected_type: Expected return type
    :type expected_type: type
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool",Checks,that,the,given,return,value,has,the,correct,type,.,,,,,274218fcd236ff9643506caa629029c9ba25a0fb,https://github.com/palantir/typedjsonrpc/blob/274218fcd236ff9643506caa629029c9ba25a0fb/typedjsonrpc/parameter_checker.py#L93-L109,train,InvalidReturnTypeError,(,"""Type of return value '{}' does not match expected type {}""",.,format,(,value,",",expected_type,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/phylesystem/helper.py,_make_phylesystem_cache_region,"def _make_phylesystem_cache_region(**kwargs):
    """"""Only intended to be called by the Phylesystem singleton.
    """"""
    global _CACHE_REGION_CONFIGURED, _REGION
    if _CACHE_REGION_CONFIGURED:
        return _REGION
    _CACHE_REGION_CONFIGURED = True
    try:
        # noinspection PyPackageRequirements
        from dogpile.cache import make_region
    except:
        _LOG.debug('dogpile.cache not available')
        return
    region = None
    trial_key = 'test_key'
    trial_val = {'test_val': [4, 3]}
    trying_redis = True
    if trying_redis:
        try:
            a = {
                'host': 'localhost',
                'port': 6379,
                'db': 0,  # default is 0
                'redis_expiration_time': 60 * 60 * 24 * 2,  # 2 days
                'distributed_lock': False  # True if multiple processes will use redis
            }
            region = make_region().configure('dogpile.cache.redis', arguments=a)
            _LOG.debug('cache region set up with cache.redis.')
            _LOG.debug('testing redis caching...')
            region.set(trial_key, trial_val)
            assert trial_val == region.get(trial_key)
            _LOG.debug('redis caching works')
            region.delete(trial_key)
            _REGION = region
            return region
        except:
            _LOG.debug('redis cache set up failed.')
            region = None
    trying_file_dbm = False
    if trying_file_dbm:
        _LOG.debug('Going to try dogpile.cache.dbm ...')
        first_par = _get_phylesystem_parent(**kwargs)[0]
        cache_db_dir = os.path.split(first_par)[0]
        cache_db = os.path.join(cache_db_dir, 'phylesystem-cachefile.dbm')
        _LOG.debug('dogpile.cache region using ""{}""'.format(cache_db))
        try:
            a = {'filename': cache_db}
            region = make_region().configure('dogpile.cache.dbm',
                                             expiration_time=36000,
                                             arguments=a)
            _LOG.debug('cache region set up with cache.dbm.')
            _LOG.debug('testing anydbm caching...')
            region.set(trial_key, trial_val)
            assert trial_val == region.get(trial_key)
            _LOG.debug('anydbm caching works')
            region.delete(trial_key)
            _REGION = region
            return region
        except:
            _LOG.debug('anydbm cache set up failed')
            _LOG.debug('exception in the configuration of the cache.')
    _LOG.debug('Phylesystem will not use caching')
    return None",python,"def _make_phylesystem_cache_region(**kwargs):
    """"""Only intended to be called by the Phylesystem singleton.
    """"""
    global _CACHE_REGION_CONFIGURED, _REGION
    if _CACHE_REGION_CONFIGURED:
        return _REGION
    _CACHE_REGION_CONFIGURED = True
    try:
        # noinspection PyPackageRequirements
        from dogpile.cache import make_region
    except:
        _LOG.debug('dogpile.cache not available')
        return
    region = None
    trial_key = 'test_key'
    trial_val = {'test_val': [4, 3]}
    trying_redis = True
    if trying_redis:
        try:
            a = {
                'host': 'localhost',
                'port': 6379,
                'db': 0,  # default is 0
                'redis_expiration_time': 60 * 60 * 24 * 2,  # 2 days
                'distributed_lock': False  # True if multiple processes will use redis
            }
            region = make_region().configure('dogpile.cache.redis', arguments=a)
            _LOG.debug('cache region set up with cache.redis.')
            _LOG.debug('testing redis caching...')
            region.set(trial_key, trial_val)
            assert trial_val == region.get(trial_key)
            _LOG.debug('redis caching works')
            region.delete(trial_key)
            _REGION = region
            return region
        except:
            _LOG.debug('redis cache set up failed.')
            region = None
    trying_file_dbm = False
    if trying_file_dbm:
        _LOG.debug('Going to try dogpile.cache.dbm ...')
        first_par = _get_phylesystem_parent(**kwargs)[0]
        cache_db_dir = os.path.split(first_par)[0]
        cache_db = os.path.join(cache_db_dir, 'phylesystem-cachefile.dbm')
        _LOG.debug('dogpile.cache region using ""{}""'.format(cache_db))
        try:
            a = {'filename': cache_db}
            region = make_region().configure('dogpile.cache.dbm',
                                             expiration_time=36000,
                                             arguments=a)
            _LOG.debug('cache region set up with cache.dbm.')
            _LOG.debug('testing anydbm caching...')
            region.set(trial_key, trial_val)
            assert trial_val == region.get(trial_key)
            _LOG.debug('anydbm caching works')
            region.delete(trial_key)
            _REGION = region
            return region
        except:
            _LOG.debug('anydbm cache set up failed')
            _LOG.debug('exception in the configuration of the cache.')
    _LOG.debug('Phylesystem will not use caching')
    return None",def,_make_phylesystem_cache_region,(,*,*,kwargs,),:,global,_CACHE_REGION_CONFIGURED,",",_REGION,if,_CACHE_REGION_CONFIGURED,:,return,_REGION,_CACHE_REGION_CONFIGURED,=,True,try,:,# noinspection PyPackageRequirements,from,dogpile,.,cache,import,make_region,except,:,_LOG,.,debug,(,'dogpile.cache not available',),return,region,=,None,trial_key,=,Only intended to be called by the Phylesystem singleton.,Only,intended,to,be,called,by,the,Phylesystem,singleton,.,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/phylesystem/helper.py#L125-L187,train,'test_key',trial_val,=,{,'test_val',:,[,4,",",3,],},trying_redis,=,True,if,trying_redis,:,try,:,a,=,{,'host',:,'localhost',",",'port',:,6379,,,,,,,,,,,,,,,,,,,,",",'db',:,0,",",# default is 0,'redis_expiration_time',:,60,*,60,*,24,*,2,",",# 2 days,'distributed_lock',:,False,# True if multiple processes will use redis,},region,=,make_region,(,),.,configure,(,'dogpile.cache.redis',",",arguments,=,a,),_LOG,.,debug,(,'cache region set up with cache.redis.',),_LOG,.,debug,(,'testing redis caching...',),region,.,set,(,trial_key,",",trial_val,),assert,trial_val,==,region,.,get,(,trial_key,),_LOG,.,debug,(,'redis caching works',),region,.,delete,(,trial_key,),_REGION,=,region,return,region,except,:,_LOG,.,debug,(,'redis cache set up failed.',),region,=,None,trying_file_dbm,=,False,if,trying_file_dbm,:,_LOG,.,debug,(,'Going to try dogpile.cache.dbm ...',),first_par,=,_get_phylesystem_parent,(,*,*,kwargs,),[,0,],cache_db_dir,=,os,.,path,.,split,(,first_par,),[,0,],cache_db,=,os,.,path,.,join,(,cache_db_dir,",",'phylesystem-cachefile.dbm',),_LOG,.,debug,(,"'dogpile.cache region using ""{}""'",.,format,(,cache_db,),),try,:,a,=,{,'filename',:,cache_db,},region,=,make_region,(,),.,configure,(,'dogpile.cache.dbm',",",expiration_time,=,36000,",",arguments,=,a,),_LOG,.,debug,(,'cache region set up with cache.dbm.',),_LOG,.,debug,(,'testing anydbm caching...',),region,.,set,(,trial_key,",",trial_val,),assert,trial_val,==,region,.,get,(,trial_key,),_LOG,.,debug,(,'anydbm caching works',),region,.,delete,(,trial_key,),_REGION,=,region,return,region,except,:,_LOG,.,debug,(,'anydbm cache set up failed',),_LOG,.,debug,(,'exception in the configuration of the cache.',),_LOG,.,debug,(,'Phylesystem will not use caching',),return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.path_for_doc,"def path_for_doc(self, doc_id):
        """"""Returns doc_dir and doc_filepath for doc_id.
        """"""
        full_path = self.path_for_doc_fn(self.repo, doc_id)
        # _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc_fn: {}'.format(self.path_for_doc_fn))
        # _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc returning: [{}]'.format(full_path))
        return full_path",python,"def path_for_doc(self, doc_id):
        """"""Returns doc_dir and doc_filepath for doc_id.
        """"""
        full_path = self.path_for_doc_fn(self.repo, doc_id)
        # _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc_fn: {}'.format(self.path_for_doc_fn))
        # _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc returning: [{}]'.format(full_path))
        return full_path",def,path_for_doc,(,self,",",doc_id,),:,full_path,=,self,.,path_for_doc_fn,(,self,.,repo,",",doc_id,),# _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc_fn: {}'.format(self.path_for_doc_fn)),# _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc returning: [{}]'.format(full_path)),return,full_path,,,,,,,,,,,,,,,,,,,,Returns doc_dir and doc_filepath for doc_id.,Returns,doc_dir,and,doc_filepath,for,doc_id,.,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L143-L149,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.current_branch,"def current_branch(self):
        """"""Return the current branch name""""""
        branch_name = git(self.gitdir, self.gitwd, ""symbolic-ref"", ""HEAD"")
        return branch_name.replace('refs/heads/', '').strip()",python,"def current_branch(self):
        """"""Return the current branch name""""""
        branch_name = git(self.gitdir, self.gitwd, ""symbolic-ref"", ""HEAD"")
        return branch_name.replace('refs/heads/', '').strip()",def,current_branch,(,self,),:,branch_name,=,git,(,self,.,gitdir,",",self,.,gitwd,",","""symbolic-ref""",",","""HEAD""",),return,branch_name,.,replace,(,'refs/heads/',",",'',),.,strip,(,),,,,,,,,,Return the current branch name,Return,the,current,branch,name,,,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L169-L172,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.branch_exists,"def branch_exists(self, branch):
        """"""Returns true or false depending on if a branch exists""""""
        try:
            git(self.gitdir, self.gitwd, ""rev-parse"", branch)
        except sh.ErrorReturnCode:
            return False
        return True",python,"def branch_exists(self, branch):
        """"""Returns true or false depending on if a branch exists""""""
        try:
            git(self.gitdir, self.gitwd, ""rev-parse"", branch)
        except sh.ErrorReturnCode:
            return False
        return True",def,branch_exists,(,self,",",branch,),:,try,:,git,(,self,.,gitdir,",",self,.,gitwd,",","""rev-parse""",",",branch,),except,sh,.,ErrorReturnCode,:,return,False,return,True,,,,,,,,,,,Returns true or false depending on if a branch exists,Returns,true,or,false,depending,on,if,a,branch,exists,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L174-L180,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.fetch,"def fetch(self, remote='origin'):
        """"""fetch from a remote""""""
        git(self.gitdir, ""fetch"", remote, _env=self.env())",python,"def fetch(self, remote='origin'):
        """"""fetch from a remote""""""
        git(self.gitdir, ""fetch"", remote, _env=self.env())",def,fetch,(,self,",",remote,=,'origin',),:,git,(,self,.,gitdir,",","""fetch""",",",remote,",",_env,=,self,.,env,(,),),,,,,,,,,,,,,,,,fetch from a remote,fetch,from,a,remote,,,,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L204-L206,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.get_version_history_for_file,"def get_version_history_for_file(self, filepath):
        """""" Return a dict representation of this file's commit history

        This uses specially formatted git-log output for easy parsing, as described here:
            http://blog.lost-theory.org/post/how-to-parse-git-log-output/
        For a full list of available fields, see:
            http://linux.die.net/man/1/git-log

        """"""
        # define the desired fields for logout output, matching the order in these lists!
        GIT_COMMIT_FIELDS = ['id',
                             'author_name',
                             'author_email',
                             'date',
                             'date_ISO_8601',
                             'relative_date',
                             'message_subject',
                             'message_body']
        GIT_LOG_FORMAT = ['%H', '%an', '%ae', '%aD', '%ai', '%ar', '%s', '%b']
        # make the final format string, using standard ASCII field/record delimiters
        GIT_LOG_FORMAT = '%x1f'.join(GIT_LOG_FORMAT) + '%x1e'
        try:
            log = git(self.gitdir,
                      self.gitwd,
                      '--no-pager',
                      'log',
                      '--format=%s' % GIT_LOG_FORMAT,
                      '--follow',  # Track file's history when moved/renamed...
                      '--find-renames=100%',  # ... but only if the contents are identical!
                      '--',
                      filepath)
            # _LOG.debug('log said ""{}""'.format(log))
            log = log.strip('\n\x1e').split(""\x1e"")
            log = [row.strip().split(""\x1f"") for row in log]
            log = [dict(zip(GIT_COMMIT_FIELDS, row)) for row in log]
        except:
            _LOG.exception('git log failed')
            raise
        return log",python,"def get_version_history_for_file(self, filepath):
        """""" Return a dict representation of this file's commit history

        This uses specially formatted git-log output for easy parsing, as described here:
            http://blog.lost-theory.org/post/how-to-parse-git-log-output/
        For a full list of available fields, see:
            http://linux.die.net/man/1/git-log

        """"""
        # define the desired fields for logout output, matching the order in these lists!
        GIT_COMMIT_FIELDS = ['id',
                             'author_name',
                             'author_email',
                             'date',
                             'date_ISO_8601',
                             'relative_date',
                             'message_subject',
                             'message_body']
        GIT_LOG_FORMAT = ['%H', '%an', '%ae', '%aD', '%ai', '%ar', '%s', '%b']
        # make the final format string, using standard ASCII field/record delimiters
        GIT_LOG_FORMAT = '%x1f'.join(GIT_LOG_FORMAT) + '%x1e'
        try:
            log = git(self.gitdir,
                      self.gitwd,
                      '--no-pager',
                      'log',
                      '--format=%s' % GIT_LOG_FORMAT,
                      '--follow',  # Track file's history when moved/renamed...
                      '--find-renames=100%',  # ... but only if the contents are identical!
                      '--',
                      filepath)
            # _LOG.debug('log said ""{}""'.format(log))
            log = log.strip('\n\x1e').split(""\x1e"")
            log = [row.strip().split(""\x1f"") for row in log]
            log = [dict(zip(GIT_COMMIT_FIELDS, row)) for row in log]
        except:
            _LOG.exception('git log failed')
            raise
        return log",def,get_version_history_for_file,(,self,",",filepath,),:,"# define the desired fields for logout output, matching the order in these lists!",GIT_COMMIT_FIELDS,=,[,'id',",",'author_name',",",'author_email',",",'date',",",'date_ISO_8601',",",'relative_date',",",'message_subject',",",'message_body',],GIT_LOG_FORMAT,=,[,'%H',",",'%an',",",'%ae',",",'%aD',",",'%ai',",",'%ar',",","Return a dict representation of this file's commit history

        This uses specially formatted git-log output for easy parsing, as described here:
            http://blog.lost-theory.org/post/how-to-parse-git-log-output/
        For a full list of available fields, see:
            http://linux.die.net/man/1/git-log",Return,a,dict,representation,of,this,file,s,commit,history,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L235-L273,train,'%s',",",'%b',],"# make the final format string, using standard ASCII field/record delimiters",GIT_LOG_FORMAT,=,'%x1f',.,join,(,GIT_LOG_FORMAT,),+,'%x1e',try,:,log,=,git,(,self,.,gitdir,",",self,.,gitwd,",",'--no-pager',,,,,,,,,,,,,,,,,,,,",",'log',",",'--format=%s',%,GIT_LOG_FORMAT,",",'--follow',",",# Track file's history when moved/renamed...,'--find-renames=100%',",",# ... but only if the contents are identical!,'--',",",filepath,),"# _LOG.debug('log said ""{}""'.format(log))",log,=,log,.,strip,(,'\n\x1e',),.,split,(,"""\x1e""",),log,=,[,row,.,strip,(,),.,split,(,"""\x1f""",),for,row,in,log,],log,=,[,dict,(,zip,(,GIT_COMMIT_FIELDS,",",row,),),for,row,in,log,],except,:,_LOG,.,exception,(,'git log failed',),raise,return,log,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase._add_and_commit,"def _add_and_commit(self, doc_filepath, author, commit_msg):
        """"""Low level function used internally when you have an absolute filepath to add and commit""""""
        try:
            git(self.gitdir, self.gitwd, ""add"", doc_filepath)
            git(self.gitdir, self.gitwd, ""commit"", author=author, message=commit_msg)
        except Exception as e:
            # We can ignore this if no changes are new,
            # otherwise raise a 400
            if ""nothing to commit"" in e.message:  # @EJM is this dangerous?
                _LOG.debug('""nothing to commit"" found in error response')
            else:
                _LOG.exception('""git commit"" failed')
                self.reset_hard()
                raise",python,"def _add_and_commit(self, doc_filepath, author, commit_msg):
        """"""Low level function used internally when you have an absolute filepath to add and commit""""""
        try:
            git(self.gitdir, self.gitwd, ""add"", doc_filepath)
            git(self.gitdir, self.gitwd, ""commit"", author=author, message=commit_msg)
        except Exception as e:
            # We can ignore this if no changes are new,
            # otherwise raise a 400
            if ""nothing to commit"" in e.message:  # @EJM is this dangerous?
                _LOG.debug('""nothing to commit"" found in error response')
            else:
                _LOG.exception('""git commit"" failed')
                self.reset_hard()
                raise",def,_add_and_commit,(,self,",",doc_filepath,",",author,",",commit_msg,),:,try,:,git,(,self,.,gitdir,",",self,.,gitwd,",","""add""",",",doc_filepath,),git,(,self,.,gitdir,",",self,.,gitwd,",","""commit""",",",author,=,author,Low level function used internally when you have an absolute filepath to add and commit,Low,level,function,used,internally,when,you,have,an,absolute,filepath,to,add,and,commit,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L275-L288,train,",",message,=,commit_msg,),except,Exception,as,e,:,"# We can ignore this if no changes are new,",# otherwise raise a 400,if,"""nothing to commit""",in,e,.,message,:,# @EJM is this dangerous?,_LOG,.,debug,(,"'""nothing to commit"" found in error response'",),else,:,_LOG,.,,,,,,,,,,,,,,,,,,,,exception,(,"'""git commit"" failed'",),self,.,reset_hard,(,),raise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase._remove_document,"def _remove_document(self, gh_user, doc_id, parent_sha, author, commit_msg=None):
        """"""Remove a document
        Remove a document on the given branch and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        # _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_id={}"".format(doc_id))
        doc_filepath = self.path_for_doc(doc_id)
        # _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_filepath={}"".format(doc_filepath))

        branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha)
        prev_file_sha = None
        if commit_msg is None:
            msg = ""Delete document '%s' via OpenTree API"" % doc_id
        else:
            msg = commit_msg
        if os.path.exists(doc_filepath):
            prev_file_sha = self.get_blob_sha_for_file(doc_filepath)
            if self.doc_type == 'nexson':
                # delete the parent directory entirely
                doc_dir = os.path.split(doc_filepath)[0]
                # _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_dir={}"".format(doc_dir))
                git(self.gitdir, self.gitwd, ""rm"", ""-rf"", doc_dir)
            elif self.doc_type in ('collection', 'favorites', 'amendment'):
                # delete just the target file
                git(self.gitdir, self.gitwd, ""rm"", doc_filepath)
            else:
                raise NotImplementedError(""No deletion rules for doc_type '{}'"".format(self.doc_type))
            git(self.gitdir,
                self.gitwd,
                ""commit"",
                author=author,
                message=msg)
        new_sha = git(self.gitdir, self.gitwd, ""rev-parse"", ""HEAD"").strip()
        return {'commit_sha': new_sha,
                'branch': branch,
                'prev_file_sha': prev_file_sha,
                }",python,"def _remove_document(self, gh_user, doc_id, parent_sha, author, commit_msg=None):
        """"""Remove a document
        Remove a document on the given branch and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        # _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_id={}"".format(doc_id))
        doc_filepath = self.path_for_doc(doc_id)
        # _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_filepath={}"".format(doc_filepath))

        branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha)
        prev_file_sha = None
        if commit_msg is None:
            msg = ""Delete document '%s' via OpenTree API"" % doc_id
        else:
            msg = commit_msg
        if os.path.exists(doc_filepath):
            prev_file_sha = self.get_blob_sha_for_file(doc_filepath)
            if self.doc_type == 'nexson':
                # delete the parent directory entirely
                doc_dir = os.path.split(doc_filepath)[0]
                # _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_dir={}"".format(doc_dir))
                git(self.gitdir, self.gitwd, ""rm"", ""-rf"", doc_dir)
            elif self.doc_type in ('collection', 'favorites', 'amendment'):
                # delete just the target file
                git(self.gitdir, self.gitwd, ""rm"", doc_filepath)
            else:
                raise NotImplementedError(""No deletion rules for doc_type '{}'"".format(self.doc_type))
            git(self.gitdir,
                self.gitwd,
                ""commit"",
                author=author,
                message=msg)
        new_sha = git(self.gitdir, self.gitwd, ""rev-parse"", ""HEAD"").strip()
        return {'commit_sha': new_sha,
                'branch': branch,
                'prev_file_sha': prev_file_sha,
                }",def,_remove_document,(,self,",",gh_user,",",doc_id,",",parent_sha,",",author,",",commit_msg,=,None,),:,"# _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_id={}"".format(doc_id))",doc_filepath,=,self,.,path_for_doc,(,doc_id,),"# _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_filepath={}"".format(doc_filepath))",branch,=,self,.,create_or_checkout_branch,(,gh_user,",",doc_id,",",parent_sha,),prev_file_sha,=,None,"Remove a document
        Remove a document on the given branch and attribute the commit to author.
        Returns the SHA of the commit on branch.",Remove,a,document,Remove,a,document,on,the,given,branch,and,attribute,the,commit,to,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L429-L465,train,if,commit_msg,is,None,:,msg,=,"""Delete document '%s' via OpenTree API""",%,doc_id,else,:,msg,=,commit_msg,if,os,.,path,.,exists,(,doc_filepath,),:,prev_file_sha,=,self,.,get_blob_sha_for_file,author,.,Returns,the,SHA,of,the,commit,on,branch,.,,,,,,,,,(,doc_filepath,),if,self,.,doc_type,==,'nexson',:,# delete the parent directory entirely,doc_dir,=,os,.,path,.,split,(,doc_filepath,),[,0,],"# _LOG.debug(""@@@@@@@@ GitActionBase._remove_document, doc_dir={}"".format(doc_dir))",git,(,self,.,gitdir,",",self,.,gitwd,",","""rm""",",","""-rf""",",",doc_dir,),elif,self,.,doc_type,in,(,'collection',",",'favorites',",",'amendment',),:,# delete just the target file,git,(,self,.,gitdir,",",self,.,gitwd,",","""rm""",",",doc_filepath,),else,:,raise,NotImplementedError,(,"""No deletion rules for doc_type '{}'""",.,format,(,self,.,doc_type,),),git,(,self,.,gitdir,",",self,.,gitwd,",","""commit""",",",author,=,author,",",message,=,msg,),new_sha,=,git,(,self,.,gitdir,",",self,.,gitwd,",","""rev-parse""",",","""HEAD""",),.,strip,(,),return,{,'commit_sha',:,new_sha,",",'branch',:,branch,",",'prev_file_sha',:,prev_file_sha,",",},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.write_document,"def write_document(self, gh_user, doc_id, file_content, branch, author, commit_msg=None):
        """"""Given a document id, temporary filename of content, branch and auth_info

        Deprecated but needed until we merge api local-dep to master...

        """"""
        parent_sha = None
        fc = tempfile.NamedTemporaryFile()
        # N.B. we currently assume file_content is text/JSON, or should be serialized from a dict
        if is_str_type(file_content):
            fc.write(file_content)
        else:
            write_as_json(file_content, fc)
        fc.flush()
        try:
            doc_filepath = self.path_for_doc(doc_id)
            doc_dir = os.path.split(doc_filepath)[0]
            if parent_sha is None:
                self.checkout_master()
                parent_sha = self.get_master_sha()
            branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha, force_branch_name=True)
            # create a document directory if this is a new doc EJM- what if it isn't?
            if not os.path.isdir(doc_dir):
                os.makedirs(doc_dir)
            shutil.copy(fc.name, doc_filepath)
            git(self.gitdir, self.gitwd, ""add"", doc_filepath)
            if commit_msg is None:
                commit_msg = ""Update document '%s' via OpenTree API"" % doc_id
            try:
                git(self.gitdir,
                    self.gitwd,
                    ""commit"",
                    author=author,
                    message=commit_msg)
            except Exception as e:
                # We can ignore this if no changes are new,
                # otherwise raise a 400
                if ""nothing to commit"" in e.message:  # @EJM is this dangerous?
                    pass
                else:
                    _LOG.exception('""git commit"" failed')
                    self.reset_hard()
                    raise
            new_sha = git(self.gitdir, self.gitwd, ""rev-parse"", ""HEAD"")
        except Exception as e:
            _LOG.exception('write_document exception')
            raise GitWorkflowError(""Could not write to document #%s ! Details: \n%s"" % (doc_id, e.message))
        finally:
            fc.close()
        return new_sha",python,"def write_document(self, gh_user, doc_id, file_content, branch, author, commit_msg=None):
        """"""Given a document id, temporary filename of content, branch and auth_info

        Deprecated but needed until we merge api local-dep to master...

        """"""
        parent_sha = None
        fc = tempfile.NamedTemporaryFile()
        # N.B. we currently assume file_content is text/JSON, or should be serialized from a dict
        if is_str_type(file_content):
            fc.write(file_content)
        else:
            write_as_json(file_content, fc)
        fc.flush()
        try:
            doc_filepath = self.path_for_doc(doc_id)
            doc_dir = os.path.split(doc_filepath)[0]
            if parent_sha is None:
                self.checkout_master()
                parent_sha = self.get_master_sha()
            branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha, force_branch_name=True)
            # create a document directory if this is a new doc EJM- what if it isn't?
            if not os.path.isdir(doc_dir):
                os.makedirs(doc_dir)
            shutil.copy(fc.name, doc_filepath)
            git(self.gitdir, self.gitwd, ""add"", doc_filepath)
            if commit_msg is None:
                commit_msg = ""Update document '%s' via OpenTree API"" % doc_id
            try:
                git(self.gitdir,
                    self.gitwd,
                    ""commit"",
                    author=author,
                    message=commit_msg)
            except Exception as e:
                # We can ignore this if no changes are new,
                # otherwise raise a 400
                if ""nothing to commit"" in e.message:  # @EJM is this dangerous?
                    pass
                else:
                    _LOG.exception('""git commit"" failed')
                    self.reset_hard()
                    raise
            new_sha = git(self.gitdir, self.gitwd, ""rev-parse"", ""HEAD"")
        except Exception as e:
            _LOG.exception('write_document exception')
            raise GitWorkflowError(""Could not write to document #%s ! Details: \n%s"" % (doc_id, e.message))
        finally:
            fc.close()
        return new_sha",def,write_document,(,self,",",gh_user,",",doc_id,",",file_content,",",branch,",",author,",",commit_msg,=,None,),:,parent_sha,=,None,fc,=,tempfile,.,NamedTemporaryFile,(,),"# N.B. we currently assume file_content is text/JSON, or should be serialized from a dict",if,is_str_type,(,file_content,),:,fc,.,write,(,file_content,),"Given a document id, temporary filename of content, branch and auth_info

        Deprecated but needed until we merge api local-dep to master...",Given,a,document,id,temporary,filename,of,content,branch,and,auth_info,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L467-L516,train,else,:,write_as_json,(,file_content,",",fc,),fc,.,flush,(,),try,:,doc_filepath,=,self,.,path_for_doc,(,doc_id,),doc_dir,=,os,.,path,.,split,,,,,,,,,,,,,,,,,,,,(,doc_filepath,),[,0,],if,parent_sha,is,None,:,self,.,checkout_master,(,),parent_sha,=,self,.,get_master_sha,(,),branch,=,self,.,create_or_checkout_branch,(,gh_user,",",doc_id,",",parent_sha,",",force_branch_name,=,True,),# create a document directory if this is a new doc EJM- what if it isn't?,if,not,os,.,path,.,isdir,(,doc_dir,),:,os,.,makedirs,(,doc_dir,),shutil,.,copy,(,fc,.,name,",",doc_filepath,),git,(,self,.,gitdir,",",self,.,gitwd,",","""add""",",",doc_filepath,),if,commit_msg,is,None,:,commit_msg,=,"""Update document '%s' via OpenTree API""",%,doc_id,try,:,git,(,self,.,gitdir,",",self,.,gitwd,",","""commit""",",",author,=,author,",",message,=,commit_msg,),except,Exception,as,e,:,"# We can ignore this if no changes are new,",# otherwise raise a 400,if,"""nothing to commit""",in,e,.,message,:,# @EJM is this dangerous?,pass,else,:,_LOG,.,exception,(,"'""git commit"" failed'",),self,.,reset_hard,(,),raise,new_sha,=,git,(,self,.,gitdir,",",self,.,gitwd,",","""rev-parse""",",","""HEAD""",),except,Exception,as,e,:,_LOG,.,exception,(,'write_document exception',),raise,GitWorkflowError,(,"""Could not write to document #%s ! Details: \n%s""",%,(,doc_id,",",e,.,message,),),finally,:,fc,.,close,(,),return,new_sha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/git_storage/git_action.py,GitActionBase.write_doc_from_tmpfile,"def write_doc_from_tmpfile(self,
                               doc_id,
                               tmpfi,
                               parent_sha,
                               auth_info,
                               commit_msg='',
                               doctype_display_name=""document""):
        """"""Given a doc_id, temporary filename of content, branch and auth_info
        """"""
        gh_user, author = get_user_author(auth_info)
        doc_filepath = self.path_for_doc(doc_id)
        doc_dir = os.path.split(doc_filepath)[0]
        if parent_sha is None:
            self.checkout_master()
            parent_sha = self.get_master_sha()
        branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha)

        # build complete (probably type-specific) commit message
        default_commit_msg = ""Update %s '%s' via OpenTree API"" % (doctype_display_name, doc_id)
        if commit_msg:
            commit_msg = ""%s\n\n(%s)"" % (commit_msg, default_commit_msg)
        else:
            commit_msg = default_commit_msg

        # create a doc directory if this is a new document  EJM- what if it isn't?
        if not os.path.isdir(doc_dir):
            os.makedirs(doc_dir)

        if os.path.exists(doc_filepath):
            prev_file_sha = self.get_blob_sha_for_file(doc_filepath)
        else:
            prev_file_sha = None
        shutil.copy(tmpfi.name, doc_filepath)
        self._add_and_commit(doc_filepath, author, commit_msg)
        new_sha = git(self.gitdir, self.gitwd, ""rev-parse"", ""HEAD"")
        _LOG.debug('Committed document ""{i}"" to branch ""{b}"" commit SHA: ""{s}""'.format(i=doc_id,
                                                                                       b=branch,
                                                                                       s=new_sha.strip()))
        return {'commit_sha': new_sha.strip(),
                'branch': branch,
                'prev_file_sha': prev_file_sha,
                }",python,"def write_doc_from_tmpfile(self,
                               doc_id,
                               tmpfi,
                               parent_sha,
                               auth_info,
                               commit_msg='',
                               doctype_display_name=""document""):
        """"""Given a doc_id, temporary filename of content, branch and auth_info
        """"""
        gh_user, author = get_user_author(auth_info)
        doc_filepath = self.path_for_doc(doc_id)
        doc_dir = os.path.split(doc_filepath)[0]
        if parent_sha is None:
            self.checkout_master()
            parent_sha = self.get_master_sha()
        branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha)

        # build complete (probably type-specific) commit message
        default_commit_msg = ""Update %s '%s' via OpenTree API"" % (doctype_display_name, doc_id)
        if commit_msg:
            commit_msg = ""%s\n\n(%s)"" % (commit_msg, default_commit_msg)
        else:
            commit_msg = default_commit_msg

        # create a doc directory if this is a new document  EJM- what if it isn't?
        if not os.path.isdir(doc_dir):
            os.makedirs(doc_dir)

        if os.path.exists(doc_filepath):
            prev_file_sha = self.get_blob_sha_for_file(doc_filepath)
        else:
            prev_file_sha = None
        shutil.copy(tmpfi.name, doc_filepath)
        self._add_and_commit(doc_filepath, author, commit_msg)
        new_sha = git(self.gitdir, self.gitwd, ""rev-parse"", ""HEAD"")
        _LOG.debug('Committed document ""{i}"" to branch ""{b}"" commit SHA: ""{s}""'.format(i=doc_id,
                                                                                       b=branch,
                                                                                       s=new_sha.strip()))
        return {'commit_sha': new_sha.strip(),
                'branch': branch,
                'prev_file_sha': prev_file_sha,
                }",def,write_doc_from_tmpfile,(,self,",",doc_id,",",tmpfi,",",parent_sha,",",auth_info,",",commit_msg,=,'',",",doctype_display_name,=,"""document""",),:,gh_user,",",author,=,get_user_author,(,auth_info,),doc_filepath,=,self,.,path_for_doc,(,doc_id,),doc_dir,=,os,.,path,"Given a doc_id, temporary filename of content, branch and auth_info",Given,a,doc_id,temporary,filename,of,content,branch,and,auth_info,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/git_storage/git_action.py#L518-L559,train,.,split,(,doc_filepath,),[,0,],if,parent_sha,is,None,:,self,.,checkout_master,(,),parent_sha,=,self,.,get_master_sha,(,),branch,=,self,.,create_or_checkout_branch,,,,,,,,,,,,,,,,,,,,(,gh_user,",",doc_id,",",parent_sha,),# build complete (probably type-specific) commit message,default_commit_msg,=,"""Update %s '%s' via OpenTree API""",%,(,doctype_display_name,",",doc_id,),if,commit_msg,:,commit_msg,=,"""%s\n\n(%s)""",%,(,commit_msg,",",default_commit_msg,),else,:,commit_msg,=,default_commit_msg,# create a doc directory if this is a new document  EJM- what if it isn't?,if,not,os,.,path,.,isdir,(,doc_dir,),:,os,.,makedirs,(,doc_dir,),if,os,.,path,.,exists,(,doc_filepath,),:,prev_file_sha,=,self,.,get_blob_sha_for_file,(,doc_filepath,),else,:,prev_file_sha,=,None,shutil,.,copy,(,tmpfi,.,name,",",doc_filepath,),self,.,_add_and_commit,(,doc_filepath,",",author,",",commit_msg,),new_sha,=,git,(,self,.,gitdir,",",self,.,gitwd,",","""rev-parse""",",","""HEAD""",),_LOG,.,debug,(,"'Committed document ""{i}"" to branch ""{b}"" commit SHA: ""{s}""'",.,format,(,i,=,doc_id,",",b,=,branch,",",s,=,new_sha,.,strip,(,),),),return,{,'commit_sha',:,new_sha,.,strip,(,),",",'branch',:,branch,",",'prev_file_sha',:,prev_file_sha,",",},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/amendments/git_actions.py,TaxonomicAmendmentsGitAction.remove_amendment,"def remove_amendment(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        """"""Remove an amendment
        Given a amendment_id, branch and optionally an
        author, remove an amendment on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        if fourth_arg is None:
            amendment_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_amendment_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, amendment_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = ""Delete Amendment '%s' via OpenTree API"" % amendment_id
        return self._remove_document(gh_user, amendment_id, parent_sha, author, commit_msg)",python,"def remove_amendment(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        """"""Remove an amendment
        Given a amendment_id, branch and optionally an
        author, remove an amendment on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        if fourth_arg is None:
            amendment_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_amendment_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, amendment_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = ""Delete Amendment '%s' via OpenTree API"" % amendment_id
        return self._remove_document(gh_user, amendment_id, parent_sha, author, commit_msg)",def,remove_amendment,(,self,",",first_arg,",",sec_arg,",",third_arg,",",fourth_arg,=,None,",",commit_msg,=,None,),:,if,fourth_arg,is,None,:,amendment_id,",",branch_name,",",author,=,first_arg,",",sec_arg,",",third_arg,gh_user,=,branch_name,.,split,(,'_amendment_',"Remove an amendment
        Given a amendment_id, branch and optionally an
        author, remove an amendment on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.",Remove,an,amendment,Given,a,amendment_id,branch,and,optionally,an,author,remove,an,amendment,on,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/amendments/git_actions.py#L92-L107,train,),[,0,],parent_sha,=,self,.,get_master_sha,(,),else,:,gh_user,",",amendment_id,",",parent_sha,",",author,=,first_arg,",",sec_arg,",",third_arg,",",fourth_arg,if,commit_msg,the,given,branch,and,attribute,the,commit,to,author,.,Returns,the,SHA,of,the,commit,on,branch,.,is,None,:,commit_msg,=,"""Delete Amendment '%s' via OpenTree API""",%,amendment_id,return,self,.,_remove_document,(,gh_user,",",amendment_id,",",parent_sha,",",author,",",commit_msg,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,InclusionRequest.create,"def create(cls, community, record, user=None, expires_at=None,
               notify=True):
        """"""Create a record inclusion request to a community.

        :param community: Community object.
        :param record: Record API object.
        :param expires_at: Time after which the request expires and shouldn't
            be resolved anymore.
        """"""
        if expires_at and expires_at < datetime.utcnow():
            raise InclusionRequestExpiryTimeError(
                community=community, record=record)

        if community.has_record(record):
            raise InclusionRequestObsoleteError(
                community=community, record=record)

        try:
            # Create inclusion request
            with db.session.begin_nested():
                obj = cls(
                    id_community=community.id,
                    id_record=record.id,
                    user=user,
                    expires_at=expires_at
                )
                db.session.add(obj)
        except (IntegrityError, FlushError):
            raise InclusionRequestExistsError(
                community=community, record=record)

        # Send signal
        inclusion_request_created.send(
            current_app._get_current_object(),
            request=obj,
            notify=notify
        )

        return obj",python,"def create(cls, community, record, user=None, expires_at=None,
               notify=True):
        """"""Create a record inclusion request to a community.

        :param community: Community object.
        :param record: Record API object.
        :param expires_at: Time after which the request expires and shouldn't
            be resolved anymore.
        """"""
        if expires_at and expires_at < datetime.utcnow():
            raise InclusionRequestExpiryTimeError(
                community=community, record=record)

        if community.has_record(record):
            raise InclusionRequestObsoleteError(
                community=community, record=record)

        try:
            # Create inclusion request
            with db.session.begin_nested():
                obj = cls(
                    id_community=community.id,
                    id_record=record.id,
                    user=user,
                    expires_at=expires_at
                )
                db.session.add(obj)
        except (IntegrityError, FlushError):
            raise InclusionRequestExistsError(
                community=community, record=record)

        # Send signal
        inclusion_request_created.send(
            current_app._get_current_object(),
            request=obj,
            notify=notify
        )

        return obj",def,create,(,cls,",",community,",",record,",",user,=,None,",",expires_at,=,None,",",notify,=,True,),:,if,expires_at,and,expires_at,<,datetime,.,utcnow,(,),:,raise,InclusionRequestExpiryTimeError,(,community,=,community,",",record,=,record,"Create a record inclusion request to a community.

        :param community: Community object.
        :param record: Record API object.
        :param expires_at: Time after which the request expires and shouldn't
            be resolved anymore.",Create,a,record,inclusion,request,to,a,community,.,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L114-L152,train,),if,community,.,has_record,(,record,),:,raise,InclusionRequestObsoleteError,(,community,=,community,",",record,=,record,),try,:,# Create inclusion request,with,db,.,session,.,begin_nested,(,,,,,,,,,,,,,,,,,,,,),:,obj,=,cls,(,id_community,=,community,.,id,",",id_record,=,record,.,id,",",user,=,user,",",expires_at,=,expires_at,),db,.,session,.,add,(,obj,),except,(,IntegrityError,",",FlushError,),:,raise,InclusionRequestExistsError,(,community,=,community,",",record,=,record,),# Send signal,inclusion_request_created,.,send,(,current_app,.,_get_current_object,(,),",",request,=,obj,",",notify,=,notify,),return,obj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,InclusionRequest.get,"def get(cls, community_id, record_uuid):
        """"""Get an inclusion request.""""""
        return cls.query.filter_by(
            id_record=record_uuid, id_community=community_id
        ).one_or_none()",python,"def get(cls, community_id, record_uuid):
        """"""Get an inclusion request.""""""
        return cls.query.filter_by(
            id_record=record_uuid, id_community=community_id
        ).one_or_none()",def,get,(,cls,",",community_id,",",record_uuid,),:,return,cls,.,query,.,filter_by,(,id_record,=,record_uuid,",",id_community,=,community_id,),.,one_or_none,(,),,,,,,,,,,,,,,,Get an inclusion request.,Get,an,inclusion,request,.,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L155-L159,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.filter_communities,"def filter_communities(cls, p, so, with_deleted=False):
        """"""Search for communities.

        Helper function which takes from database only those communities which
        match search criteria. Uses parameter 'so' to set communities in the
        correct order.

        Parameter 'page' is introduced to restrict results and return only
        slice of them for the current page. If page == 0 function will return
        all communities that match the pattern.
        """"""
        query = cls.query if with_deleted else \
            cls.query.filter(cls.deleted_at.is_(None))

        if p:
            p = p.replace(' ', '%')
            query = query.filter(db.or_(
                cls.id.ilike('%' + p + '%'),
                cls.title.ilike('%' + p + '%'),
                cls.description.ilike('%' + p + '%'),
            ))

        if so in current_app.config['COMMUNITIES_SORTING_OPTIONS']:
            order = so == 'title' and db.asc or db.desc
            query = query.order_by(order(getattr(cls, so)))
        else:
            query = query.order_by(db.desc(cls.ranking))
        return query",python,"def filter_communities(cls, p, so, with_deleted=False):
        """"""Search for communities.

        Helper function which takes from database only those communities which
        match search criteria. Uses parameter 'so' to set communities in the
        correct order.

        Parameter 'page' is introduced to restrict results and return only
        slice of them for the current page. If page == 0 function will return
        all communities that match the pattern.
        """"""
        query = cls.query if with_deleted else \
            cls.query.filter(cls.deleted_at.is_(None))

        if p:
            p = p.replace(' ', '%')
            query = query.filter(db.or_(
                cls.id.ilike('%' + p + '%'),
                cls.title.ilike('%' + p + '%'),
                cls.description.ilike('%' + p + '%'),
            ))

        if so in current_app.config['COMMUNITIES_SORTING_OPTIONS']:
            order = so == 'title' and db.asc or db.desc
            query = query.order_by(order(getattr(cls, so)))
        else:
            query = query.order_by(db.desc(cls.ranking))
        return query",def,filter_communities,(,cls,",",p,",",so,",",with_deleted,=,False,),:,query,=,cls,.,query,if,with_deleted,else,cls,.,query,.,filter,(,cls,.,deleted_at,.,is_,(,None,),),if,p,:,p,=,p,"Search for communities.

        Helper function which takes from database only those communities which
        match search criteria. Uses parameter 'so' to set communities in the
        correct order.

        Parameter 'page' is introduced to restrict results and return only
        slice of them for the current page. If page == 0 function will return
        all communities that match the pattern.",Search,for,communities,.,,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L257-L284,train,.,replace,(,' ',",",'%',),query,=,query,.,filter,(,db,.,or_,(,cls,.,id,.,ilike,(,'%',+,p,+,'%',),",",,,,,,,,,,,,,,,,,,,,cls,.,title,.,ilike,(,'%',+,p,+,'%',),",",cls,.,description,.,ilike,(,'%',+,p,+,'%',),",",),),if,so,in,current_app,.,config,[,'COMMUNITIES_SORTING_OPTIONS',],:,order,=,so,==,'title',and,db,.,asc,or,db,.,desc,query,=,query,.,order_by,(,order,(,getattr,(,cls,",",so,),),),else,:,query,=,query,.,order_by,(,db,.,desc,(,cls,.,ranking,),),return,query,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.add_record,"def add_record(self, record):
        """"""Add a record to the community.

        :param record: Record object.
        :type record: `invenio_records.api.Record`
        """"""
        key = current_app.config['COMMUNITIES_RECORD_KEY']
        record.setdefault(key, [])

        if self.has_record(record):
            current_app.logger.warning(
                'Community addition: record {uuid} is already in community '
                '""{comm}""'.format(uuid=record.id, comm=self.id))
        else:
            record[key].append(self.id)
            record[key] = sorted(record[key])
        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            if not self.oaiset.has_record(record):
                self.oaiset.add_record(record)",python,"def add_record(self, record):
        """"""Add a record to the community.

        :param record: Record object.
        :type record: `invenio_records.api.Record`
        """"""
        key = current_app.config['COMMUNITIES_RECORD_KEY']
        record.setdefault(key, [])

        if self.has_record(record):
            current_app.logger.warning(
                'Community addition: record {uuid} is already in community '
                '""{comm}""'.format(uuid=record.id, comm=self.id))
        else:
            record[key].append(self.id)
            record[key] = sorted(record[key])
        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            if not self.oaiset.has_record(record):
                self.oaiset.add_record(record)",def,add_record,(,self,",",record,),:,key,=,current_app,.,config,[,'COMMUNITIES_RECORD_KEY',],record,.,setdefault,(,key,",",[,],),if,self,.,has_record,(,record,),:,current_app,.,logger,.,warning,(,'Community addition: record {uuid} is already in community ',"'""{comm}""'",.,format,"Add a record to the community.

        :param record: Record object.
        :type record: `invenio_records.api.Record`",Add,a,record,to,the,community,.,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L286-L304,train,(,uuid,=,record,.,id,",",comm,=,self,.,id,),),else,:,record,[,key,],.,append,(,self,.,id,),record,[,key,,,,,,,,,,,,,,,,,,,,],=,sorted,(,record,[,key,],),if,current_app,.,config,[,'COMMUNITIES_OAI_ENABLED',],:,if,not,self,.,oaiset,.,has_record,(,record,),:,self,.,oaiset,.,add_record,(,record,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.remove_record,"def remove_record(self, record):
        """"""Remove an already accepted record from the community.

        :param record: Record object.
        :type record: `invenio_records.api.Record`
        """"""
        if not self.has_record(record):
            current_app.logger.warning(
                'Community removal: record {uuid} was not in community '
                '""{comm}""'.format(uuid=record.id, comm=self.id))
        else:
            key = current_app.config['COMMUNITIES_RECORD_KEY']
            record[key] = [c for c in record[key] if c != self.id]

        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            if self.oaiset.has_record(record):
                self.oaiset.remove_record(record)",python,"def remove_record(self, record):
        """"""Remove an already accepted record from the community.

        :param record: Record object.
        :type record: `invenio_records.api.Record`
        """"""
        if not self.has_record(record):
            current_app.logger.warning(
                'Community removal: record {uuid} was not in community '
                '""{comm}""'.format(uuid=record.id, comm=self.id))
        else:
            key = current_app.config['COMMUNITIES_RECORD_KEY']
            record[key] = [c for c in record[key] if c != self.id]

        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            if self.oaiset.has_record(record):
                self.oaiset.remove_record(record)",def,remove_record,(,self,",",record,),:,if,not,self,.,has_record,(,record,),:,current_app,.,logger,.,warning,(,'Community removal: record {uuid} was not in community ',"'""{comm}""'",.,format,(,uuid,=,record,.,id,",",comm,=,self,.,id,),),else,:,"Remove an already accepted record from the community.

        :param record: Record object.
        :type record: `invenio_records.api.Record`",Remove,an,already,accepted,record,from,the,community,.,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L306-L322,train,key,=,current_app,.,config,[,'COMMUNITIES_RECORD_KEY',],record,[,key,],=,[,c,for,c,in,record,[,key,],if,c,!=,self,.,id,],if,,,,,,,,,,,,,,,,,,,,current_app,.,config,[,'COMMUNITIES_OAI_ENABLED',],:,if,self,.,oaiset,.,has_record,(,record,),:,self,.,oaiset,.,remove_record,(,record,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.accept_record,"def accept_record(self, record):
        """"""Accept a record for inclusion in the community.

        :param record: Record object.
        """"""
        with db.session.begin_nested():
            req = InclusionRequest.get(self.id, record.id)
            if req is None:
                raise InclusionRequestMissingError(community=self,
                                                   record=record)
            req.delete()
            self.add_record(record)
            self.last_record_accepted = datetime.utcnow()",python,"def accept_record(self, record):
        """"""Accept a record for inclusion in the community.

        :param record: Record object.
        """"""
        with db.session.begin_nested():
            req = InclusionRequest.get(self.id, record.id)
            if req is None:
                raise InclusionRequestMissingError(community=self,
                                                   record=record)
            req.delete()
            self.add_record(record)
            self.last_record_accepted = datetime.utcnow()",def,accept_record,(,self,",",record,),:,with,db,.,session,.,begin_nested,(,),:,req,=,InclusionRequest,.,get,(,self,.,id,",",record,.,id,),if,req,is,None,:,raise,InclusionRequestMissingError,(,community,=,self,",","Accept a record for inclusion in the community.

        :param record: Record object.",Accept,a,record,for,inclusion,in,the,community,.,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L329-L341,train,record,=,record,),req,.,delete,(,),self,.,add_record,(,record,),self,.,last_record_accepted,=,datetime,.,utcnow,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.reject_record,"def reject_record(self, record):
        """"""Reject a record for inclusion in the community.

        :param record: Record object.
        """"""
        with db.session.begin_nested():
            req = InclusionRequest.get(self.id, record.id)
            if req is None:
                raise InclusionRequestMissingError(community=self,
                                                   record=record)
            req.delete()",python,"def reject_record(self, record):
        """"""Reject a record for inclusion in the community.

        :param record: Record object.
        """"""
        with db.session.begin_nested():
            req = InclusionRequest.get(self.id, record.id)
            if req is None:
                raise InclusionRequestMissingError(community=self,
                                                   record=record)
            req.delete()",def,reject_record,(,self,",",record,),:,with,db,.,session,.,begin_nested,(,),:,req,=,InclusionRequest,.,get,(,self,.,id,",",record,.,id,),if,req,is,None,:,raise,InclusionRequestMissingError,(,community,=,self,",","Reject a record for inclusion in the community.

        :param record: Record object.",Reject,a,record,for,inclusion,in,the,community,.,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L343-L353,train,record,=,record,),req,.,delete,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.delete,"def delete(self):
        """"""Mark the community for deletion.

        :param delete_time: DateTime after which to delete the community.
        :type delete_time: datetime.datetime
        :raises: CommunitiesError
        """"""
        if self.deleted_at is not None:
            raise CommunitiesError(community=self)
        else:
            self.deleted_at = datetime.utcnow()",python,"def delete(self):
        """"""Mark the community for deletion.

        :param delete_time: DateTime after which to delete the community.
        :type delete_time: datetime.datetime
        :raises: CommunitiesError
        """"""
        if self.deleted_at is not None:
            raise CommunitiesError(community=self)
        else:
            self.deleted_at = datetime.utcnow()",def,delete,(,self,),:,if,self,.,deleted_at,is,not,None,:,raise,CommunitiesError,(,community,=,self,),else,:,self,.,deleted_at,=,datetime,.,utcnow,(,),,,,,,,,,,,,"Mark the community for deletion.

        :param delete_time: DateTime after which to delete the community.
        :type delete_time: datetime.datetime
        :raises: CommunitiesError",Mark,the,community,for,deletion,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L355-L365,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.logo_url,"def logo_url(self):
        """"""Get URL to collection logo.

        :returns: Path to community logo.
        :rtype: str
        """"""
        if self.logo_ext:
            return '/api/files/{bucket}/{key}'.format(
                bucket=current_app.config['COMMUNITIES_BUCKET_UUID'],
                key='{0}/logo.{1}'.format(self.id, self.logo_ext),
            )
        return None",python,"def logo_url(self):
        """"""Get URL to collection logo.

        :returns: Path to community logo.
        :rtype: str
        """"""
        if self.logo_ext:
            return '/api/files/{bucket}/{key}'.format(
                bucket=current_app.config['COMMUNITIES_BUCKET_UUID'],
                key='{0}/logo.{1}'.format(self.id, self.logo_ext),
            )
        return None",def,logo_url,(,self,),:,if,self,.,logo_ext,:,return,'/api/files/{bucket}/{key}',.,format,(,bucket,=,current_app,.,config,[,'COMMUNITIES_BUCKET_UUID',],",",key,=,'{0}/logo.{1}',.,format,(,self,.,id,",",self,.,logo_ext,),",",),return,None,"Get URL to collection logo.

        :returns: Path to community logo.
        :rtype: str",Get,URL,to,collection,logo,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L380-L391,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.oaiset,"def oaiset(self):
        """"""Return the corresponding OAISet for given community.

        If OAIServer is not installed this property will return None.

        :returns: returns OAISet object corresponding to this community.
        :rtype: `invenio_oaiserver.models.OAISet` or None
        """"""
        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            from invenio_oaiserver.models import OAISet
            return OAISet.query.filter_by(spec=self.oaiset_spec).one()
        else:
            return None",python,"def oaiset(self):
        """"""Return the corresponding OAISet for given community.

        If OAIServer is not installed this property will return None.

        :returns: returns OAISet object corresponding to this community.
        :rtype: `invenio_oaiserver.models.OAISet` or None
        """"""
        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            from invenio_oaiserver.models import OAISet
            return OAISet.query.filter_by(spec=self.oaiset_spec).one()
        else:
            return None",def,oaiset,(,self,),:,if,current_app,.,config,[,'COMMUNITIES_OAI_ENABLED',],:,from,invenio_oaiserver,.,models,import,OAISet,return,OAISet,.,query,.,filter_by,(,spec,=,self,.,oaiset_spec,),.,one,(,),else,:,return,None,,,"Return the corresponding OAISet for given community.

        If OAIServer is not installed this property will return None.

        :returns: returns OAISet object corresponding to this community.
        :rtype: `invenio_oaiserver.models.OAISet` or None",Return,the,corresponding,OAISet,for,given,community,.,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L421-L433,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.oaiset_url,"def oaiset_url(self):
        """"""Return the OAISet URL for given community.

        :returns: URL of corresponding OAISet.
        :rtype: str
        """"""
        return url_for(
            'invenio_oaiserver.response',
            verb='ListRecords',
            metadataPrefix='oai_dc', set=self.oaiset_spec, _external=True)",python,"def oaiset_url(self):
        """"""Return the OAISet URL for given community.

        :returns: URL of corresponding OAISet.
        :rtype: str
        """"""
        return url_for(
            'invenio_oaiserver.response',
            verb='ListRecords',
            metadataPrefix='oai_dc', set=self.oaiset_spec, _external=True)",def,oaiset_url,(,self,),:,return,url_for,(,'invenio_oaiserver.response',",",verb,=,'ListRecords',",",metadataPrefix,=,'oai_dc',",",set,=,self,.,oaiset_spec,",",_external,=,True,),,,,,,,,,,,,,,,"Return the OAISet URL for given community.

        :returns: URL of corresponding OAISet.
        :rtype: str",Return,the,OAISet,URL,for,given,community,.,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L436-L445,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,Community.version_id,"def version_id(self):
        """"""Return the version of the community.

        :returns: hash which encodes the community id and its las update.
        :rtype: str
        """"""
        return hashlib.sha1('{0}__{1}'.format(
            self.id, self.updated).encode('utf-8')).hexdigest()",python,"def version_id(self):
        """"""Return the version of the community.

        :returns: hash which encodes the community id and its las update.
        :rtype: str
        """"""
        return hashlib.sha1('{0}__{1}'.format(
            self.id, self.updated).encode('utf-8')).hexdigest()",def,version_id,(,self,),:,return,hashlib,.,sha1,(,'{0}__{1}',.,format,(,self,.,id,",",self,.,updated,),.,encode,(,'utf-8',),),.,hexdigest,(,),,,,,,,,,,,"Return the version of the community.

        :returns: hash which encodes the community id and its las update.
        :rtype: str",Return,the,version,of,the,community,.,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L448-L455,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/models.py,FeaturedCommunity.get_featured_or_none,"def get_featured_or_none(cls, start_date=None):
        """"""Get the latest featured community.

        :param start_date: Date after which the featuring starts
        :returns: Community object or None
        :rtype: `invenio_communities.models.Community` or None
        """"""
        start_date = start_date or datetime.utcnow()

        comm = cls.query.filter(
            FeaturedCommunity.start_date <= start_date
        ).order_by(
            cls.start_date.desc()
        ).first()
        return comm if comm is None else comm.community",python,"def get_featured_or_none(cls, start_date=None):
        """"""Get the latest featured community.

        :param start_date: Date after which the featuring starts
        :returns: Community object or None
        :rtype: `invenio_communities.models.Community` or None
        """"""
        start_date = start_date or datetime.utcnow()

        comm = cls.query.filter(
            FeaturedCommunity.start_date <= start_date
        ).order_by(
            cls.start_date.desc()
        ).first()
        return comm if comm is None else comm.community",def,get_featured_or_none,(,cls,",",start_date,=,None,),:,start_date,=,start_date,or,datetime,.,utcnow,(,),comm,=,cls,.,query,.,filter,(,FeaturedCommunity,.,start_date,<=,start_date,),.,order_by,(,cls,.,start_date,.,desc,(,),"Get the latest featured community.

        :param start_date: Date after which the featuring starts
        :returns: Community object or None
        :rtype: `invenio_communities.models.Community` or None",Get,the,latest,featured,community,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/models.py#L486-L500,train,),.,first,(,),return,comm,if,comm,is,None,else,comm,.,community,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ARMmbed/mbed-connector-api-python,mbed_connector_api/mbed_connector_api.py,connector.getConnectorVersion,"def getConnectorVersion(self):
		""""""
		GET the current Connector version.

		:returns:  asyncResult object, populates error and result fields
		:rtype: asyncResult
		""""""
		result = asyncResult()
		data = self._getURL(""/"",versioned=False)
		result.fill(data)
		if data.status_code == 200:
			result.error = False
		else:
			result.error = response_codes(""get_mdc_version"",data.status_code)
		result.is_done = True
		return result",python,"def getConnectorVersion(self):
		""""""
		GET the current Connector version.

		:returns:  asyncResult object, populates error and result fields
		:rtype: asyncResult
		""""""
		result = asyncResult()
		data = self._getURL(""/"",versioned=False)
		result.fill(data)
		if data.status_code == 200:
			result.error = False
		else:
			result.error = response_codes(""get_mdc_version"",data.status_code)
		result.is_done = True
		return result",def,getConnectorVersion,(,self,),:,result,=,asyncResult,(,),data,=,self,.,_getURL,(,"""/""",",",versioned,=,False,),result,.,fill,(,data,),if,data,.,status_code,==,200,:,result,.,error,=,False,else,:,"GET the current Connector version.

		:returns:  asyncResult object, populates error and result fields
		:rtype: asyncResult",GET,the,current,Connector,version,.,,,,,,,,,,a5024a01dc67cc192c8bf7a70b251fcf0a3f279b,https://github.com/ARMmbed/mbed-connector-api-python/blob/a5024a01dc67cc192c8bf7a70b251fcf0a3f279b/mbed_connector_api/mbed_connector_api.py#L72-L87,train,result,.,error,=,response_codes,(,"""get_mdc_version""",",",data,.,status_code,),result,.,is_done,=,True,return,result,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ARMmbed/mbed-connector-api-python,mbed_connector_api/mbed_connector_api.py,connector.setHandler,"def setHandler(self,handler,cbfn):
		'''
		Register a handler for a particular notification type.
		These are the types of notifications that are acceptable. 
		
		| 'async-responses'
		| 'registrations-expired'
		| 'de-registrations'
		| 'reg-updates'
		| 'registrations'
		| 'notifications'

		:param str handler: name of the notification type
		:param fnptr cbfn: function to pass the notification channel messages to.
		:return: Nothing.
		'''
		if handler == ""async-responses"":
			self.async_responses_callback = cbfn
		elif handler == ""registrations-expired"":
			self.registrations_expired_callback = cbfn
		elif handler == ""de-registrations"":
			self.de_registrations_callback = cbfn
		elif handler == ""reg-updates"":
			self.reg_updates_callback = cbfn
		elif handler == ""registrations"":
			self.registrations_callback = cbfn
		elif handler == ""notifications"":
			self.notifications_callback = cbfn
		else:
			self.log.warn(""'%s' is not a legitimate notification channel option. Please check your spelling."",handler)",python,"def setHandler(self,handler,cbfn):
		'''
		Register a handler for a particular notification type.
		These are the types of notifications that are acceptable. 
		
		| 'async-responses'
		| 'registrations-expired'
		| 'de-registrations'
		| 'reg-updates'
		| 'registrations'
		| 'notifications'

		:param str handler: name of the notification type
		:param fnptr cbfn: function to pass the notification channel messages to.
		:return: Nothing.
		'''
		if handler == ""async-responses"":
			self.async_responses_callback = cbfn
		elif handler == ""registrations-expired"":
			self.registrations_expired_callback = cbfn
		elif handler == ""de-registrations"":
			self.de_registrations_callback = cbfn
		elif handler == ""reg-updates"":
			self.reg_updates_callback = cbfn
		elif handler == ""registrations"":
			self.registrations_callback = cbfn
		elif handler == ""notifications"":
			self.notifications_callback = cbfn
		else:
			self.log.warn(""'%s' is not a legitimate notification channel option. Please check your spelling."",handler)",def,setHandler,(,self,",",handler,",",cbfn,),:,if,handler,==,"""async-responses""",:,self,.,async_responses_callback,=,cbfn,elif,handler,==,"""registrations-expired""",:,self,.,registrations_expired_callback,=,cbfn,elif,handler,==,"""de-registrations""",:,self,.,de_registrations_callback,=,cbfn,elif,handler,==,"Register a handler for a particular notification type.
		These are the types of notifications that are acceptable. 
		
		| 'async-responses'
		| 'registrations-expired'
		| 'de-registrations'
		| 'reg-updates'
		| 'registrations'
		| 'notifications'

		:param str handler: name of the notification type
		:param fnptr cbfn: function to pass the notification channel messages to.
		:return: Nothing.",Register,a,handler,for,a,particular,notification,type,.,These,are,the,types,of,notifications,a5024a01dc67cc192c8bf7a70b251fcf0a3f279b,https://github.com/ARMmbed/mbed-connector-api-python/blob/a5024a01dc67cc192c8bf7a70b251fcf0a3f279b/mbed_connector_api/mbed_connector_api.py#L554-L583,train,"""reg-updates""",:,self,.,reg_updates_callback,=,cbfn,elif,handler,==,"""registrations""",:,self,.,registrations_callback,=,cbfn,elif,handler,==,"""notifications""",:,self,.,notifications_callback,=,cbfn,else,:,self,that,are,acceptable,.,|,async,-,responses,|,registrations,-,expired,|,de,-,registrations,|,reg,-,.,log,.,warn,(,"""'%s' is not a legitimate notification channel option. Please check your spelling.""",",",handler,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,updates,|,registrations,|,notifications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_doc_parser.py,JSGDocParser.as_python,"def as_python(self, infile, include_original_shex: bool=False):
        """""" Return the python representation of the document """"""
        self._context.resolve_circular_references()            # add forwards for any circular entries
        body = ''
        for k in self._context.ordered_elements():
            v = self._context.grammarelts[k]
            if isinstance(v, (JSGLexerRuleBlock, JSGObjectExpr)):
                body += v.as_python(k)
                if isinstance(v, JSGObjectExpr) and not self._context.has_typeid:
                    self._context.directives.append(f'_CONTEXT.TYPE_EXCEPTIONS.append(""{k}"")')
            elif isinstance(v, JSGForwardRef):
                pass
            elif isinstance(v, (JSGValueType, JSGArrayExpr)):
                body += f""\n\n\n{k} = {v.signature_type()}""
            else:
                raise NotImplementedError(""Unknown grammar elt for {}"".format(k))
            self._context.forward_refs.pop(k, None)

        body = '\n' + '\n'.join(self._context.directives) + body
        return _jsg_python_template.format(infile=infile,
                                           original_shex='# ' + self.text if include_original_shex else """",
                                           version=__version__,
                                           gendate=datetime.datetime.now().strftime(""%Y-%m-%d %H:%M""),
                                           body=body)",python,"def as_python(self, infile, include_original_shex: bool=False):
        """""" Return the python representation of the document """"""
        self._context.resolve_circular_references()            # add forwards for any circular entries
        body = ''
        for k in self._context.ordered_elements():
            v = self._context.grammarelts[k]
            if isinstance(v, (JSGLexerRuleBlock, JSGObjectExpr)):
                body += v.as_python(k)
                if isinstance(v, JSGObjectExpr) and not self._context.has_typeid:
                    self._context.directives.append(f'_CONTEXT.TYPE_EXCEPTIONS.append(""{k}"")')
            elif isinstance(v, JSGForwardRef):
                pass
            elif isinstance(v, (JSGValueType, JSGArrayExpr)):
                body += f""\n\n\n{k} = {v.signature_type()}""
            else:
                raise NotImplementedError(""Unknown grammar elt for {}"".format(k))
            self._context.forward_refs.pop(k, None)

        body = '\n' + '\n'.join(self._context.directives) + body
        return _jsg_python_template.format(infile=infile,
                                           original_shex='# ' + self.text if include_original_shex else """",
                                           version=__version__,
                                           gendate=datetime.datetime.now().strftime(""%Y-%m-%d %H:%M""),
                                           body=body)",def,as_python,(,self,",",infile,",",include_original_shex,:,bool,=,False,),:,self,.,_context,.,resolve_circular_references,(,),# add forwards for any circular entries,body,=,'',for,k,in,self,.,_context,.,ordered_elements,(,),:,v,=,self,.,_context,.,grammarelts,Return the python representation of the document,Return,the,python,representation,of,the,document,,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_doc_parser.py#L36-L59,train,[,k,],if,isinstance,(,v,",",(,JSGLexerRuleBlock,",",JSGObjectExpr,),),:,body,+=,v,.,as_python,(,k,),if,isinstance,(,v,",",JSGObjectExpr,),,,,,,,,,,,,,,,,,,,,and,not,self,.,_context,.,has_typeid,:,self,.,_context,.,directives,.,append,(,"f'_CONTEXT.TYPE_EXCEPTIONS.append(""{k}"")'",),elif,isinstance,(,v,",",JSGForwardRef,),:,pass,elif,isinstance,(,v,",",(,JSGValueType,",",JSGArrayExpr,),),:,body,+=,"f""\n\n\n{k} = {v.signature_type()}""",else,:,raise,NotImplementedError,(,"""Unknown grammar elt for {}""",.,format,(,k,),),self,.,_context,.,forward_refs,.,pop,(,k,",",None,),body,=,'\n',+,'\n',.,join,(,self,.,_context,.,directives,),+,body,return,_jsg_python_template,.,format,(,infile,=,infile,",",original_shex,=,'# ',+,self,.,text,if,include_original_shex,else,"""""",",",version,=,__version__,",",gendate,=,datetime,.,datetime,.,now,(,),.,strftime,(,"""%Y-%m-%d %H:%M""",),",",body,=,body,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
emirozer/bowshock,bowshock/modis.py,__getDummyDateList,"def __getDummyDateList():
    """"""
	Generate a dummy date list for testing without 
	hitting the server
	""""""

    D = []
    for y in xrange(2001, 2010):
        for d in xrange(1, 365, 1):
            D.append('A%04d%03d' % (y, d))

    return D",python,"def __getDummyDateList():
    """"""
	Generate a dummy date list for testing without 
	hitting the server
	""""""

    D = []
    for y in xrange(2001, 2010):
        for d in xrange(1, 365, 1):
            D.append('A%04d%03d' % (y, d))

    return D",def,__getDummyDateList,(,),:,D,=,[,],for,y,in,xrange,(,2001,",",2010,),:,for,d,in,xrange,(,1,",",365,",",1,),:,D,.,append,(,'A%04d%03d',%,(,y,",",d,),),"Generate a dummy date list for testing without 
	hitting the server",Generate,a,dummy,date,list,for,testing,without,hitting,the,server,,,,,9f5e053f1d54995b833b83616f37c67178c3e840,https://github.com/emirozer/bowshock/blob/9f5e053f1d54995b833b83616f37c67178c3e840/bowshock/modis.py#L92-L103,train,return,D,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
emirozer/bowshock,bowshock/modis.py,mkIntDate,"def mkIntDate(s):
    """"""
	Convert the webserver formatted dates
	to an integer format by stripping the
	leading char and casting
	""""""
    n = s.__len__()
    d = int(s[-(n - 1):n])

    return d",python,"def mkIntDate(s):
    """"""
	Convert the webserver formatted dates
	to an integer format by stripping the
	leading char and casting
	""""""
    n = s.__len__()
    d = int(s[-(n - 1):n])

    return d",def,mkIntDate,(,s,),:,n,=,s,.,__len__,(,),d,=,int,(,s,[,-,(,n,-,1,),:,n,],),return,d,,,,,,,,,,,,,"Convert the webserver formatted dates
	to an integer format by stripping the
	leading char and casting",Convert,the,webserver,formatted,dates,to,an,integer,format,by,stripping,the,leading,char,and,9f5e053f1d54995b833b83616f37c67178c3e840,https://github.com/emirozer/bowshock/blob/9f5e053f1d54995b833b83616f37c67178c3e840/bowshock/modis.py#L118-L127,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,casting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/idgen.py,IDGenerator.create_id,"def create_id(self, prefix=""guid""):
        """"""Create an ID.

        Note that if `prefix` is not provided, it will be `guid`, even if the
        `method` is `METHOD_INT`.
        """"""
        if self.method == IDGenerator.METHOD_UUID:
            id_ = str(uuid.uuid4())
        elif self.method == IDGenerator.METHOD_INT:
            id_ = self.next_int
            self.next_int += 1
        else:
            raise InvalidMethodError(self.method)

        return ""%s:%s-%s"" % (self.namespace.prefix, prefix, id_)",python,"def create_id(self, prefix=""guid""):
        """"""Create an ID.

        Note that if `prefix` is not provided, it will be `guid`, even if the
        `method` is `METHOD_INT`.
        """"""
        if self.method == IDGenerator.METHOD_UUID:
            id_ = str(uuid.uuid4())
        elif self.method == IDGenerator.METHOD_INT:
            id_ = self.next_int
            self.next_int += 1
        else:
            raise InvalidMethodError(self.method)

        return ""%s:%s-%s"" % (self.namespace.prefix, prefix, id_)",def,create_id,(,self,",",prefix,=,"""guid""",),:,if,self,.,method,==,IDGenerator,.,METHOD_UUID,:,id_,=,str,(,uuid,.,uuid4,(,),),elif,self,.,method,==,IDGenerator,.,METHOD_INT,:,id_,=,self,.,next_int,"Create an ID.

        Note that if `prefix` is not provided, it will be `guid`, even if the
        `method` is `METHOD_INT`.",Create,an,ID,.,,,,,,,,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/idgen.py#L61-L75,train,self,.,next_int,+=,1,else,:,raise,InvalidMethodError,(,self,.,method,),return,"""%s:%s-%s""",%,(,self,.,namespace,.,prefix,",",prefix,",",id_,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,grayspec,"def grayspec(k):
    """"""
    List of gray-scale colors in HSV space as web hex triplets.

    For integer argument k, returns list of `k` gray-scale colors, increasingly 
    light, linearly in the HSV color space, as web hex triplets.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    **Parameters**

            **k** :  positive integer

                    Number of gray-scale colors to return.

    **Returns**

            **glist** :  list of strings

                    List of `k` gray-scale colors.

    """"""
    ll = .5
    ul = .8
    delta = (ul - ll) / k
    return [GrayScale(t) for t in np.arange(ll, ul, delta)]",python,"def grayspec(k):
    """"""
    List of gray-scale colors in HSV space as web hex triplets.

    For integer argument k, returns list of `k` gray-scale colors, increasingly 
    light, linearly in the HSV color space, as web hex triplets.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    **Parameters**

            **k** :  positive integer

                    Number of gray-scale colors to return.

    **Returns**

            **glist** :  list of strings

                    List of `k` gray-scale colors.

    """"""
    ll = .5
    ul = .8
    delta = (ul - ll) / k
    return [GrayScale(t) for t in np.arange(ll, ul, delta)]",def,grayspec,(,k,),:,ll,=,.5,ul,=,.8,delta,=,(,ul,-,ll,),/,k,return,[,GrayScale,(,t,),for,t,in,np,.,arange,(,ll,",",ul,",",delta,),],,,"List of gray-scale colors in HSV space as web hex triplets.

    For integer argument k, returns list of `k` gray-scale colors, increasingly 
    light, linearly in the HSV color space, as web hex triplets.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    **Parameters**

            **k** :  positive integer

                    Number of gray-scale colors to return.

    **Returns**

            **glist** :  list of strings

                    List of `k` gray-scale colors.",List,of,gray,-,scale,colors,in,HSV,space,as,web,hex,triplets,.,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L477-L502,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,addrecords,"def addrecords(X, new):
    """"""
    Append one or more records to the end of a numpy recarray or ndarray .

    Can take a single record, void or tuple, or a list of records, voids or 
    tuples.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addrecords`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The array to add records to.

            **new** :  record, void or tuple, or list of them

                    Record(s) to add to `X`.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new records.

    **See also:**  :func:`tabular.spreadsheet.rowstack`

    """"""
    if isinstance(new, np.record) or isinstance(new, np.void) or \
                                                        isinstance(new, tuple):
        new = [new]
    return np.append(X, utils.fromrecords(new, type=np.ndarray,
                                              dtype=X.dtype), axis=0)",python,"def addrecords(X, new):
    """"""
    Append one or more records to the end of a numpy recarray or ndarray .

    Can take a single record, void or tuple, or a list of records, voids or 
    tuples.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addrecords`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The array to add records to.

            **new** :  record, void or tuple, or list of them

                    Record(s) to add to `X`.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new records.

    **See also:**  :func:`tabular.spreadsheet.rowstack`

    """"""
    if isinstance(new, np.record) or isinstance(new, np.void) or \
                                                        isinstance(new, tuple):
        new = [new]
    return np.append(X, utils.fromrecords(new, type=np.ndarray,
                                              dtype=X.dtype), axis=0)",def,addrecords,(,X,",",new,),:,if,isinstance,(,new,",",np,.,record,),or,isinstance,(,new,",",np,.,void,),or,isinstance,(,new,",",tuple,),:,new,=,[,new,],return,np,.,append,"Append one or more records to the end of a numpy recarray or ndarray .

    Can take a single record, void or tuple, or a list of records, voids or 
    tuples.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addrecords`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The array to add records to.

            **new** :  record, void or tuple, or list of them

                    Record(s) to add to `X`.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new records.

    **See also:**  :func:`tabular.spreadsheet.rowstack`",Append,one,or,more,records,to,the,end,of,a,numpy,recarray,or,ndarray,.,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L704-L737,train,(,X,",",utils,.,fromrecords,(,new,",",type,=,np,.,ndarray,",",dtype,=,X,.,dtype,),",",axis,=,0,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,addcols,"def addcols(X, cols, names=None):
    """"""
    Add one or more columns to a numpy ndarray.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addcols`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The recarray to add columns to.

            **cols** :  numpy ndarray, or list of arrays of columns
            
                    Column(s) to add.

            **names**:  list of strings, optional

                    Names of the new columns. Only applicable when `cols` is a 
                    list of arrays.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new columns.

    **See also:**  :func:`tabular.spreadsheet.colstack`

    """"""

    if isinstance(names,str):
        names = [n.strip() for n in names.split(',')]

    if isinstance(cols, list):
        if any([isinstance(x,np.ndarray) or isinstance(x,list) or \
                                     isinstance(x,tuple) for x in cols]):
            assert all([len(x) == len(X) for x in cols]), \
                   'Trying to add columns of wrong length.'
            assert names != None and len(cols) == len(names), \
                   'Number of columns to add must equal number of new names.'
            cols = utils.fromarrays(cols,type=np.ndarray,names = names)
        else:
            assert len(cols) == len(X), 'Trying to add column of wrong length.'
            cols = utils.fromarrays([cols], type=np.ndarray,names=names)
    else:
        assert isinstance(cols, np.ndarray)
        if cols.dtype.names == None:
            cols = utils.fromarrays([cols],type=np.ndarray, names=names)

    Replacements = [a for a in cols.dtype.names if a in X.dtype.names]
    if len(Replacements) > 0:
        print('Replacing columns', 
              [a for a in cols.dtype.names if a in X.dtype.names])

    return utils.fromarrays(
      [X[a] if a not in cols.dtype.names else cols[a] for a in X.dtype.names] + 
      [cols[a] for a in cols.dtype.names if a not in X.dtype.names], 
      type=np.ndarray,
      names=list(X.dtype.names) + [a for a in cols.dtype.names 
                                   if a not in X.dtype.names])",python,"def addcols(X, cols, names=None):
    """"""
    Add one or more columns to a numpy ndarray.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addcols`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The recarray to add columns to.

            **cols** :  numpy ndarray, or list of arrays of columns
            
                    Column(s) to add.

            **names**:  list of strings, optional

                    Names of the new columns. Only applicable when `cols` is a 
                    list of arrays.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new columns.

    **See also:**  :func:`tabular.spreadsheet.colstack`

    """"""

    if isinstance(names,str):
        names = [n.strip() for n in names.split(',')]

    if isinstance(cols, list):
        if any([isinstance(x,np.ndarray) or isinstance(x,list) or \
                                     isinstance(x,tuple) for x in cols]):
            assert all([len(x) == len(X) for x in cols]), \
                   'Trying to add columns of wrong length.'
            assert names != None and len(cols) == len(names), \
                   'Number of columns to add must equal number of new names.'
            cols = utils.fromarrays(cols,type=np.ndarray,names = names)
        else:
            assert len(cols) == len(X), 'Trying to add column of wrong length.'
            cols = utils.fromarrays([cols], type=np.ndarray,names=names)
    else:
        assert isinstance(cols, np.ndarray)
        if cols.dtype.names == None:
            cols = utils.fromarrays([cols],type=np.ndarray, names=names)

    Replacements = [a for a in cols.dtype.names if a in X.dtype.names]
    if len(Replacements) > 0:
        print('Replacing columns', 
              [a for a in cols.dtype.names if a in X.dtype.names])

    return utils.fromarrays(
      [X[a] if a not in cols.dtype.names else cols[a] for a in X.dtype.names] + 
      [cols[a] for a in cols.dtype.names if a not in X.dtype.names], 
      type=np.ndarray,
      names=list(X.dtype.names) + [a for a in cols.dtype.names 
                                   if a not in X.dtype.names])",def,addcols,(,X,",",cols,",",names,=,None,),:,if,isinstance,(,names,",",str,),:,names,=,[,n,.,strip,(,),for,n,in,names,.,split,(,"','",),],if,isinstance,(,cols,",","Add one or more columns to a numpy ndarray.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addcols`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The recarray to add columns to.

            **cols** :  numpy ndarray, or list of arrays of columns
            
                    Column(s) to add.

            **names**:  list of strings, optional

                    Names of the new columns. Only applicable when `cols` is a 
                    list of arrays.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new columns.

    **See also:**  :func:`tabular.spreadsheet.colstack`",Add,one,or,more,columns,to,a,numpy,ndarray,.,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L740-L803,train,list,),:,if,any,(,[,isinstance,(,x,",",np,.,ndarray,),or,isinstance,(,x,",",list,),or,isinstance,(,x,",",tuple,),for,,,,,,,,,,,,,,,,,,,,x,in,cols,],),:,assert,all,(,[,len,(,x,),==,len,(,X,),for,x,in,cols,],),",",'Trying to add columns of wrong length.',assert,names,!=,None,and,len,(,cols,),==,len,(,names,),",",'Number of columns to add must equal number of new names.',cols,=,utils,.,fromarrays,(,cols,",",type,=,np,.,ndarray,",",names,=,names,),else,:,assert,len,(,cols,),==,len,(,X,),",",'Trying to add column of wrong length.',cols,=,utils,.,fromarrays,(,[,cols,],",",type,=,np,.,ndarray,",",names,=,names,),else,:,assert,isinstance,(,cols,",",np,.,ndarray,),if,cols,.,dtype,.,names,==,None,:,cols,=,utils,.,fromarrays,(,[,cols,],",",type,=,np,.,ndarray,",",names,=,names,),Replacements,=,[,a,for,a,in,cols,.,dtype,.,names,if,a,in,X,.,dtype,.,names,],if,len,(,Replacements,),>,0,:,print,(,'Replacing columns',",",[,a,for,a,in,cols,.,dtype,.,names,if,a,in,X,.,dtype,.,names,],),return,utils,.,fromarrays,(,[,X,[,a,],if,a,not,in,cols,.,dtype,.,names,else,cols,[,a,],for,a,in,X,.,dtype,.,names,],+,[,cols,[,a,],for,a,in,cols,.,dtype,.,names,if,a,not,in,X,.,dtype,.,names,],",",type,=,np,.,ndarray,",",names,=,list,(,X,.,dtype,.,names,),+,[,a,for,a,in,cols,.,dtype,.,names,if,a,not,in,X,.,dtype,.,names,],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,deletecols,"def deletecols(X, cols):
    """"""
    Delete columns from a numpy ndarry or recarray.

    Can take a string giving a column name or comma-separated list of column 
    names, or a list of string column names.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.deletecols`.

    **Parameters**

            **X** :  numpy recarray or ndarray with structured dtype

                    The numpy array from which to delete columns.

            **cols** :  string or list of strings

                    Name or list of names of columns in `X`.  This can be
                    a string giving a column name or comma-separated list of 
                    column names, or a list of string column names.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy ndarray with structured dtype
                    given by `X`, excluding the columns named in `cols`.

    """"""
    if isinstance(cols, str):
        cols = cols.split(',')
    retain = [n for n in X.dtype.names if n not in cols]
    if len(retain) > 0:
        return X[retain]
    else:
        return None",python,"def deletecols(X, cols):
    """"""
    Delete columns from a numpy ndarry or recarray.

    Can take a string giving a column name or comma-separated list of column 
    names, or a list of string column names.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.deletecols`.

    **Parameters**

            **X** :  numpy recarray or ndarray with structured dtype

                    The numpy array from which to delete columns.

            **cols** :  string or list of strings

                    Name or list of names of columns in `X`.  This can be
                    a string giving a column name or comma-separated list of 
                    column names, or a list of string column names.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy ndarray with structured dtype
                    given by `X`, excluding the columns named in `cols`.

    """"""
    if isinstance(cols, str):
        cols = cols.split(',')
    retain = [n for n in X.dtype.names if n not in cols]
    if len(retain) > 0:
        return X[retain]
    else:
        return None",def,deletecols,(,X,",",cols,),:,if,isinstance,(,cols,",",str,),:,cols,=,cols,.,split,(,"','",),retain,=,[,n,for,n,in,X,.,dtype,.,names,if,n,not,in,cols,],if,"Delete columns from a numpy ndarry or recarray.

    Can take a string giving a column name or comma-separated list of column 
    names, or a list of string column names.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.deletecols`.

    **Parameters**

            **X** :  numpy recarray or ndarray with structured dtype

                    The numpy array from which to delete columns.

            **cols** :  string or list of strings

                    Name or list of names of columns in `X`.  This can be
                    a string giving a column name or comma-separated list of 
                    column names, or a list of string column names.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy ndarray with structured dtype
                    given by `X`, excluding the columns named in `cols`.",Delete,columns,from,a,numpy,ndarry,or,recarray,.,,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L806-L842,train,len,(,retain,),>,0,:,return,X,[,retain,],else,:,return,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,renamecol,"def renamecol(X, old, new):
    """"""
    Rename column of a numpy ndarray with structured dtype, in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.renamecol`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    The numpy array for which a column is to be renamed.

            **old** :  string

                    Old column name, e.g. a name in `X.dtype.names`.

            **new** :  string

                    New column name to replace `old`.

    """"""
    NewNames = tuple([n if n != old else new for n in X.dtype.names])
    X.dtype.names = NewNames",python,"def renamecol(X, old, new):
    """"""
    Rename column of a numpy ndarray with structured dtype, in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.renamecol`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    The numpy array for which a column is to be renamed.

            **old** :  string

                    Old column name, e.g. a name in `X.dtype.names`.

            **new** :  string

                    New column name to replace `old`.

    """"""
    NewNames = tuple([n if n != old else new for n in X.dtype.names])
    X.dtype.names = NewNames",def,renamecol,(,X,",",old,",",new,),:,NewNames,=,tuple,(,[,n,if,n,!=,old,else,new,for,n,in,X,.,dtype,.,names,],),X,.,dtype,.,names,=,NewNames,,,,,"Rename column of a numpy ndarray with structured dtype, in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.renamecol`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    The numpy array for which a column is to be renamed.

            **old** :  string

                    Old column name, e.g. a name in `X.dtype.names`.

            **new** :  string

                    New column name to replace `old`.",Rename,column,of,a,numpy,ndarray,with,structured,dtype,in,-,place,.,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L845-L868,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,replace,"def replace(X, old, new, strict=True, cols=None, rows=None):
    """"""
    Replace value `old` with `new` everywhere it appears in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.replace`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    Numpy array for which in-place replacement of `old` with 
                    `new` is to be done.

            **old** : string

            **new** : string

            **strict** :  boolean, optional

            *   If `strict` = `True`, replace only exact occurences of `old`.

            *   If `strict` = `False`, assume `old` and `new` are strings and   
                replace all occurences of substrings (e.g. like 
                :func:`str.replace`)

            **cols** :  list of strings, optional

                    Names of columns to make replacements in; if `None`, make 
                    replacements everywhere.

            **rows** : list of booleans or integers, optional

                    Rows to make replacements in; if `None`, make replacements 
                    everywhere.

    Note:  This function does in-place replacements.  Thus there are issues 
    handling data types here when replacement dtype is larger than original 
    dtype.  This can be resolved later by making a new array when necessary ...

    """"""

    if cols == None:
        cols = X.dtype.names
    elif isinstance(cols, str):
        cols = cols.split(',')

    if rows == None:
        rows = np.ones((len(X),), bool)

    if strict:
        new = np.array(new)
        for a in cols:
            if X.dtype[a] < new.dtype:
                print('WARNING: dtype of column', a, 
                      'is inferior to dtype of ', new, 
                      'which may cause problems.')
            try:
                X[a][(X[a] == old)[rows]] = new
            except:
                print('Replacement not made on column', a, '.')
    else:
        for a in cols:
            QuickRep = True
            try:
                colstr = ''.join(X[a][rows])
            except TypeError:
                print('Not replacing in column', a, 'due to type mismatch.')
            else:
                avoid = [ord(o) for o in utils.uniqify(old + new + colstr)]
                ok = set(range(256)).difference(avoid)
                if len(ok) > 0:
                    sep = chr(list(ok)[0])
                else:
                    ok = set(range(65536)).difference(avoid)
                    if len(ok) > 0:
                        sep = unichr(list(ok)[0])
                    else:
                        print('All unicode characters represented in column', 
                              a, ', can\t replace quickly.')
                        QuickRep = False

                if QuickRep:
                    newrows = np.array(sep.join(X[a][rows])
                                       .replace(old, new).split(sep))
                else:
                    newrows = np.array([aa.replace(old,new) for aa in 
                                        X[a][rows]])
                X[a][rows] = np.cast[X.dtype[a]](newrows)

                if newrows.dtype > X.dtype[a]:
                    print('WARNING: dtype of column', a, 'is inferior to the ' 
                          'dtype of its replacement which may cause problems '
                          '(ends of strings might get chopped off).')",python,"def replace(X, old, new, strict=True, cols=None, rows=None):
    """"""
    Replace value `old` with `new` everywhere it appears in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.replace`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    Numpy array for which in-place replacement of `old` with 
                    `new` is to be done.

            **old** : string

            **new** : string

            **strict** :  boolean, optional

            *   If `strict` = `True`, replace only exact occurences of `old`.

            *   If `strict` = `False`, assume `old` and `new` are strings and   
                replace all occurences of substrings (e.g. like 
                :func:`str.replace`)

            **cols** :  list of strings, optional

                    Names of columns to make replacements in; if `None`, make 
                    replacements everywhere.

            **rows** : list of booleans or integers, optional

                    Rows to make replacements in; if `None`, make replacements 
                    everywhere.

    Note:  This function does in-place replacements.  Thus there are issues 
    handling data types here when replacement dtype is larger than original 
    dtype.  This can be resolved later by making a new array when necessary ...

    """"""

    if cols == None:
        cols = X.dtype.names
    elif isinstance(cols, str):
        cols = cols.split(',')

    if rows == None:
        rows = np.ones((len(X),), bool)

    if strict:
        new = np.array(new)
        for a in cols:
            if X.dtype[a] < new.dtype:
                print('WARNING: dtype of column', a, 
                      'is inferior to dtype of ', new, 
                      'which may cause problems.')
            try:
                X[a][(X[a] == old)[rows]] = new
            except:
                print('Replacement not made on column', a, '.')
    else:
        for a in cols:
            QuickRep = True
            try:
                colstr = ''.join(X[a][rows])
            except TypeError:
                print('Not replacing in column', a, 'due to type mismatch.')
            else:
                avoid = [ord(o) for o in utils.uniqify(old + new + colstr)]
                ok = set(range(256)).difference(avoid)
                if len(ok) > 0:
                    sep = chr(list(ok)[0])
                else:
                    ok = set(range(65536)).difference(avoid)
                    if len(ok) > 0:
                        sep = unichr(list(ok)[0])
                    else:
                        print('All unicode characters represented in column', 
                              a, ', can\t replace quickly.')
                        QuickRep = False

                if QuickRep:
                    newrows = np.array(sep.join(X[a][rows])
                                       .replace(old, new).split(sep))
                else:
                    newrows = np.array([aa.replace(old,new) for aa in 
                                        X[a][rows]])
                X[a][rows] = np.cast[X.dtype[a]](newrows)

                if newrows.dtype > X.dtype[a]:
                    print('WARNING: dtype of column', a, 'is inferior to the ' 
                          'dtype of its replacement which may cause problems '
                          '(ends of strings might get chopped off).')",def,replace,(,X,",",old,",",new,",",strict,=,True,",",cols,=,None,",",rows,=,None,),:,if,cols,==,None,:,cols,=,X,.,dtype,.,names,elif,isinstance,(,cols,",",str,),:,cols,"Replace value `old` with `new` everywhere it appears in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.replace`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    Numpy array for which in-place replacement of `old` with 
                    `new` is to be done.

            **old** : string

            **new** : string

            **strict** :  boolean, optional

            *   If `strict` = `True`, replace only exact occurences of `old`.

            *   If `strict` = `False`, assume `old` and `new` are strings and   
                replace all occurences of substrings (e.g. like 
                :func:`str.replace`)

            **cols** :  list of strings, optional

                    Names of columns to make replacements in; if `None`, make 
                    replacements everywhere.

            **rows** : list of booleans or integers, optional

                    Rows to make replacements in; if `None`, make replacements 
                    everywhere.

    Note:  This function does in-place replacements.  Thus there are issues 
    handling data types here when replacement dtype is larger than original 
    dtype.  This can be resolved later by making a new array when necessary ...",Replace,value,old,with,new,everywhere,it,appears,in,-,place,.,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L871-L964,train,=,cols,.,split,(,"','",),if,rows,==,None,:,rows,=,np,.,ones,(,(,len,(,X,),",",),",",bool,),if,strict,,,,,,,,,,,,,,,,,,,,:,new,=,np,.,array,(,new,),for,a,in,cols,:,if,X,.,dtype,[,a,],<,new,.,dtype,:,print,(,'WARNING: dtype of column',",",a,",",'is inferior to dtype of ',",",new,",",'which may cause problems.',),try,:,X,[,a,],[,(,X,[,a,],==,old,),[,rows,],],=,new,except,:,print,(,'Replacement not made on column',",",a,",",'.',),else,:,for,a,in,cols,:,QuickRep,=,True,try,:,colstr,=,'',.,join,(,X,[,a,],[,rows,],),except,TypeError,:,print,(,'Not replacing in column',",",a,",",'due to type mismatch.',),else,:,avoid,=,[,ord,(,o,),for,o,in,utils,.,uniqify,(,old,+,new,+,colstr,),],ok,=,set,(,range,(,256,),),.,difference,(,avoid,),if,len,(,ok,),>,0,:,sep,=,chr,(,list,(,ok,),[,0,],),else,:,ok,=,set,(,range,(,65536,),),.,difference,(,avoid,),if,len,(,ok,),>,0,:,sep,=,unichr,(,list,(,ok,),[,0,],),else,:,print,(,'All unicode characters represented in column',",",a,",","', can\t replace quickly.'",),QuickRep,=,False,if,QuickRep,:,newrows,=,np,.,array,(,sep,.,join,(,X,[,a,],[,rows,],),.,replace,(,old,",",new,),.,split,(,sep,),),else,:,newrows,=,np,.,array,(,[,aa,.,replace,(,old,",",new,),for,aa,in,X,[,a,],[,rows,],],),X,[,a,],[,rows,],=,np,.,cast,[,X,.,dtype,[,a,],],(,newrows,),if,newrows,.,dtype,>,X,.,dtype,[,a,],:,print,(,'WARNING: dtype of column',",",a,",",'is inferior to the ','dtype of its replacement which may cause problems ','(ends of strings might get chopped off).',),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,rowstack,"def rowstack(seq, mode='nulls', nullvals=None):
    '''
    Vertically stack a sequence of numpy ndarrays with structured dtype

    Analog of numpy.vstack

    Implemented by the tabarray method
    :func:`tabular.tab.tabarray.rowstack` which uses 
    :func:`tabular.tabarray.tab_rowstack`.

    **Parameters**

            **seq** :  sequence of numpy recarrays

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['nulls', 'commons', 'abort']

                    Denotes how to proceed if the recarrays have different
                    dtypes, e.g. different sets of named columns.

                    *   if `mode` == ``nulls``, the resulting set of columns is
                        determined by the union of the dtypes of all recarrays
                        to be stacked, and missing data is filled with null 
                        values as defined by 
                        :func:`tabular.spreadsheet.nullvalue`; this is the 
                        default mode.

                    *   elif `mode` == ``commons``, the resulting set of 
                        columns is determined by the intersection of the dtypes 
                        of all recarrays to be stacked, e.g. common columns.

                    *   elif `mode` == ``abort``, raise an error when the
                        recarrays to stack have different dtypes.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of vertically stacking the arrays in `seq`.

    **See also:**  `numpy.vstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html>`_.

    '''

    if nullvals == None:
        nullvals = utils.DEFAULT_NULLVALUEFORMAT
    #newseq = [ss for ss in seq if len(ss) > 0]
    if len(seq) > 1:
        assert mode in ['commons','nulls','abort'], \
             ('""mode"" argument must either by ""commons"", ""abort"", or ""nulls"".')
        if mode == 'abort':
            if not all([set(l.dtype.names) == set(seq[0].dtype.names) 
                        for l in seq]):
                raise ValueError('Some column names are different.')
            else:
                mode = 'commons'
        if mode == 'nulls':
            names =  utils.uniqify(utils.listunion([list(s.dtype.names) 
                                       for s in seq if s.dtype.names != None]))
            formats = [max([s.dtype[att] for s in seq if s.dtype.names != None 
                       and att in s.dtype.names]).str for att in names]         
            dtype = np.dtype(zip(names,formats))
            return utils.fromarrays([utils.listunion([s[att].tolist() 
                        if (s.dtype.names != None and att in s.dtype.names) 
                        else [nullvals(format)] * len(s) for s in seq]) 
                        for (att, format) in zip(names, formats)], type=np.ndarray,
                        dtype=dtype)
        elif mode == 'commons':
            names = [x for x in seq[0].dtype.names 
                     if all([x in l.dtype.names for l in seq[1:]])]
            formats = [max([a.dtype[att] for a in seq]).str for att in names]
            return utils.fromrecords(utils.listunion(
                    [ar.tolist() for ar in seq]), type=np.ndarray,
                          names=names, formats=formats)
    else:
        return seq[0]",python,"def rowstack(seq, mode='nulls', nullvals=None):
    '''
    Vertically stack a sequence of numpy ndarrays with structured dtype

    Analog of numpy.vstack

    Implemented by the tabarray method
    :func:`tabular.tab.tabarray.rowstack` which uses 
    :func:`tabular.tabarray.tab_rowstack`.

    **Parameters**

            **seq** :  sequence of numpy recarrays

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['nulls', 'commons', 'abort']

                    Denotes how to proceed if the recarrays have different
                    dtypes, e.g. different sets of named columns.

                    *   if `mode` == ``nulls``, the resulting set of columns is
                        determined by the union of the dtypes of all recarrays
                        to be stacked, and missing data is filled with null 
                        values as defined by 
                        :func:`tabular.spreadsheet.nullvalue`; this is the 
                        default mode.

                    *   elif `mode` == ``commons``, the resulting set of 
                        columns is determined by the intersection of the dtypes 
                        of all recarrays to be stacked, e.g. common columns.

                    *   elif `mode` == ``abort``, raise an error when the
                        recarrays to stack have different dtypes.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of vertically stacking the arrays in `seq`.

    **See also:**  `numpy.vstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html>`_.

    '''

    if nullvals == None:
        nullvals = utils.DEFAULT_NULLVALUEFORMAT
    #newseq = [ss for ss in seq if len(ss) > 0]
    if len(seq) > 1:
        assert mode in ['commons','nulls','abort'], \
             ('""mode"" argument must either by ""commons"", ""abort"", or ""nulls"".')
        if mode == 'abort':
            if not all([set(l.dtype.names) == set(seq[0].dtype.names) 
                        for l in seq]):
                raise ValueError('Some column names are different.')
            else:
                mode = 'commons'
        if mode == 'nulls':
            names =  utils.uniqify(utils.listunion([list(s.dtype.names) 
                                       for s in seq if s.dtype.names != None]))
            formats = [max([s.dtype[att] for s in seq if s.dtype.names != None 
                       and att in s.dtype.names]).str for att in names]         
            dtype = np.dtype(zip(names,formats))
            return utils.fromarrays([utils.listunion([s[att].tolist() 
                        if (s.dtype.names != None and att in s.dtype.names) 
                        else [nullvals(format)] * len(s) for s in seq]) 
                        for (att, format) in zip(names, formats)], type=np.ndarray,
                        dtype=dtype)
        elif mode == 'commons':
            names = [x for x in seq[0].dtype.names 
                     if all([x in l.dtype.names for l in seq[1:]])]
            formats = [max([a.dtype[att] for a in seq]).str for att in names]
            return utils.fromrecords(utils.listunion(
                    [ar.tolist() for ar in seq]), type=np.ndarray,
                          names=names, formats=formats)
    else:
        return seq[0]",def,rowstack,(,seq,",",mode,=,'nulls',",",nullvals,=,None,),:,if,nullvals,==,None,:,nullvals,=,utils,.,DEFAULT_NULLVALUEFORMAT,#newseq = [ss for ss in seq if len(ss) > 0],if,len,(,seq,),>,1,:,assert,mode,in,[,'commons',",",'nulls',",",'abort',],"Vertically stack a sequence of numpy ndarrays with structured dtype

    Analog of numpy.vstack

    Implemented by the tabarray method
    :func:`tabular.tab.tabarray.rowstack` which uses 
    :func:`tabular.tabarray.tab_rowstack`.

    **Parameters**

            **seq** :  sequence of numpy recarrays

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['nulls', 'commons', 'abort']

                    Denotes how to proceed if the recarrays have different
                    dtypes, e.g. different sets of named columns.

                    *   if `mode` == ``nulls``, the resulting set of columns is
                        determined by the union of the dtypes of all recarrays
                        to be stacked, and missing data is filled with null 
                        values as defined by 
                        :func:`tabular.spreadsheet.nullvalue`; this is the 
                        default mode.

                    *   elif `mode` == ``commons``, the resulting set of 
                        columns is determined by the intersection of the dtypes 
                        of all recarrays to be stacked, e.g. common columns.

                    *   elif `mode` == ``abort``, raise an error when the
                        recarrays to stack have different dtypes.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of vertically stacking the arrays in `seq`.

    **See also:**  `numpy.vstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html>`_.",Vertically,stack,a,sequence,of,numpy,ndarrays,with,structured,dtype,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L967-L1044,train,",",(,"'""mode"" argument must either by ""commons"", ""abort"", or ""nulls"".'",),if,mode,==,'abort',:,if,not,all,(,[,set,(,l,.,dtype,.,names,),==,set,(,seq,[,0,],.,,,,,,,,,,,,,,,,,,,,dtype,.,names,),for,l,in,seq,],),:,raise,ValueError,(,'Some column names are different.',),else,:,mode,=,'commons',if,mode,==,'nulls',:,names,=,utils,.,uniqify,(,utils,.,listunion,(,[,list,(,s,.,dtype,.,names,),for,s,in,seq,if,s,.,dtype,.,names,!=,None,],),),formats,=,[,max,(,[,s,.,dtype,[,att,],for,s,in,seq,if,s,.,dtype,.,names,!=,None,and,att,in,s,.,dtype,.,names,],),.,str,for,att,in,names,],dtype,=,np,.,dtype,(,zip,(,names,",",formats,),),return,utils,.,fromarrays,(,[,utils,.,listunion,(,[,s,[,att,],.,tolist,(,),if,(,s,.,dtype,.,names,!=,None,and,att,in,s,.,dtype,.,names,),else,[,nullvals,(,format,),],*,len,(,s,),for,s,in,seq,],),for,(,att,",",format,),in,zip,(,names,",",formats,),],",",type,=,np,.,ndarray,",",dtype,=,dtype,),elif,mode,==,'commons',:,names,=,[,x,for,x,in,seq,[,0,],.,dtype,.,names,if,all,(,[,x,in,l,.,dtype,.,names,for,l,in,seq,[,1,:,],],),],formats,=,[,max,(,[,a,.,dtype,[,att,],for,a,in,seq,],),.,str,for,att,in,names,],return,utils,.,fromrecords,(,utils,.,listunion,(,[,ar,.,tolist,(,),for,ar,in,seq,],),",",type,=,np,.,ndarray,",",names,=,names,",",formats,=,formats,),else,:,return,seq,[,0,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,colstack,"def colstack(seq, mode='abort',returnnaming=False):
    """"""
    Horizontally stack a sequence of numpy ndarrays with structured dtypes

    Analog of numpy.hstack for recarrays.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.colstack` which uses 
    :func:`tabular.tabarray.tab_colstack`.

    **Parameters**

            **seq** :  sequence of numpy ndarray with structured dtype

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['first','drop','abort','rename']

                    Denotes how to proceed if when multiple recarrays share the 
                    same column name:

                    *   if `mode` == ``first``, take the column from the first
                        recarray in `seq` containing the shared column name.

                    *   elif `mode` == ``abort``, raise an error when the 
                        recarrays to stack share column names; this is the
                        default mode.

                    *   elif `mode` == ``drop``, drop any column that shares    
                        its name with any other column among the sequence of 
                        recarrays.

                    *   elif `mode` == ``rename``, for any set of all columns
                        sharing the same name, rename all columns by appending 
                        an underscore, '_', followed by an integer, starting 
                        with '0' and incrementing by 1 for each subsequent 
                        column.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of horizontally stacking the arrays in `seq`.

    **See also:**  `numpy.hstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html>`_.

    """"""
    assert mode in ['first','drop','abort','rename'], \
       'mode argument must take on value ""first"",""drop"", ""rename"", or ""abort"".'

    AllNames = utils.uniqify(utils.listunion(
                                           [list(l.dtype.names) for l in seq]))
    NameList = [(x, [i for i in range(len(seq)) if x in seq[i].dtype.names]) 
                     for x in AllNames]
    Commons = [x[0] for x in NameList if len(x[1]) > 1]

    if len(Commons) > 0 or mode == 'first':
        if mode == 'abort':
            raise ValueError('There are common column names with differing ' +              
                             'values in the columns')
        elif mode == 'drop':
            Names = [(L[0], x,x) for (x, L) in NameList if x not in Commons]
        elif mode == 'rename':
            NameDict = dict(NameList)
            Names = utils.listunion([[(i,n,n) if len(NameDict[n]) == 1 else \
               (i,n,n + '_' + str(i)) for n in s.dtype.names] \
                                   for (i,s) in enumerate(seq)])                           
    else:
        Names = [(L[0], x,x) for (x, L) in NameList]
    
    if returnnaming:
        return  utils.fromarrays([seq[i][x] for (i, x,y) in Names], 
                 type= np.ndarray,names=zip(*Names)[2]),Names
    else:
        return utils.fromarrays([seq[i][x] for (i, x,y) in Names], 
                 type= np.ndarray,names=zip(*Names)[2])",python,"def colstack(seq, mode='abort',returnnaming=False):
    """"""
    Horizontally stack a sequence of numpy ndarrays with structured dtypes

    Analog of numpy.hstack for recarrays.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.colstack` which uses 
    :func:`tabular.tabarray.tab_colstack`.

    **Parameters**

            **seq** :  sequence of numpy ndarray with structured dtype

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['first','drop','abort','rename']

                    Denotes how to proceed if when multiple recarrays share the 
                    same column name:

                    *   if `mode` == ``first``, take the column from the first
                        recarray in `seq` containing the shared column name.

                    *   elif `mode` == ``abort``, raise an error when the 
                        recarrays to stack share column names; this is the
                        default mode.

                    *   elif `mode` == ``drop``, drop any column that shares    
                        its name with any other column among the sequence of 
                        recarrays.

                    *   elif `mode` == ``rename``, for any set of all columns
                        sharing the same name, rename all columns by appending 
                        an underscore, '_', followed by an integer, starting 
                        with '0' and incrementing by 1 for each subsequent 
                        column.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of horizontally stacking the arrays in `seq`.

    **See also:**  `numpy.hstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html>`_.

    """"""
    assert mode in ['first','drop','abort','rename'], \
       'mode argument must take on value ""first"",""drop"", ""rename"", or ""abort"".'

    AllNames = utils.uniqify(utils.listunion(
                                           [list(l.dtype.names) for l in seq]))
    NameList = [(x, [i for i in range(len(seq)) if x in seq[i].dtype.names]) 
                     for x in AllNames]
    Commons = [x[0] for x in NameList if len(x[1]) > 1]

    if len(Commons) > 0 or mode == 'first':
        if mode == 'abort':
            raise ValueError('There are common column names with differing ' +              
                             'values in the columns')
        elif mode == 'drop':
            Names = [(L[0], x,x) for (x, L) in NameList if x not in Commons]
        elif mode == 'rename':
            NameDict = dict(NameList)
            Names = utils.listunion([[(i,n,n) if len(NameDict[n]) == 1 else \
               (i,n,n + '_' + str(i)) for n in s.dtype.names] \
                                   for (i,s) in enumerate(seq)])                           
    else:
        Names = [(L[0], x,x) for (x, L) in NameList]
    
    if returnnaming:
        return  utils.fromarrays([seq[i][x] for (i, x,y) in Names], 
                 type= np.ndarray,names=zip(*Names)[2]),Names
    else:
        return utils.fromarrays([seq[i][x] for (i, x,y) in Names], 
                 type= np.ndarray,names=zip(*Names)[2])",def,colstack,(,seq,",",mode,=,'abort',",",returnnaming,=,False,),:,assert,mode,in,[,'first',",",'drop',",",'abort',",",'rename',],",","'mode argument must take on value ""first"",""drop"", ""rename"", or ""abort"".'",AllNames,=,utils,.,uniqify,(,utils,.,listunion,(,[,list,(,l,.,"Horizontally stack a sequence of numpy ndarrays with structured dtypes

    Analog of numpy.hstack for recarrays.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.colstack` which uses 
    :func:`tabular.tabarray.tab_colstack`.

    **Parameters**

            **seq** :  sequence of numpy ndarray with structured dtype

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['first','drop','abort','rename']

                    Denotes how to proceed if when multiple recarrays share the 
                    same column name:

                    *   if `mode` == ``first``, take the column from the first
                        recarray in `seq` containing the shared column name.

                    *   elif `mode` == ``abort``, raise an error when the 
                        recarrays to stack share column names; this is the
                        default mode.

                    *   elif `mode` == ``drop``, drop any column that shares    
                        its name with any other column among the sequence of 
                        recarrays.

                    *   elif `mode` == ``rename``, for any set of all columns
                        sharing the same name, rename all columns by appending 
                        an underscore, '_', followed by an integer, starting 
                        with '0' and incrementing by 1 for each subsequent 
                        column.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of horizontally stacking the arrays in `seq`.

    **See also:**  `numpy.hstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html>`_.",Horizontally,stack,a,sequence,of,numpy,ndarrays,with,structured,dtypes,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L1047-L1123,train,dtype,.,names,),for,l,in,seq,],),),NameList,=,[,(,x,",",[,i,for,i,in,range,(,len,(,seq,),),if,,,,,,,,,,,,,,,,,,,,x,in,seq,[,i,],.,dtype,.,names,],),for,x,in,AllNames,],Commons,=,[,x,[,0,],for,x,in,NameList,if,len,(,x,[,1,],),>,1,],if,len,(,Commons,),>,0,or,mode,==,'first',:,if,mode,==,'abort',:,raise,ValueError,(,'There are common column names with differing ',+,'values in the columns',),elif,mode,==,'drop',:,Names,=,[,(,L,[,0,],",",x,",",x,),for,(,x,",",L,),in,NameList,if,x,not,in,Commons,],elif,mode,==,'rename',:,NameDict,=,dict,(,NameList,),Names,=,utils,.,listunion,(,[,[,(,i,",",n,",",n,),if,len,(,NameDict,[,n,],),==,1,else,(,i,",",n,",",n,+,'_',+,str,(,i,),),for,n,in,s,.,dtype,.,names,],for,(,i,",",s,),in,enumerate,(,seq,),],),else,:,Names,=,[,(,L,[,0,],",",x,",",x,),for,(,x,",",L,),in,NameList,],if,returnnaming,:,return,utils,.,fromarrays,(,[,seq,[,i,],[,x,],for,(,i,",",x,",",y,),in,Names,],",",type,=,np,.,ndarray,",",names,=,zip,(,*,Names,),[,2,],),",",Names,else,:,return,utils,.,fromarrays,(,[,seq,[,i,],[,x,],for,(,i,",",x,",",y,),in,Names,],",",type,=,np,.,ndarray,",",names,=,zip,(,*,Names,),[,2,],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/spreadsheet.py,DEFAULT_RENAMER,"def DEFAULT_RENAMER(L, Names=None):
    """"""
    Renames overlapping column names of numpy ndarrays with structured dtypes

    Rename the columns by using a simple convention:

    *   If `L` is a list, it will append the number in the list to the key 
        associated with the array.

    *   If `L` is a dictionary, the algorithm will append the string 
        representation of the key associated with an array to the overlapping 
        columns from that array.

    Default renamer function used by :func:`tabular.spreadsheet.join`

    **Parameters**

            **L** :  list or dictionary

                    Numpy recarrays with columns to be renamed.

    **Returns**

            **D** :  dictionary of dictionaries

                    Dictionary mapping each input numpy recarray to a 
                    dictionary mapping each original column name to its new 
                    name following the convention above.

    """"""
    if isinstance(L,dict):
        Names = L.keys()
        LL = L.values()
    else:
        if Names == None:
            Names = range(len(L))
        else:
            assert len(Names) == len(L)
        LL = L

    commons = Commons([l.dtype.names for l in LL])

    D = {}
    for (i,l) in zip(Names, LL):
        d = {}
        for c in commons:
            if c in l.dtype.names:
                d[c] = c + '_' + str(i)
        if d:
            D[i] = d

    return D",python,"def DEFAULT_RENAMER(L, Names=None):
    """"""
    Renames overlapping column names of numpy ndarrays with structured dtypes

    Rename the columns by using a simple convention:

    *   If `L` is a list, it will append the number in the list to the key 
        associated with the array.

    *   If `L` is a dictionary, the algorithm will append the string 
        representation of the key associated with an array to the overlapping 
        columns from that array.

    Default renamer function used by :func:`tabular.spreadsheet.join`

    **Parameters**

            **L** :  list or dictionary

                    Numpy recarrays with columns to be renamed.

    **Returns**

            **D** :  dictionary of dictionaries

                    Dictionary mapping each input numpy recarray to a 
                    dictionary mapping each original column name to its new 
                    name following the convention above.

    """"""
    if isinstance(L,dict):
        Names = L.keys()
        LL = L.values()
    else:
        if Names == None:
            Names = range(len(L))
        else:
            assert len(Names) == len(L)
        LL = L

    commons = Commons([l.dtype.names for l in LL])

    D = {}
    for (i,l) in zip(Names, LL):
        d = {}
        for c in commons:
            if c in l.dtype.names:
                d[c] = c + '_' + str(i)
        if d:
            D[i] = d

    return D",def,DEFAULT_RENAMER,(,L,",",Names,=,None,),:,if,isinstance,(,L,",",dict,),:,Names,=,L,.,keys,(,),LL,=,L,.,values,(,),else,:,if,Names,==,None,:,Names,=,range,(,"Renames overlapping column names of numpy ndarrays with structured dtypes

    Rename the columns by using a simple convention:

    *   If `L` is a list, it will append the number in the list to the key 
        associated with the array.

    *   If `L` is a dictionary, the algorithm will append the string 
        representation of the key associated with an array to the overlapping 
        columns from that array.

    Default renamer function used by :func:`tabular.spreadsheet.join`

    **Parameters**

            **L** :  list or dictionary

                    Numpy recarrays with columns to be renamed.

    **Returns**

            **D** :  dictionary of dictionaries

                    Dictionary mapping each input numpy recarray to a 
                    dictionary mapping each original column name to its new 
                    name following the convention above.",Renames,overlapping,column,names,of,numpy,ndarrays,with,structured,dtypes,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/spreadsheet.py#L1520-L1571,train,len,(,L,),),else,:,assert,len,(,Names,),==,len,(,L,),LL,=,L,commons,=,Commons,(,[,l,.,dtype,.,names,,,,,,,,,,,,,,,,,,,,for,l,in,LL,],),D,=,{,},for,(,i,",",l,),in,zip,(,Names,",",LL,),:,d,=,{,},for,c,in,commons,:,if,c,in,l,.,dtype,.,names,:,d,[,c,],=,c,+,'_',+,str,(,i,),if,d,:,D,[,i,],=,d,return,D,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
emirozer/bowshock,bowshock/helioviewer.py,getjp2image,"def getjp2image(date,
                sourceId=None,
                observatory=None,
                instrument=None,
                detector=None,
                measurement=None):
    '''
    Helioviewer.org and JHelioviewer operate off of JPEG2000 formatted image data generated from science-quality FITS files. Use the APIs below to interact directly with these intermediary JPEG2000 files.
    
    Download a JP2 image for the specified datasource that is the closest match in time to the `date` requested.

    Either `sourceId` must be specified, or the combination of `observatory`, `instrument`, `detector`, and `measurement`.

    Request Parameters:

    Parameter	Required	Type	Example	Description
    date	Required	string	2014-01-01T23:59:59Z	Desired date/time of the JP2 image. ISO 8601 combined UTC date and time UTC format.
    sourceId	Optional	number	14	Unique image datasource identifier.
    observatory	Optional	string	SDO	Observatory name.
    instrument	Optional	string	AIA	Instrument name.
    detector	Optional	string	AIA	Detector name.
    measurement	Optional	string	335	Measurement name.
    jpip	Optional	boolean	false	Optionally return a JPIP URI instead of the binary data of the image itself.
    json	Optional	boolean	false	Optionally return a JSON object.


    EXAMPLE: http://helioviewer.org/api/v1/getJP2Image/?date=2014-01-01T23:59:59Z&sourceId=14&jpip=true
    '''

    base_url = 'http://helioviewer.org/api/v1/getJP2Image/?'
    req_url = ''

    try:
        validate_iso8601(date)
        if not date[-1:] == 'Z':
            date += 'Z'
        base_url += 'date=' + date
    except:
        raise ValueError(
            ""Your date input is not in iso8601 format. ex: 2014-01-01T23:59:59"")

    if sourceId:
        if not isinstance(sourceId, int):
            logger.error(""The sourceId argument should be an int, ignoring it"")
        else:
            base_url += ""sourceId="" + str(sourceId) + ""&""

    if observatory:
        if not isinstance(observatory, str):
            logger.error(
                ""The observatory argument should be a str, ignoring it"")
        else:
            base_url += ""observatory="" + observatory + ""&""

    if instrument:
        if not isinstance(instrument, str):
            logger.error(
                ""The instrument argument should be a str, ignoring it"")
        else:
            base_url += ""instrument="" + instrument + ""&""
    if detector:
        if not isinstance(detector, str):
            logger.error(""The detector argument should be a str, ignoring it"")
        else:
            base_url += ""detector="" + detector + ""&""

    if measurement:
        if not isinstance(measurement, str):
            logger.error(
                ""The measurement argument should be a str, ignoring it"")
        else:
            base_url += ""measurement="" + detector + ""&""

    req_url += base_url + ""jpip=true""

    return dispatch_http_get(req_url)",python,"def getjp2image(date,
                sourceId=None,
                observatory=None,
                instrument=None,
                detector=None,
                measurement=None):
    '''
    Helioviewer.org and JHelioviewer operate off of JPEG2000 formatted image data generated from science-quality FITS files. Use the APIs below to interact directly with these intermediary JPEG2000 files.
    
    Download a JP2 image for the specified datasource that is the closest match in time to the `date` requested.

    Either `sourceId` must be specified, or the combination of `observatory`, `instrument`, `detector`, and `measurement`.

    Request Parameters:

    Parameter	Required	Type	Example	Description
    date	Required	string	2014-01-01T23:59:59Z	Desired date/time of the JP2 image. ISO 8601 combined UTC date and time UTC format.
    sourceId	Optional	number	14	Unique image datasource identifier.
    observatory	Optional	string	SDO	Observatory name.
    instrument	Optional	string	AIA	Instrument name.
    detector	Optional	string	AIA	Detector name.
    measurement	Optional	string	335	Measurement name.
    jpip	Optional	boolean	false	Optionally return a JPIP URI instead of the binary data of the image itself.
    json	Optional	boolean	false	Optionally return a JSON object.


    EXAMPLE: http://helioviewer.org/api/v1/getJP2Image/?date=2014-01-01T23:59:59Z&sourceId=14&jpip=true
    '''

    base_url = 'http://helioviewer.org/api/v1/getJP2Image/?'
    req_url = ''

    try:
        validate_iso8601(date)
        if not date[-1:] == 'Z':
            date += 'Z'
        base_url += 'date=' + date
    except:
        raise ValueError(
            ""Your date input is not in iso8601 format. ex: 2014-01-01T23:59:59"")

    if sourceId:
        if not isinstance(sourceId, int):
            logger.error(""The sourceId argument should be an int, ignoring it"")
        else:
            base_url += ""sourceId="" + str(sourceId) + ""&""

    if observatory:
        if not isinstance(observatory, str):
            logger.error(
                ""The observatory argument should be a str, ignoring it"")
        else:
            base_url += ""observatory="" + observatory + ""&""

    if instrument:
        if not isinstance(instrument, str):
            logger.error(
                ""The instrument argument should be a str, ignoring it"")
        else:
            base_url += ""instrument="" + instrument + ""&""
    if detector:
        if not isinstance(detector, str):
            logger.error(""The detector argument should be a str, ignoring it"")
        else:
            base_url += ""detector="" + detector + ""&""

    if measurement:
        if not isinstance(measurement, str):
            logger.error(
                ""The measurement argument should be a str, ignoring it"")
        else:
            base_url += ""measurement="" + detector + ""&""

    req_url += base_url + ""jpip=true""

    return dispatch_http_get(req_url)",def,getjp2image,(,date,",",sourceId,=,None,",",observatory,=,None,",",instrument,=,None,",",detector,=,None,",",measurement,=,None,),:,base_url,=,'http://helioviewer.org/api/v1/getJP2Image/?',req_url,=,'',try,:,validate_iso8601,(,date,),if,not,date,[,-,"Helioviewer.org and JHelioviewer operate off of JPEG2000 formatted image data generated from science-quality FITS files. Use the APIs below to interact directly with these intermediary JPEG2000 files.
    
    Download a JP2 image for the specified datasource that is the closest match in time to the `date` requested.

    Either `sourceId` must be specified, or the combination of `observatory`, `instrument`, `detector`, and `measurement`.

    Request Parameters:

    Parameter	Required	Type	Example	Description
    date	Required	string	2014-01-01T23:59:59Z	Desired date/time of the JP2 image. ISO 8601 combined UTC date and time UTC format.
    sourceId	Optional	number	14	Unique image datasource identifier.
    observatory	Optional	string	SDO	Observatory name.
    instrument	Optional	string	AIA	Instrument name.
    detector	Optional	string	AIA	Detector name.
    measurement	Optional	string	335	Measurement name.
    jpip	Optional	boolean	false	Optionally return a JPIP URI instead of the binary data of the image itself.
    json	Optional	boolean	false	Optionally return a JSON object.


    EXAMPLE: http://helioviewer.org/api/v1/getJP2Image/?date=2014-01-01T23:59:59Z&sourceId=14&jpip=true",Helioviewer,.,org,and,JHelioviewer,operate,off,of,JPEG2000,formatted,image,data,generated,from,science,9f5e053f1d54995b833b83616f37c67178c3e840,https://github.com/emirozer/bowshock/blob/9f5e053f1d54995b833b83616f37c67178c3e840/bowshock/helioviewer.py#L10-L85,train,1,:,],==,'Z',:,date,+=,'Z',base_url,+=,'date=',+,date,except,:,raise,ValueError,(,"""Your date input is not in iso8601 format. ex: 2014-01-01T23:59:59""",),if,sourceId,:,if,not,isinstance,(,sourceId,",",-,quality,FITS,files,.,Use,the,APIs,below,to,interact,directly,with,these,intermediary,JPEG2000,files,.,Download,int,),:,logger,.,error,(,"""The sourceId argument should be an int, ignoring it""",),else,:,base_url,+=,"""sourceId=""",+,str,(,sourceId,),+,"""&""",if,observatory,:,if,not,isinstance,(,observatory,",",str,),:,logger,.,error,(,"""The observatory argument should be a str, ignoring it""",),else,:,base_url,+=,"""observatory=""",+,observatory,+,"""&""",if,instrument,:,if,not,isinstance,(,instrument,",",str,),:,logger,.,error,(,"""The instrument argument should be a str, ignoring it""",),else,:,base_url,+=,"""instrument=""",+,instrument,+,"""&""",if,detector,:,if,not,isinstance,(,detector,",",str,),:,logger,.,error,(,"""The detector argument should be a str, ignoring it""",),else,:,base_url,+=,"""detector=""",+,detector,+,"""&""",if,measurement,:,if,not,isinstance,(,measurement,",",str,),:,logger,.,error,(,"""The measurement argument should be a str, ignoring it""",),else,:,base_url,+=,"""measurement=""",+,detector,+,"""&""",req_url,+=,base_url,+,"""jpip=true""",return,dispatch_http_get,(,req_url,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,a,JP2,image,for,the,specified,datasource,that,is,the,closest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,match,in,time,to,the,date,requested,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/loader.py,loads_loader,"def loads_loader(load_module: types.ModuleType, pairs: Dict[str, str]) -> Optional[JSGValidateable]:
    """"""json loader objecthook

    :param load_module: Module that contains the various types
    :param pairs: key/value tuples (In our case, they are str/str)
    :return:
    """"""
    cntxt = load_module._CONTEXT

    # If the type element is a member of the JSON, load it
    possible_type = pairs[cntxt.TYPE] if cntxt.TYPE in pairs else None
    target_class = getattr(load_module, possible_type, None) if isinstance(possible_type, str) else None
    if target_class:
        return target_class(**pairs)

    # See whether there are any exception types that are valid for the incoming data
    for type_exception in cntxt.TYPE_EXCEPTIONS:
        if not hasattr(load_module, type_exception):
            raise ValueError(UNKNOWN_TYPE_EXCEPTION.format(type_exception))
        target_class = getattr(load_module, type_exception)
        target_strict = target_class._strict
        target_class._strict = False
        try:
            rval = target_class(**pairs)
        finally:
            target_class._strict = target_strict
        if is_valid(rval):
            return rval

    # If there is not a type variable and nothing fits, just load up the first (and perhaps only) exception
    # It will later fail any is_valid tests
    if not cntxt.TYPE and cntxt.TYPE_EXCEPTIONS:
        return getattr(load_module, cntxt.TYPE_EXCEPTIONS[0])(**pairs)

    if cntxt.TYPE in pairs:
        raise ValueError(f'Unknown reference type: ""{cntxt.TYPE}"": ""{pairs[cntxt.TYPE]}""')
    else:
        raise ValueError(f'Missing ""{cntxt.TYPE}"" element')",python,"def loads_loader(load_module: types.ModuleType, pairs: Dict[str, str]) -> Optional[JSGValidateable]:
    """"""json loader objecthook

    :param load_module: Module that contains the various types
    :param pairs: key/value tuples (In our case, they are str/str)
    :return:
    """"""
    cntxt = load_module._CONTEXT

    # If the type element is a member of the JSON, load it
    possible_type = pairs[cntxt.TYPE] if cntxt.TYPE in pairs else None
    target_class = getattr(load_module, possible_type, None) if isinstance(possible_type, str) else None
    if target_class:
        return target_class(**pairs)

    # See whether there are any exception types that are valid for the incoming data
    for type_exception in cntxt.TYPE_EXCEPTIONS:
        if not hasattr(load_module, type_exception):
            raise ValueError(UNKNOWN_TYPE_EXCEPTION.format(type_exception))
        target_class = getattr(load_module, type_exception)
        target_strict = target_class._strict
        target_class._strict = False
        try:
            rval = target_class(**pairs)
        finally:
            target_class._strict = target_strict
        if is_valid(rval):
            return rval

    # If there is not a type variable and nothing fits, just load up the first (and perhaps only) exception
    # It will later fail any is_valid tests
    if not cntxt.TYPE and cntxt.TYPE_EXCEPTIONS:
        return getattr(load_module, cntxt.TYPE_EXCEPTIONS[0])(**pairs)

    if cntxt.TYPE in pairs:
        raise ValueError(f'Unknown reference type: ""{cntxt.TYPE}"": ""{pairs[cntxt.TYPE]}""')
    else:
        raise ValueError(f'Missing ""{cntxt.TYPE}"" element')",def,loads_loader,(,load_module,:,types,.,ModuleType,",",pairs,:,Dict,[,str,",",str,],),->,Optional,[,JSGValidateable,],:,cntxt,=,load_module,.,_CONTEXT,"# If the type element is a member of the JSON, load it",possible_type,=,pairs,[,cntxt,.,TYPE,],if,cntxt,.,TYPE,in,"json loader objecthook

    :param load_module: Module that contains the various types
    :param pairs: key/value tuples (In our case, they are str/str)
    :return:",json,loader,objecthook,,,,,,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/loader.py#L17-L54,train,pairs,else,None,target_class,=,getattr,(,load_module,",",possible_type,",",None,),if,isinstance,(,possible_type,",",str,),else,None,if,target_class,:,return,target_class,(,*,*,,,,,,,,,,,,,,,,,,,,pairs,),# See whether there are any exception types that are valid for the incoming data,for,type_exception,in,cntxt,.,TYPE_EXCEPTIONS,:,if,not,hasattr,(,load_module,",",type_exception,),:,raise,ValueError,(,UNKNOWN_TYPE_EXCEPTION,.,format,(,type_exception,),),target_class,=,getattr,(,load_module,",",type_exception,),target_strict,=,target_class,.,_strict,target_class,.,_strict,=,False,try,:,rval,=,target_class,(,*,*,pairs,),finally,:,target_class,.,_strict,=,target_strict,if,is_valid,(,rval,),:,return,rval,"# If there is not a type variable and nothing fits, just load up the first (and perhaps only) exception",# It will later fail any is_valid tests,if,not,cntxt,.,TYPE,and,cntxt,.,TYPE_EXCEPTIONS,:,return,getattr,(,load_module,",",cntxt,.,TYPE_EXCEPTIONS,[,0,],),(,*,*,pairs,),if,cntxt,.,TYPE,in,pairs,:,raise,ValueError,(,"f'Unknown reference type: ""{cntxt.TYPE}"": ""{pairs[cntxt.TYPE]}""'",),else,:,raise,ValueError,(,"f'Missing ""{cntxt.TYPE}"" element'",),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/loader.py,loads,"def loads(s: str, load_module: types.ModuleType, **kwargs):
    """""" Convert a JSON string into a JSGObject

    :param s: string representation of JSON document
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string
    """"""
    return json.loads(s, object_hook=lambda pairs: loads_loader(load_module, pairs), **kwargs)",python,"def loads(s: str, load_module: types.ModuleType, **kwargs):
    """""" Convert a JSON string into a JSGObject

    :param s: string representation of JSON document
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string
    """"""
    return json.loads(s, object_hook=lambda pairs: loads_loader(load_module, pairs), **kwargs)",def,loads,(,s,:,str,",",load_module,:,types,.,ModuleType,",",*,*,kwargs,),:,return,json,.,loads,(,s,",",object_hook,=,lambda,pairs,:,loads_loader,(,load_module,",",pairs,),",",*,*,kwargs,),,,"Convert a JSON string into a JSGObject

    :param s: string representation of JSON document
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string",Convert,a,JSON,string,into,a,JSGObject,,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/loader.py#L57-L65,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/loader.py,load,"def load(fp: Union[TextIO, str], load_module: types.ModuleType, **kwargs):
    """""" Convert a file name or file-like object containing stringified JSON into a JSGObject

    :param fp: file-like object to deserialize
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string
    """"""
    if isinstance(fp, str):
        with open(fp) as f:
            return loads(f.read(), load_module, **kwargs)
    else:
        return loads(fp.read(), load_module, **kwargs)",python,"def load(fp: Union[TextIO, str], load_module: types.ModuleType, **kwargs):
    """""" Convert a file name or file-like object containing stringified JSON into a JSGObject

    :param fp: file-like object to deserialize
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string
    """"""
    if isinstance(fp, str):
        with open(fp) as f:
            return loads(f.read(), load_module, **kwargs)
    else:
        return loads(fp.read(), load_module, **kwargs)",def,load,(,fp,:,Union,[,TextIO,",",str,],",",load_module,:,types,.,ModuleType,",",*,*,kwargs,),:,if,isinstance,(,fp,",",str,),:,with,open,(,fp,),as,f,:,return,loads,(,f,"Convert a file name or file-like object containing stringified JSON into a JSGObject

    :param fp: file-like object to deserialize
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string",Convert,a,file,name,or,file,-,like,object,containing,stringified,JSON,into,a,JSGObject,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/loader.py#L68-L80,train,.,read,(,),",",load_module,",",*,*,kwargs,),else,:,return,loads,(,fp,.,read,(,),",",load_module,",",*,*,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/loader.py,isinstance_,"def isinstance_(x, A_tuple):
    """""" native isinstance_ with the test for typing.Union overridden """"""
    if is_union(A_tuple):
        return any(isinstance_(x, t) for t in A_tuple.__args__)
    elif getattr(A_tuple, '__origin__', None) is not None:
        return isinstance(x, A_tuple.__origin__)
    else:
        return isinstance(x, A_tuple)",python,"def isinstance_(x, A_tuple):
    """""" native isinstance_ with the test for typing.Union overridden """"""
    if is_union(A_tuple):
        return any(isinstance_(x, t) for t in A_tuple.__args__)
    elif getattr(A_tuple, '__origin__', None) is not None:
        return isinstance(x, A_tuple.__origin__)
    else:
        return isinstance(x, A_tuple)",def,isinstance_,(,x,",",A_tuple,),:,if,is_union,(,A_tuple,),:,return,any,(,isinstance_,(,x,",",t,),for,t,in,A_tuple,.,__args__,),elif,getattr,(,A_tuple,",",'__origin__',",",None,),is,not,None,:,native isinstance_ with the test for typing.Union overridden,native,isinstance_,with,the,test,for,typing,.,Union,overridden,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/loader.py#L83-L90,train,return,isinstance,(,x,",",A_tuple,.,__origin__,),else,:,return,isinstance,(,x,",",A_tuple,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/loader.py,is_valid,"def is_valid(obj: JSGValidateable, log: Optional[Union[TextIO, Logger]] = None) -> bool:
    """""" Determine whether obj is valid

    :param obj: Object to validate
    :param log: Logger to record validation failures.  If absent, no information is recorded
    """"""
    return obj._is_valid(log)",python,"def is_valid(obj: JSGValidateable, log: Optional[Union[TextIO, Logger]] = None) -> bool:
    """""" Determine whether obj is valid

    :param obj: Object to validate
    :param log: Logger to record validation failures.  If absent, no information is recorded
    """"""
    return obj._is_valid(log)",def,is_valid,(,obj,:,JSGValidateable,",",log,:,Optional,[,Union,[,TextIO,",",Logger,],],=,None,),->,bool,:,return,obj,.,_is_valid,(,log,),,,,,,,,,,,,,"Determine whether obj is valid

    :param obj: Object to validate
    :param log: Logger to record validation failures.  If absent, no information is recorded",Determine,whether,obj,is,valid,,,,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/loader.py#L93-L99,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cydrobolt/pifx,pifx/util.py,arg_tup_to_dict,"def arg_tup_to_dict(argument_tuples):
    """"""Given a set of argument tuples, set their value in a data dictionary if not blank""""""
    data = dict()
    for arg_name, arg_val in argument_tuples:
        if arg_val is not None:
            if arg_val is True:
                arg_val = 'true'
            elif arg_val is False:
                arg_val = 'false'
            data[arg_name] = arg_val

    return data",python,"def arg_tup_to_dict(argument_tuples):
    """"""Given a set of argument tuples, set their value in a data dictionary if not blank""""""
    data = dict()
    for arg_name, arg_val in argument_tuples:
        if arg_val is not None:
            if arg_val is True:
                arg_val = 'true'
            elif arg_val is False:
                arg_val = 'false'
            data[arg_name] = arg_val

    return data",def,arg_tup_to_dict,(,argument_tuples,),:,data,=,dict,(,),for,arg_name,",",arg_val,in,argument_tuples,:,if,arg_val,is,not,None,:,if,arg_val,is,True,:,arg_val,=,'true',elif,arg_val,is,False,:,arg_val,=,'false',data,[,arg_name,"Given a set of argument tuples, set their value in a data dictionary if not blank",Given,a,set,of,argument,tuples,set,their,value,in,a,data,dictionary,if,not,c9de9c2695c3e6e72de4aa0de47b78fc13c457c3,https://github.com/cydrobolt/pifx/blob/c9de9c2695c3e6e72de4aa0de47b78fc13c457c3/pifx/util.py#L32-L43,train,],=,arg_val,return,data,,,,,,,,,,,,,,,,,,,,,,,,,,blank,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cydrobolt/pifx,pifx/util.py,handle_error,"def handle_error(response):
    """"""Raise appropriate exceptions if necessary.""""""
    status_code = response.status_code

    if status_code not in A_OK_HTTP_CODES:
        error_explanation = A_ERROR_HTTP_CODES.get(status_code)
        raise_error = ""{}: {}"".format(status_code, error_explanation)
        raise Exception(raise_error)
    else:
        return True",python,"def handle_error(response):
    """"""Raise appropriate exceptions if necessary.""""""
    status_code = response.status_code

    if status_code not in A_OK_HTTP_CODES:
        error_explanation = A_ERROR_HTTP_CODES.get(status_code)
        raise_error = ""{}: {}"".format(status_code, error_explanation)
        raise Exception(raise_error)
    else:
        return True",def,handle_error,(,response,),:,status_code,=,response,.,status_code,if,status_code,not,in,A_OK_HTTP_CODES,:,error_explanation,=,A_ERROR_HTTP_CODES,.,get,(,status_code,),raise_error,=,"""{}: {}""",.,format,(,status_code,",",error_explanation,),raise,Exception,(,raise_error,),else,:,return,Raise appropriate exceptions if necessary.,Raise,appropriate,exceptions,if,necessary,.,,,,,,,,,,c9de9c2695c3e6e72de4aa0de47b78fc13c457c3,https://github.com/cydrobolt/pifx/blob/c9de9c2695c3e6e72de4aa0de47b78fc13c457c3/pifx/util.py#L54-L63,train,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/agent/base.py,_BaseAgent.open,"async def open(self) -> '_BaseAgent':
        """"""
        Context manager entry; open wallet.
        For use when keeping agent open across multiple calls.

        :return: current object
        """"""

        LOGGER.debug('_BaseAgent.open >>>')

        # Do not open pool independently: let relying party decide when to go on-line and off-line
        await self.wallet.open()

        LOGGER.debug('_BaseAgent.open <<<')
        return self",python,"async def open(self) -> '_BaseAgent':
        """"""
        Context manager entry; open wallet.
        For use when keeping agent open across multiple calls.

        :return: current object
        """"""

        LOGGER.debug('_BaseAgent.open >>>')

        # Do not open pool independently: let relying party decide when to go on-line and off-line
        await self.wallet.open()

        LOGGER.debug('_BaseAgent.open <<<')
        return self",async,def,open,(,self,),->,'_BaseAgent',:,LOGGER,.,debug,(,'_BaseAgent.open >>>',),# Do not open pool independently: let relying party decide when to go on-line and off-line,await,self,.,wallet,.,open,(,),LOGGER,.,debug,(,'_BaseAgent.open <<<',),return,self,,,,,,,,,,,,"Context manager entry; open wallet.
        For use when keeping agent open across multiple calls.

        :return: current object",Context,manager,entry,;,open,wallet,.,For,use,when,keeping,agent,open,across,multiple,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/agent/base.py#L129-L143,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,calls,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/agent/base.py,_BaseAgent._get_rev_reg_def,"async def _get_rev_reg_def(self, rr_id: str) -> str:
        """"""
        Get revocation registry definition from ledger by its identifier. Raise AbsentRevReg
        for no such revocation registry, logging any error condition and raising BadLedgerTxn
        on bad request.

        Retrieve the revocation registry definition from the agent's revocation cache if it has it;
        cache it en passant if it does not (and such a revocation registry definition exists on the ledger).

        :param rr_id: (revocation registry) identifier string, of the format
            '<issuer-did>:4:<issuer-did>:3:CL:<schema-seq-no>:<tag>:CL_ACCUM:<tag>'
        :return: revocation registry definition json as retrieved from ledger
        """"""

        LOGGER.debug('_BaseAgent._get_rev_reg_def >>> rr_id: %s', rr_id)

        rv_json = json.dumps({})

        with REVO_CACHE.lock:
            revo_cache_entry = REVO_CACHE.get(rr_id, None)
            rr_def = revo_cache_entry.rev_reg_def if revo_cache_entry else None
            if rr_def:
                LOGGER.info('_BaseAgent._get_rev_reg_def: rev reg def for %s from cache', rr_id)
                rv_json = json.dumps(rr_def)
            else:
                get_rrd_req_json = await ledger.build_get_revoc_reg_def_request(self.did, rr_id)
                resp_json = await self._submit(get_rrd_req_json)
                try:
                    (_, rv_json) = await ledger.parse_get_revoc_reg_def_response(resp_json)
                    rr_def = json.loads(rv_json)
                except IndyError:  # ledger replied, but there is no such rev reg
                    LOGGER.debug('_BaseAgent._get_rev_reg_def: <!< no rev reg exists on %s', rr_id)
                    raise AbsentRevReg('No rev reg exists on {}'.format(rr_id))

                if revo_cache_entry is None:
                    REVO_CACHE[rr_id] = RevoCacheEntry(rr_def, None)
                else:
                    REVO_CACHE[rr_id].rev_reg_def = rr_def

        LOGGER.debug('_BaseAgent._get_rev_reg_def <<< %s', rv_json)
        return rv_json",python,"async def _get_rev_reg_def(self, rr_id: str) -> str:
        """"""
        Get revocation registry definition from ledger by its identifier. Raise AbsentRevReg
        for no such revocation registry, logging any error condition and raising BadLedgerTxn
        on bad request.

        Retrieve the revocation registry definition from the agent's revocation cache if it has it;
        cache it en passant if it does not (and such a revocation registry definition exists on the ledger).

        :param rr_id: (revocation registry) identifier string, of the format
            '<issuer-did>:4:<issuer-did>:3:CL:<schema-seq-no>:<tag>:CL_ACCUM:<tag>'
        :return: revocation registry definition json as retrieved from ledger
        """"""

        LOGGER.debug('_BaseAgent._get_rev_reg_def >>> rr_id: %s', rr_id)

        rv_json = json.dumps({})

        with REVO_CACHE.lock:
            revo_cache_entry = REVO_CACHE.get(rr_id, None)
            rr_def = revo_cache_entry.rev_reg_def if revo_cache_entry else None
            if rr_def:
                LOGGER.info('_BaseAgent._get_rev_reg_def: rev reg def for %s from cache', rr_id)
                rv_json = json.dumps(rr_def)
            else:
                get_rrd_req_json = await ledger.build_get_revoc_reg_def_request(self.did, rr_id)
                resp_json = await self._submit(get_rrd_req_json)
                try:
                    (_, rv_json) = await ledger.parse_get_revoc_reg_def_response(resp_json)
                    rr_def = json.loads(rv_json)
                except IndyError:  # ledger replied, but there is no such rev reg
                    LOGGER.debug('_BaseAgent._get_rev_reg_def: <!< no rev reg exists on %s', rr_id)
                    raise AbsentRevReg('No rev reg exists on {}'.format(rr_id))

                if revo_cache_entry is None:
                    REVO_CACHE[rr_id] = RevoCacheEntry(rr_def, None)
                else:
                    REVO_CACHE[rr_id].rev_reg_def = rr_def

        LOGGER.debug('_BaseAgent._get_rev_reg_def <<< %s', rv_json)
        return rv_json",async,def,_get_rev_reg_def,(,self,",",rr_id,:,str,),->,str,:,LOGGER,.,debug,(,'_BaseAgent._get_rev_reg_def >>> rr_id: %s',",",rr_id,),rv_json,=,json,.,dumps,(,{,},),with,REVO_CACHE,.,lock,:,revo_cache_entry,=,REVO_CACHE,.,get,(,rr_id,",","Get revocation registry definition from ledger by its identifier. Raise AbsentRevReg
        for no such revocation registry, logging any error condition and raising BadLedgerTxn
        on bad request.

        Retrieve the revocation registry definition from the agent's revocation cache if it has it;
        cache it en passant if it does not (and such a revocation registry definition exists on the ledger).

        :param rr_id: (revocation registry) identifier string, of the format
            '<issuer-did>:4:<issuer-did>:3:CL:<schema-seq-no>:<tag>:CL_ACCUM:<tag>'
        :return: revocation registry definition json as retrieved from ledger",Get,revocation,registry,definition,from,ledger,by,its,identifier,.,Raise,AbsentRevReg,for,no,such,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/agent/base.py#L289-L329,train,None,),rr_def,=,revo_cache_entry,.,rev_reg_def,if,revo_cache_entry,else,None,if,rr_def,:,LOGGER,.,info,(,'_BaseAgent._get_rev_reg_def: rev reg def for %s from cache',",",rr_id,),rv_json,=,json,.,dumps,(,rr_def,),revocation,registry,logging,any,error,condition,and,raising,BadLedgerTxn,on,bad,request,.,,,,,,,else,:,get_rrd_req_json,=,await,ledger,.,build_get_revoc_reg_def_request,(,self,.,did,",",rr_id,),resp_json,=,await,self,.,_submit,(,get_rrd_req_json,),try,:,(,_,",",rv_json,),=,await,ledger,.,parse_get_revoc_reg_def_response,(,resp_json,),rr_def,=,json,.,loads,(,rv_json,),except,IndyError,:,"# ledger replied, but there is no such rev reg",LOGGER,.,debug,(,'_BaseAgent._get_rev_reg_def: <!< no rev reg exists on %s',",",rr_id,),raise,AbsentRevReg,(,'No rev reg exists on {}',.,format,(,rr_id,),),if,revo_cache_entry,is,None,:,REVO_CACHE,[,rr_id,],=,RevoCacheEntry,(,rr_def,",",None,),else,:,REVO_CACHE,[,rr_id,],.,rev_reg_def,=,rr_def,LOGGER,.,debug,(,'_BaseAgent._get_rev_reg_def <<< %s',",",rv_json,),return,rv_json,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/agent/base.py,_BaseAgent.get_cred_def,"async def get_cred_def(self, cd_id: str) -> str:
        """"""
        Get credential definition from ledger by its identifier.

        Raise AbsentCredDef for no such credential definition, logging any error condition and raising
        BadLedgerTxn on bad request. Raise ClosedPool if cred def not in cache and pool is closed.

        Retrieve the credential definition from the agent's credential definition cache if it has it; cache it
        en passant if it does not (and if there is a corresponding credential definition on the ledger).

        :param cd_id: (credential definition) identifier string ('<issuer-did>:3:CL:<schema-seq-no>:<tag>')
        :return: credential definition json as retrieved from ledger, empty production for no such cred def
        """"""

        LOGGER.debug('_BaseAgent.get_cred_def >>> cd_id: %s', cd_id)

        rv_json = json.dumps({})

        with CRED_DEF_CACHE.lock:
            if cd_id in CRED_DEF_CACHE:
                LOGGER.info('_BaseAgent.get_cred_def: got cred def for %s from cache', cd_id)
                rv_json = json.dumps(CRED_DEF_CACHE[cd_id])
                LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)
                return rv_json

            req_json = await ledger.build_get_cred_def_request(self.did, cd_id)
            resp_json = await self._submit(req_json)
            resp = json.loads(resp_json)
            if not ('result' in resp and resp['result'].get('data', None)):
                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)
                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))
            try:
                (_, rv_json) = await ledger.parse_get_cred_def_response(resp_json)
            except IndyError:  # ledger replied, but there is no such cred def
                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)
                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))
            CRED_DEF_CACHE[cd_id] = json.loads(rv_json)
            LOGGER.info('_BaseAgent.get_cred_def: got cred def %s from ledger', cd_id)

        LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)
        return rv_json",python,"async def get_cred_def(self, cd_id: str) -> str:
        """"""
        Get credential definition from ledger by its identifier.

        Raise AbsentCredDef for no such credential definition, logging any error condition and raising
        BadLedgerTxn on bad request. Raise ClosedPool if cred def not in cache and pool is closed.

        Retrieve the credential definition from the agent's credential definition cache if it has it; cache it
        en passant if it does not (and if there is a corresponding credential definition on the ledger).

        :param cd_id: (credential definition) identifier string ('<issuer-did>:3:CL:<schema-seq-no>:<tag>')
        :return: credential definition json as retrieved from ledger, empty production for no such cred def
        """"""

        LOGGER.debug('_BaseAgent.get_cred_def >>> cd_id: %s', cd_id)

        rv_json = json.dumps({})

        with CRED_DEF_CACHE.lock:
            if cd_id in CRED_DEF_CACHE:
                LOGGER.info('_BaseAgent.get_cred_def: got cred def for %s from cache', cd_id)
                rv_json = json.dumps(CRED_DEF_CACHE[cd_id])
                LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)
                return rv_json

            req_json = await ledger.build_get_cred_def_request(self.did, cd_id)
            resp_json = await self._submit(req_json)
            resp = json.loads(resp_json)
            if not ('result' in resp and resp['result'].get('data', None)):
                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)
                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))
            try:
                (_, rv_json) = await ledger.parse_get_cred_def_response(resp_json)
            except IndyError:  # ledger replied, but there is no such cred def
                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)
                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))
            CRED_DEF_CACHE[cd_id] = json.loads(rv_json)
            LOGGER.info('_BaseAgent.get_cred_def: got cred def %s from ledger', cd_id)

        LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)
        return rv_json",async,def,get_cred_def,(,self,",",cd_id,:,str,),->,str,:,LOGGER,.,debug,(,'_BaseAgent.get_cred_def >>> cd_id: %s',",",cd_id,),rv_json,=,json,.,dumps,(,{,},),with,CRED_DEF_CACHE,.,lock,:,if,cd_id,in,CRED_DEF_CACHE,:,LOGGER,.,info,"Get credential definition from ledger by its identifier.

        Raise AbsentCredDef for no such credential definition, logging any error condition and raising
        BadLedgerTxn on bad request. Raise ClosedPool if cred def not in cache and pool is closed.

        Retrieve the credential definition from the agent's credential definition cache if it has it; cache it
        en passant if it does not (and if there is a corresponding credential definition on the ledger).

        :param cd_id: (credential definition) identifier string ('<issuer-did>:3:CL:<schema-seq-no>:<tag>')
        :return: credential definition json as retrieved from ledger, empty production for no such cred def",Get,credential,definition,from,ledger,by,its,identifier,.,,,,,,,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/agent/base.py#L331-L371,train,(,'_BaseAgent.get_cred_def: got cred def for %s from cache',",",cd_id,),rv_json,=,json,.,dumps,(,CRED_DEF_CACHE,[,cd_id,],),LOGGER,.,debug,(,'_BaseAgent.get_cred_def <<< %s',",",rv_json,),return,rv_json,req_json,=,await,ledger,,,,,,,,,,,,,,,,,,,,.,build_get_cred_def_request,(,self,.,did,",",cd_id,),resp_json,=,await,self,.,_submit,(,req_json,),resp,=,json,.,loads,(,resp_json,),if,not,(,'result',in,resp,and,resp,[,'result',],.,get,(,'data',",",None,),),:,LOGGER,.,debug,(,'_BaseAgent.get_cred_def: <!< no cred def exists on %s',",",cd_id,),raise,AbsentCredDef,(,'No cred def exists on {}',.,format,(,cd_id,),),try,:,(,_,",",rv_json,),=,await,ledger,.,parse_get_cred_def_response,(,resp_json,),except,IndyError,:,"# ledger replied, but there is no such cred def",LOGGER,.,debug,(,'_BaseAgent.get_cred_def: <!< no cred def exists on %s',",",cd_id,),raise,AbsentCredDef,(,'No cred def exists on {}',.,format,(,cd_id,),),CRED_DEF_CACHE,[,cd_id,],=,json,.,loads,(,rv_json,),LOGGER,.,info,(,'_BaseAgent.get_cred_def: got cred def %s from ledger',",",cd_id,),LOGGER,.,debug,(,'_BaseAgent.get_cred_def <<< %s',",",rv_json,),return,rv_json,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/jsglib/typing_patch_37.py,is_union,"def is_union(etype) -> bool:
    """""" Determine whether etype is a Union """"""
    return getattr(etype, '__origin__', None) is not None and \
           getattr(etype.__origin__, '_name', None) and\
           etype.__origin__._name == 'Union'",python,"def is_union(etype) -> bool:
    """""" Determine whether etype is a Union """"""
    return getattr(etype, '__origin__', None) is not None and \
           getattr(etype.__origin__, '_name', None) and\
           etype.__origin__._name == 'Union'",def,is_union,(,etype,),->,bool,:,return,getattr,(,etype,",",'__origin__',",",None,),is,not,None,and,getattr,(,etype,.,__origin__,",",'_name',",",None,),and,etype,.,__origin__,.,_name,==,'Union',,,,,Determine whether etype is a Union,Determine,whether,etype,is,a,Union,,,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/jsglib/typing_patch_37.py#L13-L17,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/fields.py,unset,"def unset(entity, *types):
    """"""Unset the TypedFields on the input `entity`.

    Args:
        entity: A mixbox.Entity object.
        *types: A variable-length list of TypedField subclasses. If not
            provided, defaults to TypedField.
    """"""
    if not types:
        types = (TypedField,)

    fields = list(entity._fields.keys())
    remove = (x for x in fields if isinstance(x, types))

    for field in remove:
        del entity._fields[field]",python,"def unset(entity, *types):
    """"""Unset the TypedFields on the input `entity`.

    Args:
        entity: A mixbox.Entity object.
        *types: A variable-length list of TypedField subclasses. If not
            provided, defaults to TypedField.
    """"""
    if not types:
        types = (TypedField,)

    fields = list(entity._fields.keys())
    remove = (x for x in fields if isinstance(x, types))

    for field in remove:
        del entity._fields[field]",def,unset,(,entity,",",*,types,),:,if,not,types,:,types,=,(,TypedField,",",),fields,=,list,(,entity,.,_fields,.,keys,(,),),remove,=,(,x,for,x,in,fields,if,isinstance,(,x,"Unset the TypedFields on the input `entity`.

    Args:
        entity: A mixbox.Entity object.
        *types: A variable-length list of TypedField subclasses. If not
            provided, defaults to TypedField.",Unset,the,TypedFields,on,the,input,entity,.,,,,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/fields.py#L17-L32,train,",",types,),),for,field,in,remove,:,del,entity,.,_fields,[,field,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/fields.py,_matches,"def _matches(field, params):
    """"""Return True if the input TypedField `field` contains instance attributes
    that match the input parameters.

    Args:
        field: A TypedField instance.
        params: A dictionary of TypedField instance attribute-to-value mappings.

    Returns:
        True if the input TypedField matches the input parameters.
    """"""
    fieldattrs = six.iteritems(params)
    return all(getattr(field, attr) == val for attr, val in fieldattrs)",python,"def _matches(field, params):
    """"""Return True if the input TypedField `field` contains instance attributes
    that match the input parameters.

    Args:
        field: A TypedField instance.
        params: A dictionary of TypedField instance attribute-to-value mappings.

    Returns:
        True if the input TypedField matches the input parameters.
    """"""
    fieldattrs = six.iteritems(params)
    return all(getattr(field, attr) == val for attr, val in fieldattrs)",def,_matches,(,field,",",params,),:,fieldattrs,=,six,.,iteritems,(,params,),return,all,(,getattr,(,field,",",attr,),==,val,for,attr,",",val,in,fieldattrs,),,,,,,,,,,"Return True if the input TypedField `field` contains instance attributes
    that match the input parameters.

    Args:
        field: A TypedField instance.
        params: A dictionary of TypedField instance attribute-to-value mappings.

    Returns:
        True if the input TypedField matches the input parameters.",Return,True,if,the,input,TypedField,field,contains,instance,attributes,that,match,the,input,parameters,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/fields.py#L35-L47,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/fields.py,iterfields,"def iterfields(klass):
    """"""Iterate over the input class members and yield its TypedFields.

    Args:
        klass: A class (usually an Entity subclass).

    Yields:
        (class attribute name, TypedField instance) tuples.
    """"""
    is_field = lambda x: isinstance(x, TypedField)

    for name, field in inspect.getmembers(klass, predicate=is_field):
        yield name, field",python,"def iterfields(klass):
    """"""Iterate over the input class members and yield its TypedFields.

    Args:
        klass: A class (usually an Entity subclass).

    Yields:
        (class attribute name, TypedField instance) tuples.
    """"""
    is_field = lambda x: isinstance(x, TypedField)

    for name, field in inspect.getmembers(klass, predicate=is_field):
        yield name, field",def,iterfields,(,klass,),:,is_field,=,lambda,x,:,isinstance,(,x,",",TypedField,),for,name,",",field,in,inspect,.,getmembers,(,klass,",",predicate,=,is_field,),:,yield,name,",",field,,,,,,,"Iterate over the input class members and yield its TypedFields.

    Args:
        klass: A class (usually an Entity subclass).

    Yields:
        (class attribute name, TypedField instance) tuples.",Iterate,over,the,input,class,members,and,yield,its,TypedFields,.,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/fields.py#L50-L62,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CybOXProject/mixbox,mixbox/fields.py,TypedField._clean,"def _clean(self, value):
        """"""Validate and clean a candidate value for this field.""""""
        if value is None:
            return None
        elif self.type_ is None:
            return value
        elif self.check_type(value):
            return value
        elif self.is_type_castable:  # noqa
            return self.type_(value)

        error_fmt = ""%s must be a %s, not a %s""
        error = error_fmt % (self.name, self.type_, type(value))
        raise TypeError(error)",python,"def _clean(self, value):
        """"""Validate and clean a candidate value for this field.""""""
        if value is None:
            return None
        elif self.type_ is None:
            return value
        elif self.check_type(value):
            return value
        elif self.is_type_castable:  # noqa
            return self.type_(value)

        error_fmt = ""%s must be a %s, not a %s""
        error = error_fmt % (self.name, self.type_, type(value))
        raise TypeError(error)",def,_clean,(,self,",",value,),:,if,value,is,None,:,return,None,elif,self,.,type_,is,None,:,return,value,elif,self,.,check_type,(,value,),:,return,value,elif,self,.,is_type_castable,:,# noqa,return,self,.,Validate and clean a candidate value for this field.,Validate,and,clean,a,candidate,value,for,this,field,.,,,,,,9097dae7a433f5b98c18171c4a5598f69a7d30af,https://github.com/CybOXProject/mixbox/blob/9097dae7a433f5b98c18171c4a5598f69a7d30af/mixbox/fields.py#L177-L190,train,type_,(,value,),error_fmt,=,"""%s must be a %s, not a %s""",error,=,error_fmt,%,(,self,.,name,",",self,.,type_,",",type,(,value,),),raise,TypeError,(,error,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/collections_store/git_actions.py,TreeCollectionsGitAction.remove_collection,"def remove_collection(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        """"""Remove a collection
        Given a collection_id, branch and optionally an
        author, remove a collection on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        if fourth_arg is None:
            collection_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_collection_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, collection_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = ""Delete Collection '%s' via OpenTree API"" % collection_id
        return self._remove_document(gh_user, collection_id, parent_sha, author, commit_msg)",python,"def remove_collection(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        """"""Remove a collection
        Given a collection_id, branch and optionally an
        author, remove a collection on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        if fourth_arg is None:
            collection_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_collection_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, collection_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = ""Delete Collection '%s' via OpenTree API"" % collection_id
        return self._remove_document(gh_user, collection_id, parent_sha, author, commit_msg)",def,remove_collection,(,self,",",first_arg,",",sec_arg,",",third_arg,",",fourth_arg,=,None,",",commit_msg,=,None,),:,if,fourth_arg,is,None,:,collection_id,",",branch_name,",",author,=,first_arg,",",sec_arg,",",third_arg,gh_user,=,branch_name,.,split,(,'_collection_',"Remove a collection
        Given a collection_id, branch and optionally an
        author, remove a collection on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.",Remove,a,collection,Given,a,collection_id,branch,and,optionally,an,author,remove,a,collection,on,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/collections_store/git_actions.py#L92-L107,train,),[,0,],parent_sha,=,self,.,get_master_sha,(,),else,:,gh_user,",",collection_id,",",parent_sha,",",author,=,first_arg,",",sec_arg,",",third_arg,",",fourth_arg,if,commit_msg,the,given,branch,and,attribute,the,commit,to,author,.,Returns,the,SHA,of,the,commit,on,branch,.,is,None,:,commit_msg,=,"""Delete Collection '%s' via OpenTree API""",%,collection_id,return,self,.,_remove_document,(,gh_user,",",collection_id,",",parent_sha,",",author,",",commit_msg,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSPC-SPAC-buyandsell/von_agent,von_agent/agent/verifier.py,Verifier.load_cache,"async def load_cache(self, archive: bool = False) -> int:
        """"""
        Load caches and archive enough to go offline and be able to verify proof
        on content marked of interest in configuration.

        Return timestamp (epoch seconds) of cache load event, also used as subdirectory
        for cache archives.

        :param archive: whether to archive caches to disk
        :return: cache load event timestamp (epoch seconds)
        """"""

        LOGGER.debug('Verifier.load_cache >>> archive: %s', archive)

        rv = int(time())
        for s_id in self.cfg.get('archive-on-close', {}).get('schema_id', {}):
            with SCHEMA_CACHE.lock:
                await self.get_schema(s_id)
        for cd_id in self.cfg.get('archive-on-close', {}).get('cred_def_id', {}):
            with CRED_DEF_CACHE.lock:
                await self.get_cred_def(cd_id)
        for rr_id in self.cfg.get('archive-on-close', {}).get('rev_reg_id', {}):
            await self._get_rev_reg_def(rr_id)
            with REVO_CACHE.lock:
                revo_cache_entry = REVO_CACHE.get(rr_id, None)
                if revo_cache_entry:
                    try:
                        await revo_cache_entry.get_state_json(self._build_rr_state_json, rv, rv)
                    except ClosedPool:
                        LOGGER.warning(
                            'Verifier %s is offline from pool %s, cannot update revo cache reg state for %s to %s',
                            self.wallet.name,
                            self.pool.name,
                            rr_id,
                            rv)

        if archive:
            Caches.archive(self.dir_cache)
        LOGGER.debug('Verifier.load_cache <<< %s', rv)
        return rv",python,"async def load_cache(self, archive: bool = False) -> int:
        """"""
        Load caches and archive enough to go offline and be able to verify proof
        on content marked of interest in configuration.

        Return timestamp (epoch seconds) of cache load event, also used as subdirectory
        for cache archives.

        :param archive: whether to archive caches to disk
        :return: cache load event timestamp (epoch seconds)
        """"""

        LOGGER.debug('Verifier.load_cache >>> archive: %s', archive)

        rv = int(time())
        for s_id in self.cfg.get('archive-on-close', {}).get('schema_id', {}):
            with SCHEMA_CACHE.lock:
                await self.get_schema(s_id)
        for cd_id in self.cfg.get('archive-on-close', {}).get('cred_def_id', {}):
            with CRED_DEF_CACHE.lock:
                await self.get_cred_def(cd_id)
        for rr_id in self.cfg.get('archive-on-close', {}).get('rev_reg_id', {}):
            await self._get_rev_reg_def(rr_id)
            with REVO_CACHE.lock:
                revo_cache_entry = REVO_CACHE.get(rr_id, None)
                if revo_cache_entry:
                    try:
                        await revo_cache_entry.get_state_json(self._build_rr_state_json, rv, rv)
                    except ClosedPool:
                        LOGGER.warning(
                            'Verifier %s is offline from pool %s, cannot update revo cache reg state for %s to %s',
                            self.wallet.name,
                            self.pool.name,
                            rr_id,
                            rv)

        if archive:
            Caches.archive(self.dir_cache)
        LOGGER.debug('Verifier.load_cache <<< %s', rv)
        return rv",async,def,load_cache,(,self,",",archive,:,bool,=,False,),->,int,:,LOGGER,.,debug,(,'Verifier.load_cache >>> archive: %s',",",archive,),rv,=,int,(,time,(,),),for,s_id,in,self,.,cfg,.,get,(,'archive-on-close',",",{,"Load caches and archive enough to go offline and be able to verify proof
        on content marked of interest in configuration.

        Return timestamp (epoch seconds) of cache load event, also used as subdirectory
        for cache archives.

        :param archive: whether to archive caches to disk
        :return: cache load event timestamp (epoch seconds)",Load,caches,and,archive,enough,to,go,offline,and,be,able,to,verify,proof,on,0b1c17cca3bd178b6e6974af84dbac1dfce5cf45,https://github.com/PSPC-SPAC-buyandsell/von_agent/blob/0b1c17cca3bd178b6e6974af84dbac1dfce5cf45/von_agent/agent/verifier.py#L165-L204,train,},),.,get,(,'schema_id',",",{,},),:,with,SCHEMA_CACHE,.,lock,:,await,self,.,get_schema,(,s_id,),for,cd_id,in,self,.,cfg,.,content,marked,of,interest,in,configuration,.,,,,,,,,,,,,,get,(,'archive-on-close',",",{,},),.,get,(,'cred_def_id',",",{,},),:,with,CRED_DEF_CACHE,.,lock,:,await,self,.,get_cred_def,(,cd_id,),for,rr_id,in,self,.,cfg,.,get,(,'archive-on-close',",",{,},),.,get,(,'rev_reg_id',",",{,},),:,await,self,.,_get_rev_reg_def,(,rr_id,),with,REVO_CACHE,.,lock,:,revo_cache_entry,=,REVO_CACHE,.,get,(,rr_id,",",None,),if,revo_cache_entry,:,try,:,await,revo_cache_entry,.,get_state_json,(,self,.,_build_rr_state_json,",",rv,",",rv,),except,ClosedPool,:,LOGGER,.,warning,(,"'Verifier %s is offline from pool %s, cannot update revo cache reg state for %s to %s'",",",self,.,wallet,.,name,",",self,.,pool,.,name,",",rr_id,",",rv,),if,archive,:,Caches,.,archive,(,self,.,dir_cache,),LOGGER,.,debug,(,'Verifier.load_cache <<< %s',",",rv,),return,rv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/permissions.py,_Permission.can,"def can(self):
        """"""Grant permission if owner or admin.""""""
        return str(current_user.get_id()) == str(self.community.id_user) or \
            DynamicPermission(ActionNeed('admin-access')).can()",python,"def can(self):
        """"""Grant permission if owner or admin.""""""
        return str(current_user.get_id()) == str(self.community.id_user) or \
            DynamicPermission(ActionNeed('admin-access')).can()",def,can,(,self,),:,return,str,(,current_user,.,get_id,(,),),==,str,(,self,.,community,.,id_user,),or,DynamicPermission,(,ActionNeed,(,'admin-access',),),.,can,(,),,,,,,,,Grant permission if owner or admin.,Grant,permission,if,owner,or,admin,.,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/permissions.py#L46-L49,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/utils.py,listunion,"def listunion(ListOfLists):
    """"""
    Take the union of a list of lists.

    Take a Python list of Python lists::

            [[l11,l12, ...], [l21,l22, ...], ... , [ln1, ln2, ...]]

    and return the aggregated list::

            [l11,l12, ..., l21, l22 , ...]

    For a list of two lists, e.g. `[a, b]`, this is like::

            a.extend(b)

    **Parameters**

            **ListOfLists** :  Python list

                    Python list of Python lists.

    **Returns**

            **u** :  Python list

                    Python list created by taking the union of the
                    lists in `ListOfLists`.

    """"""
    u = []
    for s in ListOfLists:
        if s != None:
            u.extend(s)
    return u",python,"def listunion(ListOfLists):
    """"""
    Take the union of a list of lists.

    Take a Python list of Python lists::

            [[l11,l12, ...], [l21,l22, ...], ... , [ln1, ln2, ...]]

    and return the aggregated list::

            [l11,l12, ..., l21, l22 , ...]

    For a list of two lists, e.g. `[a, b]`, this is like::

            a.extend(b)

    **Parameters**

            **ListOfLists** :  Python list

                    Python list of Python lists.

    **Returns**

            **u** :  Python list

                    Python list created by taking the union of the
                    lists in `ListOfLists`.

    """"""
    u = []
    for s in ListOfLists:
        if s != None:
            u.extend(s)
    return u",def,listunion,(,ListOfLists,),:,u,=,[,],for,s,in,ListOfLists,:,if,s,!=,None,:,u,.,extend,(,s,),return,u,,,,,,,,,,,,,,,,"Take the union of a list of lists.

    Take a Python list of Python lists::

            [[l11,l12, ...], [l21,l22, ...], ... , [ln1, ln2, ...]]

    and return the aggregated list::

            [l11,l12, ..., l21, l22 , ...]

    For a list of two lists, e.g. `[a, b]`, this is like::

            a.extend(b)

    **Parameters**

            **ListOfLists** :  Python list

                    Python list of Python lists.

    **Returns**

            **u** :  Python list

                    Python list created by taking the union of the
                    lists in `ListOfLists`.",Take,the,union,of,a,list,of,lists,.,,,,,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/utils.py#L49-L83,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yamins81/tabular,tabular/utils.py,DEFAULT_NULLVALUE,"def DEFAULT_NULLVALUE(test):
    """"""
    Returns a null value for each of various kinds of test values.

    **Parameters**

            **test** :  bool, int, float or string

                    Value to test.


    **Returns**
            **null** :  element in `[False, 0, 0.0, '']`

                    Null value corresponding to the given test value:

                    *   if `test` is a `bool`, return `False`
                    *   else if `test` is an `int`, return `0`
                    *   else if `test` is a `float`, return `0.0`
                    *   else `test` is a `str`, return `''`

    """"""
    return False if isinstance(test,bool) \
           else 0 if isinstance(test,int) \
           else 0.0 if isinstance(test,float) \
           else ''",python,"def DEFAULT_NULLVALUE(test):
    """"""
    Returns a null value for each of various kinds of test values.

    **Parameters**

            **test** :  bool, int, float or string

                    Value to test.


    **Returns**
            **null** :  element in `[False, 0, 0.0, '']`

                    Null value corresponding to the given test value:

                    *   if `test` is a `bool`, return `False`
                    *   else if `test` is an `int`, return `0`
                    *   else if `test` is a `float`, return `0.0`
                    *   else `test` is a `str`, return `''`

    """"""
    return False if isinstance(test,bool) \
           else 0 if isinstance(test,int) \
           else 0.0 if isinstance(test,float) \
           else ''",def,DEFAULT_NULLVALUE,(,test,),:,return,False,if,isinstance,(,test,",",bool,),else,0,if,isinstance,(,test,",",int,),else,0.0,if,isinstance,(,test,",",float,),else,'',,,,,,,,,"Returns a null value for each of various kinds of test values.

    **Parameters**

            **test** :  bool, int, float or string

                    Value to test.


    **Returns**
            **null** :  element in `[False, 0, 0.0, '']`

                    Null value corresponding to the given test value:

                    *   if `test` is a `bool`, return `False`
                    *   else if `test` is an `int`, return `0`
                    *   else if `test` is a `float`, return `0.0`
                    *   else `test` is a `str`, return `''`",Returns,a,null,value,for,each,of,various,kinds,of,test,values,.,,,1caf091c8c395960a9ad7078f95158b533cc52dd,https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/utils.py#L369-L394,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_objectexpr_parser.py,JSGObjectExpr.as_python,"def as_python(self, name: str) -> str:
        """""" Return the python representation of the class represented by this object """"""
        if self._map_valuetype:
            return self.map_as_python(name)
        else:
            return self.obj_as_python(name)",python,"def as_python(self, name: str) -> str:
        """""" Return the python representation of the class represented by this object """"""
        if self._map_valuetype:
            return self.map_as_python(name)
        else:
            return self.obj_as_python(name)",def,as_python,(,self,",",name,:,str,),->,str,:,if,self,.,_map_valuetype,:,return,self,.,map_as_python,(,name,),else,:,return,self,.,obj_as_python,(,name,),,,,,,,,,,,Return the python representation of the class represented by this object,Return,the,python,representation,of,the,class,represented,by,this,object,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_objectexpr_parser.py#L87-L92,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_objectexpr_parser.py,JSGObjectExpr.members_entries,"def members_entries(self, all_are_optional: bool=False) -> List[Tuple[str, str]]:
        """""" Return an ordered list of elements for the _members section

        :param all_are_optional: True means we're in a choice situation so everything is optional
        :return:
        """"""
        rval = []
        if self._members:
            for member in self._members:
                rval += member.members_entries(all_are_optional)
        elif self._choices:
            for choice in self._choices:
                rval += self._context.reference(choice).members_entries(True)
        else:
            return []
        return rval",python,"def members_entries(self, all_are_optional: bool=False) -> List[Tuple[str, str]]:
        """""" Return an ordered list of elements for the _members section

        :param all_are_optional: True means we're in a choice situation so everything is optional
        :return:
        """"""
        rval = []
        if self._members:
            for member in self._members:
                rval += member.members_entries(all_are_optional)
        elif self._choices:
            for choice in self._choices:
                rval += self._context.reference(choice).members_entries(True)
        else:
            return []
        return rval",def,members_entries,(,self,",",all_are_optional,:,bool,=,False,),->,List,[,Tuple,[,str,",",str,],],:,rval,=,[,],if,self,.,_members,:,for,member,in,self,.,_members,:,rval,+=,member,.,members_entries,"Return an ordered list of elements for the _members section

        :param all_are_optional: True means we're in a choice situation so everything is optional
        :return:",Return,an,ordered,list,of,elements,for,the,_members,section,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_objectexpr_parser.py#L196-L211,train,(,all_are_optional,),elif,self,.,_choices,:,for,choice,in,self,.,_choices,:,rval,+=,self,.,_context,.,reference,(,choice,),.,members_entries,(,True,),,,,,,,,,,,,,,,,,,,,else,:,return,[,],return,rval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/phylesystem/phylesystem_shard.py,_get_filtered_study_ids,"def _get_filtered_study_ids(shard, include_aliases=False):
    """"""Optionally filters out aliases from standard doc-id list""""""
    from peyotl.phylesystem.helper import DIGIT_PATTERN
    k = shard.get_doc_ids()
    if shard.has_aliases and (not include_aliases):
        x = []
        for i in k:
            if DIGIT_PATTERN.match(i) or ((len(i) > 1) and (i[-2] == '_')):
                pass
            else:
                x.append(i)
        return x",python,"def _get_filtered_study_ids(shard, include_aliases=False):
    """"""Optionally filters out aliases from standard doc-id list""""""
    from peyotl.phylesystem.helper import DIGIT_PATTERN
    k = shard.get_doc_ids()
    if shard.has_aliases and (not include_aliases):
        x = []
        for i in k:
            if DIGIT_PATTERN.match(i) or ((len(i) > 1) and (i[-2] == '_')):
                pass
            else:
                x.append(i)
        return x",def,_get_filtered_study_ids,(,shard,",",include_aliases,=,False,),:,from,peyotl,.,phylesystem,.,helper,import,DIGIT_PATTERN,k,=,shard,.,get_doc_ids,(,),if,shard,.,has_aliases,and,(,not,include_aliases,),:,x,=,[,],for,i,in,k,Optionally filters out aliases from standard doc-id list,Optionally,filters,out,aliases,from,standard,doc,-,id,list,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/phylesystem/phylesystem_shard.py#L18-L29,train,:,if,DIGIT_PATTERN,.,match,(,i,),or,(,(,len,(,i,),>,1,),and,(,i,[,-,2,],==,'_',),),:,,,,,,,,,,,,,,,,,,,,pass,else,:,x,.,append,(,i,),return,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/phylesystem/phylesystem_shard.py,PhylesystemShard._determine_next_study_id,"def _determine_next_study_id(self):
        """"""Return the numeric part of the newest study_id

        Checks out master branch as a side effect!
        """"""
        if self._doc_counter_lock is None:
            self._doc_counter_lock = Lock()
        prefix = self._new_study_prefix
        lp = len(prefix)
        n = 0
        # this function holds the lock for quite awhile,
        #   but it only called on the first instance of
        #   of creating a new study
        with self._doc_counter_lock:
            with self._index_lock:
                for k in self.study_index.keys():
                    if k.startswith(prefix):
                        try:
                            pn = int(k[lp:])
                            if pn > n:
                                n = pn
                        except:
                            pass
            nsi_contents = self._read_master_branch_resource(self._id_minting_file, is_json=True)
            if nsi_contents:
                self._next_study_id = nsi_contents['next_study_id']
                if self._next_study_id <= n:
                    m = 'next_study_id in {} is set lower than the ID of an existing study!'
                    m = m.format(self._id_minting_file)
                    raise RuntimeError(m)
            else:
                # legacy support for repo with no next_study_id.json file
                self._next_study_id = n
                self._advance_new_study_id()",python,"def _determine_next_study_id(self):
        """"""Return the numeric part of the newest study_id

        Checks out master branch as a side effect!
        """"""
        if self._doc_counter_lock is None:
            self._doc_counter_lock = Lock()
        prefix = self._new_study_prefix
        lp = len(prefix)
        n = 0
        # this function holds the lock for quite awhile,
        #   but it only called on the first instance of
        #   of creating a new study
        with self._doc_counter_lock:
            with self._index_lock:
                for k in self.study_index.keys():
                    if k.startswith(prefix):
                        try:
                            pn = int(k[lp:])
                            if pn > n:
                                n = pn
                        except:
                            pass
            nsi_contents = self._read_master_branch_resource(self._id_minting_file, is_json=True)
            if nsi_contents:
                self._next_study_id = nsi_contents['next_study_id']
                if self._next_study_id <= n:
                    m = 'next_study_id in {} is set lower than the ID of an existing study!'
                    m = m.format(self._id_minting_file)
                    raise RuntimeError(m)
            else:
                # legacy support for repo with no next_study_id.json file
                self._next_study_id = n
                self._advance_new_study_id()",def,_determine_next_study_id,(,self,),:,if,self,.,_doc_counter_lock,is,None,:,self,.,_doc_counter_lock,=,Lock,(,),prefix,=,self,.,_new_study_prefix,lp,=,len,(,prefix,),n,=,0,"# this function holds the lock for quite awhile,",#   but it only called on the first instance of,#   of creating a new study,with,self,.,_doc_counter_lock,:,with,"Return the numeric part of the newest study_id

        Checks out master branch as a side effect!",Return,the,numeric,part,of,the,newest,study_id,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/phylesystem/phylesystem_shard.py#L246-L279,train,self,.,_index_lock,:,for,k,in,self,.,study_index,.,keys,(,),:,if,k,.,startswith,(,prefix,),:,try,:,pn,=,int,(,k,,,,,,,,,,,,,,,,,,,,[,lp,:,],),if,pn,>,n,:,n,=,pn,except,:,pass,nsi_contents,=,self,.,_read_master_branch_resource,(,self,.,_id_minting_file,",",is_json,=,True,),if,nsi_contents,:,self,.,_next_study_id,=,nsi_contents,[,'next_study_id',],if,self,.,_next_study_id,<=,n,:,m,=,'next_study_id in {} is set lower than the ID of an existing study!',m,=,m,.,format,(,self,.,_id_minting_file,),raise,RuntimeError,(,m,),else,:,# legacy support for repo with no next_study_id.json file,self,.,_next_study_id,=,n,self,.,_advance_new_study_id,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/phylesystem/phylesystem_shard.py,PhylesystemShard._advance_new_study_id,"def _advance_new_study_id(self):
        """""" ASSUMES the caller holds the _doc_counter_lock !
        Returns the current numeric part of the next study ID, advances
        the counter to the next value, and stores that value in the
        file in case the server is restarted.
        """"""
        c = self._next_study_id
        self._next_study_id = 1 + c
        content = u'{""next_study_id"": %d}\n' % self._next_study_id
        # The content is JSON, but we hand-rolled the string above
        #       so that we can use it as a commit_msg
        self._write_master_branch_resource(content,
                                           self._id_minting_file,
                                           commit_msg=content,
                                           is_json=False)
        return c",python,"def _advance_new_study_id(self):
        """""" ASSUMES the caller holds the _doc_counter_lock !
        Returns the current numeric part of the next study ID, advances
        the counter to the next value, and stores that value in the
        file in case the server is restarted.
        """"""
        c = self._next_study_id
        self._next_study_id = 1 + c
        content = u'{""next_study_id"": %d}\n' % self._next_study_id
        # The content is JSON, but we hand-rolled the string above
        #       so that we can use it as a commit_msg
        self._write_master_branch_resource(content,
                                           self._id_minting_file,
                                           commit_msg=content,
                                           is_json=False)
        return c",def,_advance_new_study_id,(,self,),:,c,=,self,.,_next_study_id,self,.,_next_study_id,=,1,+,c,content,=,"u'{""next_study_id"": %d}\n'",%,self,.,_next_study_id,"# The content is JSON, but we hand-rolled the string above",#       so that we can use it as a commit_msg,self,.,_write_master_branch_resource,(,content,",",self,.,_id_minting_file,",",commit_msg,=,content,",",is_json,=,"ASSUMES the caller holds the _doc_counter_lock !
        Returns the current numeric part of the next study ID, advances
        the counter to the next value, and stores that value in the
        file in case the server is restarted.",ASSUMES,the,caller,holds,the,_doc_counter_lock,!,Returns,the,current,numeric,part,of,the,next,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/phylesystem/phylesystem_shard.py#L281-L296,train,False,),return,c,,,,,,,,,,,,,,,,,,,,,,,,,,,study,ID,advances,the,counter,to,the,next,value,and,stores,that,value,in,the,file,in,case,the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,server,is,restarted,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/parser_utils.py,flatten,"def flatten(l: Iterable) -> List:
    """"""Return a list of all non-list items in l

    :param l: list to be flattened
    :return:
    """"""
    rval = []
    for e in l:
        if not isinstance(e, str) and isinstance(e, Iterable):
            if len(list(e)):
                rval += flatten(e)
        else:
            rval.append(e)
    return rval",python,"def flatten(l: Iterable) -> List:
    """"""Return a list of all non-list items in l

    :param l: list to be flattened
    :return:
    """"""
    rval = []
    for e in l:
        if not isinstance(e, str) and isinstance(e, Iterable):
            if len(list(e)):
                rval += flatten(e)
        else:
            rval.append(e)
    return rval",def,flatten,(,l,:,Iterable,),->,List,:,rval,=,[,],for,e,in,l,:,if,not,isinstance,(,e,",",str,),and,isinstance,(,e,",",Iterable,),:,if,len,(,list,(,e,),),"Return a list of all non-list items in l

    :param l: list to be flattened
    :return:",Return,a,list,of,all,non,-,list,items,in,l,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/parser_utils.py#L14-L27,train,:,rval,+=,flatten,(,e,),else,:,rval,.,append,(,e,),return,rval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/parser_utils.py,flatten_unique,"def flatten_unique(l: Iterable) -> List:
    """""" Return a list of UNIQUE non-list items in l """"""
    rval = OrderedDict()
    for e in l:
        if not isinstance(e, str) and isinstance(e, Iterable):
            for ev in flatten_unique(e):
                rval[ev] = None
        else:
            rval[e] = None
    return list(rval.keys())",python,"def flatten_unique(l: Iterable) -> List:
    """""" Return a list of UNIQUE non-list items in l """"""
    rval = OrderedDict()
    for e in l:
        if not isinstance(e, str) and isinstance(e, Iterable):
            for ev in flatten_unique(e):
                rval[ev] = None
        else:
            rval[e] = None
    return list(rval.keys())",def,flatten_unique,(,l,:,Iterable,),->,List,:,rval,=,OrderedDict,(,),for,e,in,l,:,if,not,isinstance,(,e,",",str,),and,isinstance,(,e,",",Iterable,),:,for,ev,in,flatten_unique,(,e,),Return a list of UNIQUE non-list items in l,Return,a,list,of,UNIQUE,non,-,list,items,in,l,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/parser_utils.py#L30-L39,train,:,rval,[,ev,],=,None,else,:,rval,[,e,],=,None,return,list,(,rval,.,keys,(,),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/parser_utils.py,as_tokens,"def as_tokens(ctx: List[ParserRuleContext]) -> List[str]:
    """"""Return a stringified list of identifiers in ctx

    :param ctx: JSG parser item with a set of identifiers
    :return:
    """"""
    return [as_token(e) for e in ctx]",python,"def as_tokens(ctx: List[ParserRuleContext]) -> List[str]:
    """"""Return a stringified list of identifiers in ctx

    :param ctx: JSG parser item with a set of identifiers
    :return:
    """"""
    return [as_token(e) for e in ctx]",def,as_tokens,(,ctx,:,List,[,ParserRuleContext,],),->,List,[,str,],:,return,[,as_token,(,e,),for,e,in,ctx,],,,,,,,,,,,,,,,,,"Return a stringified list of identifiers in ctx

    :param ctx: JSG parser item with a set of identifiers
    :return:",Return,a,stringified,list,of,identifiers,in,ctx,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/parser_utils.py#L85-L91,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/parser_utils.py,is_valid_python,"def is_valid_python(tkn: str) -> bool:
    """"""Determine whether tkn is a valid python identifier

    :param tkn:
    :return:
    """"""
    try:
        root = ast.parse(tkn)
    except SyntaxError:
        return False
    return len(root.body) == 1 and isinstance(root.body[0], ast.Expr) and isinstance(root.body[0].value, ast.Name)",python,"def is_valid_python(tkn: str) -> bool:
    """"""Determine whether tkn is a valid python identifier

    :param tkn:
    :return:
    """"""
    try:
        root = ast.parse(tkn)
    except SyntaxError:
        return False
    return len(root.body) == 1 and isinstance(root.body[0], ast.Expr) and isinstance(root.body[0].value, ast.Name)",def,is_valid_python,(,tkn,:,str,),->,bool,:,try,:,root,=,ast,.,parse,(,tkn,),except,SyntaxError,:,return,False,return,len,(,root,.,body,),==,1,and,isinstance,(,root,.,body,[,0,],"Determine whether tkn is a valid python identifier

    :param tkn:
    :return:",Determine,whether,tkn,is,a,valid,python,identifier,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/parser_utils.py#L94-L104,train,",",ast,.,Expr,),and,isinstance,(,root,.,body,[,0,],.,value,",",ast,.,Name,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/phylesystem/git_actions.py,PhylesystemGitAction.remove_study,"def remove_study(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        """"""Remove a study
        Given a study_id, branch and optionally an
        author, remove a study on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        if fourth_arg is None:
            study_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_study_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, study_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = ""Delete Study #%s via OpenTree API"" % study_id
        return self._remove_document(gh_user, study_id, parent_sha, author, commit_msg)",python,"def remove_study(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        """"""Remove a study
        Given a study_id, branch and optionally an
        author, remove a study on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.
        """"""
        if fourth_arg is None:
            study_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_study_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, study_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = ""Delete Study #%s via OpenTree API"" % study_id
        return self._remove_document(gh_user, study_id, parent_sha, author, commit_msg)",def,remove_study,(,self,",",first_arg,",",sec_arg,",",third_arg,",",fourth_arg,=,None,",",commit_msg,=,None,),:,if,fourth_arg,is,None,:,study_id,",",branch_name,",",author,=,first_arg,",",sec_arg,",",third_arg,gh_user,=,branch_name,.,split,(,'_study_',"Remove a study
        Given a study_id, branch and optionally an
        author, remove a study on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.",Remove,a,study,Given,a,study_id,branch,and,optionally,an,author,remove,a,study,on,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/phylesystem/git_actions.py#L108-L123,train,),[,0,],parent_sha,=,self,.,get_master_sha,(,),else,:,gh_user,",",study_id,",",parent_sha,",",author,=,first_arg,",",sec_arg,",",third_arg,",",fourth_arg,if,commit_msg,the,given,branch,and,attribute,the,commit,to,author,.,Returns,the,SHA,of,the,commit,on,branch,.,is,None,:,commit_msg,=,"""Delete Study #%s via OpenTree API""",%,study_id,return,self,.,_remove_document,(,gh_user,",",study_id,",",parent_sha,",",author,",",commit_msg,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/cli.py,init,"def init():
    """"""Initialize the communities file storage.""""""
    try:
        initialize_communities_bucket()
        click.secho('Community init successful.', fg='green')
    except FilesException as e:
        click.secho(e.message, fg='red')",python,"def init():
    """"""Initialize the communities file storage.""""""
    try:
        initialize_communities_bucket()
        click.secho('Community init successful.', fg='green')
    except FilesException as e:
        click.secho(e.message, fg='red')",def,init,(,),:,try,:,initialize_communities_bucket,(,),click,.,secho,(,'Community init successful.',",",fg,=,'green',),except,FilesException,as,e,:,click,.,secho,(,e,.,message,",",fg,=,'red',),,,,,,,Initialize the communities file storage.,Initialize,the,communities,file,storage,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/cli.py#L50-L56,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/cli.py,addlogo,"def addlogo(community_id, logo):
    """"""Add logo to the community.""""""
    # Create the bucket
    c = Community.get(community_id)
    if not c:
        click.secho('Community {0} does not exist.'.format(community_id),
                    fg='red')
        return
    ext = save_and_validate_logo(logo, logo.name, c.id)
    c.logo_ext = ext
    db.session.commit()",python,"def addlogo(community_id, logo):
    """"""Add logo to the community.""""""
    # Create the bucket
    c = Community.get(community_id)
    if not c:
        click.secho('Community {0} does not exist.'.format(community_id),
                    fg='red')
        return
    ext = save_and_validate_logo(logo, logo.name, c.id)
    c.logo_ext = ext
    db.session.commit()",def,addlogo,(,community_id,",",logo,),:,# Create the bucket,c,=,Community,.,get,(,community_id,),if,not,c,:,click,.,secho,(,'Community {0} does not exist.',.,format,(,community_id,),",",fg,=,'red',),return,ext,=,save_and_validate_logo,(,logo,",",Add logo to the community.,Add,logo,to,the,community,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/cli.py#L63-L73,train,logo,.,name,",",c,.,id,),c,.,logo_ext,=,ext,db,.,session,.,commit,(,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/cli.py,request,"def request(community_id, record_id, accept):
    """"""Request a record acceptance to a community.""""""
    c = Community.get(community_id)
    assert c is not None
    record = Record.get_record(record_id)
    if accept:
        c.add_record(record)
        record.commit()
    else:
        InclusionRequest.create(community=c, record=record,
                                notify=False)
    db.session.commit()
    RecordIndexer().index_by_id(record.id)",python,"def request(community_id, record_id, accept):
    """"""Request a record acceptance to a community.""""""
    c = Community.get(community_id)
    assert c is not None
    record = Record.get_record(record_id)
    if accept:
        c.add_record(record)
        record.commit()
    else:
        InclusionRequest.create(community=c, record=record,
                                notify=False)
    db.session.commit()
    RecordIndexer().index_by_id(record.id)",def,request,(,community_id,",",record_id,",",accept,),:,c,=,Community,.,get,(,community_id,),assert,c,is,not,None,record,=,Record,.,get_record,(,record_id,),if,accept,:,c,.,add_record,(,record,),record,.,commit,Request a record acceptance to a community.,Request,a,record,acceptance,to,a,community,.,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/cli.py#L81-L93,train,(,),else,:,InclusionRequest,.,create,(,community,=,c,",",record,=,record,",",notify,=,False,),db,.,session,.,commit,(,),RecordIndexer,(,),,,,,,,,,,,,,,,,,,,,.,index_by_id,(,record,.,id,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/cli.py,remove,"def remove(community_id, record_id):
    """"""Remove a record from community.""""""
    c = Community.get(community_id)
    assert c is not None
    c.remove_record(record_id)
    db.session.commit()
    RecordIndexer().index_by_id(record_id)",python,"def remove(community_id, record_id):
    """"""Remove a record from community.""""""
    c = Community.get(community_id)
    assert c is not None
    c.remove_record(record_id)
    db.session.commit()
    RecordIndexer().index_by_id(record_id)",def,remove,(,community_id,",",record_id,),:,c,=,Community,.,get,(,community_id,),assert,c,is,not,None,c,.,remove_record,(,record_id,),db,.,session,.,commit,(,),RecordIndexer,(,),.,index_by_id,(,record_id,),,Remove a record from community.,Remove,a,record,from,community,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/cli.py#L100-L106,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/__init__.py,gen_otu_dict,"def gen_otu_dict(nex_obj, nexson_version=None):
    """"""Takes a NexSON object and returns a dict of
    otu_id -> otu_obj
    """"""
    if nexson_version is None:
        nexson_version = detect_nexson_version(nex_obj)
    if _is_by_id_hbf(nexson_version):
        otus = nex_obj['nexml']['otusById']
        if len(otus) > 1:
            d = {}
            for v in otus.values():
                d.update(v['otuById'])
            return d
        else:
            return otus.values()[0]['otuById']
    o_dict = {}
    for ob in nex_obj.get('otus', []):
        for o in ob.get('otu', []):
            oid = o['@id']
            o_dict[oid] = o
    return o_dict",python,"def gen_otu_dict(nex_obj, nexson_version=None):
    """"""Takes a NexSON object and returns a dict of
    otu_id -> otu_obj
    """"""
    if nexson_version is None:
        nexson_version = detect_nexson_version(nex_obj)
    if _is_by_id_hbf(nexson_version):
        otus = nex_obj['nexml']['otusById']
        if len(otus) > 1:
            d = {}
            for v in otus.values():
                d.update(v['otuById'])
            return d
        else:
            return otus.values()[0]['otuById']
    o_dict = {}
    for ob in nex_obj.get('otus', []):
        for o in ob.get('otu', []):
            oid = o['@id']
            o_dict[oid] = o
    return o_dict",def,gen_otu_dict,(,nex_obj,",",nexson_version,=,None,),:,if,nexson_version,is,None,:,nexson_version,=,detect_nexson_version,(,nex_obj,),if,_is_by_id_hbf,(,nexson_version,),:,otus,=,nex_obj,[,'nexml',],[,'otusById',],if,len,(,otus,),>,1,"Takes a NexSON object and returns a dict of
    otu_id -> otu_obj",Takes,a,NexSON,object,and,returns,a,dict,of,otu_id,-,>,otu_obj,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/__init__.py#L33-L53,train,:,d,=,{,},for,v,in,otus,.,values,(,),:,d,.,update,(,v,[,'otuById',],),return,d,else,:,return,otus,.,,,,,,,,,,,,,,,,,,,,values,(,),[,0,],[,'otuById',],o_dict,=,{,},for,ob,in,nex_obj,.,get,(,'otus',",",[,],),:,for,o,in,ob,.,get,(,'otu',",",[,],),:,oid,=,o,[,'@id',],o_dict,[,oid,],=,o,return,o_dict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
color/django-country,django_country/views.py,set_country,"def set_country(request):
    """"""
    Sets the chosen country in the session or cookie.

    If `next' query param is present, it redirects to a given url.
    """"""
    if request.method == 'POST':
        next = request.POST.get('next', request.GET.get('next'))
        if is_safe_url(url=next, host=request.get_host()):
            response = http.HttpResponseRedirect(next)
        else:
            response = http.HttpResponse()

        country_code = request.POST.get('country', '').upper()
        if country_code != geo.get_supported_country(country_code):
            return http.HttpResponseBadRequest()

        if hasattr(request, 'session'):
            request.session[geo.COUNTRY_SESSION_KEY] = country_code
        else:
            response.set_cookie(geo.COUNTRY_COOKIE_NAME,
                               country_code,
                               max_age=geo.COUNTRY_COOKIE_AGE,
                               path=geo.COUNTRY_COOKIE_PATH)
        return response
    else:
        return http.HttpResponseNotAllowed(['POST'])",python,"def set_country(request):
    """"""
    Sets the chosen country in the session or cookie.

    If `next' query param is present, it redirects to a given url.
    """"""
    if request.method == 'POST':
        next = request.POST.get('next', request.GET.get('next'))
        if is_safe_url(url=next, host=request.get_host()):
            response = http.HttpResponseRedirect(next)
        else:
            response = http.HttpResponse()

        country_code = request.POST.get('country', '').upper()
        if country_code != geo.get_supported_country(country_code):
            return http.HttpResponseBadRequest()

        if hasattr(request, 'session'):
            request.session[geo.COUNTRY_SESSION_KEY] = country_code
        else:
            response.set_cookie(geo.COUNTRY_COOKIE_NAME,
                               country_code,
                               max_age=geo.COUNTRY_COOKIE_AGE,
                               path=geo.COUNTRY_COOKIE_PATH)
        return response
    else:
        return http.HttpResponseNotAllowed(['POST'])",def,set_country,(,request,),:,if,request,.,method,==,'POST',:,next,=,request,.,POST,.,get,(,'next',",",request,.,GET,.,get,(,'next',),),if,is_safe_url,(,url,=,next,",",host,=,request,.,"Sets the chosen country in the session or cookie.

    If `next' query param is present, it redirects to a given url.",Sets,the,chosen,country,in,the,session,or,cookie,.,,,,,,1d272a196d998e21bb8d407e2657b88211f35232,https://github.com/color/django-country/blob/1d272a196d998e21bb8d407e2657b88211f35232/django_country/views.py#L8-L34,train,get_host,(,),),:,response,=,http,.,HttpResponseRedirect,(,next,),else,:,response,=,http,.,HttpResponse,(,),country_code,=,request,.,POST,.,get,(,,,,,,,,,,,,,,,,,,,,'country',",",'',),.,upper,(,),if,country_code,!=,geo,.,get_supported_country,(,country_code,),:,return,http,.,HttpResponseBadRequest,(,),if,hasattr,(,request,",",'session',),:,request,.,session,[,geo,.,COUNTRY_SESSION_KEY,],=,country_code,else,:,response,.,set_cookie,(,geo,.,COUNTRY_COOKIE_NAME,",",country_code,",",max_age,=,geo,.,COUNTRY_COOKIE_AGE,",",path,=,geo,.,COUNTRY_COOKIE_PATH,),return,response,else,:,return,http,.,HttpResponseNotAllowed,(,[,'POST',],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_doc_context.py,JSGDocContext.reference,"def reference(self, tkn: str):
        """""" Return the element that tkn represents""""""
        return self.grammarelts[tkn] if tkn in self.grammarelts else UndefinedElement(tkn)",python,"def reference(self, tkn: str):
        """""" Return the element that tkn represents""""""
        return self.grammarelts[tkn] if tkn in self.grammarelts else UndefinedElement(tkn)",def,reference,(,self,",",tkn,:,str,),:,return,self,.,grammarelts,[,tkn,],if,tkn,in,self,.,grammarelts,else,UndefinedElement,(,tkn,),,,,,,,,,,,,,,,,Return the element that tkn represents,Return,the,element,that,tkn,represents,,,,,,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_doc_context.py#L111-L113,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_doc_context.py,JSGDocContext.dependency_list,"def dependency_list(self, tkn: str) -> List[str]:
        """"""Return a list all of the grammarelts that depend on tkn

        :param tkn: 
        :return:
        """"""
        if tkn not in self.dependency_map:
            self.dependency_map[tkn] = [tkn]        # Force a circular reference
            self.dependency_map[tkn] = self.reference(tkn).dependency_list()
        return self.dependency_map[tkn]",python,"def dependency_list(self, tkn: str) -> List[str]:
        """"""Return a list all of the grammarelts that depend on tkn

        :param tkn: 
        :return:
        """"""
        if tkn not in self.dependency_map:
            self.dependency_map[tkn] = [tkn]        # Force a circular reference
            self.dependency_map[tkn] = self.reference(tkn).dependency_list()
        return self.dependency_map[tkn]",def,dependency_list,(,self,",",tkn,:,str,),->,List,[,str,],:,if,tkn,not,in,self,.,dependency_map,:,self,.,dependency_map,[,tkn,],=,[,tkn,],# Force a circular reference,self,.,dependency_map,[,tkn,],=,self,.,"Return a list all of the grammarelts that depend on tkn

        :param tkn: 
        :return:",Return,a,list,all,of,the,grammarelts,that,depend,on,tkn,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_doc_context.py#L139-L148,train,reference,(,tkn,),.,dependency_list,(,),return,self,.,dependency_map,[,tkn,],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_doc_context.py,JSGDocContext.dependencies,"def dependencies(self, tkn: str) -> Set[str]:
        """"""Return all the items that tkn depends on as a set

        :param tkn:
        :return:
        """"""
        return set(self.dependency_list(tkn))",python,"def dependencies(self, tkn: str) -> Set[str]:
        """"""Return all the items that tkn depends on as a set

        :param tkn:
        :return:
        """"""
        return set(self.dependency_list(tkn))",def,dependencies,(,self,",",tkn,:,str,),->,Set,[,str,],:,return,set,(,self,.,dependency_list,(,tkn,),),,,,,,,,,,,,,,,,,,,"Return all the items that tkn depends on as a set

        :param tkn:
        :return:",Return,all,the,items,that,tkn,depends,on,as,a,set,,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_doc_context.py#L150-L156,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsolbrig/pyjsg,pyjsg/parser_impl/jsg_doc_context.py,JSGDocContext.undefined_entries,"def undefined_entries(self) -> Set[str]:
        """""" Return the set of tokens that are referenced but not defined. """"""
        return as_set([[d for d in self.dependencies(k) if d not in self.grammarelts]
                       for k in self.grammarelts.keys()])",python,"def undefined_entries(self) -> Set[str]:
        """""" Return the set of tokens that are referenced but not defined. """"""
        return as_set([[d for d in self.dependencies(k) if d not in self.grammarelts]
                       for k in self.grammarelts.keys()])",def,undefined_entries,(,self,),->,Set,[,str,],:,return,as_set,(,[,[,d,for,d,in,self,.,dependencies,(,k,),if,d,not,in,self,.,grammarelts,],for,k,in,self,.,grammarelts,.,keys,(,Return the set of tokens that are referenced but not defined.,Return,the,set,of,tokens,that,are,referenced,but,not,defined,.,,,,9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7,https://github.com/hsolbrig/pyjsg/blob/9b2b8fa8e3b8448abe70b09f804a79f0f31b32b7/pyjsg/parser_impl/jsg_doc_context.py#L158-L161,train,),],),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/receivers.py,new_request,"def new_request(sender, request=None, notify=True, **kwargs):
    """"""New request for inclusion.""""""
    if current_app.config['COMMUNITIES_MAIL_ENABLED'] and notify:
        send_community_request_email(request)",python,"def new_request(sender, request=None, notify=True, **kwargs):
    """"""New request for inclusion.""""""
    if current_app.config['COMMUNITIES_MAIL_ENABLED'] and notify:
        send_community_request_email(request)",def,new_request,(,sender,",",request,=,None,",",notify,=,True,",",*,*,kwargs,),:,if,current_app,.,config,[,'COMMUNITIES_MAIL_ENABLED',],and,notify,:,send_community_request_email,(,request,),,,,,,,,,,,,New request for inclusion.,New,request,for,inclusion,.,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/receivers.py#L36-L39,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/receivers.py,inject_provisional_community,"def inject_provisional_community(sender, json=None, record=None, index=None,
                                 **kwargs):
    """"""Inject 'provisional_communities' key to ES index.""""""
    if index and not index.startswith(
            current_app.config['COMMUNITIES_INDEX_PREFIX']):
        return

    json['provisional_communities'] = list(sorted([
        r.id_community for r in InclusionRequest.get_by_record(record.id)
    ]))",python,"def inject_provisional_community(sender, json=None, record=None, index=None,
                                 **kwargs):
    """"""Inject 'provisional_communities' key to ES index.""""""
    if index and not index.startswith(
            current_app.config['COMMUNITIES_INDEX_PREFIX']):
        return

    json['provisional_communities'] = list(sorted([
        r.id_community for r in InclusionRequest.get_by_record(record.id)
    ]))",def,inject_provisional_community,(,sender,",",json,=,None,",",record,=,None,",",index,=,None,",",*,*,kwargs,),:,if,index,and,not,index,.,startswith,(,current_app,.,config,[,'COMMUNITIES_INDEX_PREFIX',],),:,return,json,[,'provisional_communities',],Inject 'provisional_communities' key to ES index.,Inject,provisional_communities,key,to,ES,index,.,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/receivers.py#L42-L51,train,=,list,(,sorted,(,[,r,.,id_community,for,r,in,InclusionRequest,.,get_by_record,(,record,.,id,),],),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/api/oti.py,_OTIWrapper.find_nodes,"def find_nodes(self, query_dict=None, exact=False, verbose=False, **kwargs):
        """"""Query on node properties. See documentation for _OTIWrapper class.""""""
        assert self.use_v1
        return self._do_query('{p}/singlePropertySearchForTreeNodes'.format(p=self.query_prefix),
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.node_search_term_set,
                              kwargs=kwargs)",python,"def find_nodes(self, query_dict=None, exact=False, verbose=False, **kwargs):
        """"""Query on node properties. See documentation for _OTIWrapper class.""""""
        assert self.use_v1
        return self._do_query('{p}/singlePropertySearchForTreeNodes'.format(p=self.query_prefix),
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.node_search_term_set,
                              kwargs=kwargs)",def,find_nodes,(,self,",",query_dict,=,None,",",exact,=,False,",",verbose,=,False,",",*,*,kwargs,),:,assert,self,.,use_v1,return,self,.,_do_query,(,'{p}/singlePropertySearchForTreeNodes',.,format,(,p,=,self,.,query_prefix,),",",query_dict,Query on node properties. See documentation for _OTIWrapper class.,Query,on,node,properties,.,See,documentation,for,_OTIWrapper,class,.,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/api/oti.py#L119-L127,train,=,query_dict,",",exact,=,exact,",",verbose,=,verbose,",",valid_keys,=,self,.,node_search_term_set,",",kwargs,=,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/api/oti.py,_OTIWrapper.find_trees,"def find_trees(self, query_dict=None, exact=False, verbose=False, wrap_response=False, **kwargs):
        """"""Query on tree properties. See documentation for _OTIWrapper class.""""""
        if self.use_v1:
            uri = '{p}/singlePropertySearchForTrees'.format(p=self.query_prefix)
        else:
            uri = '{p}/find_trees'.format(p=self.query_prefix)
        resp = self._do_query(uri,
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.tree_search_term_set,
                              kwargs=kwargs)
        if wrap_response:
            return TreeRefList(resp)
        return resp",python,"def find_trees(self, query_dict=None, exact=False, verbose=False, wrap_response=False, **kwargs):
        """"""Query on tree properties. See documentation for _OTIWrapper class.""""""
        if self.use_v1:
            uri = '{p}/singlePropertySearchForTrees'.format(p=self.query_prefix)
        else:
            uri = '{p}/find_trees'.format(p=self.query_prefix)
        resp = self._do_query(uri,
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.tree_search_term_set,
                              kwargs=kwargs)
        if wrap_response:
            return TreeRefList(resp)
        return resp",def,find_trees,(,self,",",query_dict,=,None,",",exact,=,False,",",verbose,=,False,",",wrap_response,=,False,",",*,*,kwargs,),:,if,self,.,use_v1,:,uri,=,'{p}/singlePropertySearchForTrees',.,format,(,p,=,self,.,query_prefix,),Query on tree properties. See documentation for _OTIWrapper class.,Query,on,tree,properties,.,See,documentation,for,_OTIWrapper,class,.,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/api/oti.py#L129-L143,train,else,:,uri,=,'{p}/find_trees',.,format,(,p,=,self,.,query_prefix,),resp,=,self,.,_do_query,(,uri,",",query_dict,=,query_dict,",",exact,=,exact,",",,,,,,,,,,,,,,,,,,,,verbose,=,verbose,",",valid_keys,=,self,.,tree_search_term_set,",",kwargs,=,kwargs,),if,wrap_response,:,return,TreeRefList,(,resp,),return,resp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/api/oti.py,_OTIWrapper.find_studies,"def find_studies(self, query_dict=None, exact=False, verbose=False, **kwargs):
        """"""Query on study properties. See documentation for _OTIWrapper class.""""""
        if self.use_v1:
            uri = '{p}/singlePropertySearchForStudies'.format(p=self.query_prefix)
        else:
            uri = '{p}/find_studies'.format(p=self.query_prefix)
        return self._do_query(uri,
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.study_search_term_set,
                              kwargs=kwargs)",python,"def find_studies(self, query_dict=None, exact=False, verbose=False, **kwargs):
        """"""Query on study properties. See documentation for _OTIWrapper class.""""""
        if self.use_v1:
            uri = '{p}/singlePropertySearchForStudies'.format(p=self.query_prefix)
        else:
            uri = '{p}/find_studies'.format(p=self.query_prefix)
        return self._do_query(uri,
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.study_search_term_set,
                              kwargs=kwargs)",def,find_studies,(,self,",",query_dict,=,None,",",exact,=,False,",",verbose,=,False,",",*,*,kwargs,),:,if,self,.,use_v1,:,uri,=,'{p}/singlePropertySearchForStudies',.,format,(,p,=,self,.,query_prefix,),else,:,uri,=,Query on study properties. See documentation for _OTIWrapper class.,Query,on,study,properties,.,See,documentation,for,_OTIWrapper,class,.,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/api/oti.py#L145-L156,train,'{p}/find_studies',.,format,(,p,=,self,.,query_prefix,),return,self,.,_do_query,(,uri,",",query_dict,=,query_dict,",",exact,=,exact,",",verbose,=,verbose,",",valid_keys,,,,,,,,,,,,,,,,,,,,=,self,.,study_search_term_set,",",kwargs,=,kwargs,),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpotlightData/preprocessing,setup.py,get_requirements,"def get_requirements():
    '''returns requirements array for package'''
    packages = []
    with open(""requirements.txt"", ""r"") as req_doc:
        for package in req_doc:
            packages.append(package.replace(""\n"", """"))
    return packages",python,"def get_requirements():
    '''returns requirements array for package'''
    packages = []
    with open(""requirements.txt"", ""r"") as req_doc:
        for package in req_doc:
            packages.append(package.replace(""\n"", """"))
    return packages",def,get_requirements,(,),:,packages,=,[,],with,open,(,"""requirements.txt""",",","""r""",),as,req_doc,:,for,package,in,req_doc,:,packages,.,append,(,package,.,replace,(,"""\n""",",","""""",),),return,packages,,,,,returns requirements array for package,returns,requirements,array,for,package,,,,,,,,,,,180c6472bc2642afbd7a1ece08d0b0d14968a708,https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/setup.py#L5-L11,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/amendments/amendments_umbrella.py,TaxonomicAmendmentStore,"def TaxonomicAmendmentStore(repos_dict=None,
                            repos_par=None,
                            with_caching=True,
                            assumed_doc_version=None,
                            git_ssh=None,
                            pkey=None,
                            git_action_class=TaxonomicAmendmentsGitAction,
                            mirror_info=None,
                            infrastructure_commit_author='OpenTree API <api@opentreeoflife.org>'):
    """"""Factory function for a _TaxonomicAmendmentStore object.

    A wrapper around the _TaxonomicAmendmentStore class instantiation for
    the most common use case: a singleton _TaxonomicAmendmentStore.
    If you need distinct _TaxonomicAmendmentStore objects, you'll need to
    call that class directly.
    """"""
    global _THE_TAXONOMIC_AMENDMENT_STORE
    if _THE_TAXONOMIC_AMENDMENT_STORE is None:
        _THE_TAXONOMIC_AMENDMENT_STORE = _TaxonomicAmendmentStore(repos_dict=repos_dict,
                                                                  repos_par=repos_par,
                                                                  with_caching=with_caching,
                                                                  assumed_doc_version=assumed_doc_version,
                                                                  git_ssh=git_ssh,
                                                                  pkey=pkey,
                                                                  git_action_class=git_action_class,
                                                                  mirror_info=mirror_info,
                                                                  infrastructure_commit_author=infrastructure_commit_author)
    return _THE_TAXONOMIC_AMENDMENT_STORE",python,"def TaxonomicAmendmentStore(repos_dict=None,
                            repos_par=None,
                            with_caching=True,
                            assumed_doc_version=None,
                            git_ssh=None,
                            pkey=None,
                            git_action_class=TaxonomicAmendmentsGitAction,
                            mirror_info=None,
                            infrastructure_commit_author='OpenTree API <api@opentreeoflife.org>'):
    """"""Factory function for a _TaxonomicAmendmentStore object.

    A wrapper around the _TaxonomicAmendmentStore class instantiation for
    the most common use case: a singleton _TaxonomicAmendmentStore.
    If you need distinct _TaxonomicAmendmentStore objects, you'll need to
    call that class directly.
    """"""
    global _THE_TAXONOMIC_AMENDMENT_STORE
    if _THE_TAXONOMIC_AMENDMENT_STORE is None:
        _THE_TAXONOMIC_AMENDMENT_STORE = _TaxonomicAmendmentStore(repos_dict=repos_dict,
                                                                  repos_par=repos_par,
                                                                  with_caching=with_caching,
                                                                  assumed_doc_version=assumed_doc_version,
                                                                  git_ssh=git_ssh,
                                                                  pkey=pkey,
                                                                  git_action_class=git_action_class,
                                                                  mirror_info=mirror_info,
                                                                  infrastructure_commit_author=infrastructure_commit_author)
    return _THE_TAXONOMIC_AMENDMENT_STORE",def,TaxonomicAmendmentStore,(,repos_dict,=,None,",",repos_par,=,None,",",with_caching,=,True,",",assumed_doc_version,=,None,",",git_ssh,=,None,",",pkey,=,None,",",git_action_class,=,TaxonomicAmendmentsGitAction,",",mirror_info,=,None,",",infrastructure_commit_author,=,'OpenTree API <api@opentreeoflife.org>',),:,global,_THE_TAXONOMIC_AMENDMENT_STORE,if,"Factory function for a _TaxonomicAmendmentStore object.

    A wrapper around the _TaxonomicAmendmentStore class instantiation for
    the most common use case: a singleton _TaxonomicAmendmentStore.
    If you need distinct _TaxonomicAmendmentStore objects, you'll need to
    call that class directly.",Factory,function,for,a,_TaxonomicAmendmentStore,object,.,,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/amendments/amendments_umbrella.py#L352-L379,train,_THE_TAXONOMIC_AMENDMENT_STORE,is,None,:,_THE_TAXONOMIC_AMENDMENT_STORE,=,_TaxonomicAmendmentStore,(,repos_dict,=,repos_dict,",",repos_par,=,repos_par,",",with_caching,=,with_caching,",",assumed_doc_version,=,assumed_doc_version,",",git_ssh,=,git_ssh,",",pkey,=,,,,,,,,,,,,,,,,,,,,pkey,",",git_action_class,=,git_action_class,",",mirror_info,=,mirror_info,",",infrastructure_commit_author,=,infrastructure_commit_author,),return,_THE_TAXONOMIC_AMENDMENT_STORE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/tasks.py,delete_marked_communities,"def delete_marked_communities():
    """"""Delete communities after holdout time.""""""
    # TODO: Delete the community ID from all records metadata first
    raise NotImplementedError()
    Community.query.filter_by(
        Community.delete_time > datetime.utcnow()).delete()
    db.session.commit()",python,"def delete_marked_communities():
    """"""Delete communities after holdout time.""""""
    # TODO: Delete the community ID from all records metadata first
    raise NotImplementedError()
    Community.query.filter_by(
        Community.delete_time > datetime.utcnow()).delete()
    db.session.commit()",def,delete_marked_communities,(,),:,# TODO: Delete the community ID from all records metadata first,raise,NotImplementedError,(,),Community,.,query,.,filter_by,(,Community,.,delete_time,>,datetime,.,utcnow,(,),),.,delete,(,),db,.,session,.,commit,(,),,,,,,,Delete communities after holdout time.,Delete,communities,after,holdout,time,.,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/tasks.py#L38-L44,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inveniosoftware/invenio-communities,invenio_communities/tasks.py,delete_expired_requests,"def delete_expired_requests():
    """"""Delete expired inclusion requests.""""""
    InclusionRequest.query.filter_by(
        InclusionRequest.expiry_date > datetime.utcnow()).delete()
    db.session.commit()",python,"def delete_expired_requests():
    """"""Delete expired inclusion requests.""""""
    InclusionRequest.query.filter_by(
        InclusionRequest.expiry_date > datetime.utcnow()).delete()
    db.session.commit()",def,delete_expired_requests,(,),:,InclusionRequest,.,query,.,filter_by,(,InclusionRequest,.,expiry_date,>,datetime,.,utcnow,(,),),.,delete,(,),db,.,session,.,commit,(,),,,,,,,,,,,,Delete expired inclusion requests.,Delete,expired,inclusion,requests,.,,,,,,,,,,,5c4de6783724d276ae1b6dd13a399a9e22fadc7a,https://github.com/inveniosoftware/invenio-communities/blob/5c4de6783724d276ae1b6dd13a399a9e22fadc7a/invenio_communities/tasks.py#L48-L52,train,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/__init__.py,create_content_spec,"def create_content_spec(**kwargs):
    """"""Sugar. factory for a PhyloSchema object.

    Repackages the kwargs to kwargs for PhyloSchema so that our
    PhyloSchema.__init__ does not have to be soo rich
    """"""
    format_str = kwargs.get('format', 'nexson')
    nexson_version = kwargs.get('nexson_version', 'native')
    otu_label = kwargs.get('otu_label')
    if otu_label is None:
        otu_label = kwargs.get('tip_label')
    content = kwargs.get('content')
    if content is not None:
        content_id = kwargs.get('content_id')
        if content_id is None:
            content_id = _get_content_id_from(**kwargs)
    else:
        content, content_id = _sniff_content_from_kwargs(**kwargs)
    if content is None:
        content = 'study'
    return PhyloSchema(content=content,
                       content_id=content_id,
                       format_str=format_str,
                       version=nexson_version,
                       otu_label=otu_label,
                       repo_nexml2json=kwargs.get('repo_nexml2json'),
                       bracket_ingroup=bool(kwargs.get('bracket_ingroup', False)),
                       cull_nonmatching=kwargs.get('cull_nonmatching'))",python,"def create_content_spec(**kwargs):
    """"""Sugar. factory for a PhyloSchema object.

    Repackages the kwargs to kwargs for PhyloSchema so that our
    PhyloSchema.__init__ does not have to be soo rich
    """"""
    format_str = kwargs.get('format', 'nexson')
    nexson_version = kwargs.get('nexson_version', 'native')
    otu_label = kwargs.get('otu_label')
    if otu_label is None:
        otu_label = kwargs.get('tip_label')
    content = kwargs.get('content')
    if content is not None:
        content_id = kwargs.get('content_id')
        if content_id is None:
            content_id = _get_content_id_from(**kwargs)
    else:
        content, content_id = _sniff_content_from_kwargs(**kwargs)
    if content is None:
        content = 'study'
    return PhyloSchema(content=content,
                       content_id=content_id,
                       format_str=format_str,
                       version=nexson_version,
                       otu_label=otu_label,
                       repo_nexml2json=kwargs.get('repo_nexml2json'),
                       bracket_ingroup=bool(kwargs.get('bracket_ingroup', False)),
                       cull_nonmatching=kwargs.get('cull_nonmatching'))",def,create_content_spec,(,*,*,kwargs,),:,format_str,=,kwargs,.,get,(,'format',",",'nexson',),nexson_version,=,kwargs,.,get,(,'nexson_version',",",'native',),otu_label,=,kwargs,.,get,(,'otu_label',),if,otu_label,is,None,:,otu_label,=,"Sugar. factory for a PhyloSchema object.

    Repackages the kwargs to kwargs for PhyloSchema so that our
    PhyloSchema.__init__ does not have to be soo rich",Sugar,.,factory,for,a,PhyloSchema,object,.,,,,,,,,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/__init__.py#L178-L205,train,kwargs,.,get,(,'tip_label',),content,=,kwargs,.,get,(,'content',),if,content,is,not,None,:,content_id,=,kwargs,.,get,(,'content_id',),if,content_id,,,,,,,,,,,,,,,,,,,,is,None,:,content_id,=,_get_content_id_from,(,*,*,kwargs,),else,:,content,",",content_id,=,_sniff_content_from_kwargs,(,*,*,kwargs,),if,content,is,None,:,content,=,'study',return,PhyloSchema,(,content,=,content,",",content_id,=,content_id,",",format_str,=,format_str,",",version,=,nexson_version,",",otu_label,=,otu_label,",",repo_nexml2json,=,kwargs,.,get,(,'repo_nexml2json',),",",bracket_ingroup,=,bool,(,kwargs,.,get,(,'bracket_ingroup',",",False,),),",",cull_nonmatching,=,kwargs,.,get,(,'cull_nonmatching',),),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenTreeOfLife/peyotl,peyotl/nexson_syntax/__init__.py,convert_nexson_format,"def convert_nexson_format(blob,
                          out_nexson_format,
                          current_format=None,
                          remove_old_structs=True,
                          pristine_if_invalid=False,
                          sort_arbitrary=False):
    """"""Take a dict form of NexSON and converts its datastructures to
    those needed to serialize as out_nexson_format.
    If current_format is not specified, it will be inferred.
    If `remove_old_structs` is False and different honeybadgerfish varieties
        are selected, the `blob` will be 'fat"" containing both types
        of lookup structures.
    If pristine_if_invalid is False, then the object may be corrupted if it
        is an invalid nexson struct. Setting this to False can result in
        faster translation, but if an exception is raised the object may
        be polluted with partially constructed fields for the out_nexson_format.
    """"""
    if not current_format:
        current_format = detect_nexson_version(blob)
    out_nexson_format = resolve_nexson_format(out_nexson_format)
    if current_format == out_nexson_format:
        if sort_arbitrary:
            sort_arbitrarily_ordered_nexson(blob)
        return blob
    two2zero = _is_by_id_hbf(out_nexson_format) and _is_badgerfish_version(current_format)
    zero2two = _is_by_id_hbf(current_format) and _is_badgerfish_version(out_nexson_format)
    if two2zero or zero2two:
        # go from 0.0 -> 1.0 then the 1.0->1.2 should succeed without nexml...
        blob = convert_nexson_format(blob,
                                     DIRECT_HONEY_BADGERFISH,
                                     current_format=current_format,
                                     remove_old_structs=remove_old_structs,
                                     pristine_if_invalid=pristine_if_invalid)
        current_format = DIRECT_HONEY_BADGERFISH
    ccdict = {'output_format': out_nexson_format,
              'input_format': current_format,
              'remove_old_structs': remove_old_structs,
              'pristine_if_invalid': pristine_if_invalid}
    ccfg = ConversionConfig(ccdict)
    if _is_badgerfish_version(current_format):
        converter = Badgerfish2DirectNexson(ccfg)
    elif _is_badgerfish_version(out_nexson_format):
        assert _is_direct_hbf(current_format)
        converter = Direct2BadgerfishNexson(ccfg)
    elif _is_direct_hbf(current_format) and (out_nexson_format == BY_ID_HONEY_BADGERFISH):
        converter = Direct2OptimalNexson(ccfg)
    elif _is_direct_hbf(out_nexson_format) and (current_format == BY_ID_HONEY_BADGERFISH):
        converter = Optimal2DirectNexson(ccfg)
    else:
        raise NotImplementedError('Conversion from {i} to {o}'.format(i=current_format, o=out_nexson_format))
    blob = converter.convert(blob)
    if sort_arbitrary:
        sort_arbitrarily_ordered_nexson(blob)
    return blob",python,"def convert_nexson_format(blob,
                          out_nexson_format,
                          current_format=None,
                          remove_old_structs=True,
                          pristine_if_invalid=False,
                          sort_arbitrary=False):
    """"""Take a dict form of NexSON and converts its datastructures to
    those needed to serialize as out_nexson_format.
    If current_format is not specified, it will be inferred.
    If `remove_old_structs` is False and different honeybadgerfish varieties
        are selected, the `blob` will be 'fat"" containing both types
        of lookup structures.
    If pristine_if_invalid is False, then the object may be corrupted if it
        is an invalid nexson struct. Setting this to False can result in
        faster translation, but if an exception is raised the object may
        be polluted with partially constructed fields for the out_nexson_format.
    """"""
    if not current_format:
        current_format = detect_nexson_version(blob)
    out_nexson_format = resolve_nexson_format(out_nexson_format)
    if current_format == out_nexson_format:
        if sort_arbitrary:
            sort_arbitrarily_ordered_nexson(blob)
        return blob
    two2zero = _is_by_id_hbf(out_nexson_format) and _is_badgerfish_version(current_format)
    zero2two = _is_by_id_hbf(current_format) and _is_badgerfish_version(out_nexson_format)
    if two2zero or zero2two:
        # go from 0.0 -> 1.0 then the 1.0->1.2 should succeed without nexml...
        blob = convert_nexson_format(blob,
                                     DIRECT_HONEY_BADGERFISH,
                                     current_format=current_format,
                                     remove_old_structs=remove_old_structs,
                                     pristine_if_invalid=pristine_if_invalid)
        current_format = DIRECT_HONEY_BADGERFISH
    ccdict = {'output_format': out_nexson_format,
              'input_format': current_format,
              'remove_old_structs': remove_old_structs,
              'pristine_if_invalid': pristine_if_invalid}
    ccfg = ConversionConfig(ccdict)
    if _is_badgerfish_version(current_format):
        converter = Badgerfish2DirectNexson(ccfg)
    elif _is_badgerfish_version(out_nexson_format):
        assert _is_direct_hbf(current_format)
        converter = Direct2BadgerfishNexson(ccfg)
    elif _is_direct_hbf(current_format) and (out_nexson_format == BY_ID_HONEY_BADGERFISH):
        converter = Direct2OptimalNexson(ccfg)
    elif _is_direct_hbf(out_nexson_format) and (current_format == BY_ID_HONEY_BADGERFISH):
        converter = Optimal2DirectNexson(ccfg)
    else:
        raise NotImplementedError('Conversion from {i} to {o}'.format(i=current_format, o=out_nexson_format))
    blob = converter.convert(blob)
    if sort_arbitrary:
        sort_arbitrarily_ordered_nexson(blob)
    return blob",def,convert_nexson_format,(,blob,",",out_nexson_format,",",current_format,=,None,",",remove_old_structs,=,True,",",pristine_if_invalid,=,False,",",sort_arbitrary,=,False,),:,if,not,current_format,:,current_format,=,detect_nexson_version,(,blob,),out_nexson_format,=,resolve_nexson_format,(,out_nexson_format,),if,current_format,==,"Take a dict form of NexSON and converts its datastructures to
    those needed to serialize as out_nexson_format.
    If current_format is not specified, it will be inferred.
    If `remove_old_structs` is False and different honeybadgerfish varieties
        are selected, the `blob` will be 'fat"" containing both types
        of lookup structures.
    If pristine_if_invalid is False, then the object may be corrupted if it
        is an invalid nexson struct. Setting this to False can result in
        faster translation, but if an exception is raised the object may
        be polluted with partially constructed fields for the out_nexson_format.",Take,a,dict,form,of,NexSON,and,converts,its,datastructures,to,those,needed,to,serialize,5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/__init__.py#L646-L699,train,out_nexson_format,:,if,sort_arbitrary,:,sort_arbitrarily_ordered_nexson,(,blob,),return,blob,two2zero,=,_is_by_id_hbf,(,out_nexson_format,),and,_is_badgerfish_version,(,current_format,),zero2two,=,_is_by_id_hbf,(,current_format,),and,_is_badgerfish_version,as,out_nexson_format,.,If,current_format,is,not,specified,it,will,be,inferred,.,If,remove_old_structs,is,False,and,different,(,out_nexson_format,),if,two2zero,or,zero2two,:,# go from 0.0 -> 1.0 then the 1.0->1.2 should succeed without nexml...,blob,=,convert_nexson_format,(,blob,",",DIRECT_HONEY_BADGERFISH,",",current_format,=,current_format,",",remove_old_structs,=,remove_old_structs,",",pristine_if_invalid,=,pristine_if_invalid,),current_format,=,DIRECT_HONEY_BADGERFISH,ccdict,=,{,'output_format',:,out_nexson_format,",",'input_format',:,current_format,",",'remove_old_structs',:,remove_old_structs,",",'pristine_if_invalid',:,pristine_if_invalid,},ccfg,=,ConversionConfig,(,ccdict,),if,_is_badgerfish_version,(,current_format,),:,converter,=,Badgerfish2DirectNexson,(,ccfg,),elif,_is_badgerfish_version,(,out_nexson_format,),:,assert,_is_direct_hbf,(,current_format,),converter,=,Direct2BadgerfishNexson,(,ccfg,),elif,_is_direct_hbf,(,current_format,),and,(,out_nexson_format,==,BY_ID_HONEY_BADGERFISH,),:,converter,=,Direct2OptimalNexson,(,ccfg,),elif,_is_direct_hbf,(,out_nexson_format,),and,(,current_format,==,BY_ID_HONEY_BADGERFISH,),:,converter,=,Optimal2DirectNexson,(,ccfg,),else,:,raise,NotImplementedError,(,'Conversion from {i} to {o}',.,format,(,i,=,current_format,",",o,=,out_nexson_format,),),blob,=,converter,.,convert,(,blob,),if,sort_arbitrary,:,sort_arbitrarily_ordered_nexson,(,blob,),return,blob,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,honeybadgerfish,varieties,are,selected,the,blob,will,be,fat,containing,both,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,types,of,lookup,structures,.,If,pristine_if_invalid,is,False,then,the,object,may,be,corrupted,if,it,is,an,invalid,nexson,struct,.,Setting,this,to,False,can,result,in,faster,translation,but,if,an,exception,is,raised,the,object,may,be,polluted,with,partially,constructed,fields,for,the,out_nexson_format,.
